[{"id_": "de3f8c45-f9b3-45f5-a96e-5c1bfd4b380f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node. ", "original_text": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fb42f16-ce8e-42fe-8d84-1a9070db4701", "node_type": "1", "metadata": {"window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. ", "original_text": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods. "}, "hash": "b7a9aa61dd83e1a458e3a34fd7deb779dc4cb80b0d56cbfb4457891c4243f029", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 366, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7fb42f16-ce8e-42fe-8d84-1a9070db4701", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. ", "original_text": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de3f8c45-f9b3-45f5-a96e-5c1bfd4b380f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node. ", "original_text": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul. "}, "hash": "f72f09f1565e88674e972476a9e403d80409d562dd2fb8bd7c680c8c1e3b8a4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0874bf7-9875-4b20-8c3c-e752946c2296", "node_type": "1", "metadata": {"window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. ", "original_text": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data. "}, "hash": "4ea76b3fa2919c1a010b4bdfeccb855241cd19b1a4e23931ffc2dbc865fb2751", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods. ", "mimetype": "text/plain", "start_char_idx": 366, "end_char_idx": 1226, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0874bf7-9875-4b20-8c3c-e752946c2296", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. ", "original_text": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fb42f16-ce8e-42fe-8d84-1a9070db4701", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. ", "original_text": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods. "}, "hash": "74e48bff9cee607ef4e98327d90fe9118d31fd817494f8e75c605c6af3f67e6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c3993671-c19e-4347-b9ed-43fd49d66d93", "node_type": "1", "metadata": {"window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. ", "original_text": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach. "}, "hash": "99354679535da1236df4aa2c87a180f7ab79b0594236269f87cf34a661a68f4c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data. ", "mimetype": "text/plain", "start_char_idx": 1226, "end_char_idx": 1409, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c3993671-c19e-4347-b9ed-43fd49d66d93", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. ", "original_text": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0874bf7-9875-4b20-8c3c-e752946c2296", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. ", "original_text": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data. "}, "hash": "24ef3e3f93e148983843a9bb855f66864f3331651939bd9567baa41683bfabdc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe648480-6fe2-4183-a713-dbda53b33905", "node_type": "1", "metadata": {"window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n", "original_text": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method. "}, "hash": "51a424f4032e1120ba9ce10b300d21152f117b1e378e8ec2b3a9469f1440d0b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach. ", "mimetype": "text/plain", "start_char_idx": 1409, "end_char_idx": 1538, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe648480-6fe2-4183-a713-dbda53b33905", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n", "original_text": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c3993671-c19e-4347-b9ed-43fd49d66d93", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. ", "original_text": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach. "}, "hash": "a639058ce4f866f02c4cf3e500e28a89c329a071b3d7a3f4a1dc999d82812998", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ddbbd06-43b6-406e-ab38-2f4d4ebadbf5", "node_type": "1", "metadata": {"window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors. ", "original_text": "k-Means clustering is used to predict the number of divisions on each decision tree node. "}, "hash": "e800d73a87817f880c90931ddbdc4cfb83d6e7fad34183c5cdd7f1f07694968b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method. ", "mimetype": "text/plain", "start_char_idx": 1538, "end_char_idx": 1698, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ddbbd06-43b6-406e-ab38-2f4d4ebadbf5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors. ", "original_text": "k-Means clustering is used to predict the number of divisions on each decision tree node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe648480-6fe2-4183-a713-dbda53b33905", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n", "original_text": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method. "}, "hash": "2f40bfff56dce5f9f458fa5b88a283604e06b6394ea8c3078acfd19b27b8cae5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d57867b-83ad-4821-89ac-326ea64d4589", "node_type": "1", "metadata": {"window": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V. ", "original_text": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. "}, "hash": "0a94bd198f7a31f2b03a36e251ea3c7846e9edf86f4a80dc6f6d8207d350a534", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "k-Means clustering is used to predict the number of divisions on each decision tree node. ", "mimetype": "text/plain", "start_char_idx": 1698, "end_char_idx": 1788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d57867b-83ad-4821-89ac-326ea64d4589", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V. ", "original_text": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ddbbd06-43b6-406e-ab38-2f4d4ebadbf5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Knowledge-Based Systems 195 (2020) 105659**\n\n**Contents lists available at ScienceDirect**\n\n**Knowledge-Based Systems**\n\n**journal homepage: www.elsevier.com/locate/knosys**\n\n---\n\n### K-Means-based isolation forest\u2606\n\n**Pawe\u0142 Karczmarek a,*, Adam Kiersztyn a, Witold Pedrycz b,c,d, Ebru Ale**\n\na Department of Computer Science, Lublin University of Technology, ul.  Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors. ", "original_text": "k-Means clustering is used to predict the number of divisions on each decision tree node. "}, "hash": "7be0327bef9acd431faf7bb1fda55f83e573605a2dccdfff90d9f18668cb3bd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9e86b31-23ca-4c30-83cc-623927803bac", "node_type": "1", "metadata": {"window": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. "}, "hash": "7bc20f75c1625fa6012369b9ad0bfb7e4a991f123ca41c6c2cfca16fa0d54f9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. ", "mimetype": "text/plain", "start_char_idx": 1788, "end_char_idx": 1987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9e86b31-23ca-4c30-83cc-623927803bac", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d57867b-83ad-4821-89ac-326ea64d4589", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Nadbystrzycka 36B, 20-618, Lublin, Poland\nb Department of Electrical & Computer Engineering, University of Alberta, Edmonton T6R 2V4 AB, Canada\nc Department of Electrical and Computer Engineering, King Abdulaziz University, Jeddah, 21589, Saudi Arabia\nd Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland\n\u2022 Ekol Lojistik Inc., Ekol Caddesi No:2, TR-34935 Sultanbeyli-Istanbul, Turkey\n\n---\n\n**ARTICLE INFO**\n\n*Article history:*\nReceived 28 October 2019\nReceived in revised form 27 January 2020\nAccepted 12 February 2020\nAvailable online 17 February 2020\n\n*Keywords:*\nAnomaly detection\nIsolation forest\nK-Means\nSearch tree\nSpatio-temporal datasets\n\n---\n\n**ABSTRACT**\n\nThe task of anomaly detection in data is one of the main challenges in data science because of the wide plethora of applications and despite a spectrum of available methods.  Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V. ", "original_text": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data. "}, "hash": "d100f5f3a1835bc3e6997de6d91c9aaef49b5652c14b31a5d3303d761e5779c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fea006e9-6ee3-4018-8cd2-14e1a1d32101", "node_type": "1", "metadata": {"window": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. ", "original_text": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. "}, "hash": "168debe3900e9b7905a9b515d56f766874a56ae47ab905264b9d1fc4ee80d615", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. ", "mimetype": "text/plain", "start_char_idx": 1987, "end_char_idx": 2111, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fea006e9-6ee3-4018-8cd2-14e1a1d32101", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. ", "original_text": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9e86b31-23ca-4c30-83cc-623927803bac", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Unfortunately, many of anomaly detection schemes are still imperfect i.e., they are not effective enough or act in a non-intuitive way or they are focused on a specific type of data.  In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset. "}, "hash": "1bbdf64dfcf4321afdbe00eef33318a061bf892b5f4dddbb8fc64d8b9ee8ba39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65be0059-b75a-4a79-bf74-63f015fc77ec", "node_type": "1", "metadata": {"window": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys. ", "original_text": "Moreover, it returns more intuitively appealing anomaly score values.\n\n"}, "hash": "6bbcf5cbe3685e1eb2693044f601b1b6032eb2dfe1899320e50817eca3a500ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. ", "mimetype": "text/plain", "start_char_idx": 2111, "end_char_idx": 2222, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65be0059-b75a-4a79-bf74-63f015fc77ec", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys. ", "original_text": "Moreover, it returns more intuitively appealing anomaly score values.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fea006e9-6ee3-4018-8cd2-14e1a1d32101", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In this study, the classical method of Isolation Forest is thoroughly analyzed and augmented by bringing an innovative approach.  This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. ", "original_text": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building. "}, "hash": "f5ffc525159f45674a41b10eb7325b7f2dafd4e5a200cd40c9f3d35dae0af009", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3432368c-f7c9-491d-be81-59d7b91473f1", "node_type": "1", "metadata": {"window": "k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n", "original_text": "\u00a9 2020 The Authors. "}, "hash": "dbff9a87405249123f344ea5522907680ff2ea20875663e71b7620afde03ca1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, it returns more intuitively appealing anomaly score values.\n\n", "mimetype": "text/plain", "start_char_idx": 2222, "end_char_idx": 2293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3432368c-f7c9-491d-be81-59d7b91473f1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n", "original_text": "\u00a9 2020 The Authors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65be0059-b75a-4a79-bf74-63f015fc77ec", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This is k-Means-Based Isolation Forest that allows to build a search tree based on many branches in contrast to the only two considered in the original method.  k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys. ", "original_text": "Moreover, it returns more intuitively appealing anomaly score values.\n\n"}, "hash": "f00259cfd2c385ebdc376db80f09a53799b54aedd167182e7f4d1c0e0c406dcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32b9c1df-47ee-435d-bc8e-1d4e4a23e526", "node_type": "1", "metadata": {"window": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n", "original_text": "Published by Elsevier B.V. "}, "hash": "3dc11081669e7a71799ff52938fc259bd4951b26a5f8f8e511d45b0ded92f64c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u00a9 2020 The Authors. ", "mimetype": "text/plain", "start_char_idx": 2293, "end_char_idx": 2313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32b9c1df-47ee-435d-bc8e-1d4e4a23e526", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n", "original_text": "Published by Elsevier B.V. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3432368c-f7c9-491d-be81-59d7b91473f1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "k-Means clustering is used to predict the number of divisions on each decision tree node.  As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n", "original_text": "\u00a9 2020 The Authors. "}, "hash": "442f7d039749edf7ad83ceb1ea2210bd8dd85dd7a52ad02332413f365f76c3e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9bf9a91-8cde-4ff7-9bbf-896561254c61", "node_type": "1", "metadata": {"window": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "8a9daa01877a90a3d2a2dfda7fe11e0cbe163da1954735ed1f99d2d0f9ff9b6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published by Elsevier B.V. ", "mimetype": "text/plain", "start_char_idx": 2313, "end_char_idx": 2340, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9bf9a91-8cde-4ff7-9bbf-896561254c61", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32b9c1df-47ee-435d-bc8e-1d4e4a23e526", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "As supported through experimental studies, the proposed method works effectively for data coming from various application areas including intermodal transport and geographical, spatio-temporal data.  In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n", "original_text": "Published by Elsevier B.V. "}, "hash": "7601fe17db15fb60c66d7449a4d850f78c47b43ce4e15d9e60d7f2a9d209f75e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40ecd9c0-e8ff-4464-8cee-41dec8a7a2e0", "node_type": "1", "metadata": {"window": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. ", "original_text": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. "}, "hash": "00b8f1f16cae3bd7912a37186469dfb50e971ae733666f060ef32ffd0eb82ac8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "mimetype": "text/plain", "start_char_idx": 2340, "end_char_idx": 2455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40ecd9c0-e8ff-4464-8cee-41dec8a7a2e0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. ", "original_text": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9bf9a91-8cde-4ff7-9bbf-896561254c61", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In addition, it enables a user to intuitively determine the anomaly score for an individual record of the analyzed dataset.  The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "4879c8186ab6739cc18740a55a6e8d983b373472af5a8471a6c24b3282cde6b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d86f561-cff3-4ecb-9b37-7df1a8be70c2", "node_type": "1", "metadata": {"window": "Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V. ", "original_text": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys. "}, "hash": "534d3d3d0896b123de790ff6728376d9e5d545bb05ad0663a33326f131d7f7f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. ", "mimetype": "text/plain", "start_char_idx": 2455, "end_char_idx": 2616, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d86f561-cff3-4ecb-9b37-7df1a8be70c2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V. ", "original_text": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40ecd9c0-e8ff-4464-8cee-41dec8a7a2e0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The advantage of the proposed method is that it is able to fit the data at the step of decision tree building.  Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. ", "original_text": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work. "}, "hash": "dc4dca2dcabe801985cb2925f6f7a0c74d50ef5965cb7ddafc2bbf318d6d318f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2836670-b04c-4816-b2b5-1a7da5a6fbe1", "node_type": "1", "metadata": {"window": "\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "2020.105659.\n"}, "hash": "0d528bddf8c53024b3cf9bfbe29cf9529bf67a91004916b6244e834b0ae81806", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys. ", "mimetype": "text/plain", "start_char_idx": 2616, "end_char_idx": 2690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f2836670-b04c-4816-b2b5-1a7da5a6fbe1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "2020.105659.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d86f561-cff3-4ecb-9b37-7df1a8be70c2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it returns more intuitively appealing anomaly score values.\n\n \u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V. ", "original_text": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys. "}, "hash": "fa4a802cfa158a9ddd6d442f7fdf1aafb4ec45cd1f4780043c51a91351bb7ef5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "47ebbfb0-6555-4ff7-9523-54debb18f1ef", "node_type": "1", "metadata": {"window": "Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1. ", "original_text": "* Corresponding author.\n"}, "hash": "3880731091780594c188929a15ab3ec35db685a7c4df0d78ec0dcd6674f20f34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2020.105659.\n", "mimetype": "text/plain", "start_char_idx": 2690, "end_char_idx": 2703, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "47ebbfb0-6555-4ff7-9523-54debb18f1ef", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1. ", "original_text": "* Corresponding author.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2836670-b04c-4816-b2b5-1a7da5a6fbe1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "original_text": "2020.105659.\n"}, "hash": "0a9b31b919ed9e784644a80e436f3483b7df1205ee8b44ec25b01aa07f9a517c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb080be2-5b7f-4320-99bc-4a001716e9a2", "node_type": "1", "metadata": {"window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. ", "original_text": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n"}, "hash": "0d09770a601fcb7092f5e431b8ef7fab2daeab2cd046935d24fb29fce274ee86", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "* Corresponding author.\n", "mimetype": "text/plain", "start_char_idx": 2703, "end_char_idx": 2727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb080be2-5b7f-4320-99bc-4a001716e9a2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. ", "original_text": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47ebbfb0-6555-4ff7-9523-54debb18f1ef", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1. ", "original_text": "* Corresponding author.\n"}, "hash": "0de3dbece7c3b404a1372bff1b9bf8c708b7673c662403169ce0ba3b15d835ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d272a359-997b-4ea6-a0ad-2cca941b9359", "node_type": "1", "metadata": {"window": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways. ", "original_text": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. "}, "hash": "d689d0cd3bba4e129d2e4d0fad3a767dcf8ec1e3bedf5ff93fa41ced69d4c375", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n", "mimetype": "text/plain", "start_char_idx": 2727, "end_char_idx": 2895, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d272a359-997b-4ea6-a0ad-2cca941b9359", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways. ", "original_text": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb080be2-5b7f-4320-99bc-4a001716e9a2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. ", "original_text": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n"}, "hash": "7c6f53e1c9bd32d2ce60a4e6356f4a44098098fefc0977ab5ff084b5b55938ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "693b7d59-a459-40fe-b9e8-08daee23c971", "node_type": "1", "metadata": {"window": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. ", "original_text": "Published by Elsevier B.V. "}, "hash": "58fce06d4b08ede9f73c43790a58eda9798883862d2fd8866a6f2fe04fd2febb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. ", "mimetype": "text/plain", "start_char_idx": 2895, "end_char_idx": 2970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "693b7d59-a459-40fe-b9e8-08daee23c971", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. ", "original_text": "Published by Elsevier B.V. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d272a359-997b-4ea6-a0ad-2cca941b9359", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "---\n\n\u2606 No author associated with this paper has disclosed any potential or pertinent conflicts which may be perceived to have impending conflict with this work.  For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways. ", "original_text": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors. "}, "hash": "d8a94fbe380e25bcf4e6b1d097fe0fd9de545d44785e7e0136cf260aeed630c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4148ad6f-026c-4e9b-9671-2b1282125f97", "node_type": "1", "metadata": {"window": "2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "5c4faf192833749cd78a5c155b5c55504a3cf785a04f67f9dc6a8bdee81e34a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Published by Elsevier B.V. ", "mimetype": "text/plain", "start_char_idx": 2970, "end_char_idx": 2997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4148ad6f-026c-4e9b-9671-2b1282125f97", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "693b7d59-a459-40fe-b9e8-08daee23c971", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For full disclosure statements refer to https://doi.org/10.1016/j.knosys.  2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. ", "original_text": "Published by Elsevier B.V. "}, "hash": "2c0ba97080bf2c17797ed8940d4e8c074ede8744978ea37a19b0eab15398a5a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efd142b9-f1fb-427c-9519-e5ee7edfc307", "node_type": "1", "metadata": {"window": "* Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps). ", "original_text": "### 1. "}, "hash": "a32323f326c1c8f5ce5bf41d4d2d7ceefbfd7e96d19b78ccb339cde3b2bd9b53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n", "mimetype": "text/plain", "start_char_idx": 2997, "end_char_idx": 3112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "efd142b9-f1fb-427c-9519-e5ee7edfc307", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "* Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps). ", "original_text": "### 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4148ad6f-026c-4e9b-9671-2b1282125f97", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2020.105659.\n * Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. ", "original_text": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n"}, "hash": "a48c420897f6a6166732b905b6f029494d544fe67d6882b69fe3a808eb0bd6ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da0bdc31-45de-4823-8b36-8d1825ba4168", "node_type": "1", "metadata": {"window": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n", "original_text": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. "}, "hash": "dfd5ae962e865b9166f90c55f29e2cd7ab7f3f220ea46fab084cd97fcfb6fb7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 1. ", "mimetype": "text/plain", "start_char_idx": 3112, "end_char_idx": 3119, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da0bdc31-45de-4823-8b36-8d1825ba4168", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n", "original_text": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efd142b9-f1fb-427c-9519-e5ee7edfc307", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "* Corresponding author.\n E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps). ", "original_text": "### 1. "}, "hash": "82ac010ff211585d425e57693fa97164daacb61ad36cb2b3ebd0b81a84180773", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b219910-107c-4d23-87ab-b5b31b907755", "node_type": "1", "metadata": {"window": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. ", "original_text": "An anomaly can be defined in various ways. "}, "hash": "510e71f313e20fdac2e931983bbda64d213ba3496694109f6bec056e1a385fb6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. ", "mimetype": "text/plain", "start_char_idx": 3119, "end_char_idx": 3417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b219910-107c-4d23-87ab-b5b31b907755", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. ", "original_text": "An anomaly can be defined in various ways. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da0bdc31-45de-4823-8b36-8d1825ba4168", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "E-mail addresses: pawel.karczmarek@gmail.com (P. Karczmarek), adam.kiersztyn.pl@gmail.com (A. Kiersztyn), wpedrycz@ualberta.ca (W. Pedrycz), ebru.al@ekol.com (E. Al).\n\n https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n", "original_text": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations. "}, "hash": "5d7969d3439e08b744718efdadc474f18bb99edde55b32d2efaf13d4fcb05ba2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "522557d9-3890-4177-8ac1-de747c880edb", "node_type": "1", "metadata": {"window": "Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users. ", "original_text": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. "}, "hash": "7cfda1461b8052ef7261f0311920d001a25561c9d5010786a08111a33c12acef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An anomaly can be defined in various ways. ", "mimetype": "text/plain", "start_char_idx": 3417, "end_char_idx": 3460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "522557d9-3890-4177-8ac1-de747c880edb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users. ", "original_text": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b219910-107c-4d23-87ab-b5b31b907755", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "https://doi.org/10.1016/j.knosys.2020.105659\n0950-7051/\u00a9 2020 The Authors.  Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. ", "original_text": "An anomaly can be defined in various ways. "}, "hash": "546a18e7a9b0973cdeb35a204b4457e3da57b886a4bce27975acbb45be734a33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "acd60fe5-daa0-47e0-b0b7-325cc8d41b0d", "node_type": "1", "metadata": {"window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies. ", "original_text": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. "}, "hash": "2eeb1680521126273eb951f2367e4d09c3fb463bf75a55b49b707b81508e94f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. ", "mimetype": "text/plain", "start_char_idx": 3460, "end_char_idx": 3613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "acd60fe5-daa0-47e0-b0b7-325cc8d41b0d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies. ", "original_text": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "522557d9-3890-4177-8ac1-de747c880edb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Published by Elsevier B.V.  This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users. ", "original_text": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2]. "}, "hash": "114a3981f97767c9fef28b611aaeea219eb5327a323e5fcf0063082ab5e8fedc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "291d1d60-c789-4cc8-abad-08fa100aa2f8", "node_type": "1", "metadata": {"window": "### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations. ", "original_text": "a time-related values (time stamps). "}, "hash": "6d50430deb25cac21fc9e10a15110d409397f54c4adb770e4d4615a64e154c20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. ", "mimetype": "text/plain", "start_char_idx": 3613, "end_char_idx": 4002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "291d1d60-c789-4cc8-abad-08fa100aa2f8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations. ", "original_text": "a time-related values (time stamps). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "acd60fe5-daa0-47e0-b0b7-325cc8d41b0d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).\n\n ### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies. ", "original_text": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz. "}, "hash": "51c8de5ab806783a55bbaa3a187b740fbc73cc8149cc545bb9e9f25642b1bddd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6d31012-e6b9-475a-ac60-0fbaec51944f", "node_type": "1", "metadata": {"window": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. ", "original_text": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n"}, "hash": "f2860573df62ba2bf6e72e21cea1ba11697192b174472f1e1e0a529d018d8f0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a time-related values (time stamps). ", "mimetype": "text/plain", "start_char_idx": 4002, "end_char_idx": 4039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6d31012-e6b9-475a-ac60-0fbaec51944f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. ", "original_text": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "291d1d60-c789-4cc8-abad-08fa100aa2f8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 1.  Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations. ", "original_text": "a time-related values (time stamps). "}, "hash": "f2075df9666822767bf9708a8c23c9edcf7e4839f02099311516a7307957a3e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94c79cdf-5e5d-4203-ad84-721b2d0956f9", "node_type": "1", "metadata": {"window": "An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. ", "original_text": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. "}, "hash": "e2ec4a2acc2a3b247fd67b25056e445a5dd3fea87ec91a2c9e4bc9ff468babfe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n", "mimetype": "text/plain", "start_char_idx": 4039, "end_char_idx": 4177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94c79cdf-5e5d-4203-ad84-721b2d0956f9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. ", "original_text": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6d31012-e6b9-475a-ac60-0fbaec51944f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Introduction\n\nThe task of anomaly detection also known as outlier detection in large data coming from various aspects of real life, in particular, business data, has been one of the most challenging problems because of fast increase of datasets size and their complexities in modern organizations.  An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. ", "original_text": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n"}, "hash": "8886d34401db323dbb9d57abf3f0da45d3b1ed8d3f5060fc0ab760c2289ddd12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4a2db90-6e3d-4388-ab24-04d62d00c71d", "node_type": "1", "metadata": {"window": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. ", "original_text": "There may be incorrect data present in the database, e.g., mistakenly inserted by users. "}, "hash": "fd20d61be7dd27b068e9cc5f7f23f7be5372aa071f4b66dda0ed39866da21145", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. ", "mimetype": "text/plain", "start_char_idx": 4177, "end_char_idx": 4294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4a2db90-6e3d-4388-ab24-04d62d00c71d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. ", "original_text": "There may be incorrect data present in the database, e.g., mistakenly inserted by users. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94c79cdf-5e5d-4203-ad84-721b2d0956f9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An anomaly can be defined in various ways.  It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. ", "original_text": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field. "}, "hash": "e1007d6c6fd79182dca759df98bfcc2e34d7c8fe8277dc2d4fc4cb05a8213f30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55522ddd-936e-41b5-899f-75eccdee137c", "node_type": "1", "metadata": {"window": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented. ", "original_text": "Also the distant points located far from cluster centers could be regarded as anomalies. "}, "hash": "eae88f197d085d971fe29812b9e92d7299911e515e489642fa80c766c97819f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There may be incorrect data present in the database, e.g., mistakenly inserted by users. ", "mimetype": "text/plain", "start_char_idx": 4294, "end_char_idx": 4383, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "55522ddd-936e-41b5-899f-75eccdee137c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented. ", "original_text": "Also the distant points located far from cluster centers could be regarded as anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4a2db90-6e3d-4388-ab24-04d62d00c71d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It may be a deviation in amplitude, randomly inserted value, lack of data, data of various types, and implied by many commonly encountered errors [1,2].  The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. ", "original_text": "There may be incorrect data present in the database, e.g., mistakenly inserted by users. "}, "hash": "40e85471d08d31d76a73d8bb21cc21a647c99be6c66427e267943ae6b6ea2b0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c76a74cf-1a14-46be-b912-9e612ab46ab5", "node_type": "1", "metadata": {"window": "a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n", "original_text": "One can think of anomaly as missing data in the database or a result of erroneous calculations. "}, "hash": "b93f85f512f800088d77c4fc42b65f6e745a244eab283e7cfe37dbce281cce1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also the distant points located far from cluster centers could be regarded as anomalies. ", "mimetype": "text/plain", "start_char_idx": 4383, "end_char_idx": 4472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c76a74cf-1a14-46be-b912-9e612ab46ab5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n", "original_text": "One can think of anomaly as missing data in the database or a result of erroneous calculations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55522ddd-936e-41b5-899f-75eccdee137c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The problem can be even more important in the case when multi-dimensional data are considered, in particular, spatio-temporal time series, where one subset of variables is related to the geographical positions, i.e., latitude-longitude pairs of coordinates or other positions such as points on the plots or maps, while the rest of the variables denote one or more time series values, viz.  a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented. ", "original_text": "Also the distant points located far from cluster centers could be regarded as anomalies. "}, "hash": "c03c8ded81ebcca64bd133ab9fd10a8ef2b68000921cbb98ed14d8a6dc7a5918", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d42fbef-2bfc-4a28-842f-69e86a88ec8e", "node_type": "1", "metadata": {"window": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain. ", "original_text": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. "}, "hash": "7a86bbc938ce41a1632a020bc2f7b4ecf66b6f973cbcf4d984524eea5595c853", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can think of anomaly as missing data in the database or a result of erroneous calculations. ", "mimetype": "text/plain", "start_char_idx": 4472, "end_char_idx": 4568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d42fbef-2bfc-4a28-842f-69e86a88ec8e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain. ", "original_text": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c76a74cf-1a14-46be-b912-9e612ab46ab5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "a time-related values (time stamps).  Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n", "original_text": "One can think of anomaly as missing data in the database or a result of erroneous calculations. "}, "hash": "e4b608ca90795e954a3d435c917a61dc77e2b5e955efd19dd68ce25ec9b08946", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0fcec55-19c2-48dc-a2e6-66818a47f4a5", "node_type": "1", "metadata": {"window": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig. ", "original_text": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. "}, "hash": "17cb816ff40ce7fda4b246259e9a5a12c23fc94fa606d917226a1187a05c3e04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. ", "mimetype": "text/plain", "start_char_idx": 4568, "end_char_idx": 4821, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0fcec55-19c2-48dc-a2e6-66818a47f4a5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig. ", "original_text": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d42fbef-2bfc-4a28-842f-69e86a88ec8e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Typical examples of this kind of data are demographic, geological and geophysical data or the details of intermodal transport processes.\n\n There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain. ", "original_text": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc. "}, "hash": "0137de0d6833a6a669617fe18abf04430859ff83204dd614a1df6230d84f9f25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d913e224-deba-460a-9192-6e6254534be2", "node_type": "1", "metadata": {"window": "There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1. ", "original_text": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. "}, "hash": "43ddd3bd92f1ed260b762a7ef3c9896eaf1c3241a93401e42884619135a53691", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. ", "mimetype": "text/plain", "start_char_idx": 4821, "end_char_idx": 5113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d913e224-deba-460a-9192-6e6254534be2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1. ", "original_text": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0fcec55-19c2-48dc-a2e6-66818a47f4a5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There are many types of anomalies which pose challenging problems for the scientists and practitioners in the field.  There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig. ", "original_text": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes. "}, "hash": "db5fbed19241816b1f9db1dc43758248e892017638f6188df48853759230b57c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d386e43-6023-4aec-8ebb-38caf36a5ce5", "node_type": "1", "metadata": {"window": "Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest.", "original_text": "Therefore, the literature of the topic is vast and application-oriented. "}, "hash": "1f7a7eb43e43ae6df06b512335750b4e244aa2c948e994ba20abd10c934aa6d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. ", "mimetype": "text/plain", "start_char_idx": 5113, "end_char_idx": 5256, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d386e43-6023-4aec-8ebb-38caf36a5ce5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest.", "original_text": "Therefore, the literature of the topic is vast and application-oriented. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d913e224-deba-460a-9192-6e6254534be2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There may be incorrect data present in the database, e.g., mistakenly inserted by users.  Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1. ", "original_text": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases. "}, "hash": "6d7da0e8e062d8d7a74f8a2427a5c35fb07d7e0d02707fb1038e4e4295ec9a79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "995619ff-67ac-45ff-8d5f-9f4bda92a528", "node_type": "1", "metadata": {"window": "One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points. ", "original_text": "A few main results will be recalled now.\n\n"}, "hash": "cc45a705c95dff090df2e315b82f7f597c5cbc931e6b537c09f5d9c434355492", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the literature of the topic is vast and application-oriented. ", "mimetype": "text/plain", "start_char_idx": 5256, "end_char_idx": 5329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "995619ff-67ac-45ff-8d5f-9f4bda92a528", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points. ", "original_text": "A few main results will be recalled now.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d386e43-6023-4aec-8ebb-38caf36a5ce5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Also the distant points located far from cluster centers could be regarded as anomalies.  One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest.", "original_text": "Therefore, the literature of the topic is vast and application-oriented. "}, "hash": "213ca306d761035860dbbc0e4a036d5672bcaf52d6a55ae1fc3e854b1a638c69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63ea06d0-be14-436b-8242-cb4c771a23e1", "node_type": "1", "metadata": {"window": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. ", "original_text": "The experts in data mining often list a few methods efficient in outlier detection domain. "}, "hash": "b6627d4423bad2ec993d765dcc9af91e2d2f062287522d0cbd0457137ca753ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A few main results will be recalled now.\n\n", "mimetype": "text/plain", "start_char_idx": 5329, "end_char_idx": 5371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63ea06d0-be14-436b-8242-cb4c771a23e1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. ", "original_text": "The experts in data mining often list a few methods efficient in outlier detection domain. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "995619ff-67ac-45ff-8d5f-9f4bda92a528", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "One can think of anomaly as missing data in the database or a result of erroneous calculations.  Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points. ", "original_text": "A few main results will be recalled now.\n\n"}, "hash": "53e3761a8237faf690088f3e310f2f659b540f8bc85004b3267dc5ca4129ed1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "955c9d43-bf05-4cfc-ad82-1deb329dfeba", "node_type": "1", "metadata": {"window": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n", "original_text": "Isolation forest [3,4] is almost always\n\n***\n**Fig. "}, "hash": "f4bf415f9368534e9df875106b514f42cff9dbde885f6526105214239a9a7504", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experts in data mining often list a few methods efficient in outlier detection domain. ", "mimetype": "text/plain", "start_char_idx": 5371, "end_char_idx": 5462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "955c9d43-bf05-4cfc-ad82-1deb329dfeba", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n", "original_text": "Isolation forest [3,4] is almost always\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63ea06d0-be14-436b-8242-cb4c771a23e1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Anomalies include transport delay times or extremely short vehicle transit times, in extreme cases, negative, unexpected travel trajectories, lack of transport details and characteristics, or even typos inserted by users or dataset administrators, etc.  Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. ", "original_text": "The experts in data mining often list a few methods efficient in outlier detection domain. "}, "hash": "d04c367d780728dd85712e6d5429fbdc429e23ac40217ed45e8beb36ccb4cfe5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe36aafe-07fa-47c6-b43b-41c1abd68793", "node_type": "1", "metadata": {"window": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "1. "}, "hash": "5ddd819c7f37e0ff322094c0d35499da3d7b0203c7c49118e0a104f2cd19e908", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation forest [3,4] is almost always\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 5462, "end_char_idx": 5514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe36aafe-07fa-47c6-b43b-41c1abd68793", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "955c9d43-bf05-4cfc-ad82-1deb329dfeba", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Anomalies in databases can be represented by readings of parameters of malfunctioning devices, e.g., perimetry or server temperatures variations, as well as network anomalies, various fraud attempts, e.g., impersonating someone in electronic banking systems or anomalies in a crowded scenes.  These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n", "original_text": "Isolation forest [3,4] is almost always\n\n***\n**Fig. "}, "hash": "dc5ef000bf4110d46acf14ba2b8b107bbfa8bde1d54f2cc1fe40cb67144d05ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34207f33-57c4-479b-add5-5075414eb2e9", "node_type": "1", "metadata": {"window": "Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig. ", "original_text": "An intuitive interpretation of isolation forest."}, "hash": "081fe93e1eb5a075e253f6b72a0a1cddd643ccf02d56397d8cebedb464f0b3f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. ", "mimetype": "text/plain", "start_char_idx": 5514, "end_char_idx": 5517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34207f33-57c4-479b-add5-5075414eb2e9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig. ", "original_text": "An intuitive interpretation of isolation forest."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe36aafe-07fa-47c6-b43b-41c1abd68793", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These significant areas of applications and resulting problems lead to the increased involvement in the search for outliers in huge databases.  Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "1. "}, "hash": "a1f30ec13225a2787b558d142c90b6db070052c2f8972a3042420cfd8498e6b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bed78de7-0d1c-41d1-92d5-a56e13da1a5b", "node_type": "1", "metadata": {"window": "A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2. ", "original_text": "** This figure displays a 2D scatter plot of data points. "}, "hash": "4cd7c6dd4c4a96e61337ff1511f69f8eaba4602fc6fecf713b0f83c1a3445c21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An intuitive interpretation of isolation forest.", "mimetype": "text/plain", "start_char_idx": 5517, "end_char_idx": 5565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bed78de7-0d1c-41d1-92d5-a56e13da1a5b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2. ", "original_text": "** This figure displays a 2D scatter plot of data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34207f33-57c4-479b-add5-5075414eb2e9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore, the literature of the topic is vast and application-oriented.  A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig. ", "original_text": "An intuitive interpretation of isolation forest."}, "hash": "32df0cff06680fa2ba5bc5bb68db4d8281f64846a21ee1db97507b5d24ad2b6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25ff354f-5866-4626-a173-fc2a6a1372c7", "node_type": "1", "metadata": {"window": "The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF.", "original_text": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. "}, "hash": "36dd47c9192d058d0a394f736d736c7118ba49809146bd385dbeac1341a04eb6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This figure displays a 2D scatter plot of data points. ", "mimetype": "text/plain", "start_char_idx": 5565, "end_char_idx": 5623, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "25ff354f-5866-4626-a173-fc2a6a1372c7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF.", "original_text": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bed78de7-0d1c-41d1-92d5-a56e13da1a5b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A few main results will be recalled now.\n\n The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2. ", "original_text": "** This figure displays a 2D scatter plot of data points. "}, "hash": "3614168160c0b168d159377d564d949037ab60be7a214359327a0b8b5e79e7fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d478ec6e-bda4-4f2c-b3f7-f1270ff2c9b0", "node_type": "1", "metadata": {"window": "Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot. ", "original_text": "This illustrates how an outlier can be isolated in a small number of steps.\n"}, "hash": "f43c8da329c9b7be3fdf01e9536a77ceed3b3928ad071dba822b5af61c1b15b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. ", "mimetype": "text/plain", "start_char_idx": 5623, "end_char_idx": 5785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d478ec6e-bda4-4f2c-b3f7-f1270ff2c9b0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot. ", "original_text": "This illustrates how an outlier can be isolated in a small number of steps.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25ff354f-5866-4626-a173-fc2a6a1372c7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The experts in data mining often list a few methods efficient in outlier detection domain.  Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF.", "original_text": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space. "}, "hash": "f09574ca7b00e86fa2bed9b4e420149f123b7a1795c2608f260ae8ef0c758250", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd8fbdc1-7f61-4d40-92f8-694c9fb1b0cb", "node_type": "1", "metadata": {"window": "1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "add287a396ee8db8a3baeab3fddcd8561ced8a942714733df708d287ee05300b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This illustrates how an outlier can be isolated in a small number of steps.\n", "mimetype": "text/plain", "start_char_idx": 5785, "end_char_idx": 5861, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd8fbdc1-7f61-4d40-92f8-694c9fb1b0cb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d478ec6e-bda4-4f2c-b3f7-f1270ff2c9b0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Isolation forest [3,4] is almost always\n\n***\n**Fig.  1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot. ", "original_text": "This illustrates how an outlier can be isolated in a small number of steps.\n"}, "hash": "838347cbc5399865d6e3c936ebe64ac0ae583855b8586baa73a1271235f4a032", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a43451a2-ed66-4b9c-9dc3-4c3f6dfc288c", "node_type": "1", "metadata": {"window": "An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n", "original_text": ")*\n***\n**Fig. "}, "hash": "cfa150c4c89024f49808400e058c2dcad95a339ff818be5335aaa63ee0313191", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 5861, "end_char_idx": 5990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a43451a2-ed66-4b9c-9dc3-4c3f6dfc288c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n", "original_text": ")*\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd8fbdc1-7f61-4d40-92f8-694c9fb1b0cb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1.  An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "810bc8337c2182592abc76bb85b9ec0947d0450f2fecd5848ae75e752e59baa1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7bc949b-66be-427d-817d-372c06998abc", "node_type": "1", "metadata": {"window": "** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig. ", "original_text": "2. "}, "hash": "1993091040cc85511e87fd72825d1d05dccf4fc560da9f4c46a2eb6f8f20da1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 5990, "end_char_idx": 6004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7bc949b-66be-427d-817d-372c06998abc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig. ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a43451a2-ed66-4b9c-9dc3-4c3f6dfc288c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An intuitive interpretation of isolation forest. ** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n", "original_text": ")*\n***\n**Fig. "}, "hash": "599256a4de2126f67aa0c24bb7ae81dab4fd8221e46d56417e8f53653fd6ecae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "073e8f04-921a-47d0-8ffe-b058a2596fea", "node_type": "1", "metadata": {"window": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3. ", "original_text": "Clustering of points: Intuition behind the k-Means-based IF."}, "hash": "f42503fbfe8c1164365b41925f2f00a86345397d0b63d8ac45b245111810ed52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 6004, "end_char_idx": 6007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "073e8f04-921a-47d0-8ffe-b058a2596fea", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3. ", "original_text": "Clustering of points: Intuition behind the k-Means-based IF."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7bc949b-66be-427d-817d-372c06998abc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure displays a 2D scatter plot of data points.  A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig. ", "original_text": "2. "}, "hash": "4e3b4d38aaacec77a2444403d69b90c8f048a89e1e463860c13cb52cb398e684", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1692a33-24f9-4f3d-8886-daffae313be6", "node_type": "1", "metadata": {"window": "This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset).", "original_text": "** This figure shows a similar 2D scatter plot. "}, "hash": "366125a5aed8c03af41795b5cb13a20a25adaa773001df4146fca7934e2dc57c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Clustering of points: Intuition behind the k-Means-based IF.", "mimetype": "text/plain", "start_char_idx": 6007, "end_char_idx": 6067, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c1692a33-24f9-4f3d-8886-daffae313be6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset).", "original_text": "** This figure shows a similar 2D scatter plot. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "073e8f04-921a-47d0-8ffe-b058a2596fea", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A single red point is isolated from the main cluster of points by a series of ten alternating vertical and horizontal lines that recursively partition the space.  This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3. ", "original_text": "Clustering of points: Intuition behind the k-Means-based IF."}, "hash": "1d0a0a2756814aa4241a746925bc5c68d56177d9513151b91924118df68565c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4317dd86-df47-4f0f-9b11-a26aa9cb887f", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. ", "original_text": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. "}, "hash": "21c73d561ffc9ac27249b01a23d3e909ac239eadbdac11a033be81313a02cf85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This figure shows a similar 2D scatter plot. ", "mimetype": "text/plain", "start_char_idx": 6067, "end_char_idx": 6115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4317dd86-df47-4f0f-9b11-a26aa9cb887f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. ", "original_text": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1692a33-24f9-4f3d-8886-daffae313be6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This illustrates how an outlier can be isolated in a small number of steps.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset).", "original_text": "** This figure shows a similar 2D scatter plot. "}, "hash": "e1ac2d83f39a361d96386aa2e7105534f051ea70bece8c5cb362c764a4cd872d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "353f8bdf-a3c8-4c8b-95cc-36a5edee04b3", "node_type": "1", "metadata": {"window": ")*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements. ", "original_text": "A single red point is located within one of the central clusters.\n"}, "hash": "1d89736676bd020be246d06687815ae78e539c95b0ffb03f5a6473d0ad77cc5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. ", "mimetype": "text/plain", "start_char_idx": 6115, "end_char_idx": 6277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "353f8bdf-a3c8-4c8b-95cc-36a5edee04b3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements. ", "original_text": "A single red point is located within one of the central clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4317dd86-df47-4f0f-9b11-a26aa9cb887f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. ", "original_text": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits. "}, "hash": "b2be8f138d1bdc3891627bf59123e04fdb89c8e65f84afdf25f8f4fa8eb16e15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "555563b3-8a24-4c86-940f-5461e981d3ed", "node_type": "1", "metadata": {"window": "2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n", "original_text": "***\n**Fig. "}, "hash": "1598e8a2ef30b07966fd6dbf886d84edbc5385339d65b3e7260c1f7f4938e9b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A single red point is located within one of the central clusters.\n", "mimetype": "text/plain", "start_char_idx": 6277, "end_char_idx": 6343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "555563b3-8a24-4c86-940f-5461e981d3ed", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "353f8bdf-a3c8-4c8b-95cc-36a5edee04b3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n**Fig.  2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements. ", "original_text": "A single red point is located within one of the central clusters.\n"}, "hash": "f3602090fb373b71f63b197529fdae226be80b9077ae982db2b49603f4d7c5f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30a3f93c-575a-4d7a-95bf-396b4d3cf811", "node_type": "1", "metadata": {"window": "Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig. ", "original_text": "3. "}, "hash": "6b1b87b3a983b9fb43aff8a702a779acca7a7337e8fe0f034d7d97e468b2ee5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 6343, "end_char_idx": 6354, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30a3f93c-575a-4d7a-95bf-396b4d3cf811", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig. ", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "555563b3-8a24-4c86-940f-5461e981d3ed", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2.  Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n", "original_text": "***\n**Fig. "}, "hash": "6b7d4b910d03324e17b0188d5e834528902afb567aacfdf1a191721141193f0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f278050-981a-4f9a-8001-0410d45d4db6", "node_type": "1", "metadata": {"window": "** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3. ", "original_text": "Histogram of rank differences (artificial dataset)."}, "hash": "9773288cbaf82a969971aac3c9d527a1988792a9c821ffabc484db61544e3382", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 6354, "end_char_idx": 6357, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0f278050-981a-4f9a-8001-0410d45d4db6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3. ", "original_text": "Histogram of rank differences (artificial dataset)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30a3f93c-575a-4d7a-95bf-396b4d3cf811", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Clustering of points: Intuition behind the k-Means-based IF. ** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig. ", "original_text": "3. "}, "hash": "e74ba3dbe08de4d26767c5afd8ad036cce24ee573e2c803ac05216bb541be848", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4e790e6-c487-4919-bf56-345826ca16ab", "node_type": "1", "metadata": {"window": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification.", "original_text": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. "}, "hash": "dfec7d04263f2b439208105501bb80b35b6639a30562c5395e9a280808b9c081", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Histogram of rank differences (artificial dataset).", "mimetype": "text/plain", "start_char_idx": 6357, "end_char_idx": 6408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4e790e6-c487-4919-bf56-345826ca16ab", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification.", "original_text": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f278050-981a-4f9a-8001-0410d45d4db6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure shows a similar 2D scatter plot.  The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3. ", "original_text": "Histogram of rank differences (artificial dataset)."}, "hash": "64dfdad6a04dd06b38e0b429ddaabbd11316735df0718f6cdd38999cec7d58c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db1a8dd2-b1c2-4cbb-a686-584d8f9f67cc", "node_type": "1", "metadata": {"window": "A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings. ", "original_text": "The x-axis represents rank differences, and the y-axis shows the number of elements. "}, "hash": "0372a9ea0353f87912c52c4f60664e58514cd637ac07413748d0b18e5bcb5587", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. ", "mimetype": "text/plain", "start_char_idx": 6408, "end_char_idx": 6565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db1a8dd2-b1c2-4cbb-a686-584d8f9f67cc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings. ", "original_text": "The x-axis represents rank differences, and the y-axis shows the number of elements. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4e790e6-c487-4919-bf56-345826ca16ab", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The space is partitioned by horizontal and vertical lines, but these divisions appear to align with natural clusters in the data rather than being random splits.  A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification.", "original_text": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification. "}, "hash": "3e1e17b34b5ef7a8d8d176c5f70bdec735ebf387d7579a364197ebdcc44aef88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9763c13e-3ba2-48c0-a223-3d067467f137", "node_type": "1", "metadata": {"window": "***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n", "original_text": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n"}, "hash": "8980d44c4832094196768fd4e509c730fe51da9c476802af6e5c96a7c67339a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis represents rank differences, and the y-axis shows the number of elements. ", "mimetype": "text/plain", "start_char_idx": 6565, "end_char_idx": 6650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9763c13e-3ba2-48c0-a223-3d067467f137", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n", "original_text": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db1a8dd2-b1c2-4cbb-a686-584d8f9f67cc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A single red point is located within one of the central clusters.\n ***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings. ", "original_text": "The x-axis represents rank differences, and the y-axis shows the number of elements. "}, "hash": "0d4fbb1f946ad5590695dafeb1f7c5ab938dc0679ee600aa5b7c7cdd167bb751", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "094f7e1b-02d3-4717-bea8-aea1b2774b65", "node_type": "1", "metadata": {"window": "3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. ", "original_text": "*(Caption)* **Fig. "}, "hash": "b5669176c7352e8e2b92bfd4e493b347e95513e00356b64a258dbb8e06a73761", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n", "mimetype": "text/plain", "start_char_idx": 6650, "end_char_idx": 6779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "094f7e1b-02d3-4717-bea8-aea1b2774b65", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. ", "original_text": "*(Caption)* **Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9763c13e-3ba2-48c0-a223-3d067467f137", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n", "original_text": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n"}, "hash": "ef29bcb2d9f59551afdf266c7911c4e538618c2efff03423d70ef0abad06bd92", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad958514-4de6-471a-95a7-6a91a9c85340", "node_type": "1", "metadata": {"window": "Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. ", "original_text": "3. "}, "hash": "80a5c2eac961ef08a09d1a4fca29948e36dbe9aa93c67a0073d8e9647033b533", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(Caption)* **Fig. ", "mimetype": "text/plain", "start_char_idx": 6779, "end_char_idx": 6798, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad958514-4de6-471a-95a7-6a91a9c85340", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. ", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "094f7e1b-02d3-4717-bea8-aea1b2774b65", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "3.  Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. ", "original_text": "*(Caption)* **Fig. "}, "hash": "c0679439d180470e062829ecca107200b940e9735d5ca270ac2d3bb0a39173cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15f7149e-e5de-48a0-a5b3-a63124c11950", "node_type": "1", "metadata": {"window": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster. ", "original_text": "Rank differences between IF and its k-Means-based modification."}, "hash": "282e658ec49f895e65f17a39983c9b980ee33bb4f50a8bb96e79a5c586a772bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 6798, "end_char_idx": 6801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "15f7149e-e5de-48a0-a5b3-a63124c11950", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster. ", "original_text": "Rank differences between IF and its k-Means-based modification."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad958514-4de6-471a-95a7-6a91a9c85340", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Histogram of rank differences (artificial dataset). ** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. ", "original_text": "3. "}, "hash": "31755674519384203826e162930db702ca6b93eddad21e5fc17c5e81d625071c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "957404b3-26d3-436d-97f5-9f576a5038ef", "node_type": "1", "metadata": {"window": "The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. ", "original_text": "**\n***\n\none of the methods presented at such rankings. "}, "hash": "1e662dbdf1f68b2d3c217d73065c34305279e4a3295da718e1186049c50558fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rank differences between IF and its k-Means-based modification.", "mimetype": "text/plain", "start_char_idx": 6801, "end_char_idx": 6864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "957404b3-26d3-436d-97f5-9f576a5038ef", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. ", "original_text": "**\n***\n\none of the methods presented at such rankings. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15f7149e-e5de-48a0-a5b3-a63124c11950", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a bar chart showing the frequency distribution of rank differences between the standard Isolation Forest (IF) and the k-Means-based modification.  The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster. ", "original_text": "Rank differences between IF and its k-Means-based modification."}, "hash": "362bf45716be56830508ccccf1824ad5b879166b2aa6d6116935a9c4f9304090", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ff8d0c1-2b06-4aa4-a71f-7e94323a39e3", "node_type": "1", "metadata": {"window": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data. ", "original_text": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n"}, "hash": "06144d2804a45e4dd54d36d6594aa8305f1bea4915b859573bd9f288a1cf6239", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n***\n\none of the methods presented at such rankings. ", "mimetype": "text/plain", "start_char_idx": 6864, "end_char_idx": 6919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ff8d0c1-2b06-4aa4-a71f-7e94323a39e3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data. ", "original_text": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "957404b3-26d3-436d-97f5-9f576a5038ef", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The x-axis represents rank differences, and the y-axis shows the number of elements.  The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. ", "original_text": "**\n***\n\none of the methods presented at such rankings. "}, "hash": "4d4d32c29bef641e161b5d2b535f78e932277cd812115defa7375474eaba61b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab0169a0-51d3-46ad-983b-3da91d4acd99", "node_type": "1", "metadata": {"window": "*(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. ", "original_text": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. "}, "hash": "d9e815712e0a94be031c3e708701b34084839bb3acb3499a551ae519d7df378e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n", "mimetype": "text/plain", "start_char_idx": 6919, "end_char_idx": 7132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab0169a0-51d3-46ad-983b-3da91d4acd99", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. ", "original_text": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ff8d0c1-2b06-4aa4-a71f-7e94323a39e3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The majority of points have small rank differences (close to 0), with the frequency decreasing as the rank difference increases.\n *(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data. ", "original_text": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n"}, "hash": "587e4ad3514250c6e12c491aef3723564c458c8993f218c8cf1b27070dd2bb3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06772cc9-6b92-4023-a271-b208c99a9757", "node_type": "1", "metadata": {"window": "3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed. ", "original_text": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. "}, "hash": "fd24d2a5df876a84bd2b7f41bd6b2ed9faba4f7dd407d32333278c7c8b0a1348", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. ", "mimetype": "text/plain", "start_char_idx": 7132, "end_char_idx": 7278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06772cc9-6b92-4023-a271-b208c99a9757", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed. ", "original_text": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab0169a0-51d3-46ad-983b-3da91d4acd99", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(Caption)* **Fig.  3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. ", "original_text": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters. "}, "hash": "8acb6f7e69929286158b1ec6db1bcd2711d5be0c6ab374ec76fd0f765f046e5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "efb9820c-aba8-4214-8b1b-f2fa0975f53c", "node_type": "1", "metadata": {"window": "Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. ", "original_text": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster. "}, "hash": "0ffa4432a91c0e9599173c5336bc72995b58d58127e5ca2802fd840cd0f610e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. ", "mimetype": "text/plain", "start_char_idx": 7278, "end_char_idx": 7466, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "efb9820c-aba8-4214-8b1b-f2fa0975f53c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. ", "original_text": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06772cc9-6b92-4023-a271-b208c99a9757", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "3.  Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed. ", "original_text": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training. "}, "hash": "b5ac0d0a7b941fb03a04807caedb0e85d9f54a64af52ca5431ecdb79fdd5a929", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c519936-97d7-49a8-96f9-504715bab802", "node_type": "1", "metadata": {"window": "**\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n", "original_text": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. "}, "hash": "41d4daf8f27cb84cc2010a1065c61e0c2aa4a1cf2ef24deb242ff17d1dbf159f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster. ", "mimetype": "text/plain", "start_char_idx": 7466, "end_char_idx": 7562, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c519936-97d7-49a8-96f9-504715bab802", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n", "original_text": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efb9820c-aba8-4214-8b1b-f2fa0975f53c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Rank differences between IF and its k-Means-based modification. **\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. ", "original_text": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster. "}, "hash": "8d9f4f82f91c53910efa6a401b884e2f8bca352f84aa305335e07428144852c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffe453c3-6fb7-4df5-8af2-76eee8716e20", "node_type": "1", "metadata": {"window": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet. ", "original_text": "Moreover, it seems to be faster for large datasets containing mixed data. "}, "hash": "c04a29c5dd6af2648c4cd66a4bdc2fed81545e6cce18a303735e103d6ee1f42b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. ", "mimetype": "text/plain", "start_char_idx": 7562, "end_char_idx": 7718, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ffe453c3-6fb7-4df5-8af2-76eee8716e20", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet. ", "original_text": "Moreover, it seems to be faster for large datasets containing mixed data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c519936-97d7-49a8-96f9-504715bab802", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n***\n\none of the methods presented at such rankings.  Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n", "original_text": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers. "}, "hash": "7eec6e819f5341ba51e05103e8aeaedfa210a4e984f6585e8ff7e4051388162c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd1aa580-91c2-4eed-b986-ae4066b57b20", "node_type": "1", "metadata": {"window": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. ", "original_text": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. "}, "hash": "8ac30cda8b3761b59802c3cba3ff8c3eb56825c0ecd04088fa299368ff5e96ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, it seems to be faster for large datasets containing mixed data. ", "mimetype": "text/plain", "start_char_idx": 7718, "end_char_idx": 7792, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd1aa580-91c2-4eed-b986-ae4066b57b20", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. ", "original_text": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffe453c3-6fb7-4df5-8af2-76eee8716e20", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Isolation Forest is based on the binary search trees used to find the partition of multidimensional dataset containing a particular record and estimate its anomaly score using a relatively sophisticated formula.\n\n The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet. ", "original_text": "Moreover, it seems to be faster for large datasets containing mixed data. "}, "hash": "e7dff21d5f3eba40d64d4f05d228aaa5e3561f4d1549e6f5b0bbe2f495ccb0bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d61a164e-865f-45fc-9582-bc2ca9417c31", "node_type": "1", "metadata": {"window": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. ", "original_text": "The novel structure of the search trees built in the process of training is discussed. "}, "hash": "3ea64838ef6f509d18318db9d66a4575f13d13a219006f2b7aa16d77197e4c1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. ", "mimetype": "text/plain", "start_char_idx": 7792, "end_char_idx": 7955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d61a164e-865f-45fc-9582-bc2ca9417c31", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. ", "original_text": "The novel structure of the search trees built in the process of training is discussed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd1aa580-91c2-4eed-b986-ae4066b57b20", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The main goal of this study is to depart from the classic binary search tree structure and augment the trees by engaging the concept of clusters.  Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. ", "original_text": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown. "}, "hash": "ca67554d91f1fd2091c517c15ff65172f34f535d1ee7209013b991290b9e7cd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5994f6e3-f7d7-4f99-a69c-08f948fbc573", "node_type": "1", "metadata": {"window": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig. ", "original_text": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. "}, "hash": "43d4d9a5fc3feef0cebbe73e3672a8b7433f39ef967443f79089d16b3383d0ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The novel structure of the search trees built in the process of training is discussed. ", "mimetype": "text/plain", "start_char_idx": 7955, "end_char_idx": 8042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5994f6e3-f7d7-4f99-a69c-08f948fbc573", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig. ", "original_text": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d61a164e-865f-45fc-9582-bc2ca9417c31", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Namely, the number of leafs of a tree (to be more precise, a search tree) depends on the optimal number of clusters present in the sub-partition of a dataset at the stage of the training.  Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. ", "original_text": "The novel structure of the search trees built in the process of training is discussed. "}, "hash": "352e8f7c13e195886341a80ac58ed31ab1d7edd87258bb14d85164b177832e5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5880f25a-57a4-4963-bb38-0b3bf94edaf4", "node_type": "1", "metadata": {"window": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4. ", "original_text": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n"}, "hash": "5116b51c6fe9c915be1a6ceef0dad24ca60fc0b30bfe29a33f98f61dc1c52aab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. ", "mimetype": "text/plain", "start_char_idx": 8042, "end_char_idx": 8165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5880f25a-57a4-4963-bb38-0b3bf94edaf4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4. ", "original_text": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5994f6e3-f7d7-4f99-a69c-08f948fbc573", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, the concept of anomaly score is quantified using the membership value to the cluster.  This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig. ", "original_text": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced. "}, "hash": "6a5da3207b64bbf24ea8a9041d37fabda21d6631b39803b9935f46c5d0088063", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7774beb2-bf2c-4e05-ad21-31e35e875feb", "node_type": "1", "metadata": {"window": "Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest.", "original_text": "An important advantage of the presented approach comes with its intuitive facet. "}, "hash": "f8eaddba0503072e1dce1afe89e376c10194b12fa8c9b19f1c912da272a70af2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n", "mimetype": "text/plain", "start_char_idx": 8165, "end_char_idx": 8392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7774beb2-bf2c-4e05-ad21-31e35e875feb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest.", "original_text": "An important advantage of the presented approach comes with its intuitive facet. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5880f25a-57a4-4963-bb38-0b3bf94edaf4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This augmentation of the existing method offers interesting and interpretable outcomes that offer a new deeper insight into the data and detected outliers.  Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4. ", "original_text": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n"}, "hash": "51e665e74e0d3d1ae1ff5f13b71664c92710e789b5b4cf8418ed5a0a529792a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72a4f7e-8f5e-437e-af89-07870920c2fc", "node_type": "1", "metadata": {"window": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points. ", "original_text": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. "}, "hash": "1007e433e6a52a592bab81524fd78303dbe6cc0a9110ef9717d85c932fe5bd4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An important advantage of the presented approach comes with its intuitive facet. ", "mimetype": "text/plain", "start_char_idx": 8392, "end_char_idx": 8473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b72a4f7e-8f5e-437e-af89-07870920c2fc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points. ", "original_text": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7774beb2-bf2c-4e05-ad21-31e35e875feb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it seems to be faster for large datasets containing mixed data.  In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest.", "original_text": "An important advantage of the presented approach comes with its intuitive facet. "}, "hash": "87e285fe1db6a29ae12941da7a2be6ef30c11707d6ca9a125e21f07f9731e868", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67724e5e-fb94-4524-b51f-04ee70f26513", "node_type": "1", "metadata": {"window": "The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n", "original_text": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. "}, "hash": "a1a61e95a937dfe844aba4a67fdc9b1994472f17e350b074de66d285cb7322b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. ", "mimetype": "text/plain", "start_char_idx": 8473, "end_char_idx": 8641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67724e5e-fb94-4524-b51f-04ee70f26513", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n", "original_text": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b72a4f7e-8f5e-437e-af89-07870920c2fc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In this study, a k-Means-based modification of the well-known isolation forest (IF) is presented and the main advantages and shortcomings of the method are shown.  The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points. ", "original_text": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies. "}, "hash": "df2cb40432ce380c240dbbbf1cf348a5b9b1dd29a66840eaf9052a17c3bfe4fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfacfc6a-5e09-4b2a-bf64-9a62b79276f0", "node_type": "1", "metadata": {"window": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. ", "original_text": "It\n\n***\n**Fig. "}, "hash": "cccd1e0b641530f22a34b145eb7619433e4dc2db508986a91de4d5924ca71675", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. ", "mimetype": "text/plain", "start_char_idx": 8641, "end_char_idx": 8890, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfacfc6a-5e09-4b2a-bf64-9a62b79276f0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. ", "original_text": "It\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67724e5e-fb94-4524-b51f-04ee70f26513", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The novel structure of the search trees built in the process of training is discussed.  Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n", "original_text": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest. "}, "hash": "fa13cee6c4de4b6b259b1abd42a356d301bc877be83ed51b57d04169096ed19f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ec2ad84f-6210-4cdf-a5fd-0b52eaaeff69", "node_type": "1", "metadata": {"window": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. ", "original_text": "4. "}, "hash": "5d62aa36386579cc27a78410991d9b27e4461e28a1b0128d382283f85b13981a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 8890, "end_char_idx": 8905, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ec2ad84f-6210-4cdf-a5fd-0b52eaaeff69", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. ", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfacfc6a-5e09-4b2a-bf64-9a62b79276f0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, a certain anomaly score based on the distance of a record to the clusters representing the tree leafs is introduced.  Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. ", "original_text": "It\n\n***\n**Fig. "}, "hash": "190cc45abf96e8cdd9b11a0bca86fea5ed187e79392f24d0495448a49633ed09", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f011ad87-c929-4905-82ee-5daaf612291f", "node_type": "1", "metadata": {"window": "An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest. ", "original_text": "Points isolated by k-Means-based IF but not isolated when using isolation forest."}, "hash": "ecbd583c11ffe24156aaf83254b70aa69a1671362f0768739517c2c22087ae63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 8905, "end_char_idx": 8908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f011ad87-c929-4905-82ee-5daaf612291f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest. ", "original_text": "Points isolated by k-Means-based IF but not isolated when using isolation forest."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ec2ad84f-6210-4cdf-a5fd-0b52eaaeff69", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, in a series of experiments we present in detail how the proposal works in case of various types of data, namely geographical, spatio-temporal, and mixed datasets including categorical, spatial as well as time series.\n\n An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. ", "original_text": "4. "}, "hash": "b803f28f76ffa1ea7a33bde3a96943643b1ebe2c0831158fdf899140efef5169", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5823a4a-0500-4df9-9c56-285f91948d4a", "node_type": "1", "metadata": {"window": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. ", "original_text": "** This is a 2D scatter plot showing a sparse distribution of points. "}, "hash": "11f9c48e45b49eb34bdee478444256420f69321defacc4210f350fb587f53a4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Points isolated by k-Means-based IF but not isolated when using isolation forest.", "mimetype": "text/plain", "start_char_idx": 8908, "end_char_idx": 8989, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5823a4a-0500-4df9-9c56-285f91948d4a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. ", "original_text": "** This is a 2D scatter plot showing a sparse distribution of points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f011ad87-c929-4905-82ee-5daaf612291f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An important advantage of the presented approach comes with its intuitive facet.  An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest. ", "original_text": "Points isolated by k-Means-based IF but not isolated when using isolation forest."}, "hash": "4fbb29c74014245f8d7b1dbc03cd23d8b4a385e4f73dfedcc69a1676d469649b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50ce5a64-ab89-4877-88a5-06dddcec8e91", "node_type": "1", "metadata": {"window": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n", "original_text": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n"}, "hash": "6eef71ff350ea40a76fd642b1a90917da3672ae07489ebd9ecb396bd32c32489", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This is a 2D scatter plot showing a sparse distribution of points. ", "mimetype": "text/plain", "start_char_idx": 8989, "end_char_idx": 9059, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50ce5a64-ab89-4877-88a5-06dddcec8e91", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n", "original_text": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5823a4a-0500-4df9-9c56-285f91948d4a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An intuitive anomaly score calculation process with no need of normalization shows clearly which records in a dataset are deemed suspected to be outliers or anomalies.  Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. ", "original_text": "** This is a 2D scatter plot showing a sparse distribution of points. "}, "hash": "7ebfa8bd5ebb92de768db52f27ed21e8d2c28157a127f5255988e8701a7d3a5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b82ed6cc-16b1-41fd-bc6c-b74b4012265b", "node_type": "1", "metadata": {"window": "It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows. ", "original_text": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. "}, "hash": "8c2ea08d79972a6c99398fffda304ffd45737d36136e5fd5cfe3c1757291e1c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n", "mimetype": "text/plain", "start_char_idx": 9059, "end_char_idx": 9193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b82ed6cc-16b1-41fd-bc6c-b74b4012265b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows. ", "original_text": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50ce5a64-ab89-4877-88a5-06dddcec8e91", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, the series of experiments conducted here, shows that the intermodal transport data and taxi transport details containing erroneous values can be better analyzed using our approach than the previous one, i.e., the classic isolation forest.  It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n", "original_text": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n"}, "hash": "49ca30492f8d8364d57c6cbca06774ec596090dc163022ac8ffa224148f9be55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5768f8a7-7004-4df1-8c89-f4859b333e19", "node_type": "1", "metadata": {"window": "4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed. ", "original_text": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. "}, "hash": "005e273fce890cd070c7b61bc7a9ed2051391ec861f5c2838b50c31cf30e547f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. ", "mimetype": "text/plain", "start_char_idx": 9193, "end_char_idx": 9363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5768f8a7-7004-4df1-8c89-f4859b333e19", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed. ", "original_text": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b82ed6cc-16b1-41fd-bc6c-b74b4012265b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It\n\n***\n**Fig.  4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows. ", "original_text": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data. "}, "hash": "6344759ea785d03bcbe4d5baa07077d927c458be044070bb2f383f0929d58638", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2bbf1bf-8652-4481-9103-e6dce1eaf241", "node_type": "1", "metadata": {"window": "Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled. ", "original_text": "Therefore, it represents a relatively wide spectrum of interest. "}, "hash": "a30438587b2494b79f1b5146431e3b4a3e706f76e731736c7d8d4b6eb91315fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. ", "mimetype": "text/plain", "start_char_idx": 9363, "end_char_idx": 9467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f2bbf1bf-8652-4481-9103-e6dce1eaf241", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled. ", "original_text": "Therefore, it represents a relatively wide spectrum of interest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5768f8a7-7004-4df1-8c89-f4859b333e19", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "4.  Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed. ", "original_text": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed. "}, "hash": "692eef7bd564de86922d38e396f3e97885a2145de6ebc8f1928e78437e38eacb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdb31500-b24a-4aeb-aa2e-0950a565d9ad", "node_type": "1", "metadata": {"window": "** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented. ", "original_text": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. "}, "hash": "d789349d30aa129b5e23d3ecbca351996bdd362ba8ec9e74e4532e6e7d81638c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, it represents a relatively wide spectrum of interest. ", "mimetype": "text/plain", "start_char_idx": 9467, "end_char_idx": 9532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdb31500-b24a-4aeb-aa2e-0950a565d9ad", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented. ", "original_text": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2bbf1bf-8652-4481-9103-e6dce1eaf241", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Points isolated by k-Means-based IF but not isolated when using isolation forest. ** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled. ", "original_text": "Therefore, it represents a relatively wide spectrum of interest. "}, "hash": "1a67a58d2d72b1225c0930a0f3638ddde8f506c3175d72e94c266ff3bab8ec56", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f36d4c-974e-4839-84a5-df30c80608fa", "node_type": "1", "metadata": {"window": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. ", "original_text": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n"}, "hash": "fe6e1140e699c6acf71335737e7308bc1a94a9d5b83a923c0f0d2b4485413b6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. ", "mimetype": "text/plain", "start_char_idx": 9532, "end_char_idx": 9670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69f36d4c-974e-4839-84a5-df30c80608fa", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. ", "original_text": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdb31500-b24a-4aeb-aa2e-0950a565d9ad", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a 2D scatter plot showing a sparse distribution of points.  A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented. ", "original_text": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest. "}, "hash": "98c896274200b0649145f1beb3507e445f25a06eb35f6b42406b06f2f9f7a5bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a99db88-7d9f-4ea6-b8d3-fd9d99c0538d", "node_type": "1", "metadata": {"window": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n", "original_text": "The structure of the paper is as follows. "}, "hash": "c9d28aced6816070efebd8ae9c078f9981e90fd36d27f1632ff98fe5b28612cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n", "mimetype": "text/plain", "start_char_idx": 9670, "end_char_idx": 9810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a99db88-7d9f-4ea6-b8d3-fd9d99c0538d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n", "original_text": "The structure of the paper is as follows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69f36d4c-974e-4839-84a5-df30c80608fa", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A few points, highlighted in green, are identified as anomalies by the k-Means-based method but not by the standard isolation forest.\n ***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. ", "original_text": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n"}, "hash": "f25cd687c2409c8bb35e8fe68bd3ae15835cde93d6f8e8c25cb9a0142f2e9e25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61764db7-040d-40a8-981f-d478cea4ecb1", "node_type": "1", "metadata": {"window": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2. ", "original_text": "In Section 2, the literature of the topic is overviewed. "}, "hash": "092cbf20e50a76366a6904714cd7dd35a882e0000dba9ef58479c9a7bebcd365", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The structure of the paper is as follows. ", "mimetype": "text/plain", "start_char_idx": 9810, "end_char_idx": 9852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61764db7-040d-40a8-981f-d478cea4ecb1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2. ", "original_text": "In Section 2, the literature of the topic is overviewed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a99db88-7d9f-4ea6-b8d3-fd9d99c0538d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nis worth to stress that despite the fact that the anomaly detection constitutes an intensive area of applications, we limit ourselves to the case of transport data.  The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n", "original_text": "The structure of the paper is as follows. "}, "hash": "47b61129736b43f15e7a6fe2b5cd3942e4f63693c9a94f3dde9de12c64f8b121", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90b73bc5-5ed9-47e2-abd5-ee7270405548", "node_type": "1", "metadata": {"window": "Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. ", "original_text": "In Section 3, a concept of isolation forest approach is recalled. "}, "hash": "f1f63b6909968d034ce626044c29938eac09d8578db2a2cc9508dc0ca60db867", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 2, the literature of the topic is overviewed. ", "mimetype": "text/plain", "start_char_idx": 9852, "end_char_idx": 9909, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90b73bc5-5ed9-47e2-abd5-ee7270405548", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. ", "original_text": "In Section 3, a concept of isolation forest approach is recalled. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61764db7-040d-40a8-981f-d478cea4ecb1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The motivation is that such kind of data can be either categorical, spatial, spatio-temporal, or mixed.  Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2. ", "original_text": "In Section 2, the literature of the topic is overviewed. "}, "hash": "e22b2a60d966291573b20e8f6b87d2894a6faa6b3ca62c3b212b8d49eed0c739", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ba23e9d-2665-47ad-8cb3-11b13b615a09", "node_type": "1", "metadata": {"window": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. ", "original_text": "Moreover, a proposal of its enhancement is presented. "}, "hash": "051564b48d343a8df623fa245a56a5ea31cfd0851511084a2da32408dac8b8bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 3, a concept of isolation forest approach is recalled. ", "mimetype": "text/plain", "start_char_idx": 9909, "end_char_idx": 9975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ba23e9d-2665-47ad-8cb3-11b13b615a09", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. ", "original_text": "Moreover, a proposal of its enhancement is presented. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90b73bc5-5ed9-47e2-abd5-ee7270405548", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore, it represents a relatively wide spectrum of interest.  Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. ", "original_text": "In Section 3, a concept of isolation forest approach is recalled. "}, "hash": "faf1f1500827d176e6516e78705fd3e05be95d1b57b9a0ab764a5af85fe557d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c37441ab-35e3-4de6-86a0-08641ea5dde8", "node_type": "1", "metadata": {"window": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. ", "original_text": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. "}, "hash": "130870b3b8491adb265f0389a8b963fa58eef04919613f72d7e643795ce29e27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, a proposal of its enhancement is presented. ", "mimetype": "text/plain", "start_char_idx": 9975, "end_char_idx": 10029, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c37441ab-35e3-4de6-86a0-08641ea5dde8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. ", "original_text": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ba23e9d-2665-47ad-8cb3-11b13b615a09", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, it is important to note that the results of numerical experiments prove the intuitiveness of the k-Means-based isolation forest.  It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. ", "original_text": "Moreover, a proposal of its enhancement is presented. "}, "hash": "a38cda53506cb812dfa383f02965b3fb383914b2e1469e10f7a967b47488890f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "863d7ecd-fe0d-46bd-bf54-a982d1f1507d", "node_type": "1", "metadata": {"window": "The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. ", "original_text": "Finally, conclusions and future work directions are presented in Section 5.\n\n"}, "hash": "e310ed0c63cae475f6791615f941305c59010022ca13f7b5c39eb337145de14c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. ", "mimetype": "text/plain", "start_char_idx": 10029, "end_char_idx": 10165, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "863d7ecd-fe0d-46bd-bf54-a982d1f1507d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. ", "original_text": "Finally, conclusions and future work directions are presented in Section 5.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c37441ab-35e3-4de6-86a0-08641ea5dde8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It marks the points as the isolated if they are placed, for instance, outside large groups of other points or if they are boundary values.\n\n The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. ", "original_text": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed. "}, "hash": "bacb1c475c97479f6c731e0fd761835dba9d522d3e692759660d4594ce9debd9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4110064d-a7c4-4f6a-9826-c81f609ec2b2", "node_type": "1", "metadata": {"window": "In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. ", "original_text": "### 2. "}, "hash": "ea8dc3c6de637883daa6329c81b329514def1c1137ea042c41c082c99996be80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, conclusions and future work directions are presented in Section 5.\n\n", "mimetype": "text/plain", "start_char_idx": 10165, "end_char_idx": 10242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4110064d-a7c4-4f6a-9826-c81f609ec2b2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. ", "original_text": "### 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "863d7ecd-fe0d-46bd-bf54-a982d1f1507d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The structure of the paper is as follows.  In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. ", "original_text": "Finally, conclusions and future work directions are presented in Section 5.\n\n"}, "hash": "3994a0dd88b2c362a5f9453172f7711b628ee2eb741100275f1c03d67d7bc190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d84923a3-05b9-4d20-915c-f6b0ff1bd877", "node_type": "1", "metadata": {"window": "In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters. ", "original_text": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. "}, "hash": "1647cb2acd2b1127b4a149cc6276bbaa51b0f79683233a81c28d88953a842bab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2. ", "mimetype": "text/plain", "start_char_idx": 10242, "end_char_idx": 10249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d84923a3-05b9-4d20-915c-f6b0ff1bd877", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters. ", "original_text": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4110064d-a7c4-4f6a-9826-c81f609ec2b2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In Section 2, the literature of the topic is overviewed.  In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. ", "original_text": "### 2. "}, "hash": "3a2ba462dee701dafb9ef23ecad116ca91533eaebec4bee31493d44233ab3cc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d784ce5-5cd5-44c3-9dd9-011e05cddbc2", "node_type": "1", "metadata": {"window": "Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data. ", "original_text": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. "}, "hash": "2d5c22b7fd2a74edfaa415429e348a21b3a43f4a4cb2fdf8250ca71fe9da2cac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. ", "mimetype": "text/plain", "start_char_idx": 10249, "end_char_idx": 10361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d784ce5-5cd5-44c3-9dd9-011e05cddbc2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data. ", "original_text": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d84923a3-05b9-4d20-915c-f6b0ff1bd877", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In Section 3, a concept of isolation forest approach is recalled.  Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters. ", "original_text": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique. "}, "hash": "6469dd2437240e6c72fddaf36d100cc0003a66af08a99ce9817ec433659a7cb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9485b20d-2578-4ceb-9ece-546837b1b25e", "node_type": "1", "metadata": {"window": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method. ", "original_text": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. "}, "hash": "7124c6ae42be5591b8268fb349efed939ceda01f8d85fce02441670b623bfd16", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. ", "mimetype": "text/plain", "start_char_idx": 10361, "end_char_idx": 10516, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9485b20d-2578-4ceb-9ece-546837b1b25e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method. ", "original_text": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d784ce5-5cd5-44c3-9dd9-011e05cddbc2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, a proposal of its enhancement is presented.  In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data. ", "original_text": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset. "}, "hash": "e1936c70728cd83676a4f49ee32957a1784825545b94bf32567cad04a89d6392", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db9703f6-ad4d-4a5a-aead-25e9828fb8e6", "node_type": "1", "metadata": {"window": "Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. ", "original_text": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. "}, "hash": "72163688a11a051fccc979e1dd759efffca40474f0cf779de31f7a7d6d29cf26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. ", "mimetype": "text/plain", "start_char_idx": 10516, "end_char_idx": 10786, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db9703f6-ad4d-4a5a-aead-25e9828fb8e6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. ", "original_text": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9485b20d-2578-4ceb-9ece-546837b1b25e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In Section 4, the results of numerical experiments with various publically available datasets as well as our private data are detailed.  Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method. ", "original_text": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315]. "}, "hash": "873ac4e574c54965cb3cf687a9c14cf6e38631db18ae3fe7f8396ffa1b560e25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4fa31b5-5ecf-4fc6-a4d8-cf1b074fbc30", "node_type": "1", "metadata": {"window": "### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. ", "original_text": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. "}, "hash": "a1073ed71b7a0e4314ea9206e9cf64356aa4b27fa13d25e36522fcee89ac70d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. ", "mimetype": "text/plain", "start_char_idx": 10786, "end_char_idx": 10955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4fa31b5-5ecf-4fc6-a4d8-cf1b074fbc30", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. ", "original_text": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db9703f6-ad4d-4a5a-aead-25e9828fb8e6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, conclusions and future work directions are presented in Section 5.\n\n ### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. ", "original_text": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324]. "}, "hash": "be743f54eb88845c1be533e494ad0cc9155cf1548d35794dde728b9ee5bbd344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "131bac07-9d2f-451f-bc22-8f111a9cf0f7", "node_type": "1", "metadata": {"window": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. ", "original_text": "The dataset is divided into two clusters. "}, "hash": "3eefd8efca6740ee760a8cf4ed5d1b5c723ed006fa7f374cfacd2b86eb46d3e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. ", "mimetype": "text/plain", "start_char_idx": 10955, "end_char_idx": 11090, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "131bac07-9d2f-451f-bc22-8f111a9cf0f7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. ", "original_text": "The dataset is divided into two clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4fa31b5-5ecf-4fc6-a4d8-cf1b074fbc30", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 2.  Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. ", "original_text": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records. "}, "hash": "b5f0b2c5cfc366892d2c4047d2a046051d8f4c0d654eddf5cec1012c97740a4d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7c3b34d-76fb-4cf2-ad5f-0de7f7a74b95", "node_type": "1", "metadata": {"window": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. ", "original_text": "The smallest one is considered as abnormal data. "}, "hash": "e286cbd878abf8b59d8d4b208bc66a34dea3d30b22a5f38d4f3e9d0dbb4b2870", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dataset is divided into two clusters. ", "mimetype": "text/plain", "start_char_idx": 11090, "end_char_idx": 11132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7c3b34d-76fb-4cf2-ad5f-0de7f7a74b95", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. ", "original_text": "The smallest one is considered as abnormal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "131bac07-9d2f-451f-bc22-8f111a9cf0f7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Overview of previous research\n\nOne of the classic approaches to anomaly detection is a density-based technique.  It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. ", "original_text": "The dataset is divided into two clusters. "}, "hash": "f002168bbdff4fee4718b0bf948bbacee99d017c70ee937dbdfcd122c923fea2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ce1c634-b5ae-4e46-a086-3a0a13203487", "node_type": "1", "metadata": {"window": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. ", "original_text": "Next, the anomaly scores are calculated using the IF method. "}, "hash": "cc3200eaac34d058ec87391f232a7b91b07e800839d8db8ba18f87e6b639db9d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The smallest one is considered as abnormal data. ", "mimetype": "text/plain", "start_char_idx": 11132, "end_char_idx": 11181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ce1c634-b5ae-4e46-a086-3a0a13203487", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. ", "original_text": "Next, the anomaly scores are calculated using the IF method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7c3b34d-76fb-4cf2-ad5f-0de7f7a74b95", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It includes k-nearest neighbor methods [5\u20138] and isolation forest [3,4] which is based on a forest of binary search trees trained on samples of a dataset.  Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. ", "original_text": "The smallest one is considered as abnormal data. "}, "hash": "b8bdb53e5a4cfe8645e740b8a587d7e2bf451a91ab6cca467eb0f4d62f541884", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76de8b1d-f0c1-4782-a2b5-0b4b0f333884", "node_type": "1", "metadata": {"window": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods. ", "original_text": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. "}, "hash": "eb0321462d17175c0ed9e488ee836f8fdcc84802c89615c913cdb90519d6ccfb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, the anomaly scores are calculated using the IF method. ", "mimetype": "text/plain", "start_char_idx": 11181, "end_char_idx": 11242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76de8b1d-f0c1-4782-a2b5-0b4b0f333884", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods. ", "original_text": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ce1c634-b5ae-4e46-a086-3a0a13203487", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another ways of outlier detection are the methods realizing support vector machines [9], core vector machine [10], kernel methods [11], or neural networks, in particular deep learning models such as autoencoders, long-short term memory, or self-organizing maps [12\u201315].  Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. ", "original_text": "Next, the anomaly scores are calculated using the IF method. "}, "hash": "020318f201b96a61b0541726f935282776da2002438c6af43c28161224f4c4f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6f92d94-2ae8-4855-a5fc-e98a7327d562", "node_type": "1", "metadata": {"window": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series. ", "original_text": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. "}, "hash": "febf499c4f70adecc12e1e3ea7a82ea0627882240e3245e3b89a5a5ff40c3e21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. ", "mimetype": "text/plain", "start_char_idx": 11242, "end_char_idx": 11338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6f92d94-2ae8-4855-a5fc-e98a7327d562", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series. ", "original_text": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76de8b1d-f0c1-4782-a2b5-0b4b0f333884", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Also, an interesting approach is based on cluster analysis [16,17], in particular, DBSCAN algorithm [18,19] or DBSCAN and k-Means [20], or fuzzy set techniques [21\u201324].  k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods. ", "original_text": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328]. "}, "hash": "affca1eaf00fc35dd02ebf1c467754e2f7e923fd6f3cca5256b9edb891ecda17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afaae3bb-2ed1-401c-94dd-96c6f937dbf4", "node_type": "1", "metadata": {"window": "The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31]. ", "original_text": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. "}, "hash": "f992fcfca5e5aefbe14f2ec19af76935a35877f4577188fe4581d5b8662bac72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. ", "mimetype": "text/plain", "start_char_idx": 11338, "end_char_idx": 11455, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "afaae3bb-2ed1-401c-94dd-96c6f937dbf4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31]. ", "original_text": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6f92d94-2ae8-4855-a5fc-e98a7327d562", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "k-Means was also used in a combination with IF, a so-called CBiForest [25], where authors apply it to preselection of dataset records.  The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series. ", "original_text": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29]. "}, "hash": "3cdcd131f341fb619f9057473be2340f5fa7a1bddb9566ed81d1af728d770291", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69e9d2be-1693-4d05-bf75-ce2ac4eb1ded", "node_type": "1", "metadata": {"window": "The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. ", "original_text": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. "}, "hash": "15d7665fc13ee2410cb82086804d2d66affcb67a9ef82c4ea826a43da4824e5f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. ", "mimetype": "text/plain", "start_char_idx": 11455, "end_char_idx": 11547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69e9d2be-1693-4d05-bf75-ce2ac4eb1ded", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. ", "original_text": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afaae3bb-2ed1-401c-94dd-96c6f937dbf4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The dataset is divided into two clusters.  The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31]. ", "original_text": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30]. "}, "hash": "4f9004c0c27c7c43a6a9abb2e3ef874ab6f6b13dfaad70bd26e4e70f08b01b5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f1ed597-0c9d-4c46-a65c-edce40accc6b", "node_type": "1", "metadata": {"window": "Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. ", "original_text": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. "}, "hash": "f7da4d4a5e4cf015d0f4e955905ae9ebcc1ba326409c66f7b5ac2659814bbe57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. ", "mimetype": "text/plain", "start_char_idx": 11547, "end_char_idx": 11677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f1ed597-0c9d-4c46-a65c-edce40accc6b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. ", "original_text": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69e9d2be-1693-4d05-bf75-ce2ac4eb1ded", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The smallest one is considered as abnormal data.  Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. ", "original_text": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data. "}, "hash": "2782d0dc3ec305829f3cbadbc1fc73822f19789c3f09cf5312e28a61d5cdbb70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb0053eb-5e67-4cd1-8c14-b42a2785cbbc", "node_type": "1", "metadata": {"window": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. ", "original_text": "To work with such data, the researchers came with various methods. "}, "hash": "74683ca4057707057e19efd373a8f73aff506eaf76c1a94bd334e92be6be1d02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. ", "mimetype": "text/plain", "start_char_idx": 11677, "end_char_idx": 11801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb0053eb-5e67-4cd1-8c14-b42a2785cbbc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. ", "original_text": "To work with such data, the researchers came with various methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f1ed597-0c9d-4c46-a65c-edce40accc6b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, the anomaly scores are calculated using the IF method.  Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. ", "original_text": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases. "}, "hash": "2b238472e0d9e8299c5ab11a955173182627d970fbc0779b03ac98f39c7ad7fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f969880-2867-47d0-88bc-dabb72fa5698", "node_type": "1", "metadata": {"window": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. ", "original_text": "First of them is a set of techniques based on similarity of time series. "}, "hash": "97cc01e2378cbf81de009ec668e843d1b9ee1beccb6e0703bb537e35efc7cbd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To work with such data, the researchers came with various methods. ", "mimetype": "text/plain", "start_char_idx": 11801, "end_char_idx": 11868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f969880-2867-47d0-88bc-dabb72fa5698", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. ", "original_text": "First of them is a set of techniques based on similarity of time series. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb0053eb-5e67-4cd1-8c14-b42a2785cbbc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another approach falling within this category was an application of Fuzzy c-Means, see [26\u201328].  A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. ", "original_text": "To work with such data, the researchers came with various methods. "}, "hash": "84795591c0bc889fde0384973efff8340da8123a6e82ba9622fc8455857e89de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ddfe2ca-3003-4ac5-83a0-2eab19a7d13e", "node_type": "1", "metadata": {"window": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling. ", "original_text": "It may be built on a basis of cross correlation between time series present in a dataset [31]. "}, "hash": "af5f3d1b92aa7caa4edfefb0e9807602c93c3c368522431f3da5e6385c4de2af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First of them is a set of techniques based on similarity of time series. ", "mimetype": "text/plain", "start_char_idx": 11868, "end_char_idx": 11941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ddfe2ca-3003-4ac5-83a0-2eab19a7d13e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling. ", "original_text": "It may be built on a basis of cross correlation between time series present in a dataset [31]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f969880-2867-47d0-88bc-dabb72fa5698", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A very practical method for mining high utility patterns in the case of incremental databases was presented in [29].  Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. ", "original_text": "First of them is a set of techniques based on similarity of time series. "}, "hash": "e3ed32ee7eb02b13eaf30b28be0d947f6dadb787fd89ff2f57e4c7615e0a346c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ca7f195-c4bc-49ea-86a0-b2bd453f6dca", "node_type": "1", "metadata": {"window": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. ", "original_text": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. "}, "hash": "b718e782e6ad542166dc6fa2274a77ad08a038cdcb4b44e09028eba96ec4477c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It may be built on a basis of cross correlation between time series present in a dataset [31]. ", "mimetype": "text/plain", "start_char_idx": 11941, "end_char_idx": 12036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ca7f195-c4bc-49ea-86a0-b2bd453f6dca", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. ", "original_text": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ddfe2ca-3003-4ac5-83a0-2eab19a7d13e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, evolutionary Random Weight Networks were applied to detect and identify spam [30].  A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling. ", "original_text": "It may be built on a basis of cross correlation between time series present in a dataset [31]. "}, "hash": "0af2369ef84ab5f45fbb8d9fcc010cad5806bc0b9aab510390cb91591553c4a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "330a88f4-da4a-4f4c-8e25-fb9c3bb954dc", "node_type": "1", "metadata": {"window": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. ", "original_text": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. "}, "hash": "32ac2961ae565d6abc7337c2feeb07872e3917d083e6782007ce8f4137390de7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. ", "mimetype": "text/plain", "start_char_idx": 12036, "end_char_idx": 12278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "330a88f4-da4a-4f4c-8e25-fb9c3bb954dc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. ", "original_text": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ca7f195-c4bc-49ea-86a0-b2bd453f6dca", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A particular and important kind of datasets are the databases containing spatial, temporal (time-series) or spatio-temporal data.  These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. ", "original_text": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced. "}, "hash": "0a03a72ac82bf16eb82f0b61bfb8b303cafabb616d66092f94f1f88894febbb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5a24a5f-14ef-40da-a6b0-d81dd4c9c564", "node_type": "1", "metadata": {"window": "To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. ", "original_text": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. "}, "hash": "4aa6e9bb15b02d365ab11dd6d7de5060ed991b7a7de77b5d8b5ec998c89374b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. ", "mimetype": "text/plain", "start_char_idx": 12278, "end_char_idx": 12372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5a24a5f-14ef-40da-a6b0-d81dd4c9c564", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. ", "original_text": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "330a88f4-da4a-4f4c-8e25-fb9c3bb954dc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These large datasets contain information coming from various, often weakly related systems, e.g., transportation databases.  To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. ", "original_text": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34]. "}, "hash": "956de79a4db249d8b756307c8f00ec35e8bc1b4e6eea68d881b17ba785b0b1c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76e8bee4-843a-40c1-bffb-7e0dde7e813d", "node_type": "1", "metadata": {"window": "First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. ", "original_text": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. "}, "hash": "57554500330b4af41594d24587c03087c0efbfeac85df40e0519d5e879146854", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. ", "mimetype": "text/plain", "start_char_idx": 12372, "end_char_idx": 12534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76e8bee4-843a-40c1-bffb-7e0dde7e813d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. ", "original_text": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5a24a5f-14ef-40da-a6b0-d81dd4c9c564", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "To work with such data, the researchers came with various methods.  First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. ", "original_text": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies. "}, "hash": "4e41f90ec4c920d2962af3f9baa0e0019989c244d2b9d17e529f43f488356c88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a1335b1-97f0-4eb3-8411-8c5503ea6c2f", "node_type": "1", "metadata": {"window": "It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. ", "original_text": "Another approach is based on time series modeling. "}, "hash": "70465fbfeb005e90dd99288dda3882813ab53ce8dced6dfb92272281b56c1c67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. ", "mimetype": "text/plain", "start_char_idx": 12534, "end_char_idx": 12689, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a1335b1-97f0-4eb3-8411-8c5503ea6c2f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. ", "original_text": "Another approach is based on time series modeling. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76e8bee4-843a-40c1-bffb-7e0dde7e813d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "First of them is a set of techniques based on similarity of time series.  It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. ", "original_text": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340]. "}, "hash": "856346887683ac55c9cf137b7b420d1c2d24304f88351a9331fba27ed9eb44a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d1eead5-879f-4fb2-a86a-6dbeafd5b9d5", "node_type": "1", "metadata": {"window": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n", "original_text": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. "}, "hash": "d898962ee787e42137322467f1066b48dbc6a2df6e870d5685cef1dedece5bb0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another approach is based on time series modeling. ", "mimetype": "text/plain", "start_char_idx": 12689, "end_char_idx": 12740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d1eead5-879f-4fb2-a86a-6dbeafd5b9d5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n", "original_text": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a1335b1-97f0-4eb3-8411-8c5503ea6c2f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It may be built on a basis of cross correlation between time series present in a dataset [31].  Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. ", "original_text": "Another approach is based on time series modeling. "}, "hash": "ad0d041cc580502e8fa396d1a9b65e0ddf22d3e7b9784282ed0e534047ff8bd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7379c26f-b377-41d0-a9b8-2c6abe5fbedc", "node_type": "1", "metadata": {"window": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3. ", "original_text": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. "}, "hash": "317db3b1b0c102dff81e5e6e9f35ed9b5b36a66e042e70e6fc18099e6693bf5a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. ", "mimetype": "text/plain", "start_char_idx": 12740, "end_char_idx": 12949, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7379c26f-b377-41d0-a9b8-2c6abe5fbedc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3. ", "original_text": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d1eead5-879f-4fb2-a86a-6dbeafd5b9d5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another example is the proposal [32], where a nearest neighbor-based distance between so-called discords (the longest time series) is considered, or in [33], where a special dissimilarity measure based on the size of the data was introduced.  Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n", "original_text": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc. "}, "hash": "7d8c4fcb9e09c0d3b3d3d9672b3fd34ef503ef3b78751d73234518121f5f69c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f94213f1-219a-4628-b1b1-1deaf28a709a", "node_type": "1", "metadata": {"window": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. ", "original_text": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. "}, "hash": "ec7867460767c25131a8b5fd91f3c55cf3ebf88cd9f412852a292d249f82f198", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. ", "mimetype": "text/plain", "start_char_idx": 12949, "end_char_idx": 13071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f94213f1-219a-4628-b1b1-1deaf28a709a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. ", "original_text": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7379c26f-b377-41d0-a9b8-2c6abe5fbedc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Outliers for climate changes using distance and neighborhood concepts were discussed in [34].  The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3. ", "original_text": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48]. "}, "hash": "1542cdaf43b9a0b211244f071eac81229cd94460706b302e97210ca591d993fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fdfb061-db18-4809-ad58-480d26e63b0d", "node_type": "1", "metadata": {"window": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building. ", "original_text": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. "}, "hash": "c68e5182c3494746f48c7af5d1082e255e4b9aa4a2119fd94f0bc846db1ca581", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. ", "mimetype": "text/plain", "start_char_idx": 13071, "end_char_idx": 13300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fdfb061-db18-4809-ad58-480d26e63b0d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building. ", "original_text": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f94213f1-219a-4628-b1b1-1deaf28a709a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The second approach is strictly related to the classification task, where classifiers are trained to differentiation among time series with or without anomalies.  Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. ", "original_text": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series. "}, "hash": "d6e76b558f8cc25a75c466b4d1ce3d7ea834ee4f876981235d95d5b981f18f7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60197cb1-0d44-4c84-86d6-76108e4eff5a", "node_type": "1", "metadata": {"window": "Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes. ", "original_text": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. "}, "hash": "ffdede75bb93bb4279194ccca3188efe11f55a2f09e43d0ae8bd83faa3530d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. ", "mimetype": "text/plain", "start_char_idx": 13300, "end_char_idx": 13451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60197cb1-0d44-4c84-86d6-76108e4eff5a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes. ", "original_text": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fdfb061-db18-4809-ad58-480d26e63b0d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Various authors proposed neural networks [35], selection inspired by an immune system [36], support vector machines [37], deep neural networks [13,38\u201340].  Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building. ", "original_text": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328]. "}, "hash": "06b60820cac2b3d3f5af89b88f40a885328cc309283d201c4c1c7def5d3fcec9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49b6868c-259e-42da-8920-4c01ed3661dd", "node_type": "1", "metadata": {"window": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . ", "original_text": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n"}, "hash": "82b231fe84eb5dd6e20a9708d7300be07c7e8345e8cad537149f06102031ad04", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. ", "mimetype": "text/plain", "start_char_idx": 13451, "end_char_idx": 13536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49b6868c-259e-42da-8920-4c01ed3661dd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . ", "original_text": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60197cb1-0d44-4c84-86d6-76108e4eff5a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Another approach is based on time series modeling.  The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes. ", "original_text": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50]. "}, "hash": "2dadb17c8e1ed029ea0fb390cf8c35ffc6230964cb4582268b9c645618f33989", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac13de69-cfe2-4bb7-882b-138d1d54127c", "node_type": "1", "metadata": {"window": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  . ", "original_text": "### 3. "}, "hash": "7a6a716948146a7eb32fd6e97dbd6ac96ba8e69db14951e2147fd65d376b5366", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n", "mimetype": "text/plain", "start_char_idx": 13536, "end_char_idx": 13634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac13de69-cfe2-4bb7-882b-138d1d54127c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  . ", "original_text": "### 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49b6868c-259e-42da-8920-4c01ed3661dd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The adopted models include, for instance, autoregressive model [41,42], weighted graph representation [43], hidden Markov model [44], Bayesian network [45,46], scan statistics based on expectations [47], etc.  The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . ", "original_text": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n"}, "hash": "450c8edba86c64a13f18ed54457aaae92de8874b679ef7e5dbf49cd2c74fc73d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "026d08e8-db22-40ab-813d-3b1d778bf38c", "node_type": "1", "metadata": {"window": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  . ", "original_text": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. "}, "hash": "c217b90a297cb40985d76029c342e7070d03ace7a301e8e9d0133b438d73e334", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3. ", "mimetype": "text/plain", "start_char_idx": 13634, "end_char_idx": 13641, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "026d08e8-db22-40ab-813d-3b1d778bf38c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  . ", "original_text": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac13de69-cfe2-4bb7-882b-138d1d54127c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The last set of methods has its origin in clustering, in particular, above-mentioned Fuzzy c-Means clustering, FCM, [48].  In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  . ", "original_text": "### 3. "}, "hash": "7c567610dec1d11124723b5a72bd672c9ad62088a7280810ad6c9820e810d0b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b45ac74-d177-42d8-af3e-0f6daa13c0ac", "node_type": "1", "metadata": {"window": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n", "original_text": "Let us recall the process of binary trees building. "}, "hash": "3fbdec336b0aa88a2f94964d03fb832c4ffc2982dff85faf6670d44563c90307", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. ", "mimetype": "text/plain", "start_char_idx": 13641, "end_char_idx": 13958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b45ac74-d177-42d8-af3e-0f6daa13c0ac", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n", "original_text": "Let us recall the process of binary trees building. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "026d08e8-db22-40ab-813d-3b1d778bf38c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In general, using this method, one has to assume that time series are clustered with the use of a clustering approach, next the centers of clusters are specified, and the anomaly scoring is determined for particular time series.  FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  . ", "original_text": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset. "}, "hash": "a46e30cac764040693f4fb20bcd6d2646958f796d36608b585e0e2ce2c3466a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d50d3119-413d-4e1d-bc62-03a6e0b7ba35", "node_type": "1", "metadata": {"window": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig. ", "original_text": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes. "}, "hash": "8781320ba4009f7f24161a4a0f088689d5b0771f971ab72318ac8d708ed5cd1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let us recall the process of binary trees building. ", "mimetype": "text/plain", "start_char_idx": 13958, "end_char_idx": 14010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d50d3119-413d-4e1d-bc62-03a6e0b7ba35", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig. ", "original_text": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b45ac74-d177-42d8-af3e-0f6daa13c0ac", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "FCM clustering with a reconstruction criterion to assign the anomaly scores for examined time series related with weather data was applied in [26\u201328].  K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n", "original_text": "Let us recall the process of binary trees building. "}, "hash": "fe3b7cfdb8811ba4f8a67961dcbc9b75f96336a18b665bb4c5b1f2dffc0b4140", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "254ee644-a0cd-4b96-b1b4-d055c1a0e6de", "node_type": "1", "metadata": {"window": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5. ", "original_text": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . "}, "hash": "070f98899ad121867f652b8aae7229c64d4cba3f21dc851346efc2ec4b0c5759", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes. ", "mimetype": "text/plain", "start_char_idx": 14010, "end_char_idx": 14106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "254ee644-a0cd-4b96-b1b4-d055c1a0e6de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5. ", "original_text": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d50d3119-413d-4e1d-bc62-03a6e0b7ba35", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "K-medoid clustering was proposed in [49] while fuzzy c-Medoids was proposed in [50].  An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig. ", "original_text": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes. "}, "hash": "70554a482cbfff2e1f5f5a04603f75b1ee02397405289eac5d3b1c07c6b659d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc4acb8c-fdf6-4199-b101-95bf72790702", "node_type": "1", "metadata": {"window": "### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters.", "original_text": ". "}, "hash": "0561f68efcdae8134e4f6b618d2095253ab7e3d4458b4bcb0ed5e47c563748f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . ", "mimetype": "text/plain", "start_char_idx": 14106, "end_char_idx": 14313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc4acb8c-fdf6-4199-b101-95bf72790702", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters.", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "254ee644-a0cd-4b96-b1b4-d055c1a0e6de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An interested reader can refer to the in-depth comprehensive surveys of the methods [1,2,51,52].\n\n ### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5. ", "original_text": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, . "}, "hash": "3ef7d7febbd9f9d36016c08bb0a7dae9ef75f8e2a51e0acc60394c73b569803b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbdb2fbc-ea9f-4714-bda9-7156283959b5", "node_type": "1", "metadata": {"window": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n", "original_text": ". "}, "hash": "67177e24952d86f7c40c3dd45ca9e6fb75eef39c832374ffc90c5007d2fa4d48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 14311, "end_char_idx": 14313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbdb2fbc-ea9f-4714-bda9-7156283959b5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc4acb8c-fdf6-4199-b101-95bf72790702", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 3.  Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters.", "original_text": ". "}, "hash": "2ea8dc1159fd386db5dbfe5533c4c9ce97cd5fdd6d1c73e80deb75b6e675fe1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dab0a702-ce70-49e4-b930-7375a466fb4a", "node_type": "1", "metadata": {"window": "Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n", "original_text": ", n.\n\n"}, "hash": "5e46919d49da245b76b4d89e048b9a061647b736e7dc7d43141607fb383e74b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 14313, "end_char_idx": 14315, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dab0a702-ce70-49e4-b930-7375a466fb4a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n", "original_text": ", n.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbdb2fbc-ea9f-4714-bda9-7156283959b5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Existing methodology and the proposed generalized approach\n\nThe method [3,4] is composed of two general stages, namely: (i) training based on building binary search trees completed on a basis of samples of the dataset and (ii) scoring based on searching these trees when arguments are all the records of the dataset.  Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n", "original_text": ". "}, "hash": "836dec9606f7734b2aa12081ffc35877207786246028fa20ce0cf0e60ce84b60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7425be08-6834-4458-b291-577a8093af4b", "node_type": "1", "metadata": {"window": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n", "original_text": "***\n**Fig. "}, "hash": "a7015e49f5099cae7ffc54f82d5025bdf6da9da5a2efdcff42452b06812ddce9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", n.\n\n", "mimetype": "text/plain", "start_char_idx": 14317, "end_char_idx": 14323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7425be08-6834-4458-b291-577a8093af4b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dab0a702-ce70-49e4-b930-7375a466fb4a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let us recall the process of binary trees building.  Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n", "original_text": ", n.\n\n"}, "hash": "3074a289ab5a99fbcd0266d01767feb377dc419ccd2f2ce01592a996d6f55aa8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86ee0da8-1d66-4e9b-bdc6-375f47f61310", "node_type": "1", "metadata": {"window": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n", "original_text": "5. "}, "hash": "abdef1017ef8b9c1a69b424aa66a7f25ad21730a35c01f265e07b9df08dae136", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 14323, "end_char_idx": 14334, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86ee0da8-1d66-4e9b-bdc6-375f47f61310", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n", "original_text": "5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7425be08-6834-4458-b291-577a8093af4b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let D denotes the whole dataset, containing R records, each of the records having Q attributes.  Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n", "original_text": "***\n**Fig. "}, "hash": "6e29fbf7d8013622cd52a5a4fd6bb13e951332348f4f1e903bbb754f364a9619", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7efed226-b53a-4672-a2d7-9dc47ec9de10", "node_type": "1", "metadata": {"window": ".  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n", "original_text": "Results on an artificial dataset with three distinct rectangular clusters."}, "hash": "d0b8a66fc215d51cebc5a682a6fa041294deafe45c5aadc888146f15fa213a6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5. ", "mimetype": "text/plain", "start_char_idx": 14334, "end_char_idx": 14337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7efed226-b53a-4672-a2d7-9dc47ec9de10", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n", "original_text": "Results on an artificial dataset with three distinct rectangular clusters."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86ee0da8-1d66-4e9b-bdc6-375f47f61310", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let the number of decision trees be t and the number of randomly chosen sample records used for building each tree be n, i.e., to build the tree it is used the dataset X \u2282 D containing elements xi, i = 1, .  .  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n", "original_text": "5. "}, "hash": "b3c19b239565d6d35455471e5631f4ec3548cdbfb02b23af56384794cb981810", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2285cf26-cfd5-4ac0-822c-2fd0886ce3cc", "node_type": "1", "metadata": {"window": ".  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n", "original_text": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n"}, "hash": "ffdb0d3d4a7838ae57fbe978d59e3985350c05b868b1050ce0d91705691df3a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Results on an artificial dataset with three distinct rectangular clusters.", "mimetype": "text/plain", "start_char_idx": 14337, "end_char_idx": 14411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2285cf26-cfd5-4ac0-822c-2fd0886ce3cc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n", "original_text": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7efed226-b53a-4672-a2d7-9dc47ec9de10", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  .  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n", "original_text": "Results on an artificial dataset with three distinct rectangular clusters."}, "hash": "5bc896ae548e96a8575548e7f0f642577fab8cdb5b38b0335a92dabb43bfc484", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fac87a8-821d-45d1-a01d-7b4a00ace0b6", "node_type": "1", "metadata": {"window": ", n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n", "original_text": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n"}, "hash": "a4cb311645783c1caa6f5bbf310cbf308190f437de2436279df156f4e459adcd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n", "mimetype": "text/plain", "start_char_idx": 14411, "end_char_idx": 14552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8fac87a8-821d-45d1-a01d-7b4a00ace0b6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ", n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n", "original_text": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2285cf26-cfd5-4ac0-822c-2fd0886ce3cc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  , n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n", "original_text": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n"}, "hash": "160d2dd0ef2e6da4cc5676cc8a101d247623a8085cca0d9ced8652c0b40fe890", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b6684e6-e6f2-4b55-b70e-b63e3e6d2381", "node_type": "1", "metadata": {"window": "***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n", "original_text": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n"}, "hash": "590fa7d0b97c88199ea01c4eef189df6dfb66116dd3c4ef47d22e77cdc0187ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n", "mimetype": "text/plain", "start_char_idx": 14552, "end_char_idx": 14649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b6684e6-e6f2-4b55-b70e-b63e3e6d2381", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n", "original_text": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fac87a8-821d-45d1-a01d-7b4a00ace0b6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ", n.\n\n ***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n", "original_text": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n"}, "hash": "0acc255ad022af28b97baca7894dc59cd4f27cda0dc85e08ebe87e5e21e37ff2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ac6856d-9c1d-4801-8b4c-c2a94e4f02f7", "node_type": "1", "metadata": {"window": "5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n", "original_text": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n"}, "hash": "8ab4e9d379066bfc5105671e964da4626b8c689ed1d692a0ecfa3b64b78d341c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n", "mimetype": "text/plain", "start_char_idx": 14649, "end_char_idx": 14791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ac6856d-9c1d-4801-8b4c-c2a94e4f02f7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n", "original_text": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b6684e6-e6f2-4b55-b70e-b63e3e6d2381", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n", "original_text": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n"}, "hash": "24fe717fafe88bf162ed00c91e42c6ba498f6b8493b5d67918636c609e37725f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c0c5773-c098-4633-9f17-46eff374a793", "node_type": "1", "metadata": {"window": "Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n", "original_text": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n"}, "hash": "eee255b630cbb78af56072ceabd51af1dbf78a2f5a62751c1bc2fad66a0887a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n", "mimetype": "text/plain", "start_char_idx": 14791, "end_char_idx": 14905, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c0c5773-c098-4633-9f17-46eff374a793", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n", "original_text": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ac6856d-9c1d-4801-8b4c-c2a94e4f02f7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "5.  Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n", "original_text": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n"}, "hash": "7d9412a9e8878b41413555b54f906a218b116e00bcfa2085889bad50542611a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e598786f-279d-47c7-a097-08ff7f3e66a0", "node_type": "1", "metadata": {"window": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n", "original_text": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n"}, "hash": "5258e0b91708d21d9616df51353ee912c0a81c38842715e3162f2d7914487584", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n", "mimetype": "text/plain", "start_char_idx": 14905, "end_char_idx": 15005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e598786f-279d-47c7-a097-08ff7f3e66a0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n", "original_text": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c0c5773-c098-4633-9f17-46eff374a793", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Results on an artificial dataset with three distinct rectangular clusters. ** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n", "original_text": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n"}, "hash": "5cc23335f6174c825b986fca00f2e42214f763eb0b929f23a5a5767fcff38d26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "475ee983-95f2-45e7-84a2-5bf88a4c536c", "node_type": "1", "metadata": {"window": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n"}, "hash": "aedfb5ecd2cc34ea18594ee34cad658d7390cd60a2cd549889d24d9ece230a50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n", "mimetype": "text/plain", "start_char_idx": 15005, "end_char_idx": 15141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "475ee983-95f2-45e7-84a2-5bf88a4c536c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e598786f-279d-47c7-a097-08ff7f3e66a0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure contains 10 subplots (a-j) comparing the performance of the standard Isolation Forest (IF) and the proposed k-Means-based IF.\n *   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n", "original_text": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n"}, "hash": "b8783e1f2fb76db7e4744c9a8c627aa6a31bcff8c5311e91c5e228dd2467658b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c4c4aaa-3f48-454b-8f57-d94808e6b0ca", "node_type": "1", "metadata": {"window": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows. ", "original_text": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n"}, "hash": "33cd498e2177436d4088fc5226cf0cd45b421133d9561fe9fa68634ddb0285b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n", "mimetype": "text/plain", "start_char_idx": 15141, "end_char_idx": 15253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c4c4aaa-3f48-454b-8f57-d94808e6b0ca", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows. ", "original_text": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "475ee983-95f2-45e7-84a2-5bf88a4c536c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (a) IF anomaly score heatmap: shows areas between clusters as moderately anomalous (purple).\n *   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n"}, "hash": "6c751001fbbc29bf3457699d9617df04d2f309bb8248f3ba707188390b0a10ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae4e3ea2-53bc-4eb5-82c0-e8c25fea4409", "node_type": "1", "metadata": {"window": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. ", "original_text": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n"}, "hash": "75c1e59653e45eb7c28754719ae03ef13ec5efa1ef9736f34dbb8958718b8b67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n", "mimetype": "text/plain", "start_char_idx": 15253, "end_char_idx": 15350, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae4e3ea2-53bc-4eb5-82c0-e8c25fea4409", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. ", "original_text": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c4c4aaa-3f48-454b-8f57-d94808e6b0ca", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (b) k-Means IF anomaly score heatmap: shows areas between clusters as less anomalous, focusing more on points truly far from any cluster.\n *   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows. ", "original_text": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n"}, "hash": "b6c756f7af8018f8a726e092891fa39f8c1f33e763f0dee7724dc9d251b192b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cdc7e7cc-87a8-4478-8c98-72f20980e5a8", "node_type": "1", "metadata": {"window": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. ", "original_text": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n"}, "hash": "8918ceb7f5b9f09e192b34c0abd40efe6aff2285c7999df071b0ef701b49abf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n", "mimetype": "text/plain", "start_char_idx": 15350, "end_char_idx": 15477, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cdc7e7cc-87a8-4478-8c98-72f20980e5a8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. ", "original_text": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae4e3ea2-53bc-4eb5-82c0-e8c25fea4409", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (c) Ranked scores for IF: Red (most isolated) points are scattered both between and on the edges of clusters.\n *   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. ", "original_text": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n"}, "hash": "705011abed99fc2a601964a82c1654554630ddba8f99acd324eb1cc54e337b76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5f8d8e8-d53c-4555-9d64-f22a1efa2fbf", "node_type": "1", "metadata": {"window": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element. ", "original_text": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n"}, "hash": "348f1f1afa942453be43e42c672e0f7f1d2669afe0b77bdddfaa06acf30046ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n", "mimetype": "text/plain", "start_char_idx": 15477, "end_char_idx": 15575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5f8d8e8-d53c-4555-9d64-f22a1efa2fbf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element. ", "original_text": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdc7e7cc-87a8-4478-8c98-72f20980e5a8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (d) Ranked scores for k-Means IF: Red points are more concentrated on the very sparse outliers.\n *   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. ", "original_text": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n"}, "hash": "a65c1bf017d87007ae2244d5a347266d8ab815ecdaff1ccec358c251cc197180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92f0df18-4025-4d15-a16f-17f014448027", "node_type": "1", "metadata": {"window": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "e2457554541c10600dfdd021e7eb299a74fc890f1a023eb4c255e2ad96b3471f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n", "mimetype": "text/plain", "start_char_idx": 15575, "end_char_idx": 15705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92f0df18-4025-4d15-a16f-17f014448027", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5f8d8e8-d53c-4555-9d64-f22a1efa2fbf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (e) Difference in normalized scores: shows significant differences between the methods, especially in the regions between clusters.\n *   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element. ", "original_text": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n"}, "hash": "3b004ddd4812edb86b8d047cebe4bedfba77eef112f77c617f1540a81263522d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27be635f-0786-4da7-a00d-bb9aa6026e61", "node_type": "1", "metadata": {"window": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig. ", "original_text": ")*\n***\n\nEach decision tree is constructed as follows. "}, "hash": "40b66147ae29493060255fcb29cd918cbefca329249e4bf126a1cefffcb8e3ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 15705, "end_char_idx": 15834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "27be635f-0786-4da7-a00d-bb9aa6026e61", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig. ", "original_text": ")*\n***\n\nEach decision tree is constructed as follows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92f0df18-4025-4d15-a16f-17f014448027", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (f) Difference in ranked scores: highlights the points that are ranked very differently by the two methods.\n *   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "77fd8cedbe64345e0ef74ca8ffa69c776bb22f396bdbaf01993cb21ff3281b72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a4314fa-0faf-4bf5-b4a5-d8a2b44db5c7", "node_type": "1", "metadata": {"window": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6. ", "original_text": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. "}, "hash": "32ecc0d371de4e78aeb0d461116f88d86b710555ce3adb696f276ed0979034b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nEach decision tree is constructed as follows. ", "mimetype": "text/plain", "start_char_idx": 15834, "end_char_idx": 15888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a4314fa-0faf-4bf5-b4a5-d8a2b44db5c7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6. ", "original_text": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27be635f-0786-4da7-a00d-bb9aa6026e61", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (g) Top 100 isolated points by IF: points are located on the edges and between the clusters.\n *   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig. ", "original_text": ")*\n***\n\nEach decision tree is constructed as follows. "}, "hash": "8b738eaedbde6ab7bbbe7d37266c9b8736b38007bd314dcfdbb239e1b517730a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3be09d1d-2bcc-4a79-a6d9-c0ae5a485e9f", "node_type": "1", "metadata": {"window": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements.", "original_text": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. "}, "hash": "01ebfe9bc2189c5dbc47f2396df8f3bbff29a7ec597428919d3a7425da8fd465", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. ", "mimetype": "text/plain", "start_char_idx": 15888, "end_char_idx": 16109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3be09d1d-2bcc-4a79-a6d9-c0ae5a485e9f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements.", "original_text": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a4314fa-0faf-4bf5-b4a5-d8a2b44db5c7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (h) Top 100 isolated points by k-Means IF: points are almost exclusively in the sparse regions far from the main clusters.\n *   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6. ", "original_text": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root. "}, "hash": "693ff08746e9e29f2476f290694cf36955d0d9833389ff478e277fbd6e81c9d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0052de70-5794-4080-b59a-5385e4fdf0de", "node_type": "1", "metadata": {"window": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. ", "original_text": "The process is continued until the consecutive subsets are empty or have a single element. "}, "hash": "7b0968af596e1f375ca5bc594ca80316e50ba6770d44e392cfa738d78bc69162", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. ", "mimetype": "text/plain", "start_char_idx": 16109, "end_char_idx": 16245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0052de70-5794-4080-b59a-5385e4fdf0de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. ", "original_text": "The process is continued until the consecutive subsets are empty or have a single element. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3be09d1d-2bcc-4a79-a6d9-c0ae5a485e9f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (i) Top 1000 isolated points by IF: includes points between clusters and on their perimeters.\n *   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements.", "original_text": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root. "}, "hash": "3c1e36bb74725bc5d6c6049a4b29f5f76974c5dd8687ff18048b0da515eeb3fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38151c7e-437d-4eb8-a052-82337f282564", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n", "original_text": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n"}, "hash": "b46fbb67e05d8b33da3242058fadb3a7039656a7b975744d90e7ec43341b689a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The process is continued until the consecutive subsets are empty or have a single element. ", "mimetype": "text/plain", "start_char_idx": 16245, "end_char_idx": 16336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38151c7e-437d-4eb8-a052-82337f282564", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n", "original_text": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0052de70-5794-4080-b59a-5385e4fdf0de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*   (j) Top 1000 isolated points by k-Means IF: similar to (h) but with more points, still focusing on the most distant outliers.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. ", "original_text": "The process is continued until the consecutive subsets are empty or have a single element. "}, "hash": "1860fc28199fe71dbf6b5c5bf710258819432848b1b4ca3b3144ed55ca74285e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57f80a5b-63d0-4cab-a31b-ebc6759c9aaf", "node_type": "1", "metadata": {"window": ")*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "***\n**Fig. "}, "hash": "af3e34930acd4c112dcb3db373801fe767ea5b64a0f0b06a4b3f4d014383b188", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n", "mimetype": "text/plain", "start_char_idx": 16336, "end_char_idx": 16430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "57f80a5b-63d0-4cab-a31b-ebc6759c9aaf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38151c7e-437d-4eb8-a052-82337f282564", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n", "original_text": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n"}, "hash": "5769aa6063a8a845e94896b03ab835039702b39fdbfa788308ee2d52bb6af846", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "273e3f33-dfe0-441c-907a-6f8cda9139f6", "node_type": "1", "metadata": {"window": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. ", "original_text": "6. "}, "hash": "56f38e749d13dc044339b25b038ac5705cb512959f5bec45954808d8b609ffa9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 16430, "end_char_idx": 16441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "273e3f33-dfe0-441c-907a-6f8cda9139f6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. ", "original_text": "6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57f80a5b-63d0-4cab-a31b-ebc6759c9aaf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nEach decision tree is constructed as follows.  Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "***\n**Fig. "}, "hash": "69127ce5731c86bf298ebe767061a50a56476c85224984a13e4e9d92e7990274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8da74ba3-a301-4f3a-914d-483fe24bb8fd", "node_type": "1", "metadata": {"window": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest. ", "original_text": "Plots for the dataset with 50,000 elements."}, "hash": "b748ad9fa1e511b931f1024162c4a0fb15bb281a069c9f265a9b837cabf8311b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "6. ", "mimetype": "text/plain", "start_char_idx": 16441, "end_char_idx": 16444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8da74ba3-a301-4f3a-914d-483fe24bb8fd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest. ", "original_text": "Plots for the dataset with 50,000 elements."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "273e3f33-dfe0-441c-907a-6f8cda9139f6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Using the sub-sample setX, one randomly selects attribute q and then randomly selects its threshold (cutoff) value v. It divides the set of the values of the attribute into two subsets representing two nodes of the root.  Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. ", "original_text": "6. "}, "hash": "cbe4180ea1da50160a1af87367b18e89565015a2c10af5eba0091b69fd217610", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17b1015d-2134-4083-aaf2-8d0c2ce77246", "node_type": "1", "metadata": {"window": "The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree. ", "original_text": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. "}, "hash": "0db3cdd7b090adab57e986a7a37dfffbb77bedce9152b7135da6d5976a381d7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Plots for the dataset with 50,000 elements.", "mimetype": "text/plain", "start_char_idx": 16444, "end_char_idx": 16487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17b1015d-2134-4083-aaf2-8d0c2ce77246", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree. ", "original_text": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8da74ba3-a301-4f3a-914d-483fe24bb8fd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, for each of the subsets, one randomly selects attributes and their appropriate values related to the filter coming from the root.  The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest. ", "original_text": "Plots for the dataset with 50,000 elements."}, "hash": "40606aa72418ef563bac052c9bb65b78fa40b41554f275713c7fd997ce281abe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d403eb8a-7723-4084-a031-b850db471e7d", "node_type": "1", "metadata": {"window": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) ", "original_text": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n"}, "hash": "ed984df188ae8270a94b449fc9e73f0095300e20a6d3552706830b79425841bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. ", "mimetype": "text/plain", "start_char_idx": 16487, "end_char_idx": 16663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d403eb8a-7723-4084-a031-b850db471e7d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) ", "original_text": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b1015d-2134-4083-aaf2-8d0c2ce77246", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The process is continued until the consecutive subsets are empty or have a single element.  Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree. ", "original_text": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes. "}, "hash": "5bf3df1497be9b847581cc52355357a10c422d3c2576e2d0bb13b8eda42a7cc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b961a45-cfcc-47f7-82a5-a3d81bcefe8b", "node_type": "1", "metadata": {"window": "***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "351ae5cf331d86bf14504b9de8a2e0ec9b1c111521bf9f0eb066ab9e9689e94a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n", "mimetype": "text/plain", "start_char_idx": 16663, "end_char_idx": 16876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b961a45-cfcc-47f7-82a5-a3d81bcefe8b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d403eb8a-7723-4084-a031-b850db471e7d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Obviously, it is essential to set a maximal tree depth l which is suggested to be l = log\u2082n.\n\n ***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) ", "original_text": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n"}, "hash": "6ff7c32f8df920fad7ff5a02391c29b486dbce6675d73084b8497fe8c1a7675c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "617ce272-fb43-4746-9175-943f522119db", "node_type": "1", "metadata": {"window": "6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7. ", "original_text": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. "}, "hash": "272a2a71ed25a184c145030a6b3895ca757ab7b3ab6ebff448c6df7f7341b1fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 16876, "end_char_idx": 17005, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "617ce272-fb43-4746-9175-943f522119db", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7. ", "original_text": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b961a45-cfcc-47f7-82a5-a3d81bcefe8b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "4d6b5165b08a266a2cd99678a490cd522785c6868e42c0b7cd20a9d560da8ab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "930a6a38-57a2-40b7-8ed7-b8b6d4e58258", "node_type": "1", "metadata": {"window": "Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures.", "original_text": "The trees form a forest called isolation forest. "}, "hash": "ceb95d190f013e5892b3cb4d6e0455a88ff3ab0268e6dea7444322b9e7c85a3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. ", "mimetype": "text/plain", "start_char_idx": 17005, "end_char_idx": 17119, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "930a6a38-57a2-40b7-8ed7-b8b6d4e58258", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures.", "original_text": "The trees form a forest called isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "617ce272-fb43-4746-9175-943f522119db", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "6.  Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7. ", "original_text": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset. "}, "hash": "f3fa6e06e5be69a91553071dc502cea00ae0c82ff25706d6ff20b1f5c38e9141", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a45ec5a3-98fc-4b13-9dd5-75eedf1ebbef", "node_type": "1", "metadata": {"window": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. ", "original_text": "For all elements in the original dataset, the anomaly score is calculated by searching each tree. "}, "hash": "1f6054f198b424a785caea459097843acb0d5b60b851b284ef3c780e823828a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The trees form a forest called isolation forest. ", "mimetype": "text/plain", "start_char_idx": 17119, "end_char_idx": 17168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a45ec5a3-98fc-4b13-9dd5-75eedf1ebbef", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. ", "original_text": "For all elements in the original dataset, the anomaly score is calculated by searching each tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "930a6a38-57a2-40b7-8ed7-b8b6d4e58258", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Plots for the dataset with 50,000 elements. ** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures.", "original_text": "The trees form a forest called isolation forest. "}, "hash": "dc50707b577361e76acfe0fe4465f229fd65ba13e2ff4b61a56358f0b651d3c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e140d084-ef08-4193-8a68-44d7f85d0819", "node_type": "1", "metadata": {"window": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n", "original_text": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) "}, "hash": "72fd58cbe53719f372227a454b8faa3bebee10375761590aa2f627c3e10f4b1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For all elements in the original dataset, the anomaly score is calculated by searching each tree. ", "mimetype": "text/plain", "start_char_idx": 17168, "end_char_idx": 17266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e140d084-ef08-4193-8a68-44d7f85d0819", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n", "original_text": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a45ec5a3-98fc-4b13-9dd5-75eedf1ebbef", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure is structurally identical to Figure 5, with 10 subplots (a-j) comparing IF and k-Means IF, but on a larger dataset with more complex rectangular cluster shapes.  The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. ", "original_text": "For all elements in the original dataset, the anomaly score is calculated by searching each tree. "}, "hash": "856c627e4923a8953752761035160ed1dd0af8febede0f370115759fd9d810da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6429439e-a9e0-4cb7-9567-160e4216818e", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. "}, "hash": "353fb4f8aa67db66d059fd3ff478e81e42f6c624352acdee52738dfec5329ee1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) ", "mimetype": "text/plain", "start_char_idx": 17266, "end_char_idx": 17493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6429439e-a9e0-4cb7-9567-160e4216818e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e140d084-ef08-4193-8a68-44d7f85d0819", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The observations are similar: k-Means IF tends to identify points far from any dense region as outliers, while standard IF also marks points on the boundaries and in the spaces between dense regions as anomalous.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n", "original_text": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.) "}, "hash": "40c28406c4c8e15d6b60fe00b78e37a10e2dc55732ebd056043cd85158730644", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6a648d9-3a01-4128-a907-da70d8e81758", "node_type": "1", "metadata": {"window": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n", "original_text": "7. "}, "hash": "7206f5163d518184110504113e9606c5aa30b5122ac492c31028d2a1290b4b32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 17493, "end_char_idx": 17597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b6a648d9-3a01-4128-a907-da70d8e81758", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n", "original_text": "7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6429439e-a9e0-4cb7-9567-160e4216818e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig. "}, "hash": "85ccb8a94f665f6aa0039070b91992bd61aac19f4877bf609cfbcb0fe7e270a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "864e7b25-153c-4025-824d-2cf2c68c7caf", "node_type": "1", "metadata": {"window": "The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. ", "original_text": "The results with outputs for the points grouped in 9 geometrical figures."}, "hash": "b0981800b672ff89eecc438de03a66b8c2cdda12e11aab8a2b240a5d844b8d36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7. ", "mimetype": "text/plain", "start_char_idx": 17597, "end_char_idx": 17600, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "864e7b25-153c-4025-824d-2cf2c68c7caf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. ", "original_text": "The results with outputs for the points grouped in 9 geometrical figures."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6a648d9-3a01-4128-a907-da70d8e81758", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nAt the second stage, the trees are used to determine the scores of anomaly for each point in the dataset.  The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n", "original_text": "7. "}, "hash": "6f89530db97dc8f16d34025b10d16c92d076dae43fd77d51be2265e80b731cfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "797552cc-d518-49b3-8d42-7b5724f3d3d8", "node_type": "1", "metadata": {"window": "For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n", "original_text": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. "}, "hash": "cb3ea0fc510767eee94a0568919496ca43e511bc9f5f8c42b33ad02d0f7cde96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The results with outputs for the points grouped in 9 geometrical figures.", "mimetype": "text/plain", "start_char_idx": 17600, "end_char_idx": 17673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "797552cc-d518-49b3-8d42-7b5724f3d3d8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n", "original_text": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "864e7b25-153c-4025-824d-2cf2c68c7caf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The trees form a forest called isolation forest.  For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. ", "original_text": "The results with outputs for the points grouped in 9 geometrical figures."}, "hash": "765657eac76715ee8ca72012ac23e854e63d1fcfa301f0da91c719a46298c1bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ab7354a-0e52-4b5a-90cf-ed7dc35a6d35", "node_type": "1", "metadata": {"window": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig. ", "original_text": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n"}, "hash": "84b2ed2af3bd47f7b1d000a7257e73ad464b18ec18f58948e4c516893942ec93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. ", "mimetype": "text/plain", "start_char_idx": 17673, "end_char_idx": 17834, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ab7354a-0e52-4b5a-90cf-ed7dc35a6d35", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig. ", "original_text": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "797552cc-d518-49b3-8d42-7b5724f3d3d8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For all elements in the original dataset, the anomaly score is calculated by searching each tree.  This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n", "original_text": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid. "}, "hash": "fbfd9e5072ee4adbc0553acf0d4fbafeab96d3f6c89799bf2c2ecb6148fd0a07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4e63948-8bcb-4422-bf78-595068e7a127", "node_type": "1", "metadata": {"window": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "2b9578be817803a9bb5ae6e0faca653e21eeaaad3d9e7e3bf065a94d99de7f9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n", "mimetype": "text/plain", "start_char_idx": 17834, "end_char_idx": 18015, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4e63948-8bcb-4422-bf78-595068e7a127", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ab7354a-0e52-4b5a-90cf-ed7dc35a6d35", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This score value is obtained in the following way [21]:\n\ns(x) = 2^[-E(t,M,k) / c(R)] (1)\n\nwhere\n\nc(R) = 2H(R-1) - 2(R-1)/R (2)\n\nis the average path length of binary search tree unsuccessful search process, see [21], while H(.)  is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig. ", "original_text": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n"}, "hash": "11dc25b34581e448d822d99e2f3a0515089dc090de40bf45886f6c08f7784a3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2c15ff0-564f-4cd0-95b1-04ad54722393", "node_type": "1", "metadata": {"window": "7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points.", "original_text": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n"}, "hash": "a812a254ae753cd374c568a236194350ae69cc40e0bace60df54e45ef5c868d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 18015, "end_char_idx": 18144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2c15ff0-564f-4cd0-95b1-04ad54722393", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points.", "original_text": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4e63948-8bcb-4422-bf78-595068e7a127", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "is estimated as H(R - 1) = ln(R - 1) + 0.5772156649, R is a number of records in a dataset,\n\n***\n**Fig.  7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "472264332d9504dd8ef99a0f0e4d5aaab7a4d74f7f1b45889e0226b5ced60edf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de23ee74-8f17-4e1e-9317-54ec7254f88d", "node_type": "1", "metadata": {"window": "The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. ", "original_text": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. "}, "hash": "9cb1dd9b10d8fb11151d8309d1272d5b81143354f1c38123b75e250c8ff2686e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n", "mimetype": "text/plain", "start_char_idx": 18144, "end_char_idx": 18403, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de23ee74-8f17-4e1e-9317-54ec7254f88d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. ", "original_text": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2c15ff0-564f-4cd0-95b1-04ad54722393", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "7.  The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points.", "original_text": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n"}, "hash": "0c9bd829b386a1c066930ef9adf41a5f9c3058c767210590c1d2eae2c4538403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12d73cd8-7157-48c5-831d-c7d41572d501", "node_type": "1", "metadata": {"window": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n", "original_text": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n"}, "hash": "988293ce3f4aefd15dad8bafeb6975666560bf54df9a391d0ca61bfe2c93b548", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. ", "mimetype": "text/plain", "start_char_idx": 18403, "end_char_idx": 18541, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "12d73cd8-7157-48c5-831d-c7d41572d501", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n", "original_text": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de23ee74-8f17-4e1e-9317-54ec7254f88d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The results with outputs for the points grouped in 9 geometrical figures. ** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. ", "original_text": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value. "}, "hash": "fc78616cc9a8544e9bf5db889b6ae7eab7e476a13af6997217c237661787294c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f63f0e24-c31a-4240-9e14-51158367e939", "node_type": "1", "metadata": {"window": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "***\n**Fig. "}, "hash": "495a9320bb09b9484c7f9665095d12b4b3782d124aebfed09c2c0b0880296554", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n", "mimetype": "text/plain", "start_char_idx": 18541, "end_char_idx": 18626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f63f0e24-c31a-4240-9e14-51158367e939", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12d73cd8-7157-48c5-831d-c7d41572d501", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure presents 10 subplots (a-j) comparing IF and k-Means IF on a dataset with nine separate, well-defined rectangular clusters arranged in a 3x3 grid.  The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n", "original_text": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n"}, "hash": "b73d538363af5188765692567ff9c11a1049d13cc56445477525493c069d42ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdd3f1b2-bbd9-4199-90f6-65562524d561", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig. ", "original_text": "8. "}, "hash": "e780419067d17c85a4f4c27f8e019835b818da86f1c37beb4728aa65e0702322", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 18626, "end_char_idx": 18637, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fdd3f1b2-bbd9-4199-90f6-65562524d561", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig. ", "original_text": "8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f63f0e24-c31a-4240-9e14-51158367e939", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The k-Means IF method more cleanly separates the clusters, treating the space between them as normal, while the standard IF identifies these in-between spaces as anomalous regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "***\n**Fig. "}, "hash": "df931fd1ab008bc9767c785a6349b8cadf4428e40dc177e44dab486fab2354f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84eaf9db-6df5-4d40-9bce-342f20e3e8af", "node_type": "1", "metadata": {"window": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1. ", "original_text": "Three figures and 30,000 points."}, "hash": "e7aee1c34d83cfa3b1fec5273ea7fc537beb331e93272010cccf3efabf2fcde4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8. ", "mimetype": "text/plain", "start_char_idx": 18637, "end_char_idx": 18640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84eaf9db-6df5-4d40-9bce-342f20e3e8af", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1. ", "original_text": "Three figures and 30,000 points."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdd3f1b2-bbd9-4199-90f6-65562524d561", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig. ", "original_text": "8. "}, "hash": "7c4da71695a52265396341c7a3e0d437d415721a65c9295470ba84247255c85a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6199b6d5-fb21-4bad-818f-0a783796c6f5", "node_type": "1", "metadata": {"window": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. ", "original_text": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. "}, "hash": "917934896d5bf29737812276b632a1e1366e20278d7098fff554981e0608c94d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three figures and 30,000 points.", "mimetype": "text/plain", "start_char_idx": 18640, "end_char_idx": 18672, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6199b6d5-fb21-4bad-818f-0a783796c6f5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. ", "original_text": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84eaf9db-6df5-4d40-9bce-342f20e3e8af", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nand finally [6]\n\nE(t,M,k) = (1/t) \u03a3_{i=1 to t} { \u03a3_{j=1 to M} 1 if k=1; \u03a3_{j=1 to M} 1 + c(k) otherwise } (3)\n\nHere, t is the number of trees, M is a total number of binary splits completed during search, and k is a cardinality of exit (final) node.\n\n A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1. ", "original_text": "Three figures and 30,000 points."}, "hash": "8e754910fc440b156cebf54a0dde27a8ee09ed974a9e61686ea991b6de1106b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f529f0a-bced-4678-92a4-999450440edd", "node_type": "1", "metadata": {"window": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. ", "original_text": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n"}, "hash": "8d9374444babfc4a6b59f2863f1b61f05e65d811ffd4b82251a08b4c0178df3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. ", "mimetype": "text/plain", "start_char_idx": 18672, "end_char_idx": 18802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f529f0a-bced-4678-92a4-999450440edd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. ", "original_text": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6199b6d5-fb21-4bad-818f-0a783796c6f5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "A general interpretation of the above score values is that if the score measure of an instance is close to 1, it is likely anomaly value.  If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. ", "original_text": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes. "}, "hash": "d398a74dab3c2d2a3cb1bd71552acffca314ab3e5795f608dc3bf28f0a7eed1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3228f424-3b85-4935-940a-3c172aae476d", "node_type": "1", "metadata": {"window": "***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "eda3f506ee8f4415a2682cefde74a350d1ed82e669b8d660e2aa4fb45c789ea3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n", "mimetype": "text/plain", "start_char_idx": 18802, "end_char_idx": 19039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3228f424-3b85-4935-940a-3c172aae476d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f529f0a-bced-4678-92a4-999450440edd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If s for some record is much smaller than 0.5 then it can be deemed to be \"normal\".\n\n ***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. ", "original_text": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n"}, "hash": "feb81f836abeb15fa746947926b20249adcd275a17a0fabfd4c856c47f0395df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a52c60f-e0e9-49ef-a7a0-9c26b524de4a", "node_type": "1", "metadata": {"window": "8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters. ", "original_text": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig. "}, "hash": "571ca27bf8c943324639e5aa472f751f44b76c2e7d0011f350f1e9a7907efe45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 19039, "end_char_idx": 19168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2a52c60f-e0e9-49ef-a7a0-9c26b524de4a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters. ", "original_text": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3228f424-3b85-4935-940a-3c172aae476d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "d8783d2204f1c338493c6ae4910108a11e87f8306a727127964f9a06d4e37ba0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84f8fe8-854c-4e2a-af37-a6bacd91fffd", "node_type": "1", "metadata": {"window": "Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n", "original_text": "1. "}, "hash": "c762817aae60d1d8d2310fc2cb4a403fc810cfca3b25d9bada1ffdff8710e924", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig. ", "mimetype": "text/plain", "start_char_idx": 19168, "end_char_idx": 19238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b84f8fe8-854c-4e2a-af37-a6bacd91fffd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n", "original_text": "1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a52c60f-e0e9-49ef-a7a0-9c26b524de4a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "8.  Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters. ", "original_text": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig. "}, "hash": "2b2907a4c50005f26879001478382b7c7272e77cf484246a1ee92ae70c61a81d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4bd3559-8314-4d6e-97a2-0ccaa584f686", "node_type": "1", "metadata": {"window": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed. ", "original_text": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. "}, "hash": "8cff6f4aac34005ae43ff1653eb721f8516ed3f1734a6bc6bfabd0dce9f23f82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1. ", "mimetype": "text/plain", "start_char_idx": 19238, "end_char_idx": 19241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4bd3559-8314-4d6e-97a2-0ccaa584f686", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed. ", "original_text": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84f8fe8-854c-4e2a-af37-a6bacd91fffd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Three figures and 30,000 points. ** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n", "original_text": "1. "}, "hash": "f490ccf2820570be4c27025247cd806b3be491827ec01033cc7ba10c5ce74d74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16ab9ddf-616d-412d-af9d-5c3cec7f2ecb", "node_type": "1", "metadata": {"window": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig. ", "original_text": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. "}, "hash": "bd965e0a845ee93c042b0637379e544f97662b2a477e732bee1446dceed437c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. ", "mimetype": "text/plain", "start_char_idx": 19241, "end_char_idx": 19427, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16ab9ddf-616d-412d-af9d-5c3cec7f2ecb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig. ", "original_text": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4bd3559-8314-4d6e-97a2-0ccaa584f686", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This figure shows 10 subplots (a-j) comparing IF and k-Means IF on a dataset with three rectangular clusters of varying sizes.  The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed. ", "original_text": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed. "}, "hash": "1c3ce4938e83f2e750ed5999fc0d171d15077a0abda452bb5ba832acf48d091e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cfe3eb6-ccc6-49a4-85e2-76d2c48c21e4", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9. ", "original_text": "The method is strictly binary with sensitivity to the location of the point related to the division points. "}, "hash": "50b407bfe7aa4e67638e80942e02eab9953526945814674992e8ba2535d0253c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. ", "mimetype": "text/plain", "start_char_idx": 19427, "end_char_idx": 19654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8cfe3eb6-ccc6-49a4-85e2-76d2c48c21e4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9. ", "original_text": "The method is strictly binary with sensitivity to the location of the point related to the division points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16ab9ddf-616d-412d-af9d-5c3cec7f2ecb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The pattern of results is consistent with previous figures: standard IF identifies points on the edges and in the space between clusters as outliers, whereas k-Means IF focuses on the points truly distant from the dense cluster regions.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig. ", "original_text": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function. "}, "hash": "289ef9ea362a7ed71a834404b8e423f86a1b9e9da763a909e98cd31222a1715d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "903e8ae4-1c12-4de4-863f-83929f8fe94e", "node_type": "1", "metadata": {"window": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). ", "original_text": "Also, note that the method is insensitive to the naturally observable clusters. "}, "hash": "5930cefebda781d98481f5beeee16162e033d9b17865ffa6ac4b65408480a0db", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The method is strictly binary with sensitivity to the location of the point related to the division points. ", "mimetype": "text/plain", "start_char_idx": 19654, "end_char_idx": 19762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "903e8ae4-1c12-4de4-863f-83929f8fe94e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). ", "original_text": "Also, note that the method is insensitive to the naturally observable clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cfe3eb6-ccc6-49a4-85e2-76d2c48c21e4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9. ", "original_text": "The method is strictly binary with sensitivity to the location of the point related to the division points. "}, "hash": "3a355a823229571c7ee249eb20931dab55411aeafb01aa723b36c2fc6df03fc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94426da9-4d4c-48be-9d84-5b116c98dfae", "node_type": "1", "metadata": {"window": "1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated.", "original_text": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n"}, "hash": "903ac1e734698d2a7385b3b179ddf28f9c8133ab669763ec7e4b940f53dd531c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also, note that the method is insensitive to the naturally observable clusters. ", "mimetype": "text/plain", "start_char_idx": 19762, "end_char_idx": 19842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94426da9-4d4c-48be-9d84-5b116c98dfae", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated.", "original_text": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "903e8ae4-1c12-4de4-863f-83929f8fe94e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nAn intuition behind the isolation forest is presented in Fig.  1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). ", "original_text": "Also, note that the method is insensitive to the naturally observable clusters. "}, "hash": "54b6b630ac667a044e22d730f7b77eef5bb2c9450c1fd4882515c75375b1768e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e225ff3f-b8f8-411d-8c4d-07c1058579b3", "node_type": "1", "metadata": {"window": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States. ", "original_text": "Next, an innovative modification of a method described in the previous section is discussed. "}, "hash": "6e6e236a0414414b78f5aa74023ca31ff911aea4ad57f022458013fbecd58df6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n", "mimetype": "text/plain", "start_char_idx": 19842, "end_char_idx": 19961, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e225ff3f-b8f8-411d-8c4d-07c1058579b3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States. ", "original_text": "Next, an innovative modification of a method described in the previous section is discussed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94426da9-4d4c-48be-9d84-5b116c98dfae", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1.  Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated.", "original_text": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n"}, "hash": "5f8e9df7ce79d300fd12c596e7e27a917d8f7d0277e6657d67a24b6b0f88d8fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4728fa7-9ac1-4db2-b121-0f1b34ae92ab", "node_type": "1", "metadata": {"window": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area. ", "original_text": "Let the main symbols introduced\n\n***\n**Fig. "}, "hash": "2d01623e117b66e79408120ffeeee4c1391d6c483cc5575e9b0aba6a3ba4c931", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, an innovative modification of a method described in the previous section is discussed. ", "mimetype": "text/plain", "start_char_idx": 19961, "end_char_idx": 20054, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4728fa7-9ac1-4db2-b121-0f1b34ae92ab", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area. ", "original_text": "Let the main symbols introduced\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e225ff3f-b8f8-411d-8c4d-07c1058579b3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Assuming that the dataset is divided alternately according to the x and y values of the Cartesian plane, to find the red point as the singleton in its neighborhood 10 splits are needed.  Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States. ", "original_text": "Next, an innovative modification of a method described in the previous section is discussed. "}, "hash": "0cf6dfe5dbd8a2452d15bd90f936bf393ec4d07c61febdff7054f229ee85a1d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "011d9dc6-9260-489a-96e3-377c8ee61664", "node_type": "1", "metadata": {"window": "The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n", "original_text": "9. "}, "hash": "8152c48e44cbe5827ef107cb1734dcbd3a8851e8ca8d5cfd1c8886a62ae6c2f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let the main symbols introduced\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 20054, "end_char_idx": 20098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "011d9dc6-9260-489a-96e3-377c8ee61664", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n", "original_text": "9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4728fa7-9ac1-4db2-b121-0f1b34ae92ab", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Here, one can note that regardless of whether this point is close to the limits of split area filtered after each step of split operation or not, the final value of splits is 10 which is the input (argument) to the s function.  The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area. ", "original_text": "Let the main symbols introduced\n\n***\n**Fig. "}, "hash": "f0b4bd21657319e2c07bbe2e935db6f4de190a20d8ca42083f25fff3cd8a0a35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a8b9f64-660b-4bdd-bf73-f5232b34915d", "node_type": "1", "metadata": {"window": "Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). "}, "hash": "fd050c1ebf69dc379750b8d1ecc348451cc4acf18c9879fe68bd08f2974c57ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9. ", "mimetype": "text/plain", "start_char_idx": 20098, "end_char_idx": 20101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a8b9f64-660b-4bdd-bf73-f5232b34915d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "011d9dc6-9260-489a-96e3-377c8ee61664", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The method is strictly binary with sensitivity to the location of the point related to the division points.  Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n", "original_text": "9. "}, "hash": "82dffd4796ab9d95513635aa054580662109f16c6ac641ae7f662318791372aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e20db6e-5a07-4bb2-8363-6585efd2c6ef", "node_type": "1", "metadata": {"window": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig. ", "original_text": "The blue points are the points considered as less isolated."}, "hash": "28534e5777f05adc73ff42ef7d96b365de6d76e9995674338bbee3c179d8301a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). ", "mimetype": "text/plain", "start_char_idx": 20101, "end_char_idx": 20199, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e20db6e-5a07-4bb2-8363-6585efd2c6ef", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig. ", "original_text": "The blue points are the points considered as less isolated."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a8b9f64-660b-4bdd-bf73-f5232b34915d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Also, note that the method is insensitive to the naturally observable clusters.  Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red). "}, "hash": "53223ec4be3909c4366d9ea164afc1b6356233587f487d2196fe94f1b3df4c33", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ad6a227-3535-469f-97bb-60aedcea0364", "node_type": "1", "metadata": {"window": "Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10. ", "original_text": "** This image shows a satellite view of the northeastern United States. "}, "hash": "a161232aea5eafa47e4fd8643eec56ea9704b59a3efa07cd8fb52c445a2ce105", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue points are the points considered as less isolated.", "mimetype": "text/plain", "start_char_idx": 20199, "end_char_idx": 20258, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ad6a227-3535-469f-97bb-60aedcea0364", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10. ", "original_text": "** This image shows a satellite view of the northeastern United States. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e20db6e-5a07-4bb2-8363-6585efd2c6ef", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore, the formula of anomaly scoring is strictly related to the number of splits during binary search of a tree.\n\n Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig. ", "original_text": "The blue points are the points considered as less isolated."}, "hash": "75b19889d291401a2b9aeabcf92cf294a322df4407f03001745c50997eec1806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf75b25-aec9-46a8-8e8f-982854d6e10d", "node_type": "1", "metadata": {"window": "Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). ", "original_text": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area. "}, "hash": "42e525a4128995418a1fa197abca9745237be7f06a46eae701117cabc7b725e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This image shows a satellite view of the northeastern United States. ", "mimetype": "text/plain", "start_char_idx": 20258, "end_char_idx": 20330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "abf75b25-aec9-46a8-8e8f-982854d6e10d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). ", "original_text": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ad6a227-3535-469f-97bb-60aedcea0364", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, an innovative modification of a method described in the previous section is discussed.  Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10. ", "original_text": "** This image shows a satellite view of the northeastern United States. "}, "hash": "d2de9696b32f240b29b8cbfb70b61e43f59a37e1f843e7ebdccaad65deae4494", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d200c7f-9b62-4db4-b886-85dc97b1c7f5", "node_type": "1", "metadata": {"window": "9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated.", "original_text": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n"}, "hash": "6f950769e746bf8aac096eb4ecb580dbf3ef1c4e30f34d14299d924c88aa11f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area. ", "mimetype": "text/plain", "start_char_idx": 20330, "end_char_idx": 20415, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d200c7f-9b62-4db4-b886-85dc97b1c7f5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated.", "original_text": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf75b25-aec9-46a8-8e8f-982854d6e10d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let the main symbols introduced\n\n***\n**Fig.  9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). ", "original_text": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area. "}, "hash": "924e053eeaa15298229ed6b46e10a4844851909750d7330feb3323f2591ba2ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8b5982f-9482-4604-a363-9d74d083d354", "node_type": "1", "metadata": {"window": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "2abd2ddeb01544b8a1c5cd195b3ad3741e6132c30d0d9b27338842232cc3d2e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n", "mimetype": "text/plain", "start_char_idx": 20415, "end_char_idx": 20581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8b5982f-9482-4604-a363-9d74d083d354", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d200c7f-9b62-4db4-b886-85dc97b1c7f5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "9.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated.", "original_text": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n"}, "hash": "6dbb25d0b5887e0fe46728c8a7ff5c0a3bdc7f3db735b4872ef5e4f382f8864e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b121e25-dca4-4756-af01-1c6d11562e7c", "node_type": "1", "metadata": {"window": "The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n", "original_text": ")*\n***\n**Fig. "}, "hash": "cc7443880a8eef2435c9adada0b420e4d9e1f310a9415141db085afe4bbbdf71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 20581, "end_char_idx": 20710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b121e25-dca4-4756-af01-1c6d11562e7c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n", "original_text": ")*\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8b5982f-9482-4604-a363-9d74d083d354", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red).  The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "7aebc2203d29d1a11c3b8cdaaefe154e212fde8996804807ee08107c9d048814", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eca3266b-a401-4a80-83a4-b5a93658bcb8", "node_type": "1", "metadata": {"window": "** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "10. "}, "hash": "b182bb99d071bb8a49bd5a4a8d02b0d7a7c88b41b15c6e18f76cf747160f8d43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 20710, "end_char_idx": 20724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eca3266b-a401-4a80-83a4-b5a93658bcb8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "10. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b121e25-dca4-4756-af01-1c6d11562e7c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The blue points are the points considered as less isolated. ** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n", "original_text": ")*\n***\n**Fig. "}, "hash": "4099f35716d357d01511b601e3e9514c280bb2a9fedd34374fd9b84a45b4d0f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f23b0f2-e444-4c4b-aa3b-55ea1deb27a6", "node_type": "1", "metadata": {"window": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts. ", "original_text": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). "}, "hash": "a419bd1ee069ad700b29dfe55a23e11e879b1b6e2bed2affb2b97b18a1412b81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10. ", "mimetype": "text/plain", "start_char_idx": 20724, "end_char_idx": 20728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f23b0f2-e444-4c4b-aa3b-55ea1deb27a6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts. ", "original_text": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eca3266b-a401-4a80-83a4-b5a93658bcb8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This image shows a satellite view of the northeastern United States.  Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "10. "}, "hash": "85d8dce79b5ddfb6883f51da7ccab3c303bdfa7bcc1aed5681331e525287833e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90aa8750-02ed-4398-9cd3-a0d0f9e1b4f1", "node_type": "1", "metadata": {"window": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case. ", "original_text": "The blue points are the points considered as less isolated."}, "hash": "1c69cc17ea4d8075597b6b8e0ab355d92619a340be9a5e05599859fb8640a1ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). ", "mimetype": "text/plain", "start_char_idx": 20728, "end_char_idx": 20826, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90aa8750-02ed-4398-9cd3-a0d0f9e1b4f1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case. ", "original_text": "The blue points are the points considered as less isolated."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f23b0f2-e444-4c4b-aa3b-55ea1deb27a6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Most taxi pickup points (blue) are clustered in the New York City metropolitan area.  The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts. ", "original_text": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image). "}, "hash": "11bdf8b4326df9db2b0025c80f8ce100dfee6a1bbd7cad466334fb5958f33e1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9782e46-a92e-4fe7-a6d6-8059311bec5b", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen. ", "original_text": "** This image is a zoomed-in satellite view of the New York City area. "}, "hash": "692481e28c5eb331d7d9b8e5a6a82cd142d358462749ba0b9badbfd8a78d7188", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue points are the points considered as less isolated.", "mimetype": "text/plain", "start_char_idx": 20826, "end_char_idx": 20885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9782e46-a92e-4fe7-a6d6-8059311bec5b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen. ", "original_text": "** This image is a zoomed-in satellite view of the New York City area. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90aa8750-02ed-4398-9cd3-a0d0f9e1b4f1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The top 1000 anomalous points (red) identified by the k-Means-based IF are clustered in a specific location far to the east, likely an airport or a data entry error.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case. ", "original_text": "The blue points are the points considered as less isolated."}, "hash": "01d04a2aa5d55a872aef25487a01e9b09a7788517ff88dae0061d1776789dc81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc9e2382-f649-4125-b277-6252875d51a5", "node_type": "1", "metadata": {"window": ")*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. ", "original_text": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n"}, "hash": "562abddf0f29ac4253b6a0826bf9ca48bea28d2d54384da6b1317ffee18aebc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This image is a zoomed-in satellite view of the New York City area. ", "mimetype": "text/plain", "start_char_idx": 20885, "end_char_idx": 20956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc9e2382-f649-4125-b277-6252875d51a5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. ", "original_text": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9782e46-a92e-4fe7-a6d6-8059311bec5b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen. ", "original_text": "** This image is a zoomed-in satellite view of the New York City area. "}, "hash": "8165a13551657942259221e3c157852b5dffa4fc4291c226d6482ce88d575daa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c1923b7-a1e1-4520-8e4d-6f9796eb0dc1", "node_type": "1", "metadata": {"window": "10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54]. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "a8b056655cc161eb71548c5804a63fead7e1139771152b920e80ba3d12017d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n", "mimetype": "text/plain", "start_char_idx": 20956, "end_char_idx": 21177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c1923b7-a1e1-4520-8e4d-6f9796eb0dc1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54]. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc9e2382-f649-4125-b277-6252875d51a5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n**Fig.  10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. ", "original_text": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n"}, "hash": "edceb3b8bd2bdc63232308e2bd67719ec368dd3a14c2a7f85e88e78465f7e226", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "383b5cfa-9c31-4721-a83f-56d9d5c96c95", "node_type": "1", "metadata": {"window": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, . ", "original_text": ")*\n***\n\nabove denote the same artifacts. "}, "hash": "6e1e96d33ff0dbfeae65ea7cf60845217b4abc0ada8ef87115751fea88f426ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 21177, "end_char_idx": 21306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "383b5cfa-9c31-4721-a83f-56d9d5c96c95", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, . ", "original_text": ")*\n***\n\nabove denote the same artifacts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c1923b7-a1e1-4520-8e4d-6f9796eb0dc1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "10.  Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54]. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "21804b568da45aaa24aee667795309065fc87572125a6b6c98b0aaf824c1b502", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7162c69c-27a1-4b46-93fa-105183ee26be", "node_type": "1", "metadata": {"window": "The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  . ", "original_text": "During the training the process starts similarly as in the previous case. "}, "hash": "470465c55b7357808ee8eba16235cd135e7e70bcd3cd7faf49b21b6acac03d61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nabove denote the same artifacts. ", "mimetype": "text/plain", "start_char_idx": 21306, "end_char_idx": 21347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7162c69c-27a1-4b46-93fa-105183ee26be", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  . ", "original_text": "During the training the process starts similarly as in the previous case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "383b5cfa-9c31-4721-a83f-56d9d5c96c95", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Top 1000 most isolated pickup points according to isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, . ", "original_text": ")*\n***\n\nabove denote the same artifacts. "}, "hash": "5fd0c5c58c9f89a740fa94de4d5e9d8db2a2032b3d0c2ed21ad120740374f6c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c536f789-bb5d-4904-900a-54d571966de7", "node_type": "1", "metadata": {"window": "** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  . ", "original_text": "An attribute q is randomly chosen. "}, "hash": "9bdbaaf96a56f559a8619bd3af0608060c6cb14a3ef1b54302acc1d3b23e0b05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "During the training the process starts similarly as in the previous case. ", "mimetype": "text/plain", "start_char_idx": 21347, "end_char_idx": 21421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c536f789-bb5d-4904-900a-54d571966de7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  . ", "original_text": "An attribute q is randomly chosen. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7162c69c-27a1-4b46-93fa-105183ee26be", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The blue points are the points considered as less isolated. ** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  . ", "original_text": "During the training the process starts similarly as in the previous case. "}, "hash": "e4dc04f3814aec279fa14aaa48c3d02ee319da941df47fe4f3ab3ce84b232812", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62c01cfe-6b65-4b3e-a78b-2fd91687bf64", "node_type": "1", "metadata": {"window": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. ", "original_text": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. "}, "hash": "d2169708cb2d4f5ed38146ded48518dcb741b52dffc4c41fad7e9037eaee1dbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An attribute q is randomly chosen. ", "mimetype": "text/plain", "start_char_idx": 21421, "end_char_idx": 21456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62c01cfe-6b65-4b3e-a78b-2fd91687bf64", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. ", "original_text": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c536f789-bb5d-4904-900a-54d571966de7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This image is a zoomed-in satellite view of the New York City area.  The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  . ", "original_text": "An attribute q is randomly chosen. "}, "hash": "7c3e9ab8b7e4f548e6232c28b2dae21246e26d5f89622285de10897339522896", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "466b8888-c69f-4511-bd2e-72a4a3c5e7cf", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. ", "original_text": "The number c of clusters is selected by applying the elbow rule [53,54]. "}, "hash": "92230eb7580b5e9d61d5ec4ab526b84293566cb073e2a5953fbe72cad80ec13b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. ", "mimetype": "text/plain", "start_char_idx": 21456, "end_char_idx": 21577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "466b8888-c69f-4511-bd2e-72a4a3c5e7cf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. ", "original_text": "The number c of clusters is selected by applying the elbow rule [53,54]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62c01cfe-6b65-4b3e-a78b-2fd91687bf64", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The anomalous taxi pickup points (red) identified by the standard IF are scattered throughout the region, including within Manhattan and surrounding boroughs, rather than being concentrated in a single outlying location.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. ", "original_text": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters. "}, "hash": "ab60b533aaa615a9c534555e544b0cf97ef8cf1c552bf6b7af0f274a8f0755f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a1bb32b-93f8-43af-913d-e4fd05ed267a", "node_type": "1", "metadata": {"window": ")*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here. ", "original_text": "Namely, assume that c stands for some number of clusters from the range 1, 2, . "}, "hash": "3b8331929a91a3e1ce3533bb83773fd2663ad6c14041737f4402f8883bb92acf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The number c of clusters is selected by applying the elbow rule [53,54]. ", "mimetype": "text/plain", "start_char_idx": 21577, "end_char_idx": 21650, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1a1bb32b-93f8-43af-913d-e4fd05ed267a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here. ", "original_text": "Namely, assume that c stands for some number of clusters from the range 1, 2, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "466b8888-c69f-4511-bd2e-72a4a3c5e7cf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. ", "original_text": "The number c of clusters is selected by applying the elbow rule [53,54]. "}, "hash": "a0219bb9c97114acbddd4c4d9b84c4478b80e6e72ae395922be06c49b2f1a178", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23068048-dca9-4d5d-8886-07226e713905", "node_type": "1", "metadata": {"window": "During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. ", "original_text": ". "}, "hash": "945971bfe3f5c6575ef2935699baf09b13794c4cd01ba6df9b67bed9ac04c0e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Namely, assume that c stands for some number of clusters from the range 1, 2, . ", "mimetype": "text/plain", "start_char_idx": 21650, "end_char_idx": 21730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23068048-dca9-4d5d-8886-07226e713905", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a1bb32b-93f8-43af-913d-e4fd05ed267a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nabove denote the same artifacts.  During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here. ", "original_text": "Namely, assume that c stands for some number of clusters from the range 1, 2, . "}, "hash": "18b04936e3b958b7a95fd7827cf3d2f13e498b89b2dc37d58d4b2cc86f25f5f4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b377d59c-8480-4e8e-ba96-aedecbdc8e94", "node_type": "1", "metadata": {"window": "An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters. ", "original_text": ". "}, "hash": "47a20959dd302e4afd7cbdcca817eb6fce3512b48957a7c884844b898a173884", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 21728, "end_char_idx": 21730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b377d59c-8480-4e8e-ba96-aedecbdc8e94", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23068048-dca9-4d5d-8886-07226e713905", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "During the training the process starts similarly as in the previous case.  An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. ", "original_text": ". "}, "hash": "004a6b9301b991932be364a7ef517ec12f345867dc630f3e6b368c3cc8ccb967", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71167929-3528-42e9-872f-8f374ee7192a", "node_type": "1", "metadata": {"window": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. ", "original_text": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. "}, "hash": "2275fbca23eae5e5170d184ed51513bd5a3ae71e7ed4bf2b9109d2e377f98f61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 21730, "end_char_idx": 21732, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71167929-3528-42e9-872f-8f374ee7192a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. ", "original_text": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b377d59c-8480-4e8e-ba96-aedecbdc8e94", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An attribute q is randomly chosen.  However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters. ", "original_text": ". "}, "hash": "1899721fe387680569decfd5d550382021f683b107b0dfed0f8dd2db1e3ca543", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff493fbe-863b-45db-b3ea-896becd67269", "node_type": "1", "metadata": {"window": "The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. ", "original_text": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. "}, "hash": "94f516b7e655cb5e9f0a63dde8455aa017243de960dd22090d49bb9be81372b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. ", "mimetype": "text/plain", "start_char_idx": 21734, "end_char_idx": 21831, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff493fbe-863b-45db-b3ea-896becd67269", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. ", "original_text": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71167929-3528-42e9-872f-8f374ee7192a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, the interval of values between the maximal value of the set X and its minimal value is divided into c clusters.  The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. ", "original_text": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out. "}, "hash": "556ebcf8448bfcabc596a2196bb321d240616a0bfa486b76bb002770bae46b85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18675b8d-8361-4691-8cad-e91503e70703", "node_type": "1", "metadata": {"window": "Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n", "original_text": "The data are the inputs to the elbow method and the optimal number c is selected here. "}, "hash": "8e19124ec009acff81a696b335d65a6574437af0ddca99a93acf1bd5b5fd8da0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. ", "mimetype": "text/plain", "start_char_idx": 21831, "end_char_idx": 21964, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18675b8d-8361-4691-8cad-e91503e70703", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n", "original_text": "The data are the inputs to the elbow method and the optimal number c is selected here. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff493fbe-863b-45db-b3ea-896becd67269", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The number c of clusters is selected by applying the elbow rule [53,54].  Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. ", "original_text": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error. "}, "hash": "b0e4985a7760a27c19ff9e4087758432f9ce6a8643962e26d6613dc526cd5e97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83b0df3b-6325-447d-947b-ce87c029fa3a", "node_type": "1", "metadata": {"window": ".  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score. ", "original_text": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. "}, "hash": "e591534a8bad68979fd79f6a9800cceb21a81e52fe7946338373823c2bfdf26c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The data are the inputs to the elbow method and the optimal number c is selected here. ", "mimetype": "text/plain", "start_char_idx": 21964, "end_char_idx": 22051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83b0df3b-6325-447d-947b-ce87c029fa3a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score. ", "original_text": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18675b8d-8361-4691-8cad-e91503e70703", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Namely, assume that c stands for some number of clusters from the range 1, 2, .  .  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n", "original_text": "The data are the inputs to the elbow method and the optimal number c is selected here. "}, "hash": "9a84a892aa1a2064527fe6dba6ba727f7bf3b9f9dad588a8be25938e495f82d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "358d5b8d-8c06-4f6e-a5d4-b28a5cf5a4e5", "node_type": "1", "metadata": {"window": ".  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster. ", "original_text": "Therefore an \"elbow\" point relates to the optimal number of clusters. "}, "hash": "d19e173af1aa3cda7c089c9e23e00064f271b37cb17e9ba7249e2ffea0736f0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. ", "mimetype": "text/plain", "start_char_idx": 22051, "end_char_idx": 22174, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "358d5b8d-8c06-4f6e-a5d4-b28a5cf5a4e5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster. ", "original_text": "Therefore an \"elbow\" point relates to the optimal number of clusters. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83b0df3b-6325-447d-947b-ce87c029fa3a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  .  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score. ", "original_text": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm. "}, "hash": "d5716cff6b4b7a39f49904a55c873dfda8b7817ba552f46f63b077a3175bc93f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "871579ef-4899-41d1-a4d3-01487b5782da", "node_type": "1", "metadata": {"window": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. ", "original_text": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. "}, "hash": "e00f7c19d4f5c8ae1582f71d5867081bcbf742ac8e90a18b75b7be0acccc1efa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore an \"elbow\" point relates to the optimal number of clusters. ", "mimetype": "text/plain", "start_char_idx": 22174, "end_char_idx": 22244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "871579ef-4899-41d1-a4d3-01487b5782da", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. ", "original_text": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "358d5b8d-8c06-4f6e-a5d4-b28a5cf5a4e5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  , C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster. ", "original_text": "Therefore an \"elbow\" point relates to the optimal number of clusters. "}, "hash": "5fc72cbd0848dbfe8d50990f657a977fba10ff1680681c7d11e17626ba2433a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dec6a86-e6c6-480d-80a0-3eeea399034e", "node_type": "1", "metadata": {"window": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster. ", "original_text": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. "}, "hash": "9dbfdaf234e4bc324affc4b98c303c83528c6ba22f7a55b85ed5b6bc4cf6b823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. ", "mimetype": "text/plain", "start_char_idx": 22244, "end_char_idx": 22362, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4dec6a86-e6c6-480d-80a0-3eeea399034e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster. ", "original_text": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "871579ef-4899-41d1-a4d3-01487b5782da", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ", C. The processes of clustering of the values of attribute q, say, k-Means [55] is carried out.  When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. ", "original_text": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly. "}, "hash": "bde5f22c2a3d1893e8f94fdb7777c8ceef5cf6006344bff6e2cced0e1bfb23a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cb9f035-a672-4d79-89ae-54ff60d748b8", "node_type": "1", "metadata": {"window": "The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . ", "original_text": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n"}, "hash": "f4f26428f2c9fed13d5926917fb4c8fc788338c025aabd4faab83eb206d26c9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. ", "mimetype": "text/plain", "start_char_idx": 22362, "end_char_idx": 22473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6cb9f035-a672-4d79-89ae-54ff60d748b8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . ", "original_text": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dec6a86-e6c6-480d-80a0-3eeea399034e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "When sweeping through the number of clusters, the centers and the limits of clusters are remembered along with the clustering error.  The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster. ", "original_text": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable. "}, "hash": "9b9e234165c96745f2284beaf2b18b785b2248e6c865946ce38ba59c5853af96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "263d07bf-a5be-451e-80e2-05dc3e2b05f2", "node_type": "1", "metadata": {"window": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  . ", "original_text": "Let us now discuss the process of determining the anomaly score. "}, "hash": "2fdb2d3b01c9d2f0822969f43c30bf51b154225b628ac4753ff6a4d1595a2f70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n", "mimetype": "text/plain", "start_char_idx": 22473, "end_char_idx": 22659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "263d07bf-a5be-451e-80e2-05dc3e2b05f2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  . ", "original_text": "Let us now discuss the process of determining the anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cb9f035-a672-4d79-89ae-54ff60d748b8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The data are the inputs to the elbow method and the optimal number c is selected here.  If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . ", "original_text": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n"}, "hash": "5429e99833505d3262aa209369936af6d9c495d4b0a5d16554b6a2eddc48a718", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "609ec096-f1b3-4ae7-aae7-9880806caa95", "node_type": "1", "metadata": {"window": "Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  . ", "original_text": "Each record traces the tree according to its membership to particular cluster. "}, "hash": "4dac94a0cf564ff7627aff22b88850bb5eeccb500f34d8e9a1afb4296f2e0d85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let us now discuss the process of determining the anomaly score. ", "mimetype": "text/plain", "start_char_idx": 22659, "end_char_idx": 22724, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "609ec096-f1b3-4ae7-aae7-9880806caa95", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  . ", "original_text": "Each record traces the tree according to its membership to particular cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "263d07bf-a5be-451e-80e2-05dc3e2b05f2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If the error is matched to the number of clusters and one creates the histogram of the values, it often looks like an arm.  Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  . ", "original_text": "Let us now discuss the process of determining the anomaly score. "}, "hash": "415ddd98b970a6aae675a1f4bc1d710c7e55d29219ad12dae01e6aac49d7fc20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34ed87c3-8b56-4fe6-aad7-1dad33796012", "node_type": "1", "metadata": {"window": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. ", "original_text": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. "}, "hash": "6ef8190fd1a8e10926b618375fb12992b335e4ed6fac8a9668352aeacb37886e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each record traces the tree according to its membership to particular cluster. ", "mimetype": "text/plain", "start_char_idx": 22724, "end_char_idx": 22803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34ed87c3-8b56-4fe6-aad7-1dad33796012", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. ", "original_text": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "609ec096-f1b3-4ae7-aae7-9880806caa95", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Therefore an \"elbow\" point relates to the optimal number of clusters.  If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  . ", "original_text": "Each record traces the tree according to its membership to particular cluster. "}, "hash": "ce1075db34282fa9d82899a20308ba1a2a27697b68f0f0dd62bb643091d88262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bec5d18-370e-4143-9e19-4ed4a4973551", "node_type": "1", "metadata": {"window": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. ", "original_text": "s is regarded as the membership of x to the cluster. "}, "hash": "c97dfc103df642a4c1203ee23ef022abc684ec348a77ea90edfcc7224b0866a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. ", "mimetype": "text/plain", "start_char_idx": 22803, "end_char_idx": 22945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6bec5d18-370e-4143-9e19-4ed4a4973551", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. ", "original_text": "s is regarded as the membership of x to the cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34ed87c3-8b56-4fe6-aad7-1dad33796012", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If the number of clusters is higher than this value, then the clusters do not change the model of data significantly.  On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. ", "original_text": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit. "}, "hash": "7f759cc18d505f53bd837545d238797f532c2091ce98176915ce174b04e5cb27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "536b8afb-e0ee-4ca9-8437-e4437f8b5fe9", "node_type": "1", "metadata": {"window": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n", "original_text": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . "}, "hash": "7b9901d49e9bc6aea1b07d9ca1127ac017c9de2b5308c7b7fdcccf0da32ae52a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "s is regarded as the membership of x to the cluster. ", "mimetype": "text/plain", "start_char_idx": 22945, "end_char_idx": 22998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "536b8afb-e0ee-4ca9-8437-e4437f8b5fe9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n", "original_text": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bec5d18-370e-4143-9e19-4ed4a4973551", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "On the other hand, if the value is small then the sum of clustering errors might be too high and unacceptable.  Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. ", "original_text": "s is regarded as the membership of x to the cluster. "}, "hash": "6a909e123e303d4ea822a5efe95c4790262344eab82217d8096f2c2549a84902", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fddea5e7-0ad2-43ab-b2b1-7b9422966778", "node_type": "1", "metadata": {"window": "Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure. ", "original_text": ". "}, "hash": "a4ccbfbe48e9d73edcb8921b7903e511fdb08e92bf0a68f4818b57974d2753f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . ", "mimetype": "text/plain", "start_char_idx": 22998, "end_char_idx": 23102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fddea5e7-0ad2-43ab-b2b1-7b9422966778", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "536b8afb-e0ee-4ca9-8437-e4437f8b5fe9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, the clusters and their limits create the leafs for the node and for each of these leafs one can repeat the process with the assumption that filter coming from the node is active.\n\n Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n", "original_text": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, . "}, "hash": "3a27d81600bb91b4839ebbe7a354553c3d70a96bcd0f3ccc6ef2d46f24f92a88", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0b1e7d8-722a-4423-895f-deb3df87d664", "node_type": "1", "metadata": {"window": "Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. ", "original_text": ". "}, "hash": "803338e925a55f65eb482be05bda10b5338ffb6ba444dffb2fb26a4856a4a71c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 23100, "end_char_idx": 23102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0b1e7d8-722a-4423-895f-deb3df87d664", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fddea5e7-0ad2-43ab-b2b1-7b9422966778", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Let us now discuss the process of determining the anomaly score.  Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure. ", "original_text": ". "}, "hash": "d82f75e4f53e61e0d353fe828ee6ceb275307c35b6f82a99472dc9f4979c8c52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5bceecb1-29cb-48ca-9e87-13fbd30bb443", "node_type": "1", "metadata": {"window": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. ", "original_text": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. "}, "hash": "eb02796a09e6d2ecf3be2c9904d15063de3a375a2da976a34fcf97faf1197077", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 23102, "end_char_idx": 23104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5bceecb1-29cb-48ca-9e87-13fbd30bb443", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. ", "original_text": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0b1e7d8-722a-4423-895f-deb3df87d664", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Each record traces the tree according to its membership to particular cluster.  The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. ", "original_text": ". "}, "hash": "b9ffb85ecf734c07f7f95db605bf887c3ba0d6df829865d99181f9a62e36cd20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "647082d1-7535-4322-bb79-e519108ffdaf", "node_type": "1", "metadata": {"window": "s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11. ", "original_text": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. "}, "hash": "e49dbde624287e0fc2a7ff7716689699907d02979ba2e72fa72272f2b0715290", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. ", "mimetype": "text/plain", "start_char_idx": 23106, "end_char_idx": 23312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "647082d1-7535-4322-bb79-e519108ffdaf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11. ", "original_text": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5bceecb1-29cb-48ca-9e87-13fbd30bb443", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The score value obtained at each split is\n\ns = 1 \u2212 d(x, c_q) / d(c_l, c_q) (4)\n\nwhere c_q is the cluster center and c_l is the cluster limit.  s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. ", "original_text": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results. "}, "hash": "79f48a7cb98af2b8ea69c39e4f4f8f911b579b9523e7af4eedf5ccb667187325", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ccacc86-c3a7-41d3-83df-ea23c20f27ab", "node_type": "1", "metadata": {"window": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). ", "original_text": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n"}, "hash": "429d353670d5689297d0e1fc156c6e7e27eded8c595c2d179b0b8a8f4bd543fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. ", "mimetype": "text/plain", "start_char_idx": 23312, "end_char_idx": 23449, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ccacc86-c3a7-41d3-83df-ea23c20f27ab", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). ", "original_text": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "647082d1-7535-4322-bb79-e519108ffdaf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "s is regarded as the membership of x to the cluster.  Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11. ", "original_text": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster. "}, "hash": "d20445b9d3b21de0b5719285b2796f56fca9b0e964e8044391afd7d4cadaeff0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aeb6f198-dea6-4905-b214-6b602becc4d7", "node_type": "1", "metadata": {"window": ".  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated.", "original_text": "Moreover, at the stage of construction the trees learn the data structure. "}, "hash": "d035131b741317ab37c695491079e43fd0fb6db294dd41b1ed8e584b07404e70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n", "mimetype": "text/plain", "start_char_idx": 23449, "end_char_idx": 23534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aeb6f198-dea6-4905-b214-6b602becc4d7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated.", "original_text": "Moreover, at the stage of construction the trees learn the data structure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ccacc86-c3a7-41d3-83df-ea23c20f27ab", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Final score for each dataset record is its sum of the split memberships (at each jth split, j = 1, 2, .  .  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). ", "original_text": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n"}, "hash": "be995a659264de04f7857ecf4e888e6df1395abc9c47afa538121457d9aecdc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82352a04-67fa-456b-b7f5-ea58b033d5cc", "node_type": "1", "metadata": {"window": ".  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. ", "original_text": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. "}, "hash": "b91cbd3d873c5054ac0659f06e30a2afb791b90ef66164844c765e7556d7bfdf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, at the stage of construction the trees learn the data structure. ", "mimetype": "text/plain", "start_char_idx": 23534, "end_char_idx": 23609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82352a04-67fa-456b-b7f5-ea58b033d5cc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. ", "original_text": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aeb6f198-dea6-4905-b214-6b602becc4d7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  .  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated.", "original_text": "Moreover, at the stage of construction the trees learn the data structure. "}, "hash": "aa72140c23d307e7f36a57dcb815d4c3656b8738986a4a3bb9e375692b73770d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "228ba6b9-c7a9-4de3-aea3-a5e1ac2187de", "node_type": "1", "metadata": {"window": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n", "original_text": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. "}, "hash": "4c88eb9e9ceb5b4007a0c47e5f6ce87f08df8b0f4b2211c6bbe48e2e29d1e486", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. ", "mimetype": "text/plain", "start_char_idx": 23609, "end_char_idx": 23793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "228ba6b9-c7a9-4de3-aea3-a5e1ac2187de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n", "original_text": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82352a04-67fa-456b-b7f5-ea58b033d5cc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ".  , M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. ", "original_text": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions. "}, "hash": "37f03b557567ee8465dfb18ee8e227765248898e684541f334f3663749948e57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76cefdcc-3203-4471-b1f6-334d7053c437", "node_type": "1", "metadata": {"window": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "11. "}, "hash": "67a0d413c76f1a576fbcf5765af12d17b86fa31c538418a46f9131c00b02f412", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 23793, "end_char_idx": 23879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76cefdcc-3203-4471-b1f6-334d7053c437", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "11. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "228ba6b9-c7a9-4de3-aea3-a5e1ac2187de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ", M) divided by the trees number, i.e.,\n\n\u03b1(x) = 1 - (1/t) \u03a3_{i=1 to t} \u03a3_{j=1 to M} s_j(x) (5)\n\nThis approach exhibits an evident advantage, by delivering some intuitive insights into the obtained results.  To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n", "original_text": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig. "}, "hash": "505810dd6095a789a6fee756e6a85404441934ee6425cc91f81853b2030dbe20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88f4fa59-33cc-4929-acaa-d6b241da40f2", "node_type": "1", "metadata": {"window": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less. ", "original_text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). "}, "hash": "fb9930c1503f11f4acd1117d83a61efbf9b216f444fbd8770ab8f26e08819f55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11. ", "mimetype": "text/plain", "start_char_idx": 23879, "end_char_idx": 23883, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88f4fa59-33cc-4929-acaa-d6b241da40f2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less. ", "original_text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76cefdcc-3203-4471-b1f6-334d7053c437", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "To be precise, the highly isolated records come with the anomaly score close to zero, or even zero, when they do not fit to any cluster.  Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "original_text": "11. "}, "hash": "38710462c568618b4a317ab160057889fd97e3755209f4841c364cc772671c72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9d0df8c-1356-4bb9-9ba7-4e10b1ea6b0c", "node_type": "1", "metadata": {"window": "Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n", "original_text": "The blue points are the points considered as less isolated."}, "hash": "aac1a389a8922099c315d7117924a48c7d65ccdd4fe61890faaa763c5e64e4dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). ", "mimetype": "text/plain", "start_char_idx": 23883, "end_char_idx": 23995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9d0df8c-1356-4bb9-9ba7-4e10b1ea6b0c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n", "original_text": "The blue points are the points considered as less isolated."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88f4fa59-33cc-4929-acaa-d6b241da40f2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Of course, the records being \"typical\" bring higher values of the scoring function.\n\n Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less. ", "original_text": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image). "}, "hash": "21e8e135f9bb4e76d3ce7015eb60c24cd8eeb7945cb7af731d94b9be173d7455", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b2b6e7c-499b-4437-ae85-a278f1e80dcf", "node_type": "1", "metadata": {"window": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig. ", "original_text": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. "}, "hash": "a3a7927b5566d32375e1983edacf9af3b91cd41eb7659b67f28472f9ea82bbfc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The blue points are the points considered as less isolated.", "mimetype": "text/plain", "start_char_idx": 23995, "end_char_idx": 24054, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b2b6e7c-499b-4437-ae85-a278f1e80dcf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig. ", "original_text": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9d0df8c-1356-4bb9-9ba7-4e10b1ea6b0c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, at the stage of construction the trees learn the data structure.  The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n", "original_text": "The blue points are the points considered as less isolated."}, "hash": "c068d57b47ae7a48c45904de550981bf07668c8ecae6fc57686c06cb8da632b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13c014a8-a3e0-4619-ac4f-d4c1c9c068c2", "node_type": "1", "metadata": {"window": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2. ", "original_text": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n"}, "hash": "53c43561f1640cbe9ef895ade5ff1795a1ea335c912688a4407c3b6f9e4fc6c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. ", "mimetype": "text/plain", "start_char_idx": 24054, "end_char_idx": 24157, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13c014a8-a3e0-4619-ac4f-d4c1c9c068c2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2. ", "original_text": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b2b6e7c-499b-4437-ae85-a278f1e80dcf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The number of clusters of chosen argument values, filtered at the earliest levels of tree building, is chosen to fit this structure and show its variety as well as similarity regions.  Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig. ", "original_text": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9. "}, "hash": "d8d5ef68540f85846dd91c367d41139639876d7ee520a30787fd4319ae2584e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5360f7ff-b6cd-409c-91d8-dbaeb86acf8b", "node_type": "1", "metadata": {"window": "11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "d571967d75ba89b8ef3d4b1bf8549e8252d001b0f23ac234bbefb19653805611", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n", "mimetype": "text/plain", "start_char_idx": 24157, "end_char_idx": 24371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5360f7ff-b6cd-409c-91d8-dbaeb86acf8b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13c014a8-a3e0-4619-ac4f-d4c1c9c068c2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it means that the depth of the tree is lower and, therefore, at\n\n***\n**Fig.  11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2. ", "original_text": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n"}, "hash": "35d32d00abd40267208d7e38eeb8371ee798f9cd735b41b92af25e584412674f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "974069eb-783a-49c0-bc2e-bb2f1d7f0f29", "node_type": "1", "metadata": {"window": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. ", "original_text": ")*\n***\n\nthe level of searching the time is less. "}, "hash": "64f2ac946fe31fb1c57edbd9cd463e6d4f889ca3a8ddfcb7513f44276c749731", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.", "mimetype": "text/plain", "start_char_idx": 24371, "end_char_idx": 24500, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "974069eb-783a-49c0-bc2e-bb2f1d7f0f29", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. ", "original_text": ")*\n***\n\nthe level of searching the time is less. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5360f7ff-b6cd-409c-91d8-dbaeb86acf8b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "11.  Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. ", "original_text": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article."}, "hash": "225dcd746d78f17f122c884caef7e2bd836bbff2e5f807375810fd2e14e2efb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3342a978-e4e4-4761-9088-8d1f01e3478c", "node_type": "1", "metadata": {"window": "The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. ", "original_text": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n"}, "hash": "d1d1245a31b6f6380f9a444448e18a055685ce9915a93e2374efb919eecf1a01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n***\n\nthe level of searching the time is less. ", "mimetype": "text/plain", "start_char_idx": 24500, "end_char_idx": 24549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3342a978-e4e4-4761-9088-8d1f01e3478c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. ", "original_text": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "974069eb-783a-49c0-bc2e-bb2f1d7f0f29", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Top 1000 most isolated pickup points according to k-Means-based isolation forest (marked in red, zoomed image).  The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. ", "original_text": ")*\n***\n\nthe level of searching the time is less. "}, "hash": "15a3e28df0404430e8454afdfd7d5be1802d275a301c54e519b6a6fb237a18e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f046e225-315a-4beb-af09-93f2c71a8b4c", "node_type": "1", "metadata": {"window": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. ", "original_text": "The overall process is intuitively presented in Fig. "}, "hash": "a12b30fcf43f2536d2f6a0d80d98274f03ed73fa258acfd1a4c3260215dd72ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n", "mimetype": "text/plain", "start_char_idx": 24549, "end_char_idx": 24819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f046e225-315a-4beb-af09-93f2c71a8b4c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. ", "original_text": "The overall process is intuitively presented in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3342a978-e4e4-4761-9088-8d1f01e3478c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The blue points are the points considered as less isolated. ** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. ", "original_text": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n"}, "hash": "8bbd3ab0b9267f6a63c42826a92f1fde54995ebf60bdff5c6e953fecc7ea3d16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb01f5eb-60fb-4e6e-bd73-d92ed6a310c1", "node_type": "1", "metadata": {"window": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. ", "original_text": "2. "}, "hash": "b01945dcebff162638338d7d4128696518042822474f781c88d7d8af0e002792", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The overall process is intuitively presented in Fig. ", "mimetype": "text/plain", "start_char_idx": 24819, "end_char_idx": 24872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb01f5eb-60fb-4e6e-bd73-d92ed6a310c1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f046e225-315a-4beb-af09-93f2c71a8b4c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is another zoomed-in satellite view, focusing on the location of the red points from Figure 9.  The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. ", "original_text": "The overall process is intuitively presented in Fig. "}, "hash": "1f4ab90e2a4d05b3fc87feccdaf9955e78ba58a5da5bb830e299ed9d30c6a744", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cca8f03e-62f7-4627-b425-ec1bd37fb8d4", "node_type": "1", "metadata": {"window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. ", "original_text": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. "}, "hash": "7d1347eb3a481d87ecefe2b1fa91bcda26c40e48bed86767232538a10702ceeb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 24872, "end_char_idx": 24875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cca8f03e-62f7-4627-b425-ec1bd37fb8d4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. ", "original_text": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb01f5eb-60fb-4e6e-bd73-d92ed6a310c1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The anomalous points are tightly clustered at what appears to be JFK Airport, indicating that the k-Means-based method has identified this location as an outlier relative to the main concentration of taxi pickups.\n *(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. ", "original_text": "2. "}, "hash": "b5268afb1be44088a41f656b4d39de1f7f0efaed6358fe041913e73d85baa35a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83c963f3-173a-44af-9b30-12410768cf18", "node_type": "1", "metadata": {"window": ")*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. ", "original_text": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. "}, "hash": "63e410b9c4261e0c8f115ca7e6d0da3a45e7149730e22e9c1fee60728f89b531", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. ", "mimetype": "text/plain", "start_char_idx": 24875, "end_char_idx": 25106, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83c963f3-173a-44af-9b30-12410768cf18", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. ", "original_text": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cca8f03e-62f7-4627-b425-ec1bd37fb8d4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "*(For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article. )*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. ", "original_text": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split. "}, "hash": "8d50bd37e78c7d5cfef2975e52948a0d96525400d81ab60b755cf3138232f537", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "beeb732d-44da-4004-af8c-57b0b256b787", "node_type": "1", "metadata": {"window": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n", "original_text": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. "}, "hash": "3a61b070a320fe165c7777e03b5fb4b1e9d20e61f6487d6ac52a8ec788883090", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. ", "mimetype": "text/plain", "start_char_idx": 25106, "end_char_idx": 25307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "beeb732d-44da-4004-af8c-57b0b256b787", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n", "original_text": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83c963f3-173a-44af-9b30-12410768cf18", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": ")*\n***\n\nthe level of searching the time is less.  Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. ", "original_text": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster. "}, "hash": "f3b62dd21012e0aa8df32cc4ad9fd672ce0e0acd33d3fef5386f9da97bca15de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "020cd499-ea07-4700-aef4-35a00419fae5", "node_type": "1", "metadata": {"window": "The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4. ", "original_text": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. "}, "hash": "9432cee76c7f3f57d39d4b5a6b32c115d6c275d22dcbfab08955531f43b75f21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. ", "mimetype": "text/plain", "start_char_idx": 25307, "end_char_idx": 25503, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "020cd499-ea07-4700-aef4-35a00419fae5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4. ", "original_text": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "beeb732d-44da-4004-af8c-57b0b256b787", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, the number of nodes at each split point is not important from the functionality point of view and computing overhead (see the experimental section) since their number is not high and, in practice, oscillates between 2 and 4 rather than assuming higher values.\n\n The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n", "original_text": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting. "}, "hash": "be157942c38885c889fd2f92cf8a074c4e3235a75be56ab1a413dc94515f5ba9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc034aa5-c066-4b14-8d40-a7b60f6044ed", "node_type": "1", "metadata": {"window": "2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. ", "original_text": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. "}, "hash": "0b7a3bfa04163194df19c68f474a597fc45695d0a81e27c4574a918272c2d663", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. ", "mimetype": "text/plain", "start_char_idx": 25503, "end_char_idx": 25634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc034aa5-c066-4b14-8d40-a7b60f6044ed", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. ", "original_text": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "020cd499-ea07-4700-aef4-35a00419fae5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The overall process is intuitively presented in Fig.  2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4. ", "original_text": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers. "}, "hash": "4662cf555f981c4570e5eebcec792805f3a65b22287675c83e313b4ee1cc4800", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89c48910-4d2c-49dd-b559-b7fca90e3e26", "node_type": "1", "metadata": {"window": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. ", "original_text": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. "}, "hash": "a9837b6b0eed97b4c2db869a6d6ed1e30b194583c4969c262cfb71ef613b7e1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. ", "mimetype": "text/plain", "start_char_idx": 25634, "end_char_idx": 25793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "89c48910-4d2c-49dd-b559-b7fca90e3e26", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. ", "original_text": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc034aa5-c066-4b14-8d40-a7b60f6044ed", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2.  If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. ", "original_text": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly. "}, "hash": "6996b591ba3b4e38ac626e206a0e0912f7b7672d0262154d60fdc21af75b4fd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d6e94f2-a820-4b2b-a696-3ef07200d793", "node_type": "1", "metadata": {"window": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points. ", "original_text": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. "}, "hash": "a6b875cb64e2a9cbf15f46fc384e3c05e8468d68968e71f594c8386f02f3b447", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. ", "mimetype": "text/plain", "start_char_idx": 25793, "end_char_idx": 25889, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d6e94f2-a820-4b2b-a696-3ef07200d793", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points. ", "original_text": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89c48910-4d2c-49dd-b559-b7fca90e3e26", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "If one starts to divide the points horizontally and then alternatively according to their x and y values, it is possible to note that the red point belongs to the central cluster (relatively near to its center) in the first split.  Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. ", "original_text": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level. "}, "hash": "5a2d62ac7dd2ec8188997f69e8817fcb24e7045bfedf87b0c13fc6e209e45629", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f1261eb-1126-4877-b364-26d618423dc1", "node_type": "1", "metadata": {"window": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. ", "original_text": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n"}, "hash": "c77ed0313aa767762892729518a35966582b72cce8be32e3f7cc3da8525c299f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. ", "mimetype": "text/plain", "start_char_idx": 25889, "end_char_idx": 26067, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f1261eb-1126-4877-b364-26d618423dc1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. ", "original_text": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d6e94f2-a820-4b2b-a696-3ef07200d793", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Next, again, it belongs to the second cluster (vertically), higher cluster (horizontally), and to the left cluster (vertically), to the lower cluster (horizontally), and, finally, to the left cluster.  The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points. ", "original_text": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case. "}, "hash": "5c17a9d6b3ff6d1703237148ef6f898e2d784cb495401885a5b20056aa512211", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d40418cb-6109-451d-b03a-0ea74209c68c", "node_type": "1", "metadata": {"window": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n", "original_text": "### 4. "}, "hash": "7bab2247c36e53dec69622e18e29b7ad44fbf4a6052ebf2582101fd68ab119ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n", "mimetype": "text/plain", "start_char_idx": 26067, "end_char_idx": 26187, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d40418cb-6109-451d-b03a-0ea74209c68c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n", "original_text": "### 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f1261eb-1126-4877-b364-26d618423dc1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The sum of its membership grades is around 3.5 \u2248 0.7 + 0.2 + 0.2 + 0.6 + 0.9 + 0.9, where the particular values are the membership grades to the clusters it belongs to at each stage of splitting.  It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. ", "original_text": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n"}, "hash": "a06509bdf77a1018e92a82d7fda80f3880e2b5d749ef13ff91967bd067069a80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2002ca29-c86b-4241-bc05-741d012195cd", "node_type": "1", "metadata": {"window": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. ", "original_text": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. "}, "hash": "86223b65c68ab56d10b1522f248c94039723e537a59c3e16a40f92cf9e668ebf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4. ", "mimetype": "text/plain", "start_char_idx": 26187, "end_char_idx": 26194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2002ca29-c86b-4241-bc05-741d012195cd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. ", "original_text": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d40418cb-6109-451d-b03a-0ea74209c68c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is noticeable that the values depict its real locations in relation to the cluster centers and are not binary but real numbers.  Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n", "original_text": "### 4. "}, "hash": "eb53e1ef781141ff6e6d6cc95999b264a10a81b03061616abf8af3cccb38da7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "992422c4-dcd4-451c-a26f-81e73e919ecc", "node_type": "1", "metadata": {"window": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. ", "original_text": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. "}, "hash": "c3ff7ae1146f016dd7cce9cdaf56ee6824a5f245c0143490ef3085b0aae969c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. ", "mimetype": "text/plain", "start_char_idx": 26194, "end_char_idx": 26351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "992422c4-dcd4-451c-a26f-81e73e919ecc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. ", "original_text": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2002ca29-c86b-4241-bc05-741d012195cd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, the method of division of each part of the tree is more intuitive because it divides the set into clusters in relation to data not totally randomly.  From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. ", "original_text": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted. "}, "hash": "92b4aebace62b4e34feddedcdffa045505c64b8b08dbc01fa96e1bff6000ad62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41c97767-d1f5-4860-8685-e50ca463c081", "node_type": "1", "metadata": {"window": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n", "original_text": "Each of the subsets contains 30,000 or 50,000 points. "}, "hash": "7f17f4624ad6cf66ef5ccbbfee6e40176d9cce3148ff38bce2a391370a828fb7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. ", "mimetype": "text/plain", "start_char_idx": 26351, "end_char_idx": 26469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41c97767-d1f5-4860-8685-e50ca463c081", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n", "original_text": "Each of the subsets contains 30,000 or 50,000 points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "992422c4-dcd4-451c-a26f-81e73e919ecc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "From one perspective the trees are \"wider\", namely the trees may have more nodes at each level.  From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. ", "original_text": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space. "}, "hash": "d8b821beda5589d2149d535ed31c2f3f49872c4126a9f2a3bb5eae843f7b7cc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48c2c7a7-bcfc-4c01-9430-dd26915ca5e0", "node_type": "1", "metadata": {"window": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1. ", "original_text": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. "}, "hash": "08294d6bc6b82d9aa1c9207e0f534df92bb843a7276e5ecaf17c761f95f2db5d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each of the subsets contains 30,000 or 50,000 points. ", "mimetype": "text/plain", "start_char_idx": 26469, "end_char_idx": 26523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48c2c7a7-bcfc-4c01-9430-dd26915ca5e0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1. ", "original_text": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41c97767-d1f5-4860-8685-e50ca463c081", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "From the second one, they are not deep and the process of searching using recursive procedures can be shortened as the trace does not go as deep as in the isolation forest case.  However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n", "original_text": "Each of the subsets contains 30,000 or 50,000 points. "}, "hash": "be74f963b73bb022f90bbe0596affe2c1609cd02f12fad3a7bb0bdd9af741e7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24a37579-6337-477a-b2be-6e81d6719c31", "node_type": "1", "metadata": {"window": "### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. ", "original_text": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n"}, "hash": "f5bf495892061a430554c5eab9f557a9a9e6639dc063cb1a365ce98675d552da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. ", "mimetype": "text/plain", "start_char_idx": 26523, "end_char_idx": 26688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24a37579-6337-477a-b2be-6e81d6719c31", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. ", "original_text": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48c2c7a7-bcfc-4c01-9430-dd26915ca5e0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, more number of comparisons at each node and membership calculations impact the time of the program execution.\n\n ### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1. ", "original_text": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates. "}, "hash": "1967777d620a11072ba69dbd11fe6781817a39d0729cbef19e8ebcc39704a6d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "025390f5-c04c-463a-9695-fbbeeffe8c96", "node_type": "1", "metadata": {"window": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs. ", "original_text": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. "}, "hash": "27408b9dee6689e6c30ff362daad890b10fee69a3a8993a8715f4c07838e93a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n", "mimetype": "text/plain", "start_char_idx": 26688, "end_char_idx": 26791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "025390f5-c04c-463a-9695-fbbeeffe8c96", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs. ", "original_text": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24a37579-6337-477a-b2be-6e81d6719c31", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 4.  Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. ", "original_text": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n"}, "hash": "c3f56dad98de7c18616ba432dec09025877f890919e135c839e4ccc64f34d72f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a4d232e-c6c9-434e-a6ff-dce450d6f2a3", "node_type": "1", "metadata": {"window": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no. ", "original_text": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. "}, "hash": "fb05c14e46afabae06f40ecafb7be66fc601739de15e1e7185b75fd1eab0b7e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. ", "mimetype": "text/plain", "start_char_idx": 26791, "end_char_idx": 26935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a4d232e-c6c9-434e-a6ff-dce450d6f2a3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no. ", "original_text": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "025390f5-c04c-463a-9695-fbbeeffe8c96", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Numerical experiments\n\nA series of experiments for isolation forest and k-Means isolation forest in-depth analysis using three datasets have been conducted.  The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs. ", "original_text": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256. "}, "hash": "d969e99f387c4821f312c69ecd19b2c3733c63f10e7aabdc53d65c7e2519153d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd758df8-44ac-433b-b3cf-0dfd5335b0d9", "node_type": "1", "metadata": {"window": "Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig. ", "original_text": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n"}, "hash": "c2e9a6b62921e2ed1ce1f6f374fb300aa6200702ce0a19d65c655f05f9b5eca0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. ", "mimetype": "text/plain", "start_char_idx": 26935, "end_char_idx": 27074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd758df8-44ac-433b-b3cf-0dfd5335b0d9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig. ", "original_text": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a4d232e-c6c9-434e-a6ff-dce450d6f2a3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The first of them is the set of randomly generated points with uniform distribution located in two-dimensional space.  Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no. ", "original_text": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest. "}, "hash": "288c749c8202af3997abd9731f13431e28a3bbf426cee240ea56d6446366a86c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1068031e-eb89-4b5b-a967-8b6d502a7d93", "node_type": "1", "metadata": {"window": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points. ", "original_text": "#### 4.1. "}, "hash": "c2975bd797d9a3cddb53873c5b73d195fc5e45567e8e16946854dbd9fbed56d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n", "mimetype": "text/plain", "start_char_idx": 27074, "end_char_idx": 27288, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1068031e-eb89-4b5b-a967-8b6d502a7d93", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points. ", "original_text": "#### 4.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd758df8-44ac-433b-b3cf-0dfd5335b0d9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Each of the subsets contains 30,000 or 50,000 points.  The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig. ", "original_text": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n"}, "hash": "b219a31c5b0690bbf7b2246d5c274730e39601e69c62a386210d98d65f07df55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "514fdee9-e1b6-477d-9cb7-43c657a4e1b7", "node_type": "1", "metadata": {"window": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. ", "original_text": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. "}, "hash": "45c4c8dc447f0c74db44e69ab73f0be199b57cac830dc324a48a9bb0f4cbcbee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 4.1. ", "mimetype": "text/plain", "start_char_idx": 27288, "end_char_idx": 27298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "514fdee9-e1b6-477d-9cb7-43c657a4e1b7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. ", "original_text": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1068031e-eb89-4b5b-a967-8b6d502a7d93", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The second one is the publicly available New York City Taxi Trip Data [56] containing records of New York taxi travels including times and geographical coordinates.  Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points. ", "original_text": "#### 4.1. "}, "hash": "12d6345dbaea33c5da41a4a08da0b27c35d40aab27258b97e0f71329189a7ce6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba0993d9-f918-4148-b4a0-2094c7981f8b", "node_type": "1", "metadata": {"window": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. ", "original_text": "Figs. "}, "hash": "52c80769e8d9e64a3a91d7c16d2b11a240aa2dd6f8a52a30486fb9c21ea66b42", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. ", "mimetype": "text/plain", "start_char_idx": 27298, "end_char_idx": 27508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba0993d9-f918-4148-b4a0-2094c7981f8b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. ", "original_text": "Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "514fdee9-e1b6-477d-9cb7-43c657a4e1b7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, we have analyzed the private dataset obtained from the logistic company operating in Europe.\n\n Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. ", "original_text": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated. "}, "hash": "56efbbf3e464839ac53e1edcaaab7baac06384e6dfb114ac9713e7e1c75075ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5e59972-454f-4c15-98c9-da93c9ed163c", "node_type": "1", "metadata": {"window": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. ", "original_text": "no. "}, "hash": "70f105ddc006ee923e5657776c7be65568dccfee2cf430e7be3b47904a6da2be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figs. ", "mimetype": "text/plain", "start_char_idx": 27508, "end_char_idx": 27514, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5e59972-454f-4c15-98c9-da93c9ed163c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. ", "original_text": "no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba0993d9-f918-4148-b4a0-2094c7981f8b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Following the recommendations presented in [3,4], the number of decision trees was set to t = 100 and the number of samples n = 128 or n = 256.  In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. ", "original_text": "Figs. "}, "hash": "156a1dfd2bf7c156630ebce07531ab769b1d922f53f81af5e4b3662b01d4755a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd27bc48-68b2-4d8b-8e2a-12135beaccc5", "node_type": "1", "metadata": {"window": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n", "original_text": "1, 3, and 6 present the results of experiments for 30,000 points while Fig. "}, "hash": "c4f53da850608dea3cb7f6bc759b6e7b1802ca35fe1aa529f832d9f81e82a791", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "no. ", "mimetype": "text/plain", "start_char_idx": 27514, "end_char_idx": 27518, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd27bc48-68b2-4d8b-8e2a-12135beaccc5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n", "original_text": "1, 3, and 6 present the results of experiments for 30,000 points while Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5e59972-454f-4c15-98c9-da93c9ed163c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In our case it is always used the first of the values to compare both methods, namely isolation forest and k-Means-based isolation forest.  It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. ", "original_text": "no. "}, "hash": "16e0aa2145ef97bb3d6b4019105268e65249d9adc14285c93b53d6870f87d612", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f23c2845-99fc-4875-8d84-ac48c2c7e148", "node_type": "1", "metadata": {"window": "#### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. ", "original_text": "2 depicts the results for 50,000 artificial points. "}, "hash": "22660ab8b84dfbdac9955bbaef175cf3dca9d7c8d05e795c2c40ab217aef97be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1, 3, and 6 present the results of experiments for 30,000 points while Fig. ", "mimetype": "text/plain", "start_char_idx": 27518, "end_char_idx": 27594, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f23c2845-99fc-4875-8d84-ac48c2c7e148", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. ", "original_text": "2 depicts the results for 50,000 artificial points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd27bc48-68b2-4d8b-8e2a-12135beaccc5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is worth noting that for comparison purposes all the experiments were conducted with the following constraints: All the trees have maximal depth set to 9 and there are always 100 trees used in each experiment.\n\n #### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n", "original_text": "1, 3, and 6 present the results of experiments for 30,000 points while Fig. "}, "hash": "04b869684d024b2cc7515124cf2b0feafae2234a28d66f3198db03d59e99399d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c860bc64-00bc-4125-8251-67ac597947aa", "node_type": "1", "metadata": {"window": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even. ", "original_text": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. "}, "hash": "61dd8c75cbd41f45e7cf5faa6e81f63f6ed0f4882f37e10192ed70596c313653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2 depicts the results for 50,000 artificial points. ", "mimetype": "text/plain", "start_char_idx": 27594, "end_char_idx": 27646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c860bc64-00bc-4125-8251-67ac597947aa", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even. ", "original_text": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f23c2845-99fc-4875-8d84-ac48c2c7e148", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.1.  Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. ", "original_text": "2 depicts the results for 50,000 artificial points. "}, "hash": "0b811ad94570ba3ab9ad4ef741cc89492dfe855665968975867ceaea66129c13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84a687c-6e0b-478b-8d50-95418be1f956", "node_type": "1", "metadata": {"window": "Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. ", "original_text": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. "}, "hash": "c1d5d823424b7924e51405a97e31801e353df7e28739f391e872bc869d7d5941", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. ", "mimetype": "text/plain", "start_char_idx": 27646, "end_char_idx": 27762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b84a687c-6e0b-478b-8d50-95418be1f956", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. ", "original_text": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c860bc64-00bc-4125-8251-67ac597947aa", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Artificial datasets\n\nIn the first series of experiments, four kinds of artificial sets containing points grouped in geometrical figures and a few points being located outside these figures have been generated.  Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even. ", "original_text": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated. "}, "hash": "02c81ac5764807d919b2c2540ce3208d2b6149fe2b5a61ea553d8cf71b35e409", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d442b561-a034-4f95-88fb-96765a2e5536", "node_type": "1", "metadata": {"window": "no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods. ", "original_text": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. "}, "hash": "6871f4c506c827ee11f5a8d61f75c10914ebc9dbcc4dc51c1201cd1bb6c2c365", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. ", "mimetype": "text/plain", "start_char_idx": 27762, "end_char_idx": 27881, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d442b561-a034-4f95-88fb-96765a2e5536", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods. ", "original_text": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84a687c-6e0b-478b-8d50-95418be1f956", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Figs.  no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. ", "original_text": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones. "}, "hash": "3027f1af52d97c585589cbf7b442fd570f1c7c8311300c74c6537fe3643d342a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e21ed5d9-14b7-4e01-8348-4b2ce66f9e30", "node_type": "1", "metadata": {"window": "1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig. ", "original_text": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n"}, "hash": "04be0e1e53d57fcf9f75952f96fb190491bd65f69fec46f347ae59ebfab29187", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. ", "mimetype": "text/plain", "start_char_idx": 27881, "end_char_idx": 28044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e21ed5d9-14b7-4e01-8348-4b2ce66f9e30", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig. ", "original_text": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d442b561-a034-4f95-88fb-96765a2e5536", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "no.  1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods. ", "original_text": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal. "}, "hash": "620cec6343d5dc6a688c7376df08312bb2efdc6af6d535316585bab15957de1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed44e311-4f5d-4b3c-85c7-328a961facf2", "node_type": "1", "metadata": {"window": "2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3. ", "original_text": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. "}, "hash": "99c47236723d37fd3b715035629a4aaec123799a6fa1cc5c6329c3638082261d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n", "mimetype": "text/plain", "start_char_idx": 28044, "end_char_idx": 28194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed44e311-4f5d-4b3c-85c7-328a961facf2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3. ", "original_text": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e21ed5d9-14b7-4e01-8348-4b2ce66f9e30", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1, 3, and 6 present the results of experiments for 30,000 points while Fig.  2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig. ", "original_text": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n"}, "hash": "d7512b5a5a71b29546068360d5cf9e0ed0e63017c944af447ee651cbf27c5dcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "054843df-52ff-45f6-89ea-c4eea8dcf9f2", "node_type": "1", "metadata": {"window": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. ", "original_text": "In turn, in the case of the isolation forest method, this distribution is more even. "}, "hash": "f5ed929af7fdd75d1826da22229bf4949876b0af37a44c54ab822243f151aabc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. ", "mimetype": "text/plain", "start_char_idx": 28194, "end_char_idx": 28398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "054843df-52ff-45f6-89ea-c4eea8dcf9f2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. ", "original_text": "In turn, in the case of the isolation forest method, this distribution is more even. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed44e311-4f5d-4b3c-85c7-328a961facf2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2 depicts the results for 50,000 artificial points.  The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3. ", "original_text": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries. "}, "hash": "6bffa9f76596928ce41f285d2fbecd655bafbf9b04f35630514262232b2156fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdbd05d5-d960-421a-ac11-7a295bb65aec", "node_type": "1", "metadata": {"window": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large. ", "original_text": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. "}, "hash": "6fbe0cd848bd88565799373ce6263e6e88ba2ec8e297b8cd61b92cf1ac7f61e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In turn, in the case of the isolation forest method, this distribution is more even. ", "mimetype": "text/plain", "start_char_idx": 28398, "end_char_idx": 28483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fdbd05d5-d960-421a-ac11-7a295bb65aec", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large. ", "original_text": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "054843df-52ff-45f6-89ea-c4eea8dcf9f2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The plots show that k-Means-based version of isolation forest does not mark the stripes inside figures as isolated.  Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. ", "original_text": "In turn, in the case of the isolation forest method, this distribution is more even. "}, "hash": "f17083489755dbd3a0b9695faa583cdb35e68941c8c233a877d5059c504c5d65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65c78eeb-c92d-430d-bbf7-3a99fb0c24ba", "node_type": "1", "metadata": {"window": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig. ", "original_text": "5\u20138) in the classification of both methods. "}, "hash": "80eda6ce1bb5f0b0f7cf303425c7cea4eb47bad86ea12a1a9b6b76f89b84fd1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. ", "mimetype": "text/plain", "start_char_idx": 28483, "end_char_idx": 28592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65c78eeb-c92d-430d-bbf7-3a99fb0c24ba", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig. ", "original_text": "5\u20138) in the classification of both methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdbd05d5-d960-421a-ac11-7a295bb65aec", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, it shows that the points located inside the geometrical figures are strictly \"normal\" and not isolated ones.  The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large. ", "original_text": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs. "}, "hash": "99282ccc53fb3bb5d2ad9c5d6aec875d51d46a6a53d140a033220572b15a5d43", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ac6a80a-4f11-4e20-bd2b-a9e5fe4012f5", "node_type": "1", "metadata": {"window": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4. ", "original_text": "The rank difference histogram is shown in Fig. "}, "hash": "ca23a23f60ae420fa3a99195ae065a9e4f551304b375f79e5405607e129ace77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5\u20138) in the classification of both methods. ", "mimetype": "text/plain", "start_char_idx": 28592, "end_char_idx": 28636, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ac6a80a-4f11-4e20-bd2b-a9e5fe4012f5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4. ", "original_text": "The rank difference histogram is shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65c78eeb-c92d-430d-bbf7-3a99fb0c24ba", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The plots (b) of all the figures show that the points laying outside the figures but in-between two figures are not \"strongly\" isolated according to our proposal.  However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig. ", "original_text": "5\u20138) in the classification of both methods. "}, "hash": "227ec047ae49c2e44e85938755cb8074aa0616dde8e6eb0b4da4d620f0e8b2a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0b029e6-7c23-4e10-b060-4e4e11c45955", "node_type": "1", "metadata": {"window": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n", "original_text": "3. "}, "hash": "4f1e3457845246e55a07a37e9fa996237ea61b5539c484ded36c55401654ad7d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rank difference histogram is shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 28636, "end_char_idx": 28683, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0b029e6-7c23-4e10-b060-4e4e11c45955", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ac6a80a-4f11-4e20-bd2b-a9e5fe4012f5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, the plots showing ranks of top 1000 most isolated points (j) show that these coordinates are also high in the hierarchy of isolated points.\n\n From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4. ", "original_text": "The rank difference histogram is shown in Fig. "}, "hash": "75b6323f043a26a825d1de5cd4921b6a9d71b06ebffeee9a8dd8a06d8e108767", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d30b05b-c9d3-4fe6-a203-8f8853947052", "node_type": "1", "metadata": {"window": "In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2. ", "original_text": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. "}, "hash": "732465713b460f36e7b2d74fb8f57880d48b02a33f357479cf411916181998c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 28683, "end_char_idx": 28686, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d30b05b-c9d3-4fe6-a203-8f8853947052", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2. ", "original_text": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0b029e6-7c23-4e10-b060-4e4e11c45955", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "From the analysis, it should be noted that the proposed method assigns a low isolation score to points positioned at the center of clusters, while the ratio increases when approaching cluster boundaries.  In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n", "original_text": "3. "}, "hash": "2e804da2ed657cb46d76e149762488a151003a21d0f83050c32d3819605d4273", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66c59365-8c54-4239-afbd-9c42e5a1493e", "node_type": "1", "metadata": {"window": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. ", "original_text": "This value is close to 30,000 and therefore, it seems to be too large. "}, "hash": "89d4c5deeb0741891f0e5d404905156a8f588f303da4d54b45eac999fb52feb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. ", "mimetype": "text/plain", "start_char_idx": 28686, "end_char_idx": 28846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "66c59365-8c54-4239-afbd-9c42e5a1493e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. ", "original_text": "This value is close to 30,000 and therefore, it seems to be too large. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d30b05b-c9d3-4fe6-a203-8f8853947052", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In turn, in the case of the isolation forest method, this distribution is more even.  This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2. ", "original_text": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000. "}, "hash": "7ada1c700e4b0379acea028dcb7b419b33bb8bca829b45ad088bf0cc783ac664", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37bd0b41-390d-4750-a1e1-1e3f47faa1ca", "node_type": "1", "metadata": {"window": "5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data.", "original_text": "The twenty points with the largest difference in rank are presented in Fig. "}, "hash": "5ecf6f92aa3012fa719fee5e31f14f712d006a3ffeb034efaab57edd00b45f59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This value is close to 30,000 and therefore, it seems to be too large. ", "mimetype": "text/plain", "start_char_idx": 28846, "end_char_idx": 28917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37bd0b41-390d-4750-a1e1-1e3f47faa1ca", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data.", "original_text": "The twenty points with the largest difference in rank are presented in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66c59365-8c54-4239-afbd-9c42e5a1493e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This observation is very well visible in charts (e) and (f) of all the figures presenting differences (Figs.  5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. ", "original_text": "This value is close to 30,000 and therefore, it seems to be too large. "}, "hash": "16068a4791b9159d320f8510c5573cffbbc7d757f39aa3abeb66e334789221e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d298f20f-25b8-49d2-80d9-b6aee2e4e155", "node_type": "1", "metadata": {"window": "The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. ", "original_text": "4. "}, "hash": "5e5b80757411508f55eacb02c655bad7a325f327fe41dc04e038b91eb3c6a3c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The twenty points with the largest difference in rank are presented in Fig. ", "mimetype": "text/plain", "start_char_idx": 28917, "end_char_idx": 28993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d298f20f-25b8-49d2-80d9-b6aee2e4e155", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. ", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37bd0b41-390d-4750-a1e1-1e3f47faa1ca", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "5\u20138) in the classification of both methods.  The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data.", "original_text": "The twenty points with the largest difference in rank are presented in Fig. "}, "hash": "43259385cf7266da8584b6d4cfb35f56db1c430b73ce18c730b7fe21c09bfd81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e92e371-083b-45a0-b807-9f316da28a89", "node_type": "1", "metadata": {"window": "3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12. ", "original_text": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n"}, "hash": "08287f819240bc2f94006a0a31f270cfe64f5e29e2d543b2c6f0f947f880fa11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 28993, "end_char_idx": 28996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e92e371-083b-45a0-b807-9f316da28a89", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12. ", "original_text": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d298f20f-25b8-49d2-80d9-b6aee2e4e155", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The rank difference histogram is shown in Fig.  3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. ", "original_text": "4. "}, "hash": "672a1967cc0b67ffbbedf58a344a117928e68f367e0ed2b8471d4853b035b434", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ec4d587-aa7b-4aee-84e7-da305bd9c74a", "node_type": "1", "metadata": {"window": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation).", "original_text": "#### 4.2. "}, "hash": "e72b1442e0b421d0bb579f0ccaf959351bf795ec788c8d53cc6ae04fa30e19bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n", "mimetype": "text/plain", "start_char_idx": 28996, "end_char_idx": 29145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ec4d587-aa7b-4aee-84e7-da305bd9c74a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation).", "original_text": "#### 4.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e92e371-083b-45a0-b807-9f316da28a89", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "3.  It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12. ", "original_text": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n"}, "hash": "af0625c78fa6409610db7530c093bec60fe0520269121b8f4efc50b1f0f5bbcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d12d92e-616d-4604-b4f4-b27c21fee84e", "node_type": "1", "metadata": {"window": "This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. ", "original_text": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. "}, "hash": "1293b5266848cecc6fb64476db61c0a909f3fa9d8991f58117a843ba2f278c87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 4.2. ", "mimetype": "text/plain", "start_char_idx": 29145, "end_char_idx": 29155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d12d92e-616d-4604-b4f4-b27c21fee84e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. ", "original_text": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ec4d587-aa7b-4aee-84e7-da305bd9c74a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It can be seen that the majority of the distribution mass is focused on small values, however, for up to 3 elements, the rank difference module is over 25,000.  This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation).", "original_text": "#### 4.2. "}, "hash": "7d7f92539c9cebf781097358127d065bbede20e10b50004998d51088acead5e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7482d2f-6f6b-4abe-9564-ada626322793", "node_type": "1", "metadata": {"window": "The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n", "original_text": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data."}, "hash": "f84e68e56cdbe88e313b122c249bb7c91e79eedec9bea20a09def0243adc4550", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. ", "mimetype": "text/plain", "start_char_idx": 29155, "end_char_idx": 29276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7482d2f-6f6b-4abe-9564-ada626322793", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n", "original_text": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d12d92e-616d-4604-b4f4-b27c21fee84e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This value is close to 30,000 and therefore, it seems to be too large.  The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. ", "original_text": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013. "}, "hash": "2d6e550984b0a2e52214ce2203fea2311212ad2bceb17297936b8181b9a06d53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58fdc78e-bd66-42c6-b387-ef073fff755a", "node_type": "1", "metadata": {"window": "4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. ", "original_text": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. "}, "hash": "765a501d88010d84e0a17d2c3e719facd7704771dc876d1f48d824f9b12d04b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data.", "mimetype": "text/plain", "start_char_idx": 29276, "end_char_idx": 29351, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58fdc78e-bd66-42c6-b387-ef073fff755a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. ", "original_text": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7482d2f-6f6b-4abe-9564-ada626322793", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The twenty points with the largest difference in rank are presented in Fig.  4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n", "original_text": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data."}, "hash": "c70fb49cb93f54a19ce1abfde5c5a2a34f80a25d4f3b5c845812acd7d4c46514", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dad32b0a-ca52-45be-a90e-6dc6883cfd8d", "node_type": "1", "metadata": {"window": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. ", "original_text": "12. "}, "hash": "465ff3de9b6619bd6761a2e58dd47783f7bedcb29f67e2f5ce07b899a98a6823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 29351, "end_char_idx": 30723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dad32b0a-ca52-45be-a90e-6dc6883cfd8d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. ", "original_text": "12. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58fdc78e-bd66-42c6-b387-ef073fff755a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "4.  These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. ", "original_text": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig. "}, "hash": "dd109fea4fcea69100985d98f00e873d655a262272ab87d8b17c3f2b6e985c14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56e36674-73b6-4b58-968e-c63c3e2fdaa2", "node_type": "1", "metadata": {"window": "#### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios. ", "original_text": "Histogram of rank differences (train transportation)."}, "hash": "6f3860da80ddcaa1319dd95bfc9274ff198d0cb89853df816e837a665a7643ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12. ", "mimetype": "text/plain", "start_char_idx": 30723, "end_char_idx": 30727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "56e36674-73b6-4b58-968e-c63c3e2fdaa2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios. ", "original_text": "Histogram of rank differences (train transportation)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dad32b0a-ca52-45be-a90e-6dc6883cfd8d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These are noise points that the k-Means-based method classified as isolated points, while the IF method made a completely different classification.\n\n #### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. ", "original_text": "12. "}, "hash": "ff6ce0e0b9e3fa259fa1aa204d4f08a81d6d6279fd4580ec459d0d27b2d5052b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8909f238-1656-4f90-85ae-02e921044b8e", "node_type": "1", "metadata": {"window": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros. ", "original_text": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. "}, "hash": "abc871772bd489f37969b9ed235d06efeb93cab32587bcbe948b305f96fe6b64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Histogram of rank differences (train transportation).", "mimetype": "text/plain", "start_char_idx": 30727, "end_char_idx": 30780, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8909f238-1656-4f90-85ae-02e921044b8e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros. ", "original_text": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56e36674-73b6-4b58-968e-c63c3e2fdaa2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.2.  NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios. ", "original_text": "Histogram of rank differences (train transportation)."}, "hash": "bc7b3a25ab99f12b36903823bb1d1923209bbafdf3f4f2e50dddb000fece6546", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de250697-202d-4df5-8e7e-8fb3f400b65a", "node_type": "1", "metadata": {"window": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). ", "original_text": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n"}, "hash": "8248a432037f9504fd98dd3753504170afcd606e2ddf72cabe16f60c16594aa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. ", "mimetype": "text/plain", "start_char_idx": 30780, "end_char_idx": 30916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de250697-202d-4df5-8e7e-8fb3f400b65a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). ", "original_text": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8909f238-1656-4f90-85ae-02e921044b8e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "NYC taxi trip data\n\nThis dataset is a collection of randomly chosen 3,386,426 records of NYC Taxi Trip Data set in 2013.  Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros. ", "original_text": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset. "}, "hash": "c5f6999be0d0a66cfd0835dfc689942a850b0414d4bc78d238df6d131226276b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42170f3-1e69-4ed7-bc12-ba7f7d8d385d", "node_type": "1", "metadata": {"window": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n", "original_text": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. "}, "hash": "a643837895864c1484371955aaf4a49c9c3683d653dbc621ace3345d9a3754cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n", "mimetype": "text/plain", "start_char_idx": 30916, "end_char_idx": 31085, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e42170f3-1e69-4ed7-bc12-ba7f7d8d385d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n", "original_text": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de250697-202d-4df5-8e7e-8fb3f400b65a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Each record has\n\n**Table 1**\n**Example of data from the NYC Taxi Trip Data. **\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). ", "original_text": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n"}, "hash": "02af6c7d8c3228b40278f697108fda783cfe12cded91f0129b4cd5ced33c20bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93b32ee1-2610-40ba-819c-bfb912c24d55", "node_type": "1", "metadata": {"window": "12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). ", "original_text": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. "}, "hash": "056ea2c9a7b7f4694847f9edba65954ed34c2e413ef5781c179ad903e7b02ba5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. ", "mimetype": "text/plain", "start_char_idx": 31085, "end_char_idx": 31369, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93b32ee1-2610-40ba-819c-bfb912c24d55", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). ", "original_text": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42170f3-1e69-4ed7-bc12-ba7f7d8d385d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Medallion | Hack license | Vendor id | Rate code | Store and fwd flag | Pickup datetime | Dropoff datetime | Passenger count | Trip time in secs | Trip distance | Pickup longitude | Pickup latitude | Drop-off longitude | Drop-off latitude |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 0 | 0 | 0 | 1 | 0 | 1 956 600 | 1 957 200 | 1 | 600 | 1.51 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 200 | 1 957 620 | 1 | 420 | 1.43 | -736.5 | 40.744366 | 0 | 0 |\n| 0 | 0 | 0 | 1 | 0 | 1 957 680 | 1 959 000 | 1 | 1320 | 4.81 | -736.5 | 40.744366 | 0 | 0 |\n| 1 | 1 | 0 | 1 | 0 | 732 660 | 733 620 | 1 | 960 | 1.56 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 1 | 1 | 0 | 1 | 0 | 731 340 | 732 420 | 1 | 1080 | 4.46 | -79.822159 | 42.86293 | -79.822159 | 42.86293 |\n| 2 | 2 | 0 | 1 | 0 | 2 282 880 | 2 283 600 | 2 | 720 | 3.63 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 2 | 2 | 0 | 1 | 0 | 2 283 660 | 2 284 140 | 2 | 480 | 1.49 | -78.800003 | 40.743423 | -73.975182 | 40.770119 |\n| 3 | 3 | 0 | 1 | 0 | 1 936 200 | 1 936 620 | 6 | 420 | 1.25 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 3 | 3 | 0 | 1 | 0 | 1 935 240 | 1 935 780 | 6 | 540 | 1.36 | -74.713081 | 39.992546 | -74.713081 | 39.992546 |\n| 4 | 4 | 0 | 1 | 0 | 1 003 920 | 1 004 520 | 1 | 600 | 2.34 | -74.67514 | 41.908611 | -74.67514 | 41.908611 |\n\n***\n**Fig.  12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n", "original_text": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude. "}, "hash": "75e0d69011e8565f7a4ea4a75f40ae1246de547606602a91c00be11563246e2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d404bb7-e814-4a5d-a96a-fb4ce5450301", "node_type": "1", "metadata": {"window": "Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form. ", "original_text": "We have conducted experiment in two scenarios. "}, "hash": "d85d75137e465d49896f57d0520a34a6fad3b2adedd084d8d79326d5d605a90e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. ", "mimetype": "text/plain", "start_char_idx": 31369, "end_char_idx": 31555, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d404bb7-e814-4a5d-a96a-fb4ce5450301", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form. ", "original_text": "We have conducted experiment in two scenarios. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93b32ee1-2610-40ba-819c-bfb912c24d55", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "12.  Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). ", "original_text": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013. "}, "hash": "e8edad3144232d6a7cc77ce7057534dc78a904e27c904867751037cbdb898921", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e8f5954-ce1a-4a1c-8126-450c1ccccf17", "node_type": "1", "metadata": {"window": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records. ", "original_text": "In the first one, all the missing values have been filled by zeros. "}, "hash": "ddf3a21d03796da1032d6ae3be826cbb47e7b5887d8f6ee8eceab9c226c614bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We have conducted experiment in two scenarios. ", "mimetype": "text/plain", "start_char_idx": 31555, "end_char_idx": 31602, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e8f5954-ce1a-4a1c-8126-450c1ccccf17", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records. ", "original_text": "In the first one, all the missing values have been filled by zeros. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d404bb7-e814-4a5d-a96a-fb4ce5450301", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Histogram of rank differences (train transportation). ** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form. ", "original_text": "We have conducted experiment in two scenarios. "}, "hash": "a60011df21563ac4d3ed65951295fc2233997818ccb61e89bebe7ce1ca46dab7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7431282b-5cad-4f81-bb9c-f6ad66498c91", "node_type": "1", "metadata": {"window": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. ", "original_text": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). "}, "hash": "10aa44b43e2f8c6bc2e3da2cdf2d241d8f099dd32b1d75d17aa657ea438c04a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the first one, all the missing values have been filled by zeros. ", "mimetype": "text/plain", "start_char_idx": 31602, "end_char_idx": 31670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7431282b-5cad-4f81-bb9c-f6ad66498c91", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. ", "original_text": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e8f5954-ce1a-4a1c-8126-450c1ccccf17", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This bar chart displays the frequency of differences in anomaly rankings between the two methods for a train transportation dataset.  The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records. ", "original_text": "In the first one, all the missing values have been filled by zeros. "}, "hash": "5deec2768eff495c8a2d3c1d0a22746731334ac43c60825312a6c4abe0184239", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fde66daf-cc25-44d5-8bab-24fa54c0bd48", "node_type": "1", "metadata": {"window": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. ", "original_text": "The example of data after conversion to numbers is presented in Table 1.\n\n"}, "hash": "569f78613cc8a5704c916b5edd25ee30af9d344a6011905735079626457ddcbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). ", "mimetype": "text/plain", "start_char_idx": 31670, "end_char_idx": 31849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fde66daf-cc25-44d5-8bab-24fa54c0bd48", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. ", "original_text": "The example of data after conversion to numbers is presented in Table 1.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7431282b-5cad-4f81-bb9c-f6ad66498c91", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The distribution is spread out across a wide range of rank differences, indicating significant disagreement between the methods on which data points are most anomalous.\n ***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. ", "original_text": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records). "}, "hash": "4912644366e65ad2d05b9f4ed1529fc5b19336de1aaba37a40a598cf833ffeec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "caa6c73e-a710-4bef-ab7d-70685217d6a2", "node_type": "1", "metadata": {"window": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n", "original_text": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). "}, "hash": "fae70a4667ccf5b2afd5d76aa0c7c517621c7c2796138a1b6b2e998fe8828661", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The example of data after conversion to numbers is presented in Table 1.\n\n", "mimetype": "text/plain", "start_char_idx": 31849, "end_char_idx": 31923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "caa6c73e-a710-4bef-ab7d-70685217d6a2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n", "original_text": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fde66daf-cc25-44d5-8bab-24fa54c0bd48", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nthe following attributes: medallion (anonymized), hack license (anonymized), vendor id, rate code, store and fwd flag, pickup datetime, dropoff datetime, passenger count, trip time in secs, trip distance, pickup longitude, pickup latitude, drop-off longitude, drop-off latitude.  To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. ", "original_text": "The example of data after conversion to numbers is presented in Table 1.\n\n"}, "hash": "332706df80746107aba3941456dda0c7c4e9efc61a0bbf015578f557c708cda2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "994812c7-a3e7-4e1e-bde5-414884c94cd9", "node_type": "1", "metadata": {"window": "We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. ", "original_text": "However, it is difficult to present a concise summary of the results in a graphical form. "}, "hash": "8b2bb93b5139cef68bc3d6e321ad9924f5247204c546e789951fc849bb8a7938", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). ", "mimetype": "text/plain", "start_char_idx": 31923, "end_char_idx": 32039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "994812c7-a3e7-4e1e-bde5-414884c94cd9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. ", "original_text": "However, it is difficult to present a concise summary of the results in a graphical form. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "caa6c73e-a710-4bef-ab7d-70685217d6a2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "To the need of our calculations the string values were encoded as the positions in dictionary, date time values were converted to the numbers of seconds from the beginning of year 2013.  We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n", "original_text": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated). "}, "hash": "6e5384ea2ec53326b4da568246bb3fd353325bc9dbd53427053668018d1a8d16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb630d92-8541-4922-97bc-54285d822023", "node_type": "1", "metadata": {"window": "In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig. ", "original_text": "Hence, we restrict our presentation to the two-dimensional, geographical records. "}, "hash": "9b47d09ece70675b6c49230a9502aa767bcbbd48c075c7c2a36b95041d8ef73f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, it is difficult to present a concise summary of the results in a graphical form. ", "mimetype": "text/plain", "start_char_idx": 32039, "end_char_idx": 32129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb630d92-8541-4922-97bc-54285d822023", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig. ", "original_text": "Hence, we restrict our presentation to the two-dimensional, geographical records. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "994812c7-a3e7-4e1e-bde5-414884c94cd9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "We have conducted experiment in two scenarios.  In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. ", "original_text": "However, it is difficult to present a concise summary of the results in a graphical form. "}, "hash": "17b176ce786b904c53dcce0effe4c1209f9948e424194e9a8f96c485e9b681cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71f51a67-03ce-48c6-b414-1989af0b89aa", "node_type": "1", "metadata": {"window": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9. ", "original_text": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. "}, "hash": "49bfc0cb914889eb1ae72c2a9fdc76aebacdc1411094dfc61bfd9d0359b8fda0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hence, we restrict our presentation to the two-dimensional, geographical records. ", "mimetype": "text/plain", "start_char_idx": 32129, "end_char_idx": 32211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71f51a67-03ce-48c6-b414-1989af0b89aa", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9. ", "original_text": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb630d92-8541-4922-97bc-54285d822023", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In the first one, all the missing values have been filled by zeros.  During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig. ", "original_text": "Hence, we restrict our presentation to the two-dimensional, geographical records. "}, "hash": "9239a82ff72940f00d84356c8d165b9439801b562c1a67fdc147d93c244c49b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb43c418-87f8-47e0-a919-bf28d57554ad", "node_type": "1", "metadata": {"window": "The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. ", "original_text": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. "}, "hash": "ca35893c64cc8d13737521d3a85ca02a17cead9f013a94c70972d9d04c7d8c48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. ", "mimetype": "text/plain", "start_char_idx": 32211, "end_char_idx": 32432, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb43c418-87f8-47e0-a919-bf28d57554ad", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. ", "original_text": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71f51a67-03ce-48c6-b414-1989af0b89aa", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "During the latter one, to present the results graphically, the experiments have been conducted for non-empty pickup longitude and latitude geographic positions (737,462 records).  The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9. ", "original_text": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated. "}, "hash": "bc8e8e536bd9bcf20d1b792d485bafa635c8af503ee97728a050e3629668962c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f51734c-c70f-4d94-b705-dfce08e71607", "node_type": "1", "metadata": {"window": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs. ", "original_text": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n"}, "hash": "11422b1f573b6847e4b9f6929c9232bbc53c84772f3a062d4cafb7a21827b0a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. ", "mimetype": "text/plain", "start_char_idx": 32432, "end_char_idx": 32610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f51734c-c70f-4d94-b705-dfce08e71607", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs. ", "original_text": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb43c418-87f8-47e0-a919-bf28d57554ad", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The example of data after conversion to numbers is presented in Table 1.\n\n In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. ", "original_text": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively. "}, "hash": "53f782d3bdc1b64800c1de7ef17da375ec2750ff09344aa40371badb3671521c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bca18533-b353-4e90-a8d4-d5ba418954ec", "node_type": "1", "metadata": {"window": "However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. ", "original_text": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. "}, "hash": "ee5f2373ecbf2cd5f0400ab38bda2c6d8550d00b06c0bd17c145be7df11e000e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n", "mimetype": "text/plain", "start_char_idx": 32610, "end_char_idx": 32747, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bca18533-b353-4e90-a8d4-d5ba418954ec", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. ", "original_text": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f51734c-c70f-4d94-b705-dfce08e71607", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In the first case, for instance, all the records containing the negative values were marked as abnormal (isolated).  However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs. ", "original_text": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n"}, "hash": "03233c850c0bd230945e26d9316c298b36efa142cbdde52e9cd0fa6523afd12e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "719f864a-9809-4624-ab51-95cd23282926", "node_type": "1", "metadata": {"window": "Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n", "original_text": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig. "}, "hash": "2f3837111735a01de19bbdafbd44449685daf8228d3b970548a75946d1e9240e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. ", "mimetype": "text/plain", "start_char_idx": 32747, "end_char_idx": 32911, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "719f864a-9809-4624-ab51-95cd23282926", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n", "original_text": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bca18533-b353-4e90-a8d4-d5ba418954ec", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, it is difficult to present a concise summary of the results in a graphical form.  Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. ", "original_text": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method. "}, "hash": "9f59757c45dde890dacca2e99b98623ceb1ce5460bcca1601725ed468dee2499", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d191f97-3f47-4b89-921c-dac456f72311", "node_type": "1", "metadata": {"window": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3. ", "original_text": "9. "}, "hash": "db9f1f6be79c555d6921e0ab9f5cabe7cf3d78139ec234fa1adf50ba95ec8201", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig. ", "mimetype": "text/plain", "start_char_idx": 32911, "end_char_idx": 33004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d191f97-3f47-4b89-921c-dac456f72311", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3. ", "original_text": "9. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "719f864a-9809-4624-ab51-95cd23282926", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hence, we restrict our presentation to the two-dimensional, geographical records.  What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n", "original_text": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig. "}, "hash": "a0dbfa9cf6eb21c79481750c3b526e66d9100ab1562be3fc36a9ab196e62c20a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a1b6010-f8d5-48b9-9e33-25353da78e28", "node_type": "1", "metadata": {"window": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. ", "original_text": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. "}, "hash": "c8b9c629317c233745b8b37217aebc01b5594547e1d2b3232e05ecf2ac2f6657", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9. ", "mimetype": "text/plain", "start_char_idx": 33004, "end_char_idx": 33007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a1b6010-f8d5-48b9-9e33-25353da78e28", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. ", "original_text": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d191f97-3f47-4b89-921c-dac456f72311", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "What is important here is that for 99.98% records with zero values replacing the empty fields k-Means-based isolation forest returns the minimal possible value, i.e., 0 which means that the records are strictly isolated.  On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3. ", "original_text": "9. "}, "hash": "7d9eb322fec843f5470311accdbfcc70df1e718a4cc3cf1aee526e0fae99ce69", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6528c2a-4991-4aae-ba99-0d9875bc518e", "node_type": "1", "metadata": {"window": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments. ", "original_text": "Two next plots, Figs. "}, "hash": "d97a18043a147f2c4b6b0c0c6c6a4ae692f689e9452fb3976c1b9bab559de185", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. ", "mimetype": "text/plain", "start_char_idx": 33007, "end_char_idx": 33133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e6528c2a-4991-4aae-ba99-0d9875bc518e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments. ", "original_text": "Two next plots, Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a1b6010-f8d5-48b9-9e33-25353da78e28", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "On the other hand isolation forest for 99.56% records returns the isolation score about 0.46 which is in-between minimal and maximal values, namely 0.41 and 0.502, respectively.  This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. ", "original_text": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible. "}, "hash": "c5f904c5fcc9a7ef409a827543db04ee9281e2c60d51ed87977c00c793f0a507", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de0d2b55-80a4-4daa-b375-7cf27fb36e59", "node_type": "1", "metadata": {"window": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports. ", "original_text": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. "}, "hash": "7a7a9c1eef7001782997bbf7420d8448fca54f5028e5befb770bfd4538034c72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Two next plots, Figs. ", "mimetype": "text/plain", "start_char_idx": 33133, "end_char_idx": 33155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de0d2b55-80a4-4daa-b375-7cf27fb36e59", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports. ", "original_text": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6528c2a-4991-4aae-ba99-0d9875bc518e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "This means that the proposed innovative aspect highly improves the isolation forest giving more intuitive understanding of the results.\n\n In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments. ", "original_text": "Two next plots, Figs. "}, "hash": "e75ded9cf2e2337300ded9bbe757f08b6fd4702924384d2b7d3c0dde08193b04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8d8e8f83-ef4d-49c7-ab78-8dbcca6096d5", "node_type": "1", "metadata": {"window": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. ", "original_text": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n"}, "hash": "dfa07fc6d79a254219017abff0fb0d236426745507c276dc18ce4831b80635aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. ", "mimetype": "text/plain", "start_char_idx": 33155, "end_char_idx": 33342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8d8e8f83-ef4d-49c7-ab78-8dbcca6096d5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. ", "original_text": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de0d2b55-80a4-4daa-b375-7cf27fb36e59", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In the second case, where only pickup longitude and latitude values were analyzed, the enhancement of IF significantly improves the original version of the method.  First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports. ", "original_text": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area. "}, "hash": "8d4ba07d4715f0fb9e831351f8c434a1f8e864565c120ea8ec1012046a0b6ea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c0117b5-128f-4e72-8258-80db97a738f1", "node_type": "1", "metadata": {"window": "9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values. ", "original_text": "#### 4.3. "}, "hash": "593a904031c844c0656aaec716e567530445cc2ea5a8bbecedec99a78743f517", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n", "mimetype": "text/plain", "start_char_idx": 33342, "end_char_idx": 33527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c0117b5-128f-4e72-8258-80db97a738f1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values. ", "original_text": "#### 4.3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8d8e8f83-ef4d-49c7-ab78-8dbcca6096d5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "First, it classifies the outside (in relation to New York City) points as outliers, see Fig.  9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. ", "original_text": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n"}, "hash": "3fcefc662f5a0ff764c4c9d00f5817495a300995481858ef205abe9c91d942af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abf618af-c22b-40fc-be97-40ee95ecee63", "node_type": "1", "metadata": {"window": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. ", "original_text": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. "}, "hash": "cd238de280dbe0131c500d5da4af294b9cfa95f5d3b50a9457eeb038ffa323b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 4.3. ", "mimetype": "text/plain", "start_char_idx": 33527, "end_char_idx": 33537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "abf618af-c22b-40fc-be97-40ee95ecee63", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. ", "original_text": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c0117b5-128f-4e72-8258-80db97a738f1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "9.  We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values. ", "original_text": "#### 4.3. "}, "hash": "8ac70aca193a03894a2f2e6df004b1fb0dd4f54cc6b71f6f8e80ac8cc47dad5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64971141-d5b3-4b01-99b3-00dc66f5adeb", "node_type": "1", "metadata": {"window": "Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n", "original_text": "There were included two datasets in our experiments. "}, "hash": "5ebd5b713c69d373a65cc698551ff149ec65e04fcbb14cfaea20b0a721992a44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. ", "mimetype": "text/plain", "start_char_idx": 33537, "end_char_idx": 33734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64971141-d5b3-4b01-99b3-00dc66f5adeb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n", "original_text": "There were included two datasets in our experiments. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abf618af-c22b-40fc-be97-40ee95ecee63", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "We do not present similar graphics for the whole points area with respect to isolation forest since the points are invisible.  Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. ", "original_text": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used. "}, "hash": "c455fceb51c8a368bd3cf31c92fc1dffbde3969a4bd00a731c43e8fdeeac9d55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74445e1b-7741-4be4-942f-0913b12c3cbb", "node_type": "1", "metadata": {"window": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n", "original_text": "The first was the set of chosen ship transportation data and the second was related to train transports. "}, "hash": "5eb764a7e01c1e784ff653e1d2a1297a113cec7117ab5c081c0c6d3130254621", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There were included two datasets in our experiments. ", "mimetype": "text/plain", "start_char_idx": 33734, "end_char_idx": 33787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74445e1b-7741-4be4-942f-0913b12c3cbb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n", "original_text": "The first was the set of chosen ship transportation data and the second was related to train transports. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64971141-d5b3-4b01-99b3-00dc66f5adeb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Two next plots, Figs.  10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n", "original_text": "There were included two datasets in our experiments. "}, "hash": "6d451dcc7a9fd923b34a9cd3fdd385447f13d8f92e4a2ad77a879f9b30f6531f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9287d89-ef37-4cf1-a47c-2da0a4aa20de", "node_type": "1", "metadata": {"window": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca. ", "original_text": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. "}, "hash": "195c6d1499925df1599f9d826510990eb288d70853d2b3d4f1717e954b41a779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first was the set of chosen ship transportation data and the second was related to train transports. ", "mimetype": "text/plain", "start_char_idx": 33787, "end_char_idx": 33892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9287d89-ef37-4cf1-a47c-2da0a4aa20de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca. ", "original_text": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74445e1b-7741-4be4-942f-0913b12c3cbb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "10 and 11, show that k-Means-based version of isolation forest marks the outside points as outliers while isolation forest seeks such kind of points more likely inside the New York area.  Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n", "original_text": "The first was the set of chosen ship transportation data and the second was related to train transports. "}, "hash": "4b9fd710ec1dc7ae2ef965661fa7c72bab7f2c45f62aa9eeed71d04aba5cc813", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cff0d97-a904-458d-98c4-82003efcff16", "node_type": "1", "metadata": {"window": "#### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca. ", "original_text": "All the categorical string values were converted to the integral values. "}, "hash": "f7e1b677803f1f52503038a2c42850183cf3c0ebf6b7e4b17adc93f5307a64c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. ", "mimetype": "text/plain", "start_char_idx": 33892, "end_char_idx": 34271, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8cff0d97-a904-458d-98c4-82003efcff16", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca. ", "original_text": "All the categorical string values were converted to the integral values. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9287d89-ef37-4cf1-a47c-2da0a4aa20de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Here, it is observable that the above-discussed property of k-Means-based IF that it looks for outliers more outside the region of interest is important in many areas of applications.\n\n #### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca. ", "original_text": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time. "}, "hash": "26139e6187236f3f8dc077bd548b65ebc031e6da4b958f79cbb5387ab75a969b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d12615b5-a58b-4734-9409-15876c8ec0be", "node_type": "1", "metadata": {"window": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. ", "original_text": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. "}, "hash": "ae17fc25dd5f15420dbc09f3de6be4a448fc1ddbfb708858962c2fc3e606a1fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All the categorical string values were converted to the integral values. ", "mimetype": "text/plain", "start_char_idx": 34271, "end_char_idx": 34344, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d12615b5-a58b-4734-9409-15876c8ec0be", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. ", "original_text": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cff0d97-a904-458d-98c4-82003efcff16", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.3.  Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca. ", "original_text": "All the categorical string values were converted to the integral values. "}, "hash": "23ec3215623c0254bd702d3cbc8ad5d849d25d59d6c4515ec9035280f9f19e5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c25d32e7-bacc-4f63-a660-c209defec76c", "node_type": "1", "metadata": {"window": "There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13. ", "original_text": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n"}, "hash": "c35c427cea498029b76d9ed8d3f453c44d2d5ac632451d96ad352bdb11e851a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. ", "mimetype": "text/plain", "start_char_idx": 34344, "end_char_idx": 34710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c25d32e7-bacc-4f63-a660-c209defec76c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13. ", "original_text": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d12615b5-a58b-4734-9409-15876c8ec0be", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Intermodal transportation dataset\n\nIn the series of experiments presented here, the datasets of train and ship transportation data of one of European intermodal transport companies have been used.  There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. ", "original_text": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date. "}, "hash": "4de8ff9d971a2cb069be13f5b6ba9f528a69cd170c1daba49f16921f3fce784b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cffd9770-7f29-49ce-8e8d-eeb652545b85", "node_type": "1", "metadata": {"window": "The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. ", "original_text": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n"}, "hash": "36a2278026c664a95e665aa83ebc7de3b0331574bddef6478a16c68111a18b66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n", "mimetype": "text/plain", "start_char_idx": 34710, "end_char_idx": 34801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cffd9770-7f29-49ce-8e8d-eeb652545b85", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. ", "original_text": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c25d32e7-bacc-4f63-a660-c209defec76c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There were included two datasets in our experiments.  The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13. ", "original_text": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n"}, "hash": "6fe5f1750cddfc77f7f1184ea4874feb7554be2ee05e59941fdf7ab4900ff1fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10718aab-8bd9-41a0-891d-a2ae7f3977e5", "node_type": "1", "metadata": {"window": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4. ", "original_text": "An interesting observation is that the clustering-based method offers much more different rank values (ca. "}, "hash": "809f25e732faac2df71dff20adddbdb51319b869405a503d238dc47db69baeb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n", "mimetype": "text/plain", "start_char_idx": 34801, "end_char_idx": 35002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10718aab-8bd9-41a0-891d-a2ae7f3977e5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4. ", "original_text": "An interesting observation is that the clustering-based method offers much more different rank values (ca. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cffd9770-7f29-49ce-8e8d-eeb652545b85", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The first was the set of chosen ship transportation data and the second was related to train transports.  The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. ", "original_text": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n"}, "hash": "6d9d48bc70b2fc7f74cb8f6c3a762156c346d3ff74a1476c57931b234a0c640a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08e463aa-e288-48c4-81f8-6d91b24150de", "node_type": "1", "metadata": {"window": "All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results. ", "original_text": "12,500) of anomaly scores than IF (ca. "}, "hash": "1ea5d32683bdd4c069348ddf8f3aec3c47bfb03220cb44b6fb5da4aa229ce616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An interesting observation is that the clustering-based method offers much more different rank values (ca. ", "mimetype": "text/plain", "start_char_idx": 35002, "end_char_idx": 35109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "08e463aa-e288-48c4-81f8-6d91b24150de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results. ", "original_text": "12,500) of anomaly scores than IF (ca. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10718aab-8bd9-41a0-891d-a2ae7f3977e5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The first set we have chosen was the data containing 26,384 records with the following attributes: service group code, service type, unit category, unit type, unit kind, ride type id, departure port, arrival port, and the differences between the time the unit has arrived to port and departure, arrival, departure from port, and minimal order og, i.e., minimal realization time.  All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4. ", "original_text": "An interesting observation is that the clustering-based method offers much more different rank values (ca. "}, "hash": "278cf9f4be192f1f30e4ab7cad537ae479fd7c05b77bbc6009eb395fdea7ccad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "857db02b-f5c8-4182-bb05-3fc6e8472623", "node_type": "1", "metadata": {"window": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way. ", "original_text": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. "}, "hash": "43cbc90a27df2e2912b128ae7f5d894f79bb5b9cf48a7d11f8e19fc34ff405f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12,500) of anomaly scores than IF (ca. ", "mimetype": "text/plain", "start_char_idx": 35109, "end_char_idx": 35148, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "857db02b-f5c8-4182-bb05-3fc6e8472623", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way. ", "original_text": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08e463aa-e288-48c4-81f8-6d91b24150de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "All the categorical string values were converted to the integral values.  Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results. ", "original_text": "12,500) of anomaly scores than IF (ca. "}, "hash": "ae615d6cc002f8ee7969ff0ee26c57c2a80f6d6383e1cd0b7c310fdafda40c44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd063f68-30ad-4e2c-8529-b357e5020f8e", "node_type": "1", "metadata": {"window": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data. ", "original_text": "12 and 13. "}, "hash": "18708be3df87f89f202abe84b171222c90bef8838508666c69fb16e549677bc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. ", "mimetype": "text/plain", "start_char_idx": 35148, "end_char_idx": 35272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd063f68-30ad-4e2c-8529-b357e5020f8e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data. ", "original_text": "12 and 13. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "857db02b-f5c8-4182-bb05-3fc6e8472623", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Similarly, the data containing the following train transportation info have been prepared: service group code, service type, unit category, unit type, unit kind, departure station name, arrival station name, and the differences between the time a unit has arrived the station and train departure, train arrival, unit departure arrival station, and nearest due date.  The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way. ", "original_text": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs. "}, "hash": "3545b2b294a670773c0f62db758fe35b9b6fa83bc1109e3f220e9c52b27baa7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5803cd39-8dda-4b51-917a-664d0d94ac32", "node_type": "1", "metadata": {"window": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig. ", "original_text": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. "}, "hash": "9932752ea585c2cc3d8c2fe9bb26bde2c677c9e5a0c8c57e8331f322c3cbc5c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12 and 13. ", "mimetype": "text/plain", "start_char_idx": 35272, "end_char_idx": 35283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5803cd39-8dda-4b51-917a-664d0d94ac32", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig. ", "original_text": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd063f68-30ad-4e2c-8529-b357e5020f8e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The examples of the data contained in these two datasets are presented in Tables 2 and 3.\n\n Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data. ", "original_text": "12 and 13. "}, "hash": "244e1fb5911dd353804aa1eda6dd3d998d67778c90a51c883da7b9cfbe27dee5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cad4fbd-6ae3-4876-89f4-0640a912dfc3", "node_type": "1", "metadata": {"window": "An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work. ", "original_text": "In particular, this fact is noticeable in Table 4. "}, "hash": "82b798d1f8d04898d52cde41681bbd617bef3ab5cc8f881e19500ae237871f30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. ", "mimetype": "text/plain", "start_char_idx": 35283, "end_char_idx": 35441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cad4fbd-6ae3-4876-89f4-0640a912dfc3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work. ", "original_text": "In particular, this fact is noticeable in Table 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5803cd39-8dda-4b51-917a-664d0d94ac32", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Both IF and the proposed approach classifies the records with the values like date train arrival difference, unit departure arrival station difference, and nearest due date difference being negative.\n\n An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig. ", "original_text": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points. "}, "hash": "627f5c095d6587d24c2a4cca2799e81c95a5838bcb674eb66598f3f5a1cbe472", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b98240-23f2-4550-b8e8-3a142bb4e0a7", "node_type": "1", "metadata": {"window": "12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs. ", "original_text": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results. "}, "hash": "1b37e9e8e2c9bf2129fa03ededbc0cc5468be961ef2603412d8224ea6943ab84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, this fact is noticeable in Table 4. ", "mimetype": "text/plain", "start_char_idx": 35441, "end_char_idx": 35492, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10b98240-23f2-4550-b8e8-3a142bb4e0a7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs. ", "original_text": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cad4fbd-6ae3-4876-89f4-0640a912dfc3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An interesting observation is that the clustering-based method offers much more different rank values (ca.  12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work. ", "original_text": "In particular, this fact is noticeable in Table 4. "}, "hash": "e3529adc84f92caa745a78216bf9ee2ea18b329f953a26d40cd498a7201eb206", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f98fe2a-ee2a-477c-9ae6-b98d416fa699", "node_type": "1", "metadata": {"window": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. ", "original_text": "Finally, it is worth to note, that both methods treat the categorical data in a similar way. "}, "hash": "8f61fb1ff6e97da30eb509d26bb7cf562f04f472146b8bf2f1924cbb9073e730", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results. ", "mimetype": "text/plain", "start_char_idx": 35492, "end_char_idx": 35601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f98fe2a-ee2a-477c-9ae6-b98d416fa699", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. ", "original_text": "Finally, it is worth to note, that both methods treat the categorical data in a similar way. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b98240-23f2-4550-b8e8-3a142bb4e0a7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "12,500) of anomaly scores than IF (ca.  1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs. ", "original_text": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results. "}, "hash": "df2ad2094708faf446510d4416648b0b3c530854df7535746569ebccbdd4a41e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2251efbd-86a0-40da-96eb-88ad1dcb6927", "node_type": "1", "metadata": {"window": "12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies. ", "original_text": "Analogical results were obtained for ship transportation data. "}, "hash": "e800db1fe1af3481aefd58ff5b1f7710c64845a7b8862405f2fad1e6369e34a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, it is worth to note, that both methods treat the categorical data in a similar way. ", "mimetype": "text/plain", "start_char_idx": 35601, "end_char_idx": 35694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2251efbd-86a0-40da-96eb-88ad1dcb6927", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies. ", "original_text": "Analogical results were obtained for ship transportation data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f98fe2a-ee2a-477c-9ae6-b98d416fa699", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1750), and that there are significant differences between these rankings of points in a context of anomaly score, see Figs.  12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. ", "original_text": "Finally, it is worth to note, that both methods treat the categorical data in a similar way. "}, "hash": "d38720b591b2861f451a3356b5c5bfb2485ca76730475f9052ec2de73a317dc5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "213eacc6-a771-4479-aac1-06ab8289bbea", "node_type": "1", "metadata": {"window": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n", "original_text": "Again, the histogram depicted at Fig. "}, "hash": "3a47b75b71222202b4f62a5b3fd75ecd999b30b5df7df8c8ed23ec4042acca84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Analogical results were obtained for ship transportation data. ", "mimetype": "text/plain", "start_char_idx": 35694, "end_char_idx": 35757, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "213eacc6-a771-4479-aac1-06ab8289bbea", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n", "original_text": "Again, the histogram depicted at Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2251efbd-86a0-40da-96eb-88ad1dcb6927", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "12 and 13.  Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies. ", "original_text": "Analogical results were obtained for ship transportation data. "}, "hash": "62045d4bf691344606a6cc389c3fac3b1f8a6ad103524187f73b07024839ab42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b1c6dcc-0c0d-417e-ae3e-84110e725f0a", "node_type": "1", "metadata": {"window": "In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4. ", "original_text": "16 shows the differences between the methods' way of work. "}, "hash": "db645d3af22bdaa8c17d87dcd3159e63dedd6430f931e25b41f67e160fad5427", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Again, the histogram depicted at Fig. ", "mimetype": "text/plain", "start_char_idx": 35757, "end_char_idx": 35795, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b1c6dcc-0c0d-417e-ae3e-84110e725f0a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4. ", "original_text": "16 shows the differences between the methods' way of work. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "213eacc6-a771-4479-aac1-06ab8289bbea", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Despite being similar in terms of the classification results, it is easy to note that our proposal is more precise and differentiates between various points.  In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n", "original_text": "Again, the histogram depicted at Fig. "}, "hash": "13a607751e753be77781f8dc83183d3139e367963d0e33334faa3daf84d024ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a31b067-4116-4dcd-a5df-48a6d603dcc9", "node_type": "1", "metadata": {"window": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. ", "original_text": "Interesting insights may be obtained from Figs. "}, "hash": "06fdf778e833695775d480c35ddcca81d7f934dd1cc1d852b0c5874d3045a76e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16 shows the differences between the methods' way of work. ", "mimetype": "text/plain", "start_char_idx": 35795, "end_char_idx": 35854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a31b067-4116-4dcd-a5df-48a6d603dcc9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. ", "original_text": "Interesting insights may be obtained from Figs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b1c6dcc-0c0d-417e-ae3e-84110e725f0a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In particular, this fact is noticeable in Table 4.  For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4. ", "original_text": "16 shows the differences between the methods' way of work. "}, "hash": "1c905e30bf2b4313507b5048ad76b5f469fadab059d779cf0c6fc890e48fc353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "292fbc5c-198d-4e71-b305-a5b4e98b4e85", "node_type": "1", "metadata": {"window": "Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. ", "original_text": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. "}, "hash": "d609d4330b2b4c5ccebd1f1260fc86946d8f20994aa1f9f67b4c02a54c26874d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interesting insights may be obtained from Figs. ", "mimetype": "text/plain", "start_char_idx": 35854, "end_char_idx": 35902, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "292fbc5c-198d-4e71-b305-a5b4e98b4e85", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. ", "original_text": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a31b067-4116-4dcd-a5df-48a6d603dcc9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For various records, IF returns the same anomaly scores while our proposal differentiates the final results.  Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. ", "original_text": "Interesting insights may be obtained from Figs. "}, "hash": "62c1cfd5a93ebcf42556eae160c3155ef8360add86620a4c26fd3a4803b5ee63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f176dee1-d243-40a9-a994-6e4371cfe3ee", "node_type": "1", "metadata": {"window": "Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++. ", "original_text": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies. "}, "hash": "6ed323e52a60f01978ef1c78f51a6873ac5ed3bf284f0e23227878f44e7b318c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. ", "mimetype": "text/plain", "start_char_idx": 35902, "end_char_idx": 36040, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f176dee1-d243-40a9-a994-6e4371cfe3ee", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++. ", "original_text": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "292fbc5c-198d-4e71-b305-a5b4e98b4e85", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, it is worth to note, that both methods treat the categorical data in a similar way.  Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. ", "original_text": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented. "}, "hash": "4b68c5738c7c825f718ce25f02073c42d5b6a5473e9210f3b68a4400c71a96f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7172b53a-8e6a-48a9-a568-df2dbeef545f", "node_type": "1", "metadata": {"window": "Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results. ", "original_text": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n"}, "hash": "db2409e138823e1cc5f0473f5f8627960c0fea820f1d1c6049159147934a701e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies. ", "mimetype": "text/plain", "start_char_idx": 36040, "end_char_idx": 36117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7172b53a-8e6a-48a9-a568-df2dbeef545f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results. ", "original_text": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f176dee1-d243-40a9-a994-6e4371cfe3ee", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Analogical results were obtained for ship transportation data.  Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++. ", "original_text": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies. "}, "hash": "2827f97961a4955075d5a93e5ed7a51a8fc768af2912f84380aa49388bbde07c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4ac72f7-ca7f-43e2-bba2-9726de9c87bf", "node_type": "1", "metadata": {"window": "16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n", "original_text": "#### 4.4. "}, "hash": "22c517ecb19b38bbc6285f4094d3122ddc34054c2e28d8fe11f1593ae8e15e31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n", "mimetype": "text/plain", "start_char_idx": 36117, "end_char_idx": 36203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b4ac72f7-ca7f-43e2-bba2-9726de9c87bf", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n", "original_text": "#### 4.4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7172b53a-8e6a-48a9-a568-df2dbeef545f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Again, the histogram depicted at Fig.  16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results. ", "original_text": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n"}, "hash": "2a9992dcf3c53bb018599ea960fb0550f7376a4845e4c4a87d4fb60b1455f97d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb8e4796-e191-4eac-93d1-f9359d4e8644", "node_type": "1", "metadata": {"window": "Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest. ", "original_text": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. "}, "hash": "74fd38be3cfdd24c2d2c8f61cf1cee6c00ce59aa42c93d072e1c375d285615da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "#### 4.4. ", "mimetype": "text/plain", "start_char_idx": 36203, "end_char_idx": 36213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb8e4796-e191-4eac-93d1-f9359d4e8644", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest. ", "original_text": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4ac72f7-ca7f-43e2-bba2-9726de9c87bf", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "16 shows the differences between the methods' way of work.  Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n", "original_text": "#### 4.4. "}, "hash": "744639f1135acf332da3e8e747a7424a5a35d637d437fc2558b669a78d07e1d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28e031c0-418a-4ed4-873d-c77a7ae55189", "node_type": "1", "metadata": {"window": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest. ", "original_text": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. "}, "hash": "daa64d639a615e3ca5473b1477121080f17507723210c8bc573772768c229eda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. ", "mimetype": "text/plain", "start_char_idx": 36213, "end_char_idx": 36348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28e031c0-418a-4ed4-873d-c77a7ae55189", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest. ", "original_text": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb8e4796-e191-4eac-93d1-f9359d4e8644", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Interesting insights may be obtained from Figs.  14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest. ", "original_text": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest. "}, "hash": "260ad69ccceb3dea8bd0fa4072c38c2cc99abd843251116d264b244bb8218cf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32d813c8-ab5f-4d3a-ad70-1a5265bcd06a", "node_type": "1", "metadata": {"window": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. ", "original_text": "The application was written in C++. "}, "hash": "7ea477da45fc85289589003a2da7977fb88ec0a45f7cbbd5244b34ae8165f928", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. ", "mimetype": "text/plain", "start_char_idx": 36348, "end_char_idx": 36451, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32d813c8-ab5f-4d3a-ad70-1a5265bcd06a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. ", "original_text": "The application was written in C++. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28e031c0-418a-4ed4-873d-c77a7ae55189", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "14 and 15, where dependencies between anonymized train routes and the time differences between train departure and arrival are presented.  Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest. ", "original_text": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM. "}, "hash": "3cd81cc520df1eba1b775bf26e18c002d89033b0672a56bd2fb2452ed2861597", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0509e090-947b-4830-8b55-dcf599c0e624", "node_type": "1", "metadata": {"window": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster. ", "original_text": "No threads were used to compare the results. "}, "hash": "271d3e41ea243ee566a4d46fdd99fedd8d4f9b582e6a158214251c88bc13627e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The application was written in C++. ", "mimetype": "text/plain", "start_char_idx": 36451, "end_char_idx": 36487, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0509e090-947b-4830-8b55-dcf599c0e624", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster. ", "original_text": "No threads were used to compare the results. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32d813c8-ab5f-4d3a-ad70-1a5265bcd06a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Both IF and k-Means-Based IF are highly accurate in searching for anomalies.  However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. ", "original_text": "The application was written in C++. "}, "hash": "bcf19af5395da16538fbf12a7fc17d1dcf27242b719fd5112c2d7a36154bd4a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f4f1fe9-c4e1-4b9a-bb2d-ad9675c1a8c9", "node_type": "1", "metadata": {"window": "#### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning. ", "original_text": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n"}, "hash": "114d90a3b5d5023f8c788d591f4c2db2e8b673d7f2355fab4ea381d891bf198d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "No threads were used to compare the results. ", "mimetype": "text/plain", "start_char_idx": 36487, "end_char_idx": 36532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f4f1fe9-c4e1-4b9a-bb2d-ad9675c1a8c9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning. ", "original_text": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0509e090-947b-4830-8b55-dcf599c0e624", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, the k-Means-based version more likely finds the outliers with limit values.\n\n #### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster. ", "original_text": "No threads were used to compare the results. "}, "hash": "bb886d7f7a466dfc6de39cc056deb9c90000f46206382fc0149268e31bb83d57", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d50a270-dc73-4b9e-af0d-0ce699afe654", "node_type": "1", "metadata": {"window": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. ", "original_text": "The tests show an interesting property of k-Means-based isolation forest. "}, "hash": "1381001e87ea8b4e3a4e217c40298f375dee5b4e176c83aa156e119320986780", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n", "mimetype": "text/plain", "start_char_idx": 36532, "end_char_idx": 36711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d50a270-dc73-4b9e-af0d-0ce699afe654", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. ", "original_text": "The tests show an interesting property of k-Means-based isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f4f1fe9-c4e1-4b9a-bb2d-ad9675c1a8c9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "#### 4.4.  Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning. ", "original_text": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n"}, "hash": "2e147185282e9c0748604b3ee5ffc80db0554486164e078bdd12c52bc5ecd2bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09109a04-ad87-476c-9ab8-57b181f88976", "node_type": "1", "metadata": {"window": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n", "original_text": "In most cases, its execution time is quite longer than execution time of the isolation forest. "}, "hash": "0c2a2597a2d23cc34bbde3eac49c1eb65170d8302eebc52dd01bcb7b6e46aadc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The tests show an interesting property of k-Means-based isolation forest. ", "mimetype": "text/plain", "start_char_idx": 36711, "end_char_idx": 36785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09109a04-ad87-476c-9ab8-57b181f88976", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n", "original_text": "In most cases, its execution time is quite longer than execution time of the isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d50a270-dc73-4b9e-af0d-0ce699afe654", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Execution times\n\nHere, we discuss computing overhead observed during running both isolation forest and k-Means-based isolation forest.  All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. ", "original_text": "The tests show an interesting property of k-Means-based isolation forest. "}, "hash": "11cc6daed5c1da916c17bee9d0d1fe8760d778bfddbd96b9ceee8e8d841bbdb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "139c9e3e-5201-4f1e-9990-5357ac1703ea", "node_type": "1", "metadata": {"window": "The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5. ", "original_text": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. "}, "hash": "f3883494efff8f80c2fde218387d5c7b224dd31b4cc0c8c2aff2dbf8d7958918", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In most cases, its execution time is quite longer than execution time of the isolation forest. ", "mimetype": "text/plain", "start_char_idx": 36785, "end_char_idx": 36880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "139c9e3e-5201-4f1e-9990-5357ac1703ea", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5. ", "original_text": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09109a04-ad87-476c-9ab8-57b181f88976", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "All the tests were conducted on the computer equipped with Windows 10 64 bit architecture, 2.4 GB RAM.  The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n", "original_text": "In most cases, its execution time is quite longer than execution time of the isolation forest. "}, "hash": "a06b381a97a242a0a4fc161336a4f287d960ec4108e3c83a7a2dd9cc72da22db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9550204f-d231-4688-9f51-d352828a75e0", "node_type": "1", "metadata": {"window": "No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. ", "original_text": "However, in case of large databases containing geographical points it works faster. "}, "hash": "2e77d10dec36ddac5d78d6fee5391dda009c3568c946da605ef534f308e30436", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. ", "mimetype": "text/plain", "start_char_idx": 36880, "end_char_idx": 36991, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9550204f-d231-4688-9f51-d352828a75e0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. ", "original_text": "However, in case of large databases containing geographical points it works faster. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "139c9e3e-5201-4f1e-9990-5357ac1703ea", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The application was written in C++.  No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5. ", "original_text": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded. "}, "hash": "0a63f862db0a22d35ef00eb33cb51cf609a505300129e17961cf5dfb7545cec2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "931cb087-2691-439e-b710-63dae67458c4", "node_type": "1", "metadata": {"window": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized).", "original_text": "The cause may be hidden in the structure of data which are relatively clustered at the beginning. "}, "hash": "21a7bdce9911ce4516649fbe548cdc106e4f43c66096a9d8103d4f04e587dd01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, in case of large databases containing geographical points it works faster. ", "mimetype": "text/plain", "start_char_idx": 36991, "end_char_idx": 37075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "931cb087-2691-439e-b710-63dae67458c4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized).", "original_text": "The cause may be hidden in the structure of data which are relatively clustered at the beginning. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9550204f-d231-4688-9f51-d352828a75e0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "No threads were used to compare the results.  The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. ", "original_text": "However, in case of large databases containing geographical points it works faster. "}, "hash": "6a6559aeee5aecba3ec5f0792fd714d76e69acc2b79c9b1d43804aae8a38ae87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2f544dd-b057-404f-8f1e-5aecfdfeabe0", "node_type": "1", "metadata": {"window": "The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. ", "original_text": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. "}, "hash": "14b3246a4aa8d78d8b16aa1a32e90a8ef61dc15844e5cc10c3216cf9fdb9736f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The cause may be hidden in the structure of data which are relatively clustered at the beginning. ", "mimetype": "text/plain", "start_char_idx": 37075, "end_char_idx": 37173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2f544dd-b057-404f-8f1e-5aecfdfeabe0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. ", "original_text": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "931cb087-2691-439e-b710-63dae67458c4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The times (see, Table 5) are presented for the sum for 100 iterations of sample tree building and scoring through the whole dataset according to the sample tree with no average.\n\n The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized).", "original_text": "The cause may be hidden in the structure of data which are relatively clustered at the beginning. "}, "hash": "5a5237dd27ee13ed0f17c5b4d1aec0aa5a2cfa3e9a2809dade0a6823326fe2ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c76d3781-c36f-46a1-b3db-aeb1f954d0eb", "node_type": "1", "metadata": {"window": "In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ. ", "original_text": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n"}, "hash": "9bdde513396bbf3df8281022047e4b324db32ca4bfb743b6e49159d91b4582b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. ", "mimetype": "text/plain", "start_char_idx": 37173, "end_char_idx": 37289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c76d3781-c36f-46a1-b3db-aeb1f954d0eb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ. ", "original_text": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2f544dd-b057-404f-8f1e-5aecfdfeabe0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The tests show an interesting property of k-Means-based isolation forest.  In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. ", "original_text": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size. "}, "hash": "4fe0aeaf94dfb36517a0e6ee954429e3e2cde85328e719e86f7903b8a01fc862", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40ef8431-c527-4f67-b9cb-c86822fd83db", "node_type": "1", "metadata": {"window": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. ", "original_text": "### 5. "}, "hash": "f18c4701af787b5c40ee852226b19aa8051129cf08fe5242b7a35e5a12e701de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n", "mimetype": "text/plain", "start_char_idx": 37289, "end_char_idx": 37601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40ef8431-c527-4f67-b9cb-c86822fd83db", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. ", "original_text": "### 5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c76d3781-c36f-46a1-b3db-aeb1f954d0eb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "In most cases, its execution time is quite longer than execution time of the isolation forest.  It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ. ", "original_text": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n"}, "hash": "5caf033937b4b49d991e462a77c4386946aecb189655e6b838f75404f4450b9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0056000b-68ce-4f75-a058-6fa157acbca8", "node_type": "1", "metadata": {"window": "However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n", "original_text": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. "}, "hash": "1f392245ecdff5d9b6503ee7e94bdd4f3f20ad4afef534821235be33bf028313", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 5. ", "mimetype": "text/plain", "start_char_idx": 37601, "end_char_idx": 37608, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0056000b-68ce-4f75-a058-6fa157acbca8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n", "original_text": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40ef8431-c527-4f67-b9cb-c86822fd83db", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is because of more comparisons are done at each split and the distances to the cluster centers are yielded.  However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. ", "original_text": "### 5. "}, "hash": "9fabdb1352ab2133dc24c404a9a9a3b7fd92be9f8d1a8d4f0b7f44c21a68f6a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "903f842a-1051-46ec-afcb-a3b0c7bbead3", "node_type": "1", "metadata": {"window": "The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized).", "original_text": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized)."}, "hash": "babe7526715e559dddb1b583babff5277b5ce768ff3a9d6b83858a5ddac23be4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. ", "mimetype": "text/plain", "start_char_idx": 37608, "end_char_idx": 37800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "903f842a-1051-46ec-afcb-a3b0c7bbead3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized).", "original_text": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0056000b-68ce-4f75-a058-6fa157acbca8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "However, in case of large databases containing geographical points it works faster.  The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n", "original_text": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method. "}, "hash": "f23b520ccb2d924022e7037aae59e191a5294aad4d02ff4f8994470e6a0b63be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03a8f5c9-f428-49e1-a21a-5ad6171ac7a1", "node_type": "1", "metadata": {"window": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. ", "original_text": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. "}, "hash": "267ae9ee0968c4e71fdb6387089c5f41885b6fe8f3cba508f166e050b0fa726b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized).", "mimetype": "text/plain", "start_char_idx": 37800, "end_char_idx": 37905, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "03a8f5c9-f428-49e1-a21a-5ad6171ac7a1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. ", "original_text": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "903f842a-1051-46ec-afcb-a3b0c7bbead3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The cause may be hidden in the structure of data which are relatively clustered at the beginning.  Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized).", "original_text": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized)."}, "hash": "4c9d19f18dcece8476e27fa74afdc1c81f430cfa2c3400aab91bbd9359082124", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70c38b95-4f59-4f85-9c78-fd400dc8cb7f", "node_type": "1", "metadata": {"window": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n", "original_text": "port | Roro arrival port | Roro departure differ. "}, "hash": "b3a52025e451c1fe46c8155d4549e60baea1e0fc280815f2690d6beed8896cbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. ", "mimetype": "text/plain", "start_char_idx": 37905, "end_char_idx": 37994, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70c38b95-4f59-4f85-9c78-fd400dc8cb7f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n", "original_text": "port | Roro arrival port | Roro departure differ. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03a8f5c9-f428-49e1-a21a-5ad6171ac7a1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Here, it is worth to note that the times of execution are, in fact, negligible from the point of view of data size.  The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. ", "original_text": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart. "}, "hash": "ce5c7767065f37c92a0e3f24d877576df249c1151ddc53715a029251a395c656", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "980329e4-fe45-4926-b969-7a211b6a31e1", "node_type": "1", "metadata": {"window": "### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n", "original_text": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. "}, "hash": "87caf050ef1ee08b8849244f5781470d704a8ba20501744c99685c156a5637bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "port | Roro arrival port | Roro departure differ. ", "mimetype": "text/plain", "start_char_idx": 37994, "end_char_idx": 38044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "980329e4-fe45-4926-b969-7a211b6a31e1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n", "original_text": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70c38b95-4f59-4f85-9c78-fd400dc8cb7f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The method is called just once for the dataset and, taking into account, the experts' time to analyze the scores, the time of execution is not long, reaching at most less than 18 s in the case of k-Means-based isolation forest or less than quarter of an hour in the case of isolation forest \u2014 in one case only.\n\n ### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n", "original_text": "port | Roro arrival port | Roro departure differ. "}, "hash": "65104ccf7d2e5e670b5a9ff9e14e7d1772aed47437480a7eb2db5944da45c19e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f55afa2-929f-435f-ab11-0e91b4218156", "node_type": "1", "metadata": {"window": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest.", "original_text": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n"}, "hash": "3f42810fbb0b00a1e4d9d420503e5e89aee7732e434ed97ff07749af730ee14f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. ", "mimetype": "text/plain", "start_char_idx": 38044, "end_char_idx": 38127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f55afa2-929f-435f-ab11-0e91b4218156", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest.", "original_text": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "980329e4-fe45-4926-b969-7a211b6a31e1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### 5.  Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n", "original_text": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ. "}, "hash": "235126d8c7645084ec7ee842d3875b7600cb0431078534f8a76c034eca4542f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95ca12ca-377b-4d92-a799-2ab4e2a65b93", "node_type": "1", "metadata": {"window": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.) ", "original_text": "**Table 3**\n**Example of data from the train transportation dataset (anonymized)."}, "hash": "c737881d70c87a519e465bdf570773b4e379936732abbad9fdefd821fe2c3036", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n", "mimetype": "text/plain", "start_char_idx": 38127, "end_char_idx": 38634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95ca12ca-377b-4d92-a799-2ab4e2a65b93", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.) ", "original_text": "**Table 3**\n**Example of data from the train transportation dataset (anonymized)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f55afa2-929f-435f-ab11-0e91b4218156", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conclusions and future work\n\nIn the study, we have presented a new approach to the isolation forest-based methods of outlier detection, which is a novel k-Means-based isolation forest method.  It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest.", "original_text": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n"}, "hash": "42438c5166f2621a0bb3ed8161bcbf813bcf2318cd4d15bd2b3779dd04c44dd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b1fd6df-af83-4de5-afb0-0f7680231e8c", "node_type": "1", "metadata": {"window": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.) ", "original_text": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. "}, "hash": "62b641fafb595a69df9c748057df4cad5f73ef9313bbba538b2e65435ef43f94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Table 3**\n**Example of data from the train transportation dataset (anonymized).", "mimetype": "text/plain", "start_char_idx": 38634, "end_char_idx": 38715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b1fd6df-af83-4de5-afb0-0f7680231e8c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.) ", "original_text": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95ca12ca-377b-4d92-a799-2ab4e2a65b93", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "It is based on the data\n\n**Table 2**\n**Example of data from the ship transportation dataset (anonymized). **\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.) ", "original_text": "**Table 3**\n**Example of data from the train transportation dataset (anonymized)."}, "hash": "56ae04479f75c0b58904028fbbbc63a46de02b8dcbfbf9d10708f7b9b1351abc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c7c7488-e935-4eeb-84e3-012a7c14f294", "node_type": "1", "metadata": {"window": "port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.) ", "original_text": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n"}, "hash": "10de7154787e07643c242e2ae6bc0e1158380fcd15f66489e4e121d00bdf87d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. ", "mimetype": "text/plain", "start_char_idx": 38715, "end_char_idx": 38968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c7c7488-e935-4eeb-84e3-012a7c14f294", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.) ", "original_text": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b1fd6df-af83-4de5-afb0-0f7680231e8c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Service type | Unit category | Unit type | Unit kind | Ride type id | Roro depart.  port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.) ", "original_text": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff. "}, "hash": "a3c0c66aab94863ef4e0ebb2407b94de555c3cfcc4040a09a3722bb257b1ec24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "647f3276-96f5-4eef-9ea8-5fd605ad8c06", "node_type": "1", "metadata": {"window": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.) ", "original_text": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n"}, "hash": "df64fac30b385440fb158422bcb459cb38cd98974cf0e1afeabd6401a373a72d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n", "mimetype": "text/plain", "start_char_idx": 38968, "end_char_idx": 39420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "647f3276-96f5-4eef-9ea8-5fd605ad8c06", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.) ", "original_text": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c7c7488-e935-4eeb-84e3-012a7c14f294", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "port | Roro arrival port | Roro departure differ.  | Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.) ", "original_text": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n"}, "hash": "328060d4bfb2b325556dc82c329e23e5e58db463fd2ba8feb377866297f2796b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bafa1c9-8302-4123-94c1-3726ae7d52f8", "node_type": "1", "metadata": {"window": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. ", "original_text": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest."}, "hash": "780b4528fb11c9f6d6042112c3c07f00cd244602f1d6991a6460440423e1fff4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n", "mimetype": "text/plain", "start_char_idx": 39420, "end_char_idx": 39486, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6bafa1c9-8302-4123-94c1-3726ae7d52f8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. ", "original_text": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "647f3276-96f5-4eef-9ea8-5fd605ad8c06", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Roro arrival differ.\u00aa | Unit departure from port differ.\u00aa | Min order og differ.  | Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.) ", "original_text": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n"}, "hash": "03283c7059da89488ac1bf8fd4cca058dfce425061c60c795e265a33d43a19a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa5fe75d-2341-4ae3-9124-2ce670e1d857", "node_type": "1", "metadata": {"window": "**Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13. ", "original_text": "**\n\n| Date train departure dif (min.) "}, "hash": "2abd1def010101d5529269a502f33d12033ab06bd2192801d99c802f508a6bf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest.", "mimetype": "text/plain", "start_char_idx": 39486, "end_char_idx": 39589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aa5fe75d-2341-4ae3-9124-2ce670e1d857", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13. ", "original_text": "**\n\n| Date train departure dif (min.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bafa1c9-8302-4123-94c1-3726ae7d52f8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Service type\u00aa |\n|---|---|---|---|---|---|---|---|---|---|---|---|\n| Feg | Qrs | Uvw | Rpo | Cba | X | Y | 33 | 4854 | 5687 | 7188 | Feg |\n| Gfe | Rsq | Vwx | Rpo | Dcb | X | Y | 274 | 5095 | 6409 | 12 409 | Gfe |\n| Fed | Qrs | Uvw | Rpo | Cba | X | Y | 201 | 5022 | 6336 | 12 276 | Fed |\n| Gfe | Rsq | Vwx | Opr | Cba | X | Z | 1576 | 6397 | 7711 | 13 051 | Gfe |\n| Fed | Qrs | Wxy | Opr | Dcb | Y | W | 859 | 5680 | 7234 | 6574 | Fed |\n\n\u00aaMeans columns removed from the series of test with 9 attributes.\n\n **Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. ", "original_text": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest."}, "hash": "e2a1e5ac4b02d45b5f01368162f0cb0e66cfd799f6a9a4afefede51ce9e68792", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b1a8c1b-8909-45b4-af9c-7620c4ef0140", "node_type": "1", "metadata": {"window": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences.", "original_text": "| Date train arrival dif (min.) "}, "hash": "13049c8687f18266579d8f229ce6c5ad6308f7369e9ca3d6941b0ef24f9689ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Date train departure dif (min.) ", "mimetype": "text/plain", "start_char_idx": 39589, "end_char_idx": 39627, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b1a8c1b-8909-45b4-af9c-7620c4ef0140", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences.", "original_text": "| Date train arrival dif (min.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa5fe75d-2341-4ae3-9124-2ce670e1d857", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Table 3**\n**Example of data from the train transportation dataset (anonymized). **\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13. ", "original_text": "**\n\n| Date train departure dif (min.) "}, "hash": "091e4921e129c998aec1e173fa2a7995ccb3de7d0d1ef6cdba3146166405da61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "389ecaf8-1a6f-4e43-a81d-1c0868110851", "node_type": "1", "metadata": {"window": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. ", "original_text": "| Unit departure arr station dif (min.) "}, "hash": "502c5c25b14a474966dfae62c5d09ece322af1da64297c04ea2ac755d84e09ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Date train arrival dif (min.) ", "mimetype": "text/plain", "start_char_idx": 39627, "end_char_idx": 39659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "389ecaf8-1a6f-4e43-a81d-1c0868110851", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. ", "original_text": "| Unit departure arr station dif (min.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b1a8c1b-8909-45b4-af9c-7620c4ef0140", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Service group code | Service type | Unit category | Unit type | Unit kind | Departure station name | Arrival station name | Date train departure diff.\u1d47 | Date train arrival diff.\u00aa \u1d47 | Unit departure arrival station diff.\u00aa | Nearest due date diff.  |\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences.", "original_text": "| Date train arrival dif (min.) "}, "hash": "4e96dc4be50a3f2ef7c689064be03dbe2050d4028596fc89973b2b0b698349af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b408da9b-49ef-4027-907a-39cc80909d34", "node_type": "1", "metadata": {"window": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum. ", "original_text": "| Nearest due date dif (min.) "}, "hash": "46201adb01d6a25a5e1ab3585d47fd8cf29d7b52546a742d3b1a19ce21be4452", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Unit departure arr station dif (min.) ", "mimetype": "text/plain", "start_char_idx": 39659, "end_char_idx": 39699, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b408da9b-49ef-4027-907a-39cc80909d34", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum. ", "original_text": "| Nearest due date dif (min.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "389ecaf8-1a6f-4e43-a81d-1c0868110851", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "|\n|---|---|---|---|---|---|---|---|---|---|---|\n| Abc | Def | Ijl | Kmn | Opr | A | C | 470 | 1245 | 1640 | -3642 |\n| Abc | Efg | Jlk | Mno | Opr | B | D | 983 | 2472 | 3589 | 1553 |\n| Bcd | Def | Jlk | Mno | Prs | A | E | 763 | 1518 | 7458 | 3858 |\n| Cde | Efg | Lkm | Nop | Prs | B | D | 1285 | 2560 | 2734 | 4735 |\n| Abc | Fgh | Lkm | Nop | Prs | B | D | 983 | 2472 | 3247 | 1553 |\n\n\u00aaMean columns removed from the series of test with 10 attributes.\n \u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. ", "original_text": "| Unit departure arr station dif (min.) "}, "hash": "7aea70b77983313e74900a16f65092d103fd43cdaa91df1c3ce662f176babeb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb820501-c1d3-462c-b798-59703f45c507", "node_type": "1", "metadata": {"window": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n", "original_text": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. "}, "hash": "109e463c4b26000cda6e532d860a969b948e6470aa6130418c049c2589ac43b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Nearest due date dif (min.) ", "mimetype": "text/plain", "start_char_idx": 39699, "end_char_idx": 39729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb820501-c1d3-462c-b798-59703f45c507", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n", "original_text": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b408da9b-49ef-4027-907a-39cc80909d34", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "\u1d47Mean columns removed from the series of test with 9 attributes.\n\n **Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum. ", "original_text": "| Nearest due date dif (min.) "}, "hash": "5070661f403abaac1dc71f245cdf298cdac2c285b408fd0895a3a83eda05d5ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b9b0bc4-7d90-40e7-bb2b-98da5377962c", "node_type": "1", "metadata": {"window": "**\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs. ", "original_text": "13. "}, "hash": "845e6a6e6b5db33003150edbcf47168a5d515194ae8448ff547aae28869535ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 39729, "end_char_idx": 40392, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b9b0bc4-7d90-40e7-bb2b-98da5377962c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs. ", "original_text": "13. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb820501-c1d3-462c-b798-59703f45c507", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Table 4**\n**Anomaly score values obtained by the isolation forest and k-Means-based isolation forest. **\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n", "original_text": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig. "}, "hash": "17c083a83cc1cd64355cb8d2df991c57cbe162f845352d7c3218acadc386d0e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "096b77e8-4ae1-40e7-91ce-6413f29aebe0", "node_type": "1", "metadata": {"window": "| Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. ", "original_text": "Modules of rank differences."}, "hash": "bf67641327f1797404aad893f907ef65206f52d9a9f7cace64c0a06189791c2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13. ", "mimetype": "text/plain", "start_char_idx": 40392, "end_char_idx": 40396, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "096b77e8-4ae1-40e7-91ce-6413f29aebe0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. ", "original_text": "Modules of rank differences."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b9b0bc4-7d90-40e7-bb2b-98da5377962c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Date train departure dif (min.)  | Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs. ", "original_text": "13. "}, "hash": "c3ab9c596401a223d75f0357bd1af19158d86a999772aedf13fc5713c2429e32", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31e6d332-743f-4c5f-a44c-1fa9d523c609", "node_type": "1", "metadata": {"window": "| Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n", "original_text": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. "}, "hash": "5b0cff9a7075a10ee7c134700dc7fe70bca6e0efaf3361f535083ad348c280e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Modules of rank differences.", "mimetype": "text/plain", "start_char_idx": 40396, "end_char_idx": 40424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31e6d332-743f-4c5f-a44c-1fa9d523c609", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n", "original_text": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "096b77e8-4ae1-40e7-91ce-6413f29aebe0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Date train arrival dif (min.)  | Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. ", "original_text": "Modules of rank differences."}, "hash": "577ea84ab8b6e2ac50d3ef11573e759aef4e18b27af750b8d56ced8bcd4eb8a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cf168cb-f662-435b-87e5-04aabb1b513b", "node_type": "1", "metadata": {"window": "| Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. ", "original_text": "Points are colored on a blue-to-red spectrum. "}, "hash": "464538ac06e7ba75dd6f2521b7c96f5169c3106238021dc2fd5a1365be4110ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. ", "mimetype": "text/plain", "start_char_idx": 40424, "end_char_idx": 40573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cf168cb-f662-435b-87e5-04aabb1b513b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. ", "original_text": "Points are colored on a blue-to-red spectrum. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31e6d332-743f-4c5f-a44c-1fa9d523c609", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Unit departure arr station dif (min.)  | Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n", "original_text": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset. "}, "hash": "b6492cdac31fb5654fd8427f2f445b90f623dbf6d69b68660a0278effea9d0a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8aa2fb3f-a6ea-46fa-b56c-9067193d07de", "node_type": "1", "metadata": {"window": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. ", "original_text": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n"}, "hash": "0b47d9ab3e4c1aae72d78e1e8039b79dc33324fd14a8f5d8ff4a29e94f7fe33e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Points are colored on a blue-to-red spectrum. ", "mimetype": "text/plain", "start_char_idx": 40573, "end_char_idx": 40619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8aa2fb3f-a6ea-46fa-b56c-9067193d07de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. ", "original_text": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cf168cb-f662-435b-87e5-04aabb1b513b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| Nearest due date dif (min.)  | k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. ", "original_text": "Points are colored on a blue-to-red spectrum. "}, "hash": "2c965fad9724444f5f8be06475b68a08872ff8079613a8f52c4560a4cffa85e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77baa647-191a-4975-999f-28ffd2b9ec4e", "node_type": "1", "metadata": {"window": "13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14. ", "original_text": "***\n\nstructured in a form of trees having at each node an optimal number of leafs. "}, "hash": "cd0e8c24e12cf6f983eb467ddab4168a02b62c414c6fb8a15244491182b0d2c0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n", "mimetype": "text/plain", "start_char_idx": 40619, "end_char_idx": 40793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77baa647-191a-4975-999f-28ffd2b9ec4e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14. ", "original_text": "***\n\nstructured in a form of trees having at each node an optimal number of leafs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8aa2fb3f-a6ea-46fa-b56c-9067193d07de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "| k-Means-based | IF | Rank with k-Means IF | Rank with IF |\n|---|---|---|---|---|---|---|---|\n| 1860 | 2760 | 4740 | 10860 | 4.18365 | 0.448107 | 795 | 38 |\n| 1588 | 2488 | 4428 | 4828 | 4.18689 | 0.448107 | 796 | 38 |\n| 1525 | 2965 | 3505 | 8185 | 4.18711 | 0.448107 | 797 | 38 |\n| 1566 | 2466 | 4406 | 4806 | 4.18732 | 0.448107 | 798 | 38 |\n| 1560 | 2460 | 4400 | 4800 | 4.18736 | 0.448107 | 799 | 38 |\n| 1410 | 2910 | 9223 | 8130 | 4.21224 | 0.448107 | 811 | 38 |\n| 1479 | 2379 | 4319 | 4719 | 4.21568 | 0.448107 | 812 | 38 |\n| 1295 | 2195 | 4176 | 215 | 4.2277 | 0.448107 | 818 | 38 |\n| 616 | 1322 | 4596 | 6456 | 4.24703 | 0.448107 | 832 | 38 |\n\n***\n**Fig.  13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. ", "original_text": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n"}, "hash": "14cfb08cc5b12969edfca36a8dad6114367cd87cb0c699eedc03fe4ace4337b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e02d34fe-6382-414b-ad0f-3a42d98a6cd4", "node_type": "1", "metadata": {"window": "Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. ", "original_text": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. "}, "hash": "d5a6205ce3c34c25c3cf8fc0df944b522f27a9a248050b99264563f3d984a129", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nstructured in a form of trees having at each node an optimal number of leafs. ", "mimetype": "text/plain", "start_char_idx": 40793, "end_char_idx": 40876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e02d34fe-6382-414b-ad0f-3a42d98a6cd4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. ", "original_text": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77baa647-191a-4975-999f-28ffd2b9ec4e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "13.  Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14. ", "original_text": "***\n\nstructured in a form of trees having at each node an optimal number of leafs. "}, "hash": "5eccd98070bd000bf8c4213be730e291365bf99ff4d7c2c67783a24300d9e598", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e0342a8-3acd-4615-bbd9-484484b9a34a", "node_type": "1", "metadata": {"window": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values.", "original_text": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n"}, "hash": "ed3f7772c7bd785bd55d34fe8c3852effabd6a5340cd1c6ca712c7958184fd95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. ", "mimetype": "text/plain", "start_char_idx": 40876, "end_char_idx": 40986, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e0342a8-3acd-4615-bbd9-484484b9a34a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values.", "original_text": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e02d34fe-6382-414b-ad0f-3a42d98a6cd4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Modules of rank differences. ** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. ", "original_text": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree. "}, "hash": "1eaba82d668ba392c49d99b9c3d5b3c4c5a81ae09ce211cf564a4458960cee2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fe91e76-cb15-4a43-83de-a1a392c2bd37", "node_type": "1", "metadata": {"window": "Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes. ", "original_text": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. "}, "hash": "65179901cfa6f1cf5dc491a0dfd1f2f11d3d90bc12b5016b2215773c65c9a003", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n", "mimetype": "text/plain", "start_char_idx": 40986, "end_char_idx": 41263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0fe91e76-cb15-4a43-83de-a1a392c2bd37", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes. ", "original_text": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e0342a8-3acd-4615-bbd9-484484b9a34a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a scatter plot comparing the anomaly ranks assigned by k-Means-based IF (x-axis) and standard IF (y-axis) for the transportation dataset.  Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values.", "original_text": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n"}, "hash": "cd74ec17e1f94042ed4c512bb5a4daeba7111e7ec1eaf595bb86712ab39efd8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a7e7bc4-9e17-48c7-9e6b-f4903d9f4a74", "node_type": "1", "metadata": {"window": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. ", "original_text": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. "}, "hash": "aa89d53fc86084c2355b61438a46952dc80fb5daa9026cc69ce46f5aad5eecf3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. ", "mimetype": "text/plain", "start_char_idx": 41263, "end_char_idx": 41410, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a7e7bc4-9e17-48c7-9e6b-f4903d9f4a74", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. ", "original_text": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fe91e76-cb15-4a43-83de-a1a392c2bd37", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Points are colored on a blue-to-red spectrum.  There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes. ", "original_text": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc. "}, "hash": "d44ce438b2c7a216b0c1ba4e29afc5f299bbf010c56b51cee475c3c1c2bf7e7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce0aceba-441e-4905-a71f-271d1ac0b570", "node_type": "1", "metadata": {"window": "***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n", "original_text": "14. "}, "hash": "1b1d5c01e1d8ab60121d1be6f03afef563b2116e6e0fd060460c6aa5420d2df3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 41410, "end_char_idx": 41520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ce0aceba-441e-4905-a71f-271d1ac0b570", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n", "original_text": "14. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a7e7bc4-9e17-48c7-9e6b-f4903d9f4a74", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "There is a general positive correlation, but also a significant amount of scatter, indicating that for many data points, the two methods assign very different anomaly ranks.\n ***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. ", "original_text": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig. "}, "hash": "6eea59facf63396da3a87876d12340c5426e37197091edbdab136621ee204b70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18e4dd28-74e5-41e4-90cc-b03f9a7e780d", "node_type": "1", "metadata": {"window": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig. ", "original_text": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. "}, "hash": "cdde6bcf6ca0e1bc950e5c0fa296f795cbac76288418be4e4b128e274475e980", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14. ", "mimetype": "text/plain", "start_char_idx": 41520, "end_char_idx": 41524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18e4dd28-74e5-41e4-90cc-b03f9a7e780d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig. ", "original_text": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce0aceba-441e-4905-a71f-271d1ac0b570", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nstructured in a form of trees having at each node an optimal number of leafs.  These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n", "original_text": "14. "}, "hash": "de44f614b4cc0fac8de7eb894398a20518650e72ab3fe114b2e67ddf1322714a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16959a4b-f625-4774-a518-ebaa135d8c35", "node_type": "1", "metadata": {"window": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15. ", "original_text": "Note that the datasets contains negative values."}, "hash": "d7ae475c456c7349c2f0ea1210fd300db2d64eb53143def12880e9bf7dcbbeec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. ", "mimetype": "text/plain", "start_char_idx": 41524, "end_char_idx": 41645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16959a4b-f625-4774-a518-ebaa135d8c35", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15. ", "original_text": "Note that the datasets contains negative values."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18e4dd28-74e5-41e4-90cc-b03f9a7e780d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "These leafs are created on a basis of analysis of the training dataset filtered at earlier nodes of the tree.  During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig. ", "original_text": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF. "}, "hash": "660901fc692fcfe09f716049ef0eb60d408774b130493b44a4a33e95e1111262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "218a566d-6c23-4051-b9fe-5c83f0d54414", "node_type": "1", "metadata": {"window": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF.", "original_text": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes. "}, "hash": "af963a0f11b401e2cf70789d63f973cf2a9b7902cade93b5e47d4718ef78bdde", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the datasets contains negative values.", "mimetype": "text/plain", "start_char_idx": 41645, "end_char_idx": 41693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "218a566d-6c23-4051-b9fe-5c83f0d54414", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF.", "original_text": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16959a4b-f625-4774-a518-ebaa135d8c35", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "During the experiments, the method has demonstrated its efficiency and high level of precision in searching of isolation as well as anomaly records in various kinds of datasets containing geographical points, transportation information, or mixed (including categorical) data.\n\n Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15. ", "original_text": "Note that the datasets contains negative values."}, "hash": "9636a8510cbccca704cdd635c825e417b946646e5ec0abc532640164577393b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "172f3aee-0de8-4e31-8d99-0ce98a90107a", "node_type": "1", "metadata": {"window": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. ", "original_text": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. "}, "hash": "736133206996457aeff21a416d3859df4c2f95b3eb9bf93ffac54c45cccfc5b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes. ", "mimetype": "text/plain", "start_char_idx": 41693, "end_char_idx": 41803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "172f3aee-0de8-4e31-8d99-0ce98a90107a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. ", "original_text": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "218a566d-6c23-4051-b9fe-5c83f0d54414", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Future work directions may be an in-depth analysis of the method in other branches of applications, e.g., network anomalies, fraud detection, etc.  Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF.", "original_text": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes. "}, "hash": "973692a347fac68c587f59ded299e32e47f3ddbb7c1abaf0f08f1d9ee157acad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7745e6c1-26cc-4e1c-92a0-6d6d7eb3d18b", "node_type": "1", "metadata": {"window": "14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n", "original_text": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n"}, "hash": "cda8a1bb78af777ea1a225918d5b76abd144531823eddc70f6a12121f89738c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. ", "mimetype": "text/plain", "start_char_idx": 41803, "end_char_idx": 41918, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7745e6c1-26cc-4e1c-92a0-6d6d7eb3d18b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n", "original_text": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "172f3aee-0de8-4e31-8d99-0ce98a90107a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Moreover, an interesting ways of work seem to be various kinds of Granular Computing related data\n\n***\n**Fig.  14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. ", "original_text": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences. "}, "hash": "515ff7da41a97596909495c2b6ab56f12ac9fdc1418cbfd718487b5f49eca4e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fad4a385-b8e5-4248-baf0-97b41a71102d", "node_type": "1", "metadata": {"window": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. ", "original_text": "***\n**Fig. "}, "hash": "088f96e27437942d9f4fcee226ca71941fd997b54091886aa4e6ea5ef78316a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n", "mimetype": "text/plain", "start_char_idx": 41918, "end_char_idx": 42045, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fad4a385-b8e5-4248-baf0-97b41a71102d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. ", "original_text": "***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7745e6c1-26cc-4e1c-92a0-6d6d7eb3d18b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "14.  Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n", "original_text": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n"}, "hash": "4bd29bfb5a27215f180d60e0379fc0d4730eec8e245c1181f09b2451022e875f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "59277575-d78c-4096-9032-db300ed373bd", "node_type": "1", "metadata": {"window": "Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. ", "original_text": "15. "}, "hash": "c4035f383c1633beeb610e336deb0285e37969eda4ed36b5640b57f09eeb8027", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 42045, "end_char_idx": 42056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "59277575-d78c-4096-9032-db300ed373bd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. ", "original_text": "15. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fad4a385-b8e5-4248-baf0-97b41a71102d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Relations between anonymized train routes and times between train departures and arrivals to the stations in case of IF.  Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. ", "original_text": "***\n**Fig. "}, "hash": "28d4edfa50bbcc9c6a83285e00ed6a8806ec9a73f07baf8ef2f7e8d2fa075d26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e97305d-67b3-45f3-930f-63b562eabf40", "node_type": "1", "metadata": {"window": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n", "original_text": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF."}, "hash": "7f25ca2260fa34540bd26d1f94d02b60eb51a74f557da05f05a201ad37e20d69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15. ", "mimetype": "text/plain", "start_char_idx": 42056, "end_char_idx": 42060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e97305d-67b3-45f3-930f-63b562eabf40", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n", "original_text": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "59277575-d78c-4096-9032-db300ed373bd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Note that the datasets contains negative values. ** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. ", "original_text": "15. "}, "hash": "efc72e4fcdefcd17e0433c58a9f6cd4e06ba219a4c188ff9cb910c3b01f91d01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9445b98a-8e27-4037-9252-7207dae1861c", "node_type": "1", "metadata": {"window": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. ", "original_text": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. "}, "hash": "939558fad85ac49621ab22b042ade88755552f7e3b8a6ae28c42b05971a0a2e4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF.", "mimetype": "text/plain", "start_char_idx": 42060, "end_char_idx": 42241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9445b98a-8e27-4037-9252-7207dae1861c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. ", "original_text": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e97305d-67b3-45f3-930f-63b562eabf40", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This is a scatter plot showing the difference in departure times versus different anonymized train routes.  For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n", "original_text": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF."}, "hash": "dd3f2c1b726f0cd2c3feeefc4d93bc86a94210bc7b88336d5b74ac664b26a5d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c44f332-0bde-4c7b-9d34-c1f11f22156d", "node_type": "1", "metadata": {"window": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods.", "original_text": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n"}, "hash": "62d5d72a400969b23490de10970f481eec93b6f49f8c85076fdf0f2fba35f148", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. ", "mimetype": "text/plain", "start_char_idx": 42241, "end_char_idx": 42345, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c44f332-0bde-4c7b-9d34-c1f11f22156d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods.", "original_text": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9445b98a-8e27-4037-9252-7207dae1861c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "For each categorical route on the x-axis, there is a vertical scatter of points representing the time differences.  Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. ", "original_text": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF. "}, "hash": "2643ff8352c8d4f12bb4d17e5755c624f050e135ca2ca4717abe6485c0dbb665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d82d25c-7552-46ae-aec5-7954c346b6bc", "node_type": "1", "metadata": {"window": "***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. ", "original_text": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. "}, "hash": "52dbeccdbae63847bef266867d58fbfcd6e1c7c83ef0dd72303a9b75fbd352f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n", "mimetype": "text/plain", "start_char_idx": 42345, "end_char_idx": 42557, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6d82d25c-7552-46ae-aec5-7954c346b6bc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. ", "original_text": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c44f332-0bde-4c7b-9d34-c1f11f22156d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Some routes show a wider variance and more extreme (positive and negative) values, which IF identifies as potential anomalies.\n ***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods.", "original_text": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n"}, "hash": "03a08ca1df890de2401a6efdc06ccc126dfba2cdb35fa10e34a1c09196d55831", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8b7be9d-f321-4b25-8749-a23e250867c2", "node_type": "1", "metadata": {"window": "15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16. ", "original_text": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. "}, "hash": "86f715f4f30859c3b37c9df01b56c8cf6779bce4f48c9f7dbe856d413bce5f63", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. ", "mimetype": "text/plain", "start_char_idx": 42557, "end_char_idx": 42648, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8b7be9d-f321-4b25-8749-a23e250867c2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16. ", "original_text": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d82d25c-7552-46ae-aec5-7954c346b6bc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n**Fig.  15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. ", "original_text": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets. "}, "hash": "8bd3670a9752201915d6ecc9c2383c5ec8b55a3a9464cac8a25450136b70a00f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d42b50c4-9770-462d-817a-1e58ba30ad17", "node_type": "1", "metadata": {"window": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation.", "original_text": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n"}, "hash": "80a44fa460af828edd1b58754a2f2d8a78aba788660b5e4b972d4c752a687f81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. ", "mimetype": "text/plain", "start_char_idx": 42648, "end_char_idx": 42768, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d42b50c4-9770-462d-817a-1e58ba30ad17", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation.", "original_text": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8b7be9d-f321-4b25-8749-a23e250867c2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "15.  Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16. ", "original_text": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining. "}, "hash": "a7a31612299d77e438c6169ed6361366588eeb87080ed3a107f864a393be6db9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b58279f6-0736-4c80-a230-14d19e7b212f", "node_type": "1", "metadata": {"window": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset. ", "original_text": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. "}, "hash": "150a95cd336e6f27a58675b02334371176d3982bf5e9f1720e2cd74f202e0616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n", "mimetype": "text/plain", "start_char_idx": 42768, "end_char_idx": 42898, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b58279f6-0736-4c80-a230-14d19e7b212f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset. ", "original_text": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d42b50c4-9770-462d-817a-1e58ba30ad17", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Relations between anonymized train routes and times between train departures and arrivals to the stations (note that the table contains negative values) in case of k-Means-based IF. ** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation.", "original_text": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n"}, "hash": "481ce75c61769ce89bf3730df7d9ae7d876cdf9d99bf9183952b4dd4a433eea7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06634a43-6538-48a1-adb1-80f710cb2855", "node_type": "1", "metadata": {"window": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n", "original_text": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods."}, "hash": "a2117e906137e7427a21fa86dad1a166880f5d19621bfbdc8b4e2d55db5e90f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. ", "mimetype": "text/plain", "start_char_idx": 42898, "end_char_idx": 43194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "06634a43-6538-48a1-adb1-80f710cb2855", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n", "original_text": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b58279f6-0736-4c80-a230-14d19e7b212f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This plot is similar to Figure 14, but it reflects the anomalies identified by the k-Means-based IF.  The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset. ", "original_text": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization. "}, "hash": "321094ad6754bdcc9f6798ee2c059d0f183bacb62e6393318c80128d3a319c8d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81513697-9f36-4b73-a3a4-cd52ec352cca", "node_type": "1", "metadata": {"window": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization. ", "original_text": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. "}, "hash": "0ed587a8ec460a2199ff2bd3efc1ed10b1a981167b0eff2c4d6e0869ca8eedb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods.", "mimetype": "text/plain", "start_char_idx": 43194, "end_char_idx": 43318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "81513697-9f36-4b73-a3a4-cd52ec352cca", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization. ", "original_text": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "06634a43-6538-48a1-adb1-80f710cb2855", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The distribution of points is similar, but the color-coding (not fully described, but implied to be related to anomaly score) might differ, highlighting different points as anomalous compared to the standard IF.\n ***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n", "original_text": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods."}, "hash": "75c98dfbea47852146040291906706f0ed37f92e0ac861d2a7350bbe7f329e8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92d48142-9213-4e65-867b-ba9d45e28f28", "node_type": "1", "metadata": {"window": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. ", "original_text": "16. "}, "hash": "4737d321d0ad2e4661ca6ff50bc84051e4fb29ee70c2e190ae202aa9d7af02ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. ", "mimetype": "text/plain", "start_char_idx": 43318, "end_char_idx": 43932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92d48142-9213-4e65-867b-ba9d45e28f28", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. ", "original_text": "16. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81513697-9f36-4b73-a3a4-cd52ec352cca", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nstructures which may appear in the tree nodes such as fuzzy, rough, or shadowed sets.  An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization. ", "original_text": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig. "}, "hash": "510659c094372d5e21bfa8d42247f4a2d9e1e0b36ba3424956f14c42c2fb1e6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "858ae847-49f2-418f-9455-218bb03d1011", "node_type": "1", "metadata": {"window": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n", "original_text": "Histogram of rank differences for the data describing ship transportation."}, "hash": "0dc6ab6a9bf0fda8686b027f14eee8e7ab47519601d8f8df51b283228c58a258", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "16. ", "mimetype": "text/plain", "start_char_idx": 43932, "end_char_idx": 43936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "858ae847-49f2-418f-9455-218bb03d1011", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n", "original_text": "Histogram of rank differences for the data describing ship transportation."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92d48142-9213-4e65-867b-ba9d45e28f28", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "An application of the method to the task of classification based on an ensemble of classifiers is also worth examining.  Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. ", "original_text": "16. "}, "hash": "7f3dcbcf8e10522d77658ffdd914417d80b17f3edcb38f7d9c05640b6eaccceb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1c0e1a4-b72d-451a-8a34-bd74e7edbc39", "node_type": "1", "metadata": {"window": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "original_text": "** This bar chart shows the frequency of rank differences for the ship transportation dataset. "}, "hash": "93ff21cb92cf48a9da530d11edff0a7f93fb5313ef39ddc6545217c017a8434b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Histogram of rank differences for the data describing ship transportation.", "mimetype": "text/plain", "start_char_idx": 43936, "end_char_idx": 44010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1c0e1a4-b72d-451a-8a34-bd74e7edbc39", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "original_text": "** This bar chart shows the frequency of rank differences for the ship transportation dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "858ae847-49f2-418f-9455-218bb03d1011", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Finally, an extensive comparative analysis of various clustering-based methods is one of possible directions of further studies.\n\n **CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n", "original_text": "Histogram of rank differences for the data describing ship transportation."}, "hash": "58156fafac59e5836d374d34296dbd0d5a63803a328648c857d148c7f0fc7623", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b7402f5-248b-4275-a658-ed12de38b466", "node_type": "1", "metadata": {"window": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n", "original_text": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n"}, "hash": "67da08df0fdd2b4e6169b29d50bf5d983395fc8ab13d6284260938d925f83a24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** This bar chart shows the frequency of rank differences for the ship transportation dataset. ", "mimetype": "text/plain", "start_char_idx": 44010, "end_char_idx": 44105, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b7402f5-248b-4275-a658-ed12de38b466", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n", "original_text": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1c0e1a4-b72d-451a-8a34-bd74e7edbc39", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**CRediT authorship contribution statement**\n\n**Pawe\u0142 Karczmarek:** Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Data curation, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision, Project administration, Funding acquisition, Visualization.  **Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "original_text": "** This bar chart shows the frequency of rank differences for the ship transportation dataset. "}, "hash": "565c3bdbc92aa4899da496b1dcc3a8804a3112c361bd203f909678a5b1f67f1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "291a30cb-ed8a-4684-82ea-80f37a0be80c", "node_type": "1", "metadata": {"window": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. ", "original_text": "***\n\nData curation, Writing \u2013 review & editing, Visualization. "}, "hash": "e1ac2c996e9c46dfb11385fb2ddc6b72a5e9e8dfd30ab0a82a998301b98d6b1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n", "mimetype": "text/plain", "start_char_idx": 44105, "end_char_idx": 44312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "291a30cb-ed8a-4684-82ea-80f37a0be80c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. ", "original_text": "***\n\nData curation, Writing \u2013 review & editing, Visualization. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b7402f5-248b-4275-a658-ed12de38b466", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Adam Kiersztyn:** Validation, Formal analysis, Investigation, Resources,\n\n**Table 5**\n**Computing overhead of the methods. **\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n", "original_text": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n"}, "hash": "304ed9d1b3af0f839a27865bd5df97f93f7eaf14ca014cccf03bec430ad1b54e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44aed83c-9704-4465-bec5-efec6324da70", "node_type": "1", "metadata": {"window": "16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv. ", "original_text": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. "}, "hash": "8460f91431ed30ae66a1c8a5ab340501e83026b0b4c686c8f44681b4c4956c09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nData curation, Writing \u2013 review & editing, Visualization. ", "mimetype": "text/plain", "start_char_idx": 44312, "end_char_idx": 44375, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44aed83c-9704-4465-bec5-efec6324da70", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv. ", "original_text": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "291a30cb-ed8a-4684-82ea-80f37a0be80c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**\n\n| Dataset kind | Number of records | Number of attributes | Isolation forest (s) | k-Means-based isolation forest (s) |\n|---|---|---|---|---|\n| Artificial set | 30 000 | 2 | 4.584 | 2.524 |\n| Artificial set | 50 000 | 2 | 6.934 | 2.774 |\n| NYC Taxi | 3 386 426 | 14 | 792.66 | 17.444 |\n| NYC Taxi (geographical positions) | 737 462 | 2 | 54.687 | 7.47 |\n| Ship transport | 79 477 | 9 | 14.111 | 10.691 |\n| Ship transport | 9890 | 12 | 2.142 | 7.986 |\n| Train transport | 26 384 | 11 | 5.912 | 7.792 |\n| Train transport | 26 384 | 10 | 5.23 | 6.792 |\n| Train transport | 26 384 | 9 | 4.723 | 8.12 |\n\n***\n**Fig.  16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. ", "original_text": "***\n\nData curation, Writing \u2013 review & editing, Visualization. "}, "hash": "3bc279ba222356ed2c3a274150ef97ff5a7a10c7cfc8e350c52d8b2fde9df55c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e57ba0eb-a7ff-405b-9cfa-20045218794b", "node_type": "1", "metadata": {"window": "Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n", "original_text": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n"}, "hash": "e1a033dd40041271674b3beffca738de19f37cae675e8002ee9494ba905f50a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. ", "mimetype": "text/plain", "start_char_idx": 44375, "end_char_idx": 44512, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e57ba0eb-a7ff-405b-9cfa-20045218794b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n", "original_text": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44aed83c-9704-4465-bec5-efec6324da70", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "16.  Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv. ", "original_text": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision. "}, "hash": "867b87cafdf3623a3ee32559cba3a02c0068603061ce012e42105edf4e65abcd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77352501-d5c6-4573-8baf-923d34c91244", "node_type": "1", "metadata": {"window": "** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A. ", "original_text": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. "}, "hash": "69f2978023bf86935946767a04ebe16cca02fd92ef36e8a5a98248e4948355d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n", "mimetype": "text/plain", "start_char_idx": 44512, "end_char_idx": 44610, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77352501-d5c6-4573-8baf-923d34c91244", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A. ", "original_text": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e57ba0eb-a7ff-405b-9cfa-20045218794b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Histogram of rank differences for the data describing ship transportation. ** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n", "original_text": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n"}, "hash": "676ce01116c99336b90facccaabdb673e91b1e5151b53df1bd7a43abb63497d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63dd8022-5b68-4762-9160-2e554efb3ad2", "node_type": "1", "metadata": {"window": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "original_text": "2018/28/Z/ST6/00563).\n\n"}, "hash": "a8eaf2878b02afd4b9a51245a56a4888456b4bdbafd45685283f50a74cd3634b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. ", "mimetype": "text/plain", "start_char_idx": 44610, "end_char_idx": 44713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63dd8022-5b68-4762-9160-2e554efb3ad2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "original_text": "2018/28/Z/ST6/00563).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77352501-d5c6-4573-8baf-923d34c91244", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "** This bar chart shows the frequency of rank differences for the ship transportation dataset.  The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A. ", "original_text": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no. "}, "hash": "ca24a51bb255538c5f6a33529336bf8d6416ebd04cd087eadce4762153182a7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90b00a25-15ea-429f-9195-025be6954fd2", "node_type": "1", "metadata": {"window": "***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. ", "original_text": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. "}, "hash": "84ad6e339cf80d432572b38b30eeadc05d5d08ca73dc5b411dad049aecb5b7ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2018/28/Z/ST6/00563).\n\n", "mimetype": "text/plain", "start_char_idx": 44713, "end_char_idx": 44736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90b00a25-15ea-429f-9195-025be6954fd2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. ", "original_text": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63dd8022-5b68-4762-9160-2e554efb3ad2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "The distribution shows a large number of records with small rank differences, but also a significant tail with larger differences, again indicating cases where the two methods disagree on the anomaly level.\n ***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "original_text": "2018/28/Z/ST6/00563).\n\n"}, "hash": "62b08095d5f952d357c13b3392ef5a1b838e6468f16cbd6b8dfec8c25916d665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6999a48-7eab-4979-8ed7-789f76500ea0", "node_type": "1", "metadata": {"window": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf. ", "original_text": "Surv. "}, "hash": "46c542fdba3b7ceaa9c372180c6e13cc59e6adee5bde87e24bc4618a72586f23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. ", "mimetype": "text/plain", "start_char_idx": 44736, "end_char_idx": 44833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6999a48-7eab-4979-8ed7-789f76500ea0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf. ", "original_text": "Surv. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90b00a25-15ea-429f-9195-025be6954fd2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "***\n\nData curation, Writing \u2013 review & editing, Visualization.  **Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. ", "original_text": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput. "}, "hash": "4d531548ff36c4463cd43b7aa43caed2beab0ea4a6e3d822c1cb872fe0929234", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4b6a3a2-979b-4a82-8a20-5efddaa49c37", "node_type": "1", "metadata": {"window": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag. ", "original_text": "(CSUR) 41 (3) (2009) 1\u201372.\n"}, "hash": "5e0e0bd885edadd2b6be59acf9b4fd74a1592050e91e62897dca4cf060f29659", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Surv. ", "mimetype": "text/plain", "start_char_idx": 44833, "end_char_idx": 44839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4b6a3a2-979b-4a82-8a20-5efddaa49c37", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag. ", "original_text": "(CSUR) 41 (3) (2009) 1\u201372.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6999a48-7eab-4979-8ed7-789f76500ea0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Witold Pedrycz:** Conceptualization, Methodology, Validation, Investigation, Formal analysis, Writing \u2013 review & editing, Supervision.  **Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf. ", "original_text": "Surv. "}, "hash": "a2436e6dbfad75288035c50874ea6189aa254977a4e707aecc1656e767914c36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "757ee6de-d836-40ac-bc3b-f721dee20509", "node_type": "1", "metadata": {"window": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n", "original_text": "[2] R.A.A. "}, "hash": "b94101fd3fbff3ba92814c74977d6bc30c5c0151504a12f24e382b960aa3aa13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(CSUR) 41 (3) (2009) 1\u201372.\n", "mimetype": "text/plain", "start_char_idx": 44839, "end_char_idx": 44866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "757ee6de-d836-40ac-bc3b-f721dee20509", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n", "original_text": "[2] R.A.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4b6a3a2-979b-4a82-8a20-5efddaa49c37", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "**Ebru Al:** Resources, Project administration, Funding acquisition, Writing \u2013 review & editing.\n\n ### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag. ", "original_text": "(CSUR) 41 (3) (2009) 1\u201372.\n"}, "hash": "c760b5042886fef82e53793bdac07c16757d31275454c41c9979e7573b855691", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "901b7b9a-2d98-4a51-9fbb-b5fbddd035be", "node_type": "1", "metadata": {"window": "2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T. ", "original_text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. "}, "hash": "09fd1ccb59e24447b433cfcc27a001a3b4a4dc0ff660fbcd50b2b2a05f302ba5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] R.A.A. ", "mimetype": "text/plain", "start_char_idx": 44866, "end_char_idx": 44877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "901b7b9a-2d98-4a51-9fbb-b5fbddd035be", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T. ", "original_text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "757ee6de-d836-40ac-bc3b-f721dee20509", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### Acknowledgment\n\nFunded by the National Science Centre, Poland under CHIST-ERA programme (Grant no.  2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n", "original_text": "[2] R.A.A. "}, "hash": "f57ac558197c87fdf4ec6c65b35c6d3baf603c01cc1ccd7c568ec9d59985cce9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f41706-bd6b-424d-9304-6422fec2d532", "node_type": "1", "metadata": {"window": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M. ", "original_text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. "}, "hash": "470002b10700dedeadd245c6deaad854c3c9001aacc05c6ea35660c79d91a607", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. ", "mimetype": "text/plain", "start_char_idx": 44877, "end_char_idx": 44916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84f41706-bd6b-424d-9304-6422fec2d532", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M. ", "original_text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "901b7b9a-2d98-4a51-9fbb-b5fbddd035be", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2018/28/Z/ST6/00563).\n\n ### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T. ", "original_text": "Habeeb, F. Nasaruddin, A. Gani, I.A.T. "}, "hash": "92b7ed469c4b9b3db638daaaa646220dbe44681ee56e8022543231b2cb8256f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b509cac3-8b77-436d-a62c-908bcc835da1", "node_type": "1", "metadata": {"window": "Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "J. Inf. "}, "hash": "745ef16e70538c319112fd7260c5258d309080f79690615192ad1f67f729e440", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. ", "mimetype": "text/plain", "start_char_idx": 44916, "end_char_idx": 45012, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b509cac3-8b77-436d-a62c-908bcc835da1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "J. Inf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84f41706-bd6b-424d-9304-6422fec2d532", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "### References\n\n[1] V. Chandola, A. Banerjee, V. Kumar, Anomaly detection: A survey, ACM Comput.  Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M. ", "original_text": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int. "}, "hash": "0de85c0815416419e88dea1022e89679075915395b72a54339bc98ecf3e9ff60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f3cdd6a-e701-4fe2-9636-de31eeccd53e", "node_type": "1", "metadata": {"window": "(CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. ", "original_text": "Manag. "}, "hash": "19611a39525f39b2c9435026ac37dab61b6a04fc8cb9ca788f14572d3b851e61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J. Inf. ", "mimetype": "text/plain", "start_char_idx": 45012, "end_char_idx": 45020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f3cdd6a-e701-4fe2-9636-de31eeccd53e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "(CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. ", "original_text": "Manag. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b509cac3-8b77-436d-a62c-908bcc835da1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Surv.  (CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "J. Inf. "}, "hash": "468b6c3c1bc4a16d263b6e6aef64eddb2a464f40294e4b99df5a370587e5c743", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01d2e83a-f128-433a-9442-e7c00c7cb73d", "node_type": "1", "metadata": {"window": "[2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n", "original_text": "45 (2019) 289\u2013307.\n"}, "hash": "6e2df140c66c295d68a295a2f0d9de12c58e71b829fa22583cdab5c7c1e803bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Manag. ", "mimetype": "text/plain", "start_char_idx": 45020, "end_char_idx": 45027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01d2e83a-f128-433a-9442-e7c00c7cb73d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n", "original_text": "45 (2019) 289\u2013307.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f3cdd6a-e701-4fe2-9636-de31eeccd53e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "(CSUR) 41 (3) (2009) 1\u201372.\n [2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. ", "original_text": "Manag. "}, "hash": "aa672cc1278f8421c9171dadab6adf7429f51fa5dfa11abee7fac8b175539927", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "117b332a-79de-40e1-a3ab-727004ae7e42", "node_type": "1", "metadata": {"window": "Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T. ", "original_text": "[3] F.T. "}, "hash": "869b4e50947c5766146af6afe45ae61b6d5aca94d668a23c827e98e0d10446c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "45 (2019) 289\u2013307.\n", "mimetype": "text/plain", "start_char_idx": 45027, "end_char_idx": 45046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "117b332a-79de-40e1-a3ab-727004ae7e42", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T. ", "original_text": "[3] F.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01d2e83a-f128-433a-9442-e7c00c7cb73d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[2] R.A.A.  Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n", "original_text": "45 (2019) 289\u2013307.\n"}, "hash": "6292e154e3205fc874386834f3bdb682f45cce0cc87dbe7e3a6e68ec4247bbcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67ef7350-ed58-4dcc-bd37-61c8d7c9ee7f", "node_type": "1", "metadata": {"window": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M. ", "original_text": "Liu, K.M. "}, "hash": "c10e881db022f2200f57112554421c1bed73f4129bc28b2938623d50826aa388", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] F.T. ", "mimetype": "text/plain", "start_char_idx": 45046, "end_char_idx": 45055, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67ef7350-ed58-4dcc-bd37-61c8d7c9ee7f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M. ", "original_text": "Liu, K.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "117b332a-79de-40e1-a3ab-727004ae7e42", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Habeeb, F. Nasaruddin, A. Gani, I.A.T.  Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T. ", "original_text": "[3] F.T. "}, "hash": "30520d18d5f16970a14ab9bf1190ef04e27c98dfbf540b2cae779a0dfb653396", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f6071e2-d738-4322-87b5-ff2c2065cab1", "node_type": "1", "metadata": {"window": "J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "Ting, Z.-H. "}, "hash": "d2c50f2c3bf152ab2537b03878092625f7f526566cd59eb69eb7d73978cdc7c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu, K.M. ", "mimetype": "text/plain", "start_char_idx": 45055, "end_char_idx": 45065, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f6071e2-d738-4322-87b5-ff2c2065cab1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "Ting, Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67ef7350-ed58-4dcc-bd37-61c8d7c9ee7f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hashem, E. Ahmed, M. Imran, Real-time big data processing for anomaly detection: A survey, Int.  J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M. ", "original_text": "Liu, K.M. "}, "hash": "3e08f1ee794f4c3d634bd4e1811969ac38a496c03b1216ed66fed5e59b21ce2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8dc49bc-f154-46f0-8505-54fcfb6d0e75", "node_type": "1", "metadata": {"window": "Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans. ", "original_text": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. "}, "hash": "4ec2ba8a6116919b0678f5baf82a368eeda3e4ee85b736b96d2a9b5ea6b28216", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ting, Z.-H. ", "mimetype": "text/plain", "start_char_idx": 45065, "end_char_idx": 45077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8dc49bc-f154-46f0-8505-54fcfb6d0e75", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans. ", "original_text": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f6071e2-d738-4322-87b5-ff2c2065cab1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J. Inf.  Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H. ", "original_text": "Ting, Z.-H. "}, "hash": "5cc22627e5d0159e94c778f0b9cd77121303b3ef9e6481c20afbf789ef154d99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e5410f2-68f5-4dda-bcb4-dbf3a0473c68", "node_type": "1", "metadata": {"window": "45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl. ", "original_text": "413\u2013422.\n"}, "hash": "5c4d3f5880e2e339c85976ac142a4a526ff6aed01f8a24fc3848a93d8f79b928", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. ", "mimetype": "text/plain", "start_char_idx": 45077, "end_char_idx": 45173, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e5410f2-68f5-4dda-bcb4-dbf3a0473c68", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl. ", "original_text": "413\u2013422.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8dc49bc-f154-46f0-8505-54fcfb6d0e75", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Manag.  45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans. ", "original_text": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp. "}, "hash": "9617bac230137f9a8ba29fa5223c9001e17d527ef2c6206df34486df3975f920", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a951475c-9d2a-4947-b0ef-6fcfe8a5ee27", "node_type": "1", "metadata": {"window": "[3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov. ", "original_text": "[4] F.T. "}, "hash": "a405038974b75dd4c6d42737080b26febf851093e15aee18ace24c6ce8b7fa87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "413\u2013422.\n", "mimetype": "text/plain", "start_char_idx": 45173, "end_char_idx": 45182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a951475c-9d2a-4947-b0ef-6fcfe8a5ee27", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov. ", "original_text": "[4] F.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e5410f2-68f5-4dda-bcb4-dbf3a0473c68", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "45 (2019) 289\u2013307.\n [3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl. ", "original_text": "413\u2013422.\n"}, "hash": "48afbe157ba545990920ebb07999289d5a9dd2ee5d408aacab5d0a8e919dfb74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fd55553-9202-44b1-b49b-33a906c3f025", "node_type": "1", "metadata": {"window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n", "original_text": "Liu, K.M. "}, "hash": "b9b458b37e69103dbf12f3d6f83ece7f59e28c52a0d15984faa0b0b816090818", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] F.T. ", "mimetype": "text/plain", "start_char_idx": 45182, "end_char_idx": 45191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0fd55553-9202-44b1-b49b-33a906c3f025", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n", "original_text": "Liu, K.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a951475c-9d2a-4947-b0ef-6fcfe8a5ee27", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[3] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov. ", "original_text": "[4] F.T. "}, "hash": "a98fe4278ec21b0b8c5bbeb7c461176fe8b1653600e444324df8219a5db0ab89", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ba2346f-2fe6-4a5a-acbe-f1c558128104", "node_type": "1", "metadata": {"window": "Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. ", "original_text": "Ting, Z.-H. "}, "hash": "483ecc3dbcd73be14c93c85c21d42fb0b5f834c14741e3d8d7a95b276cc4a098", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu, K.M. ", "mimetype": "text/plain", "start_char_idx": 45191, "end_char_idx": 45201, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ba2346f-2fe6-4a5a-acbe-f1c558128104", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. ", "original_text": "Ting, Z.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fd55553-9202-44b1-b49b-33a906c3f025", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n", "original_text": "Liu, K.M. "}, "hash": "0850b07bfbc1ef5cd1f03a5122e89d20671367988a1e0fb00e0facb33940a04e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f52a4e0e-45e7-4693-933f-7022482c09a1", "node_type": "1", "metadata": {"window": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp. ", "original_text": "Zhou, Isolation-based anomaly detection, ACM Trans. "}, "hash": "4cedbe1b28f628f665f04f0b486f0fd15ecaea478511dac2d3c189a97feadb62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ting, Z.-H. ", "mimetype": "text/plain", "start_char_idx": 45201, "end_char_idx": 45213, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f52a4e0e-45e7-4693-933f-7022482c09a1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp. ", "original_text": "Zhou, Isolation-based anomaly detection, ACM Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ba2346f-2fe6-4a5a-acbe-f1c558128104", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. ", "original_text": "Ting, Z.-H. "}, "hash": "7ebc0ff3836e301d05ec91b095bc604345e437ad72396363b2da223b07765e9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c7bf35a-acfd-497b-b319-95e09a5ba21e", "node_type": "1", "metadata": {"window": "413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n", "original_text": "Knowl. "}, "hash": "2e554db81ac2d6ce1dcab148c15489ed8167ed0986de648d365d0546219c7c03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Zhou, Isolation-based anomaly detection, ACM Trans. ", "mimetype": "text/plain", "start_char_idx": 45213, "end_char_idx": 45265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c7bf35a-acfd-497b-b319-95e09a5ba21e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f52a4e0e-45e7-4693-933f-7022482c09a1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Zhou, Isolation forest, in: 2008 Eighth IEEE International Conference on Data Mining, 2008, pp.  413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp. ", "original_text": "Zhou, Isolation-based anomaly detection, ACM Trans. "}, "hash": "0e5b75dfa7099ad7929ea2f62f0596d54323a2243c1c45d87f33940220d5bc37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "923a893d-5a97-46f5-8956-316617fb229c", "node_type": "1", "metadata": {"window": "[4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n", "original_text": "Discov. "}, "hash": "3f8aba479a415cf0662368af51d56940458d994dd534beb3ea7a8c9398422494", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 45265, "end_char_idx": 45272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "923a893d-5a97-46f5-8956-316617fb229c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n", "original_text": "Discov. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c7bf35a-acfd-497b-b319-95e09a5ba21e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "413\u2013422.\n [4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n", "original_text": "Knowl. "}, "hash": "d64d14b5c012d43708d1028e9630a55a3466e7195ea409b77dd99d2ec182f8de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f252fc1f-abaa-4749-8be4-4099e560c64a", "node_type": "1", "metadata": {"window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. ", "original_text": "Data (TKDD) 6 (1) (2012) article (3).\n"}, "hash": "fb9fbf9d5e34d62d45cd8cf562fb4998da8aabf905624d7dd8e37997b152b75b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discov. ", "mimetype": "text/plain", "start_char_idx": 45272, "end_char_idx": 45280, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f252fc1f-abaa-4749-8be4-4099e560c64a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. ", "original_text": "Data (TKDD) 6 (1) (2012) article (3).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "923a893d-5a97-46f5-8956-316617fb229c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[4] F.T.  Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n", "original_text": "Discov. "}, "hash": "55e5bcade143e9e89f9498cc48ed5cbd8b9d8cad0494011071fa3faeb04cbb6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2708111-d741-4514-acaa-00be0eb5af63", "node_type": "1", "metadata": {"window": "Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf. ", "original_text": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. "}, "hash": "9a45222f85acc91348613effcb447b45ff7cd6fe88984fb6802658e5fbe5dbec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data (TKDD) 6 (1) (2012) article (3).\n", "mimetype": "text/plain", "start_char_idx": 45280, "end_char_idx": 45318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2708111-d741-4514-acaa-00be0eb5af63", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf. ", "original_text": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f252fc1f-abaa-4749-8be4-4099e560c64a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Liu, K.M.  Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. ", "original_text": "Data (TKDD) 6 (1) (2012) article (3).\n"}, "hash": "6668aa41d7ad45c43c30bceafdb90ff9d2a23f9e3c528ef3aa96ce40b6f2c809", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9f742cb-05cc-4361-837c-2bda2d48b881", "node_type": "1", "metadata": {"window": "Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp. ", "original_text": "2431, 2002, pp. "}, "hash": "29a6b7f788d00785564f46e580f0626cca72fd1474988340adc127f617c8ea6e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. ", "mimetype": "text/plain", "start_char_idx": 45318, "end_char_idx": 45497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c9f742cb-05cc-4361-837c-2bda2d48b881", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp. ", "original_text": "2431, 2002, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2708111-d741-4514-acaa-00be0eb5af63", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ting, Z.-H.  Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf. ", "original_text": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol. "}, "hash": "6d4c28c44cd0fa6b31a03a75f60b4f570f8cf939c8ea69488e027c127f763756", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36938886-bacd-4315-9e77-d0fe5ecfce9b", "node_type": "1", "metadata": {"window": "Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n", "original_text": "15\u201326.\n"}, "hash": "d871166fcfb1af2a11e1b4d4796262095953207230863c041e05a6ef5c6dc296", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2431, 2002, pp. ", "mimetype": "text/plain", "start_char_idx": 45497, "end_char_idx": 45513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36938886-bacd-4315-9e77-d0fe5ecfce9b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n", "original_text": "15\u201326.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9f742cb-05cc-4361-837c-2bda2d48b881", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Zhou, Isolation-based anomaly detection, ACM Trans.  Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp. ", "original_text": "2431, 2002, pp. "}, "hash": "7a70fc2cb322eca8cb7ae26489cde4490cd67ac779be7d92a0f4bf42cc0747c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "741b83e2-e151-4d4c-818e-ed95c6635a0e", "node_type": "1", "metadata": {"window": "Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B. ", "original_text": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n"}, "hash": "692878515d45b7dbb0b10806ffd5657d502e4e322436eaf3d309180d5feb41ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "15\u201326.\n", "mimetype": "text/plain", "start_char_idx": 45513, "end_char_idx": 45520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "741b83e2-e151-4d4c-818e-ed95c6635a0e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B. ", "original_text": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36938886-bacd-4315-9e77-d0fe5ecfce9b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n", "original_text": "15\u201326.\n"}, "hash": "78d924dc7b81a0edd682bcd2df5b350142172aa4affeb6b7d1f264f4352a10ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bda2bfe-93c9-465c-b30c-3620ff66410c", "node_type": "1", "metadata": {"window": "Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T. ", "original_text": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. "}, "hash": "032ee341b5d5a1280b2f27fb84d004c443b3cbe3db76e89ffc3177714ae2c544", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n", "mimetype": "text/plain", "start_char_idx": 45520, "end_char_idx": 45579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bda2bfe-93c9-465c-b30c-3620ff66410c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T. ", "original_text": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "741b83e2-e151-4d4c-818e-ed95c6635a0e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Discov.  Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B. ", "original_text": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n"}, "hash": "31e163408133c2cea6368e56323248c3d4f96bf34996bab9b65b95d3c39dea7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "319397e0-39c1-4902-b969-f5d6391484f7", "node_type": "1", "metadata": {"window": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. ", "original_text": "Conf. "}, "hash": "6e859389d8e13ba3548151939f9309c749415f5dfa26e94800d4fa6ef683584e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. ", "mimetype": "text/plain", "start_char_idx": 45579, "end_char_idx": 45725, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "319397e0-39c1-4902-b969-f5d6391484f7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bda2bfe-93c9-465c-b30c-3620ff66410c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data (TKDD) 6 (1) (2012) article (3).\n [5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T. ", "original_text": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int. "}, "hash": "9af07624dbc6ecd87b2ed01b4bebe975e3791c25908bf8844a357cf5b8a1eef2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a435c437-3ffd-4850-9a9d-686eb14d67b2", "node_type": "1", "metadata": {"window": "2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n", "original_text": "on Management of Data, 2000, pp. "}, "hash": "024122e1554e79896011b9b36ffa8e17f777a0c2f8c5495ad56493ca4e5c682c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 45725, "end_char_idx": 45731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a435c437-3ffd-4850-9a9d-686eb14d67b2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n", "original_text": "on Management of Data, 2000, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "319397e0-39c1-4902-b969-f5d6391484f7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[5] F. Angiulli, C. Pizzuti, Fast outlier detection in high dimensional spaces, in: Principles of Data Mining and Knowledge Discovery, in: Lecture Notes in Computer Science, vol.  2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. ", "original_text": "Conf. "}, "hash": "f4642d42dc5067d9bb3025257910e3cb7df81f276d08d71c3b5bdfbae6782f3e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "356db3eb-61ba-4f4e-9fc2-95ae987f2820", "node_type": "1", "metadata": {"window": "15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C. ", "original_text": "427\u2013438.\n"}, "hash": "e60682d8d391b8153a0eb7fc7f94a5b94ad4fea71453ffb45d274353dc2a69ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Management of Data, 2000, pp. ", "mimetype": "text/plain", "start_char_idx": 45731, "end_char_idx": 45764, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "356db3eb-61ba-4f4e-9fc2-95ae987f2820", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C. ", "original_text": "427\u2013438.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a435c437-3ffd-4850-9a9d-686eb14d67b2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2431, 2002, pp.  15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n", "original_text": "on Management of Data, 2000, pp. "}, "hash": "717858e474bc13c45d822ef1433c7a7cfd3a97c534caf8b13179e48a05d783ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1efdf5d6-2dce-42b3-991d-6e2cd6028c59", "node_type": "1", "metadata": {"window": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J. ", "original_text": "[8] E.B. "}, "hash": "5b12f253df3375c75a4e3f9abc102c470241d9c7383f41e2ecdba511aed37c97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "427\u2013438.\n", "mimetype": "text/plain", "start_char_idx": 45764, "end_char_idx": 45773, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1efdf5d6-2dce-42b3-991d-6e2cd6028c59", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J. ", "original_text": "[8] E.B. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "356db3eb-61ba-4f4e-9fc2-95ae987f2820", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "15\u201326.\n [6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C. ", "original_text": "427\u2013438.\n"}, "hash": "b0d2940509a3b6ef4e9d8834c951296d0ecdca08aef4508ec99329118c7adc04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f544f8b-0241-4321-9434-188855874163", "node_type": "1", "metadata": {"window": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C. ", "original_text": "Knorr, R.T. "}, "hash": "875d63685773ecafe449f68fe44e50495a40ca8c366c78aeb83fd1488242fe0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8] E.B. ", "mimetype": "text/plain", "start_char_idx": 45773, "end_char_idx": 45782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f544f8b-0241-4321-9434-188855874163", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C. ", "original_text": "Knorr, R.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1efdf5d6-2dce-42b3-991d-6e2cd6028c59", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[6] H. Kim, Isolation forest step by step, 2019, [online].\n [7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J. ", "original_text": "[8] E.B. "}, "hash": "6b71ca70d324704b5be970c84fbdcb3d502fa0f245e1c3bc44ed27f064916832", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c819eb06-20fa-4fb4-b990-4eb5e8fb2701", "node_type": "1", "metadata": {"window": "Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. ", "original_text": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. "}, "hash": "a6488f1604caa0168ff4d6abf3fbc76828db5bca203977fa74659c65f3456adf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knorr, R.T. ", "mimetype": "text/plain", "start_char_idx": 45782, "end_char_idx": 45794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c819eb06-20fa-4fb4-b990-4eb5e8fb2701", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. ", "original_text": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f544f8b-0241-4321-9434-188855874163", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[7] S. Ramaswamy, R. Rastogi, K. Shim, Efficient algorithms for mining outliers from large data sets, in: Proceedings of the 2000 ACM SIGMOD Int.  Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C. ", "original_text": "Knorr, R.T. "}, "hash": "45474652bf0cf8d5f59843fda534c25e7db5a8b4181efbdf3a5a10a22032d234", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3777e05a-e713-4070-b93a-28b68f2c07f2", "node_type": "1", "metadata": {"window": "on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n", "original_text": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n"}, "hash": "34e60283d6706c221ff076091b422159529448bdada015d1ebfc44f7791e4b64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. ", "mimetype": "text/plain", "start_char_idx": 45794, "end_char_idx": 45874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3777e05a-e713-4070-b93a-28b68f2c07f2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n", "original_text": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c819eb06-20fa-4fb4-b990-4eb5e8fb2701", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. ", "original_text": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int. "}, "hash": "ba8d51fdcca7c34b27188cb0ad689bf6221b427a40ef1f356529fb8949ca2d76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ed9d91f-9882-4d9d-8c88-c2b375822e0b", "node_type": "1", "metadata": {"window": "427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. ", "original_text": "[9] B. Sch\u00f6lkopf, J.C. "}, "hash": "bd9b4647c0f9acf47d4348867120461dab279f687912f30c1f1c0e4d75d78da6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n", "mimetype": "text/plain", "start_char_idx": 45874, "end_char_idx": 45923, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ed9d91f-9882-4d9d-8c88-c2b375822e0b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. ", "original_text": "[9] B. Sch\u00f6lkopf, J.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3777e05a-e713-4070-b93a-28b68f2c07f2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Management of Data, 2000, pp.  427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n", "original_text": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n"}, "hash": "66fc0b491e302a55e88aba1f4114cb052dfae2d80c413dd6539c768dcc46255f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbdac6c0-8269-4271-9a0c-8df709d2f125", "node_type": "1", "metadata": {"window": "[8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n", "original_text": "Platt, J. Shawe-Taylor, A.J. "}, "hash": "e8ebc08bc4cc16e26dd27fefc954dc8d431e9999ea5fa6c227715fb59b0513b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] B. Sch\u00f6lkopf, J.C. ", "mimetype": "text/plain", "start_char_idx": 45923, "end_char_idx": 45946, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fbdac6c0-8269-4271-9a0c-8df709d2f125", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n", "original_text": "Platt, J. Shawe-Taylor, A.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ed9d91f-9882-4d9d-8c88-c2b375822e0b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "427\u2013438.\n [8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. ", "original_text": "[9] B. Sch\u00f6lkopf, J.C. "}, "hash": "885ac3a06aa5c89888f2d6589d5e4e7ac8bb64816d23377ff952a646ec62be23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dca47862-ef84-4677-a1d0-c8a395fb525d", "node_type": "1", "metadata": {"window": "Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. ", "original_text": "Smola, R.C. "}, "hash": "3e6a34ff8b948c80e7d47c3c3847f625421283ab45150ded82ab21eb84fd13bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Platt, J. Shawe-Taylor, A.J. ", "mimetype": "text/plain", "start_char_idx": 45946, "end_char_idx": 45975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dca47862-ef84-4677-a1d0-c8a395fb525d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. ", "original_text": "Smola, R.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbdac6c0-8269-4271-9a0c-8df709d2f125", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[8] E.B.  Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n", "original_text": "Platt, J. Shawe-Taylor, A.J. "}, "hash": "9ebf093f8bc163f605501383daf7cc6ba3d20f7ff3f8151091941ef13729a3ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "199f841d-4877-4d66-8368-51e8d113853e", "node_type": "1", "metadata": {"window": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n", "original_text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. "}, "hash": "7754a1f87ece3a6d0068885462a1fe9cbc4dbe68e12090d8d7d805ae15d637b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Smola, R.C. ", "mimetype": "text/plain", "start_char_idx": 45975, "end_char_idx": 45987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "199f841d-4877-4d66-8368-51e8d113853e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n", "original_text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dca47862-ef84-4677-a1d0-c8a395fb525d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knorr, R.T.  Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. ", "original_text": "Smola, R.C. "}, "hash": "e2c88e33da95e35b56b4e69050a3497f3fc5cfc1a012712a164fb1ec6df832f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d30db65c-a93c-44a0-94a0-68e1ab0e8707", "node_type": "1", "metadata": {"window": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. ", "original_text": "13 (7) (2001) 1443\u20131471.\n"}, "hash": "946653a57cc4712e2521204ebbbc792bd926cbd503b8e77024d9686e1f868318", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. ", "mimetype": "text/plain", "start_char_idx": 45987, "end_char_idx": 46073, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d30db65c-a93c-44a0-94a0-68e1ab0e8707", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. ", "original_text": "13 (7) (2001) 1443\u20131471.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "199f841d-4877-4d66-8368-51e8d113853e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ng, V. Tucakov, Distance-based outliers: Algorithms and applications, VLDB Int.  J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n", "original_text": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput. "}, "hash": "bb41a6a218e0a0206229d0c70d078ccfcef48cd31285e60d92e44ef77b350035", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "888e42e8-0e32-44f5-9071-5edd7da6e87f", "node_type": "1", "metadata": {"window": "[9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n", "original_text": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. "}, "hash": "081b9f66437bfa77e7f5ebd096da188848ef9253145e189d8eac8c83acb37410", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "13 (7) (2001) 1443\u20131471.\n", "mimetype": "text/plain", "start_char_idx": 46073, "end_char_idx": 46098, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "888e42e8-0e32-44f5-9071-5edd7da6e87f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n", "original_text": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d30db65c-a93c-44a0-94a0-68e1ab0e8707", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J. Very Large Data Bases 8 (3\u20134) (2000) 237\u2013253.\n [9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. ", "original_text": "13 (7) (2001) 1443\u20131471.\n"}, "hash": "a4beee8f2f4b5e9b40dbac05771ba216aa8b16a21511d2d3c9796774cde3600f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b734041-ef93-4149-9769-ccf80399c9eb", "node_type": "1", "metadata": {"window": "Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M. ", "original_text": "40 (2013) 1\u20136.\n"}, "hash": "3bc67e6f519d377410c6f4da2b7043bf4bd620db7b3428919d97bc70c3dc3a74", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. ", "mimetype": "text/plain", "start_char_idx": 46098, "end_char_idx": 46244, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b734041-ef93-4149-9769-ccf80399c9eb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M. ", "original_text": "40 (2013) 1\u20136.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "888e42e8-0e32-44f5-9071-5edd7da6e87f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[9] B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n", "original_text": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst. "}, "hash": "332a9362b409bc9eca9ed9405eb3c8371edc0da7d088bdee074bba81e2d151cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0eff60c-5ee2-4ed4-8693-cbf7ae0ddcc4", "node_type": "1", "metadata": {"window": "Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. ", "original_text": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. "}, "hash": "41042f94b2eece94e901cb68d9761b5d6103e1f4e58c8fffca87efd3843d1bea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "40 (2013) 1\u20136.\n", "mimetype": "text/plain", "start_char_idx": 46244, "end_char_idx": 46259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b0eff60c-5ee2-4ed4-8693-cbf7ae0ddcc4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. ", "original_text": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b734041-ef93-4149-9769-ccf80399c9eb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Platt, J. Shawe-Taylor, A.J.  Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M. ", "original_text": "40 (2013) 1\u20136.\n"}, "hash": "7c7db7956e5c8fe5a3d6c4a0511d8b8a6b0eb02c0af95619bc25bb9932c132e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1161e1f6-6ae8-4b15-9262-fc3835724a1e", "node_type": "1", "metadata": {"window": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n", "original_text": "139 (2018) 50\u201363.\n"}, "hash": "04d272b8ddc7c1d0121f1392418d0029babb466a54b7d79319901ff5afc0b38f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. ", "mimetype": "text/plain", "start_char_idx": 46259, "end_char_idx": 46382, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1161e1f6-6ae8-4b15-9262-fc3835724a1e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n", "original_text": "139 (2018) 50\u201363.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0eff60c-5ee2-4ed4-8693-cbf7ae0ddcc4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Smola, R.C.  Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. ", "original_text": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst. "}, "hash": "a680e44d92559e69bac392732d46e3733985c5299aca27a448019bba5489ec41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "956981c5-0c64-4ec4-a1ae-2b4d86f2eb89", "node_type": "1", "metadata": {"window": "13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G. ", "original_text": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. "}, "hash": "8cc32b7af310d19ab7ed18bfca46ce11af6998aaaaaf1ab089969314a2b30ab5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "139 (2018) 50\u201363.\n", "mimetype": "text/plain", "start_char_idx": 46382, "end_char_idx": 46400, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "956981c5-0c64-4ec4-a1ae-2b4d86f2eb89", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G. ", "original_text": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1161e1f6-6ae8-4b15-9262-fc3835724a1e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Williamson, Estimating the support of a high-dimensional distribution, Neural Comput.  13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n", "original_text": "139 (2018) 50\u201363.\n"}, "hash": "067807be1bc63ce1813f8762f76cda486a974969b90d53b6f0f81c5602b082f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6ff03fa-2aad-49f1-9aca-1e6d3c24cc7f", "node_type": "1", "metadata": {"window": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "original_text": "71 (2014) 322\u2013338.\n"}, "hash": "f270f3b18c64b4a50dd313a85c626772e135b2ddc7860ddaa7612aa6ba3fea69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. ", "mimetype": "text/plain", "start_char_idx": 46400, "end_char_idx": 46624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6ff03fa-2aad-49f1-9aca-1e6d3c24cc7f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "original_text": "71 (2014) 322\u2013338.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "956981c5-0c64-4ec4-a1ae-2b4d86f2eb89", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "13 (7) (2001) 1443\u20131471.\n [10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G. ", "original_text": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst. "}, "hash": "8c9c246910fa72801c1f264b56a4c2f9172d8dea26f2c0c9fb61ae9165ac62c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d984c9c-8a19-4582-aae9-3ee2bafbe7de", "node_type": "1", "metadata": {"window": "40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n", "original_text": "[13] S.M. "}, "hash": "e812d5c34c7a588d0036ff2b1d91f41bec2daf8a14ed9971da8bdcc441f6b0ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "71 (2014) 322\u2013338.\n", "mimetype": "text/plain", "start_char_idx": 46624, "end_char_idx": 46643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d984c9c-8a19-4582-aae9-3ee2bafbe7de", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n", "original_text": "[13] S.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6ff03fa-2aad-49f1-9aca-1e6d3c24cc7f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[10] X. Gan, J. Duanmu, J. Wang, W. Cong, Anomaly intrusion detection based on PLS feature extraction and core vector machine, Knowl.-Based Syst.  40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "original_text": "71 (2014) 322\u2013338.\n"}, "hash": "9686cfa98c28e7225ab6210efe27f881118623b1b4708b8c10b69ae9155402ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801c8dbb-899d-4c34-9e40-906e5223ae3a", "node_type": "1", "metadata": {"window": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C. ", "original_text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. "}, "hash": "c52e737fdeb844b5fd15a372fbe30669193fe70082d7819a8aebc61eebc42a84", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[13] S.M. ", "mimetype": "text/plain", "start_char_idx": 46643, "end_char_idx": 46653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "801c8dbb-899d-4c34-9e40-906e5223ae3a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C. ", "original_text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d984c9c-8a19-4582-aae9-3ee2bafbe7de", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "40 (2013) 1\u20136.\n [11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n", "original_text": "[13] S.M. "}, "hash": "52c625b13767911056aac7eb391b8a5f3061446feba6470e6eace46ea01f2f97", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a907337-73ef-4ec4-88bb-ee3ca1cac7f0", "node_type": "1", "metadata": {"window": "139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "original_text": "58 (2016) 121\u2013134.\n"}, "hash": "7bd25bb2918eb4560153e851e290cd17a15cc5d3f8f508b9983b3dc5543b568e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. ", "mimetype": "text/plain", "start_char_idx": 46653, "end_char_idx": 46823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a907337-73ef-4ec4-88bb-ee3ca1cac7f0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "original_text": "58 (2016) 121\u2013134.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801c8dbb-899d-4c34-9e40-906e5223ae3a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[11] L. Zhang, J. Lin, R. Karim, Adaptive kernel density-based anomaly detection for nonlinear systems, Knowl.-Based Syst.  139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C. ", "original_text": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit. "}, "hash": "a7cc997725a6b5746159d0e228219cc6fe4a0cdbd7a13a636827c0ec63b80444", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a6958b0a-18ee-42bc-b092-2ad7a84536fc", "node_type": "1", "metadata": {"window": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n", "original_text": "[14] P. Malhotra, L. Vig, G.G. "}, "hash": "c76c318e057bf2989c46389204e4a962b9b3fe856577ccb9bf15c862bb1782f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "58 (2016) 121\u2013134.\n", "mimetype": "text/plain", "start_char_idx": 46823, "end_char_idx": 46842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a6958b0a-18ee-42bc-b092-2ad7a84536fc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n", "original_text": "[14] P. Malhotra, L. Vig, G.G. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a907337-73ef-4ec4-88bb-ee3ca1cac7f0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "139 (2018) 50\u201363.\n [12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "original_text": "58 (2016) 121\u2013134.\n"}, "hash": "b4ccf6bb672dfa23a9494921937b846e64c59779f9357fbef7ff8de842fa28fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7664de06-6851-4167-9145-6d8b2309c0d0", "node_type": "1", "metadata": {"window": "71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B. ", "original_text": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. "}, "hash": "1cef754326c76b2025a5fea9f6ba67aa0e8548b02ce3117abc531bd29fff1a19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] P. Malhotra, L. Vig, G.G. ", "mimetype": "text/plain", "start_char_idx": 46842, "end_char_idx": 46873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7664de06-6851-4167-9145-6d8b2309c0d0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B. ", "original_text": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6958b0a-18ee-42bc-b092-2ad7a84536fc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[12] E. de la Hoz, E. de la Hoz, A. Ortiz, J. Ortega, A. Mart\u00ednez-\u00c1lvarez, Feature selection by multi-objective optimisation: Application to network anomaly detection by hierarchical self-organising maps, Knowl.-Based Syst.  71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n", "original_text": "[14] P. Malhotra, L. Vig, G.G. "}, "hash": "2fa961b289c70a8449e0d61039c0307dd37af229f5d70fdba823f4e826f8f1d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a2efc02-8dd2-4493-938c-fd1800fdb0cd", "node_type": "1", "metadata": {"window": "[13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. ", "original_text": "89\u201394.\n"}, "hash": "2c7195e86d8802c788c25da3bd0d359517e06687d533bfdfd7072d9698b1fe6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. ", "mimetype": "text/plain", "start_char_idx": 46873, "end_char_idx": 47076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a2efc02-8dd2-4493-938c-fd1800fdb0cd", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. ", "original_text": "89\u201394.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7664de06-6851-4167-9145-6d8b2309c0d0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "71 (2014) 322\u2013338.\n [13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B. ", "original_text": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp. "}, "hash": "8a3ce01e9e7887de792afe5d43083b1f493f5bf56208165caff4057932890eab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ebcb8ea-c687-4a25-a7f3-c6196e2650ef", "node_type": "1", "metadata": {"window": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl. ", "original_text": "[15] C. Zhou, R.C. "}, "hash": "cc132bed75a755154e402a9623990374453f3e38d38f8acf9ee25d96e6b60ace", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "89\u201394.\n", "mimetype": "text/plain", "start_char_idx": 47076, "end_char_idx": 47083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ebcb8ea-c687-4a25-a7f3-c6196e2650ef", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl. ", "original_text": "[15] C. Zhou, R.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a2efc02-8dd2-4493-938c-fd1800fdb0cd", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[13] S.M.  Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. ", "original_text": "89\u201394.\n"}, "hash": "5514c064c4c08bd0da1ca5d8b6c0b0fc9c6e58a9ba46970ead8c4b08aedd6095", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c044802-e1e3-41c6-adf6-df00c10e1a61", "node_type": "1", "metadata": {"window": "58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov. ", "original_text": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. "}, "hash": "2f0d25017a93e749b7929e5e7dbc3d4e03c949c8bda8d4a73db38e57bda40654", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[15] C. Zhou, R.C. ", "mimetype": "text/plain", "start_char_idx": 47083, "end_char_idx": 47102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c044802-e1e3-41c6-adf6-df00c10e1a61", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov. ", "original_text": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ebcb8ea-c687-4a25-a7f3-c6196e2650ef", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Erfani, S. Rajasegarar, S. Karunasekera, C. Leckie, High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning, Pattern Recognit.  58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl. ", "original_text": "[15] C. Zhou, R.C. "}, "hash": "d0e5555fdc384d9a381f02c8774751af2beeb114825cacf3fb7c0711e4a77fe1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c675592b-ecbf-4b24-8419-9e68a2baef54", "node_type": "1", "metadata": {"window": "[14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n", "original_text": "665\u2013674.\n"}, "hash": "2cd89de7aa563ec17a084effbd6868667a79acfbb6661eeca8a06a5d7bd3a92c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. ", "mimetype": "text/plain", "start_char_idx": 47102, "end_char_idx": 47294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c675592b-ecbf-4b24-8419-9e68a2baef54", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n", "original_text": "665\u2013674.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c044802-e1e3-41c6-adf6-df00c10e1a61", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "58 (2016) 121\u2013134.\n [14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov. ", "original_text": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp. "}, "hash": "9e948ced7b3287e796a5fffb3874fefef5e6ded5cfdf025f77cb4f89fcaac2dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad215a1c-2476-4082-b2e8-f51c218d9572", "node_type": "1", "metadata": {"window": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. ", "original_text": "[16] R.J.G.B. "}, "hash": "dd00f29e6fc20899fccef1703eaf309e7f8ab9369d3f4ec6c32b5d093872342f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "665\u2013674.\n", "mimetype": "text/plain", "start_char_idx": 47294, "end_char_idx": 47303, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad215a1c-2476-4082-b2e8-f51c218d9572", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. ", "original_text": "[16] R.J.G.B. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c675592b-ecbf-4b24-8419-9e68a2baef54", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[14] P. Malhotra, L. Vig, G.G.  Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n", "original_text": "665\u2013674.\n"}, "hash": "b0a9b8a082a9ae8bed08abb6b56767c9698c7b392b2182cc8d03e0bc060b40ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6034085b-f94a-42cb-a8a6-5a35a42f4ea1", "node_type": "1", "metadata": {"window": "89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett. ", "original_text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. "}, "hash": "9d2a20e74d6894839b707cf462c76ab13f288b4ac851449b2dae9d265c4d3333", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] R.J.G.B. ", "mimetype": "text/plain", "start_char_idx": 47303, "end_char_idx": 47317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6034085b-f94a-42cb-a8a6-5a35a42f4ea1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett. ", "original_text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad215a1c-2476-4082-b2e8-f51c218d9572", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Shroff, P. Agarwal, Long short term memory networks for anomaly detection in time series, in: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, 2015, pp.  89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. ", "original_text": "[16] R.J.G.B. "}, "hash": "9513ec40b1866b46da3f36a8c64032d6e49af5582101ce8d3243d1728a67ec78", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "175979cd-a9f4-4c59-bd6e-a903c41f47f8", "node_type": "1", "metadata": {"window": "[15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n", "original_text": "Knowl. "}, "hash": "d25e2db081b8eb2ca40d96e1900607de54bbce6db935d2292328bf1c284e404d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. ", "mimetype": "text/plain", "start_char_idx": 47317, "end_char_idx": 47461, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "175979cd-a9f4-4c59-bd6e-a903c41f47f8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6034085b-f94a-42cb-a8a6-5a35a42f4ea1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "89\u201394.\n [15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett. ", "original_text": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans. "}, "hash": "02d048349d7e2b0ee89566724304c655111fb6a00e85015ae20ad7c8d217eb5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "501dba22-8620-4945-ba38-b95c10400f9d", "node_type": "1", "metadata": {"window": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. ", "original_text": "Discov. "}, "hash": "c0cc817dc60f24bc88504980ab315c8b2db9cd2b3ecf6a9253227fd85cddbdab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 47461, "end_char_idx": 47468, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "501dba22-8620-4945-ba38-b95c10400f9d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. ", "original_text": "Discov. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "175979cd-a9f4-4c59-bd6e-a903c41f47f8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[15] C. Zhou, R.C.  Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n", "original_text": "Knowl. "}, "hash": "fb4a94f7cc2f7d9586de3ac76ce3d20f3357ccb4f97d6254a3a30c7622abee1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cf27335-65fe-45d8-aa08-146cbb9363e4", "node_type": "1", "metadata": {"window": "665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl. ", "original_text": "Data 10 (1) (2015) 5.\n"}, "hash": "fd3bb12fde28824427151e7355ea140bddf0c1625747b37200ba1b8e6fbb84cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discov. ", "mimetype": "text/plain", "start_char_idx": 47468, "end_char_idx": 47476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6cf27335-65fe-45d8-aa08-146cbb9363e4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl. ", "original_text": "Data 10 (1) (2015) 5.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "501dba22-8620-4945-ba38-b95c10400f9d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Paffenroth, Anomaly detection with robust deep autoencoders, in: KDD \u201917 Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, 2017, pp.  665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. ", "original_text": "Discov. "}, "hash": "e1195379a2cb77934802593dbea51200ccbd2c1c979aadc31aa45ccd06400396", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61ce6607-2c87-4123-adb5-eef96e05f31e", "node_type": "1", "metadata": {"window": "[16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n", "original_text": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. "}, "hash": "fcb362a8b2f32041249b01aa5404ce4aa0befa61d746dfaee33a05665d0e0fff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data 10 (1) (2015) 5.\n", "mimetype": "text/plain", "start_char_idx": 47476, "end_char_idx": 47498, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61ce6607-2c87-4123-adb5-eef96e05f31e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n", "original_text": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cf27335-65fe-45d8-aa08-146cbb9363e4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "665\u2013674.\n [16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl. ", "original_text": "Data 10 (1) (2015) 5.\n"}, "hash": "103b82064916dba8727fd7f971b4dc5f8501eb71867fead5863a1251af914dfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63420b2d-9809-44f6-8f28-a647241ec7a3", "node_type": "1", "metadata": {"window": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. ", "original_text": "Lett. "}, "hash": "3eada70e6fceb6161a4676fcac54fe6406883c16d724016ba799a0f9edb98624", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. ", "mimetype": "text/plain", "start_char_idx": 47498, "end_char_idx": 47586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63420b2d-9809-44f6-8f28-a647241ec7a3", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. ", "original_text": "Lett. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61ce6607-2c87-4123-adb5-eef96e05f31e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[16] R.J.G.B.  Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n", "original_text": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit. "}, "hash": "a396325ee1aafb7bc78325351919f5adb6c9ee5f63a7e108c4ab2aec86b470fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c318685-f73a-4b89-8782-2cf634aadf11", "node_type": "1", "metadata": {"window": "Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput. ", "original_text": "24 (9\u201310) (2003) 1641\u20131650.\n"}, "hash": "6433d99d7bb67a5be1a2937292b68cd3ab74fde1cc405030f579f89d8132bbd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lett. ", "mimetype": "text/plain", "start_char_idx": 47586, "end_char_idx": 47592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c318685-f73a-4b89-8782-2cf634aadf11", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput. ", "original_text": "24 (9\u201310) (2003) 1641\u20131650.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63420b2d-9809-44f6-8f28-a647241ec7a3", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Campello, D. Moulavi, A. Zimek, J. Sander, Hierarchical density estimates for data clustering, visualization, and outlier detection, ACM Trans.  Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. ", "original_text": "Lett. "}, "hash": "80f8a125f39855401d44e8ad144d839c5a2a9c1cdcdc2385ec643e18eeb8d482", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b5c6ae1-333b-4d9b-b90e-87b1c0bfa75f", "node_type": "1", "metadata": {"window": "Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur. ", "original_text": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. "}, "hash": "b213094eeb3a6653407347b58c4d9741d62c8f61d04ff9449ba1f1a08ef509b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "24 (9\u201310) (2003) 1641\u20131650.\n", "mimetype": "text/plain", "start_char_idx": 47592, "end_char_idx": 47620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b5c6ae1-333b-4d9b-b90e-87b1c0bfa75f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur. ", "original_text": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c318685-f73a-4b89-8782-2cf634aadf11", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput. ", "original_text": "24 (9\u201310) (2003) 1641\u20131650.\n"}, "hash": "ba7ff7e09c49299920848c9bd7961842251706f44261b966dee9a7b529fd8dfd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2fde348-a022-494c-b74f-49764f185416", "node_type": "1", "metadata": {"window": "Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n", "original_text": "Appl. "}, "hash": "46aff7613b3a7c9e6ab445e540947efab53871770733145988775b6784f2a4bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. ", "mimetype": "text/plain", "start_char_idx": 47620, "end_char_idx": 47720, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f2fde348-a022-494c-b74f-49764f185416", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n", "original_text": "Appl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b5c6ae1-333b-4d9b-b90e-87b1c0bfa75f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Discov.  Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur. ", "original_text": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal. "}, "hash": "948c0f0977fe78363d3c726cec28d0c2e26412de7e97dc158462d9bae4e4f087", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed46934d-6580-4f3e-b851-a55e3e2038f5", "node_type": "1", "metadata": {"window": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. ", "original_text": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n"}, "hash": "e905f282ef72a3319980e463302a4f17324766ea4afc293f761147d7bbf7697e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appl. ", "mimetype": "text/plain", "start_char_idx": 47720, "end_char_idx": 47726, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed46934d-6580-4f3e-b851-a55e3e2038f5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. ", "original_text": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2fde348-a022-494c-b74f-49764f185416", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data 10 (1) (2015) 5.\n [17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n", "original_text": "Appl. "}, "hash": "d4fc87faf54d6fcf6ff91bb01518cebb14f5a70f186099ee38a0149a2360967f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dedc6ea9-03ac-4a4c-8cb8-bf0136976b75", "node_type": "1", "metadata": {"window": "Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl. ", "original_text": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. "}, "hash": "0e2d3b4299c7bb0e00d025edecaf7ea935c927ed2d0c67732f21b91010119eda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n", "mimetype": "text/plain", "start_char_idx": 47726, "end_char_idx": 47779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dedc6ea9-03ac-4a4c-8cb8-bf0136976b75", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl. ", "original_text": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed46934d-6580-4f3e-b851-a55e3e2038f5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[17] Z. He, X. Xu, S. Deng, Discovering cluster-based local outliers, Pattern Recognit.  Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. ", "original_text": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n"}, "hash": "b5d386fe8d21f314088365b281300b0ede92410739bedb43352e837c5f1c40e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c8a28a1-efb9-4261-8495-b42d39265520", "node_type": "1", "metadata": {"window": "24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n", "original_text": "Comput. "}, "hash": "2edd9c66106a8b04d4b80bdf98efaec400c84390fb3caa779254c3b7e1a0971d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. ", "mimetype": "text/plain", "start_char_idx": 47779, "end_char_idx": 47870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c8a28a1-efb9-4261-8495-b42d39265520", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n", "original_text": "Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dedc6ea9-03ac-4a4c-8cb8-bf0136976b75", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Lett.  24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl. ", "original_text": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw. "}, "hash": "ee61983b19d9bbc3547f524c492f54e4e85be7c4219ab81a09547fa9290ed35c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "197383b5-6d27-4585-b5fc-69c1e096202e", "node_type": "1", "metadata": {"window": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H. ", "original_text": "Secur. "}, "hash": "503f543dc2be2d24a0c76ff4e6c067cbff73f4474726934d2ee07537bb32241b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comput. ", "mimetype": "text/plain", "start_char_idx": 47870, "end_char_idx": 47878, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "197383b5-6d27-4585-b5fc-69c1e096202e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H. ", "original_text": "Secur. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c8a28a1-efb9-4261-8495-b42d39265520", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "24 (9\u201310) (2003) 1641\u20131650.\n [18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n", "original_text": "Comput. "}, "hash": "e5f936c08073bfa80fee77255bb5cffd6634105eaae74ee653481ca81390a679", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbcd8123-c7b0-4b9e-bbff-9935e5e4fe44", "node_type": "1", "metadata": {"window": "Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M. ", "original_text": "8 (2007) 43\u201346.\n"}, "hash": "551b84823650ddfb0b040b13cb2364b148a35648d995b80e67acc594525d4331", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Secur. ", "mimetype": "text/plain", "start_char_idx": 47878, "end_char_idx": 47885, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbcd8123-c7b0-4b9e-bbff-9935e5e4fe44", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M. ", "original_text": "8 (2007) 43\u201346.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "197383b5-6d27-4585-b5fc-69c1e096202e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[18] R. Scitovski, K. Sabo, DBSCAN-like clustering method for various data densities, Pattern Anal.  Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H. ", "original_text": "Secur. "}, "hash": "685391f33f681217cd8de80e9bdd62fb63b84bf4c6bfb7661e8b20424267c339", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "433fff23-3e43-4447-a358-f054ca8e0b5c", "node_type": "1", "metadata": {"window": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. ", "original_text": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. "}, "hash": "ecf33af545463896c7aa45c6a834c95d1aecb98f33c5c812d95dd5394e64efa0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 (2007) 43\u201346.\n", "mimetype": "text/plain", "start_char_idx": 47885, "end_char_idx": 47901, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "433fff23-3e43-4447-a358-f054ca8e0b5c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. ", "original_text": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbcd8123-c7b0-4b9e-bbff-9935e5e4fe44", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  (2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M. ", "original_text": "8 (2007) 43\u201346.\n"}, "hash": "49a6a2fe12c43d3729fa0bbcd2bfaa1ea7808e1a9c698249494e3fa9043e4bf1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88b5c942-b620-40eb-953b-0708aa714a0b", "node_type": "1", "metadata": {"window": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n", "original_text": "Appl. "}, "hash": "916f50bca56a190f1f4c4c480b526a19993a2eeb1ec261ab49cfa3d38991eb02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. ", "mimetype": "text/plain", "start_char_idx": 47901, "end_char_idx": 48010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88b5c942-b620-40eb-953b-0708aa714a0b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n", "original_text": "Appl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "433fff23-3e43-4447-a358-f054ca8e0b5c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "(2019) http://dx.doi.org/10.1007/s10044-019-00809-z.\n [19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. ", "original_text": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput. "}, "hash": "23ee8a98a1dfd1b9ba4fa9ed21d2c3ad1577bf974646796bba97b5099a062c0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1fca001-e21f-42e3-88b7-05f63fa4cd98", "node_type": "1", "metadata": {"window": "Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. ", "original_text": "7 (2010) 1916\u20131918.\n"}, "hash": "0a7b0b89649cf5e1954c018c3bf7cdca7cad98c0573119a93e727f18644833a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appl. ", "mimetype": "text/plain", "start_char_idx": 48010, "end_char_idx": 48016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b1fca001-e21f-42e3-88b7-05f63fa4cd98", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. ", "original_text": "7 (2010) 1916\u20131918.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88b5c942-b620-40eb-953b-0708aa714a0b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[19] Z. Wu, J. Huang, Application of DBSCAN cluster algorithm in anormaly detection, Netw.  Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n", "original_text": "Appl. "}, "hash": "3d9b718936fd43f21306cd2c847f347f607f906b380acc3404a1cc4c6f515f10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db911d32-ee94-4b14-a375-969d41c0ce42", "node_type": "1", "metadata": {"window": "Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2. ", "original_text": "[21] W. Chimphlee, A.H. "}, "hash": "b9b90e1b114711ab956f1649fe44846bae48582672025e68c3c272b5d0c04830", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "7 (2010) 1916\u20131918.\n", "mimetype": "text/plain", "start_char_idx": 48016, "end_char_idx": 48036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db911d32-ee94-4b14-a375-969d41c0ce42", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2. ", "original_text": "[21] W. Chimphlee, A.H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b1fca001-e21f-42e3-88b7-05f63fa4cd98", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Comput.  Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. ", "original_text": "7 (2010) 1916\u20131918.\n"}, "hash": "6692f83f0928df8b6674564462b45b6047464798a12921549f0c03f014d1b594", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3264368c-183f-4e6e-a3b7-e1b245de8787", "node_type": "1", "metadata": {"window": "8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp. ", "original_text": "Abdullah, M.N.M. "}, "hash": "6657f0edb16fd5c5a56cf8b2bb7bc610e10b2d0e98ecde4b92d0db95f5bf4f7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[21] W. Chimphlee, A.H. ", "mimetype": "text/plain", "start_char_idx": 48036, "end_char_idx": 48060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3264368c-183f-4e6e-a3b7-e1b245de8787", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp. ", "original_text": "Abdullah, M.N.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db911d32-ee94-4b14-a375-969d41c0ce42", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Secur.  8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2. ", "original_text": "[21] W. Chimphlee, A.H. "}, "hash": "893b49efc8e860f23170d45c455b868b621e59680d1a0ad75c8c4070a950ebdd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adcf9092-315c-47a7-a987-8bb6d0436127", "node_type": "1", "metadata": {"window": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n", "original_text": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. "}, "hash": "dc46070764e6933a9fff36309987d44c13edbb3dd5d8f78f9ddb9cf567cb79f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Abdullah, M.N.M. ", "mimetype": "text/plain", "start_char_idx": 48060, "end_char_idx": 48077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "adcf9092-315c-47a7-a987-8bb6d0436127", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n", "original_text": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3264368c-183f-4e6e-a3b7-e1b245de8787", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "8 (2007) 43\u201346.\n [20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp. ", "original_text": "Abdullah, M.N.M. "}, "hash": "3b23120990c9cbfd3ab04e7be9fe8bcc270c01b6c1c49e23584f38250d127215", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf843418-20f5-4107-af59-d7cace9f3713", "node_type": "1", "metadata": {"window": "Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D. ", "original_text": "329\u2013334.\n"}, "hash": "c938958a4f05154823aad8a6bffe755bba953a25f4dec96b7bd602ed28ff3950", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. ", "mimetype": "text/plain", "start_char_idx": 48077, "end_char_idx": 48263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cf843418-20f5-4107-af59-d7cace9f3713", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D. ", "original_text": "329\u2013334.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adcf9092-315c-47a7-a987-8bb6d0436127", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[20] J. Li, X. Hu, Efficient mixed clustering algorithm and its application in anomaly detection, J. Comput.  Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n", "original_text": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp. "}, "hash": "a6c45dffe7471e46994eb2864257834b3320455439a247e1bf795850840059d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83df4ff0-d5e6-4792-bf15-1860e343f039", "node_type": "1", "metadata": {"window": "7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. ", "original_text": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. "}, "hash": "f91ab28c9f3928ab2edae413e2c750d27873d25d94e9aec47f3033556a71c8cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "329\u2013334.\n", "mimetype": "text/plain", "start_char_idx": 48263, "end_char_idx": 48272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83df4ff0-d5e6-4792-bf15-1860e343f039", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. ", "original_text": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf843418-20f5-4107-af59-d7cace9f3713", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D. ", "original_text": "329\u2013334.\n"}, "hash": "bfb5daa212c84de497c91998db1af5b801888d9d33547e27019ff6d7be1ff4b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8710f4a1-0ef6-4371-84d3-9f7f185d8237", "node_type": "1", "metadata": {"window": "[21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput. ", "original_text": "2. "}, "hash": "401d0fd4fa79deebfb9a6d5ac56ee4765f73f1905a8bfafaeab55d16b41c9904", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. ", "mimetype": "text/plain", "start_char_idx": 48272, "end_char_idx": 48437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8710f4a1-0ef6-4371-84d3-9f7f185d8237", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput. ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83df4ff0-d5e6-4792-bf15-1860e343f039", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "7 (2010) 1916\u20131918.\n [21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. ", "original_text": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol. "}, "hash": "3d61536a425417c46dbb515547bd29bdd5a4a2e4acab7b73f064f7f5ebc3e475", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "682eabe2-0019-4683-8374-7fe4ec0f0ce0", "node_type": "1", "metadata": {"window": "Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl. ", "original_text": "St Louis, 2003, pp. "}, "hash": "e4a05558cef50ef9582c467c9c5fcca8e3568d5d9d93750f8e46f03058e0b7a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 48437, "end_char_idx": 48440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "682eabe2-0019-4683-8374-7fe4ec0f0ce0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl. ", "original_text": "St Louis, 2003, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8710f4a1-0ef6-4371-84d3-9f7f185d8237", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[21] W. Chimphlee, A.H.  Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput. ", "original_text": "2. "}, "hash": "1b72de01d3c1743b3008cde76034e4231edda934a68d72720ab046119a2fd054", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "564d419e-223a-4d53-a765-d5d9ee280756", "node_type": "1", "metadata": {"window": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n", "original_text": "1219\u20131224.\n"}, "hash": "9277cc01394baa76414d2cdff65b3f22c6ddf40f185386cf0c552969f75b9dbe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "St Louis, 2003, pp. ", "mimetype": "text/plain", "start_char_idx": 48440, "end_char_idx": 48460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "564d419e-223a-4d53-a765-d5d9ee280756", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n", "original_text": "1219\u20131224.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "682eabe2-0019-4683-8374-7fe4ec0f0ce0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Abdullah, M.N.M.  Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl. ", "original_text": "St Louis, 2003, pp. "}, "hash": "42845cdd34e54f8f62569055fdec22aaa65331399ef3c89ae3f5138dddfbd596", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9c13677-779b-47fe-99b9-1288398accec", "node_type": "1", "metadata": {"window": "329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H. ", "original_text": "[23] X.D. "}, "hash": "7f168ff372b8300845860b19ab2c67e35920b64bd7dd5a505333298bab9274fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1219\u20131224.\n", "mimetype": "text/plain", "start_char_idx": 48460, "end_char_idx": 48471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9c13677-779b-47fe-99b9-1288398accec", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H. ", "original_text": "[23] X.D. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "564d419e-223a-4d53-a765-d5d9ee280756", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Sap, S. Srinoy, S. Chimphlee, Anomaly-based intrusion detection using fuzzy rough clustering, in: 2006 International Conference on Hybrid Information Technology, Cheju Island, 2006, pp.  329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n", "original_text": "1219\u20131224.\n"}, "hash": "187170ab18c56096496da52c6c970e129d0b600e1c68d4cb5a4853e0ae81d750", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49f8ab78-babb-498a-bae8-028544a4c265", "node_type": "1", "metadata": {"window": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. ", "original_text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. "}, "hash": "4fbee56e3957ec4e710ca15356506e70030ebd5de1acc10e38d60fc77dc88f80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[23] X.D. ", "mimetype": "text/plain", "start_char_idx": 48471, "end_char_idx": 48481, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49f8ab78-babb-498a-bae8-028544a4c265", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. ", "original_text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9c13677-779b-47fe-99b9-1288398accec", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "329\u2013334.\n [22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H. ", "original_text": "[23] X.D. "}, "hash": "29267529b9ee1bee25a2c8d9037cad460a491c0f9159a2e80064efe917fb6a49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d6b8621b-deff-4721-b24f-18f9feaf3121", "node_type": "1", "metadata": {"window": "2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n", "original_text": "Comput. "}, "hash": "d9b558bb6e652e3818444c4f462ece3263d1fb57a8b81b7a3aae97be186f3875", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. ", "mimetype": "text/plain", "start_char_idx": 48481, "end_char_idx": 48620, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d6b8621b-deff-4721-b24f-18f9feaf3121", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n", "original_text": "Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49f8ab78-babb-498a-bae8-028544a4c265", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[22] J. Gomez, F. Gonzalez, D. Dasgupta, An immuno-fuzzy approach to anomaly detection, in: The 12th IEEE International Conference on Fuzzy Systems, FUZZ \u201903., Vol.  2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. ", "original_text": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw. "}, "hash": "efc68096f9ed8cf0f970d323bce0fd3de321e48a9a16e0b2a9cd5b17e3bf8d10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23fe146a-ddf7-459e-b6d5-0d82ff7c2954", "node_type": "1", "metadata": {"window": "St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "original_text": "Appl. "}, "hash": "546ec01e2e0036228d7f8db82de1bfd2a22763902c4f87ffe5f13f7faeb1210a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comput. ", "mimetype": "text/plain", "start_char_idx": 48620, "end_char_idx": 48628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23fe146a-ddf7-459e-b6d5-0d82ff7c2954", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "original_text": "Appl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d6b8621b-deff-4721-b24f-18f9feaf3121", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "2.  St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n", "original_text": "Comput. "}, "hash": "2b3a546eda6b474a944940803541a3e0bbe49ac8a632d99abfe70dbe08d99709", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ee94df1-be83-4133-a340-0bd491f9c33a", "node_type": "1", "metadata": {"window": "1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n", "original_text": "32 (6) (2009) 1219\u20131228.\n"}, "hash": "c5b0903ce043303b4fa5ba20723e675cdf8b55f37b91eed1a30a62924a19b823", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appl. ", "mimetype": "text/plain", "start_char_idx": 48628, "end_char_idx": 48634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1ee94df1-be83-4133-a340-0bd491f9c33a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n", "original_text": "32 (6) (2009) 1219\u20131228.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23fe146a-ddf7-459e-b6d5-0d82ff7c2954", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "St Louis, 2003, pp.  1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "original_text": "Appl. "}, "hash": "c1314e949c9b5efd10ed2e2a27f754e2331edd690d4ee94aed6c42c0f83c259d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad70800b-0f0b-4745-98a0-779f462069ae", "node_type": "1", "metadata": {"window": "[23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "original_text": "[24] C.-H. "}, "hash": "68db2a25d68f9331a86eb6eb64100e1793210155c76a6705f506b22f89a26a94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "32 (6) (2009) 1219\u20131228.\n", "mimetype": "text/plain", "start_char_idx": 48634, "end_char_idx": 48659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad70800b-0f0b-4745-98a0-779f462069ae", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "original_text": "[24] C.-H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ee94df1-be83-4133-a340-0bd491f9c33a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1219\u20131224.\n [23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n", "original_text": "32 (6) (2009) 1219\u20131228.\n"}, "hash": "3cca35706840e383388f1bc6087fc19c96ffea712ec47d28369b6649b40a3281", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b635fe6-3586-44e0-b475-fa06abad9345", "node_type": "1", "metadata": {"window": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n", "original_text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. "}, "hash": "c72f2d989db97ad05dc087f5e976718a2ea56db4d786dd8c6e2bcf47c28aa0e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[24] C.-H. ", "mimetype": "text/plain", "start_char_idx": 48659, "end_char_idx": 48670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b635fe6-3586-44e0-b475-fa06abad9345", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n", "original_text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad70800b-0f0b-4745-98a0-779f462069ae", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[23] X.D.  Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "original_text": "[24] C.-H. "}, "hash": "cb8e9af92f9033557f7a6fad499eb625a5eb4c4a6065bbbd090055d4101df08d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "017ec510-3115-4d45-8295-c437d4e4728b", "node_type": "1", "metadata": {"window": "Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. ", "original_text": "40 (2007) 2373\u20132391.\n"}, "hash": "4ae8dda09bb7e8290c3b8dd55514538e43176e5621aac9e398166b334cfddf90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. ", "mimetype": "text/plain", "start_char_idx": 48670, "end_char_idx": 48829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "017ec510-3115-4d45-8295-c437d4e4728b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. ", "original_text": "40 (2007) 2373\u20132391.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b635fe6-3586-44e0-b475-fa06abad9345", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hoang, J. Hu, P. Bertok, A program-based anomaly intrusion detection scheme using multiple detection engines and fuzzy inference, J. Netw.  Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n", "original_text": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit. "}, "hash": "8d6e6e2ac9fc6aed5037670d8d9db4ecba288e3f2e5f3aed445cb09d3ea1b25e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1eb4113e-5063-411a-934e-0a4d0ff30b8b", "node_type": "1", "metadata": {"window": "Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst. ", "original_text": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. "}, "hash": "85cacae73ed54616f2866b40a8b9e058d20e3b81cb2f84c1aac40a9e056b4714", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "40 (2007) 2373\u20132391.\n", "mimetype": "text/plain", "start_char_idx": 48829, "end_char_idx": 48850, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1eb4113e-5063-411a-934e-0a4d0ff30b8b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst. ", "original_text": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "017ec510-3115-4d45-8295-c437d4e4728b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Comput.  Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. ", "original_text": "40 (2007) 2373\u20132391.\n"}, "hash": "01298b43e1a1e6fa9825c5b5a7720f033a240460cdb13017ff62792f1ce51ab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2378d03e-1dac-46ff-a52d-ccffa558a8fc", "node_type": "1", "metadata": {"window": "32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n", "original_text": "217\u2013222.\n"}, "hash": "172eb4ace49abc2746bb811515edc258bf65c44ab659caae723458acdb7f5ac3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. ", "mimetype": "text/plain", "start_char_idx": 48850, "end_char_idx": 49068, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2378d03e-1dac-46ff-a52d-ccffa558a8fc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n", "original_text": "217\u2013222.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1eb4113e-5063-411a-934e-0a4d0ff30b8b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst. ", "original_text": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp. "}, "hash": "5ca1052fb96ad165ffc1203dd99dde39a6b1c3d091985a1fc8725c0f54d29f45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2a77793-d8a3-493e-b150-109ec0871dc9", "node_type": "1", "metadata": {"window": "[24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. ", "original_text": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. "}, "hash": "63d5bca1b0776e2aea3febe2ed8db7058b4c1f2fdb072861c5f66bd6d80fb29e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "217\u2013222.\n", "mimetype": "text/plain", "start_char_idx": 49068, "end_char_idx": 49077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2a77793-d8a3-493e-b150-109ec0871dc9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. ", "original_text": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2378d03e-1dac-46ff-a52d-ccffa558a8fc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "32 (6) (2009) 1219\u20131228.\n [24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n", "original_text": "217\u2013222.\n"}, "hash": "da2ed6fb3d81e6dbde65ab5b1c417a67b50f061451496b809d6668a882be517e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c67b5b5d-1984-4489-8d08-665a0772f6d2", "node_type": "1", "metadata": {"window": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst. ", "original_text": "1513\u20131518.\n"}, "hash": "b7d0551dec9222d9e927a5a065a6c577c29dc7a7241bacfe6be39cb76007edf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. ", "mimetype": "text/plain", "start_char_idx": 49077, "end_char_idx": 49278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c67b5b5d-1984-4489-8d08-665a0772f6d2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst. ", "original_text": "1513\u20131518.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2a77793-d8a3-493e-b150-109ec0871dc9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[24] C.-H.  Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. ", "original_text": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp. "}, "hash": "1d2fd43103fb32bfa298783007b8481b76b3a0eab3e99e91b49167cbd96669dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f148dde-1193-46f6-a75a-ae022e327746", "node_type": "1", "metadata": {"window": "40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n", "original_text": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. "}, "hash": "afcc7f5e58ec05a63df360b4788d52de91acea66ababaaa4600cd8e7ef184e75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1513\u20131518.\n", "mimetype": "text/plain", "start_char_idx": 49278, "end_char_idx": 49289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f148dde-1193-46f6-a75a-ae022e327746", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n", "original_text": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c67b5b5d-1984-4489-8d08-665a0772f6d2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Tsang, S. Kwong, H. Wang, Genetic-fuzzy rule mining approach and evaluation of feature selection techniques for anomaly intrusion detection, Pattern Recognit.  40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst. ", "original_text": "1513\u20131518.\n"}, "hash": "7eeb7d461467e400620e8dcd09045961aeeb2dd1daecf7d8cb752e32bf356a14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b7acc42-0ca5-44b4-abe4-fcabf4dc809e", "node_type": "1", "metadata": {"window": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. ", "original_text": "Fuzzy Syst. "}, "hash": "ace171831f8e96d05d12792ab0bc3a6a78f8f8ae5709046e5ede814c5e80e924", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. ", "mimetype": "text/plain", "start_char_idx": 49289, "end_char_idx": 49400, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b7acc42-0ca5-44b4-abe4-fcabf4dc809e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. ", "original_text": "Fuzzy Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f148dde-1193-46f6-a75a-ae022e327746", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "40 (2007) 2373\u20132391.\n [25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n", "original_text": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans. "}, "hash": "c39dad49c6c11b8823150243079fdc121d962572a76354d57f931cf3163c17f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8a4227a-6968-4c32-9bad-7e2a355189f0", "node_type": "1", "metadata": {"window": "217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n", "original_text": "21 (5) (2013) 855\u2013868.\n"}, "hash": "758fbba9c0785b7c9f38ed4b9850fdfe691f5e71c18a83085af306025d4ff627", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fuzzy Syst. ", "mimetype": "text/plain", "start_char_idx": 49400, "end_char_idx": 49412, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8a4227a-6968-4c32-9bad-7e2a355189f0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n", "original_text": "21 (5) (2013) 855\u2013868.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b7acc42-0ca5-44b4-abe4-fcabf4dc809e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[25] J. Liu, J. Tian, Z. Cai, Y. Zhou, R. Luo, R. Wang, A hybrid semi-supervised approach for financial fraud detection, in: 2017 International Conference on Machine Learning and Cybernetics (ICMLC), Ningbo, 2017, pp.  217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. ", "original_text": "Fuzzy Syst. "}, "hash": "a09e264fd9c2c08d8557444b2ae74e0243fe0751ef75fa5726878ed56926612e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b9f55d8-f347-4b04-9202-9d64563de728", "node_type": "1", "metadata": {"window": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M. ", "original_text": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. "}, "hash": "b62d4ac64264c8dae6d69a3c6beb7cdb16e3226cb2fbdefba4bc79c58843ed10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21 (5) (2013) 855\u2013868.\n", "mimetype": "text/plain", "start_char_idx": 49412, "end_char_idx": 49435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b9f55d8-f347-4b04-9202-9d64563de728", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M. ", "original_text": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8a4227a-6968-4c32-9bad-7e2a355189f0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "217\u2013222.\n [26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n", "original_text": "21 (5) (2013) 855\u2013868.\n"}, "hash": "a4d7330f6f87ac82d9d37c51d7a23bd954714c864281073b74f20ec7fef72386", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4b89b1d-d3f6-4cbb-ba01-87db03f212bc", "node_type": "1", "metadata": {"window": "1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A. ", "original_text": "Fuzzy Syst. "}, "hash": "990f4c467ae7a2d077277d9f5974976529f9525235fdbea43b9d1d3c198a139b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. ", "mimetype": "text/plain", "start_char_idx": 49435, "end_char_idx": 49572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4b89b1d-d3f6-4cbb-ba01-87db03f212bc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A. ", "original_text": "Fuzzy Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b9f55d8-f347-4b04-9202-9d64563de728", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[26] H. Izakian, W. Pedrycz, Anomaly detection in time series data using a fuzzy c-means clustering, in: 2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS), Edmonton, AB, 2013, pp.  1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M. ", "original_text": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans. "}, "hash": "91934cc0c9571480a2b1c050e3d431f87fa4a80a97a78cdd21b090ad0a09db24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eac292f0-abd8-45d8-9ef3-44113b982d8d", "node_type": "1", "metadata": {"window": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A. ", "original_text": "22 (6) (2014) 1612\u20131624.\n"}, "hash": "9c68ffd8cf618d8f6a5ede63fda86d2dd9c21badf70fb123cc362a1b87cecd54", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fuzzy Syst. ", "mimetype": "text/plain", "start_char_idx": 49572, "end_char_idx": 49584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eac292f0-abd8-45d8-9ef3-44113b982d8d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A. ", "original_text": "22 (6) (2014) 1612\u20131624.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4b89b1d-d3f6-4cbb-ba01-87db03f212bc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1513\u20131518.\n [27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A. ", "original_text": "Fuzzy Syst. "}, "hash": "e32ab0590368898ccd06a89c0d1544e7ed95731ee5892b0b1fd7c0dacaa621b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6dba12e4-2d1e-4810-9a98-6d466c1e819f", "node_type": "1", "metadata": {"window": "Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. ", "original_text": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. "}, "hash": "108a1e3586fc0971cbeaca96f124a7be2a9976e9a8b251e70696e4f54af1740d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "22 (6) (2014) 1612\u20131624.\n", "mimetype": "text/plain", "start_char_idx": 49584, "end_char_idx": 49609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6dba12e4-2d1e-4810-9a98-6d466c1e819f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. ", "original_text": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eac292f0-abd8-45d8-9ef3-44113b982d8d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[27] H. Izakian, W. Pedrycz, I. Jamal, Clustering spatiotemporal data: An augmented fuzzy c-means, IEEE Trans.  Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A. ", "original_text": "22 (6) (2014) 1612\u20131624.\n"}, "hash": "f678e91966399154dbf4e85e75e5ebf391b235479906cfc82c59c95de92c2b42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4458b5c3-298d-421f-8150-1e14b664e212", "node_type": "1", "metadata": {"window": "21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n", "original_text": "124 (2017) 188\u2013206.\n"}, "hash": "8e7d9d1a9ce8c723f44cde5f7493b7257b410072710309bf561122c8486f6153", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. ", "mimetype": "text/plain", "start_char_idx": 49609, "end_char_idx": 49777, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4458b5c3-298d-421f-8150-1e14b664e212", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n", "original_text": "124 (2017) 188\u2013206.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6dba12e4-2d1e-4810-9a98-6d466c1e819f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fuzzy Syst.  21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. ", "original_text": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst. "}, "hash": "f50c4dbd56cffa91fd4044a162e5a3ef94851a41cdac0bbcb8d59d294fcf9dd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "399785f3-287e-413a-a80b-caa0cd1caf96", "node_type": "1", "metadata": {"window": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M. ", "original_text": "[30] H. Faris, A.M. "}, "hash": "291b21f52081ffc7b56d4fe0a634a24e0b16733f6aff8e86ba7c9c24f5b5cc88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "124 (2017) 188\u2013206.\n", "mimetype": "text/plain", "start_char_idx": 49777, "end_char_idx": 49797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "399785f3-287e-413a-a80b-caa0cd1caf96", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M. ", "original_text": "[30] H. Faris, A.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4458b5c3-298d-421f-8150-1e14b664e212", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "21 (5) (2013) 855\u2013868.\n [28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n", "original_text": "124 (2017) 188\u2013206.\n"}, "hash": "5056927829ee4beb11ad97e9d7b7a7bf179d78aec4b29a1d007fc41782a6318d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bad4478f-5d8a-4b5b-914d-908f6290f3f5", "node_type": "1", "metadata": {"window": "Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F. ", "original_text": "Al-Zoubi, A.A. "}, "hash": "252171f2196775fd318fd561795a88158043e34e72ef5c6c4e7b0fc6103fbd6b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[30] H. Faris, A.M. ", "mimetype": "text/plain", "start_char_idx": 49797, "end_char_idx": 49817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bad4478f-5d8a-4b5b-914d-908f6290f3f5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F. ", "original_text": "Al-Zoubi, A.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "399785f3-287e-413a-a80b-caa0cd1caf96", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[28] H. Izakian, W. Pedrycz, Anomaly detection and characterization in spatial time series data: A cluster-centric approach, IEEE Trans.  Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M. ", "original_text": "[30] H. Faris, A.M. "}, "hash": "4ac9ec20c3ea5a78ee61d9e6011cf38a3e909ca3921862afdf73fe6e1bda38d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7dd07337-e72b-4ddf-81dc-33e5899b09c6", "node_type": "1", "metadata": {"window": "22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. ", "original_text": "Heidari, I. Aljarah, M. Mafarja, M.A. "}, "hash": "24b5c9e0c779f9dbdd11e5709298dabb9ceb77b8c11016c822a3fc9d6fd00051", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Al-Zoubi, A.A. ", "mimetype": "text/plain", "start_char_idx": 49817, "end_char_idx": 49832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7dd07337-e72b-4ddf-81dc-33e5899b09c6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. ", "original_text": "Heidari, I. Aljarah, M. Mafarja, M.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bad4478f-5d8a-4b5b-914d-908f6290f3f5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fuzzy Syst.  22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F. ", "original_text": "Al-Zoubi, A.A. "}, "hash": "9e88c476fcd7f0d68a2b0d5cfb7470e1efacf2d119a8f219fa71fcc71e3d7908", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ead77a39-64a0-4c52-a8c0-145c51143f70", "node_type": "1", "metadata": {"window": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not. ", "original_text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. "}, "hash": "551e834a4b89ad9c1a11a8c3f5fa8db7cee952f10b55e47828626081604c88a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Heidari, I. Aljarah, M. Mafarja, M.A. ", "mimetype": "text/plain", "start_char_idx": 49832, "end_char_idx": 49870, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ead77a39-64a0-4c52-a8c0-145c51143f70", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not. ", "original_text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7dd07337-e72b-4ddf-81dc-33e5899b09c6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "22 (6) (2014) 1612\u20131624.\n [29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. ", "original_text": "Heidari, I. Aljarah, M. Mafarja, M.A. "}, "hash": "feed69d6a18917d02b01aa4558e35e99e8d36d820a7fcc3fe5b945e0e99206d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2114337-f1a7-48c7-9500-0cddf7a687c7", "node_type": "1", "metadata": {"window": "124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron. ", "original_text": "Fusion 48 (2019) 67\u201383.\n"}, "hash": "4a0fb3ad78a08efa0e6c687f6cbf52ee6a05da5650ea285b6a13c0e9ca7caa01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. ", "mimetype": "text/plain", "start_char_idx": 49870, "end_char_idx": 50032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2114337-f1a7-48c7-9500-0cddf7a687c7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron. ", "original_text": "Fusion 48 (2019) 67\u201383.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ead77a39-64a0-4c52-a8c0-145c51143f70", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[29] U. Yun, H. Ryang, G. Lee, H. Fujita, An efficient algorithm for mining high utility patterns from incremental databases with one database scan, Knowl.-Based Syst.  124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not. ", "original_text": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf. "}, "hash": "ca29293a3cd0799edb5e9a23f2b51aa83f8bb848fe57b51f21532a26e04471da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c632910b-c42b-4034-8aaa-24fc8fdf7717", "node_type": "1", "metadata": {"window": "[30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc. ", "original_text": "[31] P. Protopapas, J.M. "}, "hash": "d08a94c21c271148f8b149e9dfb736d17b10eb243047680671ddb1bacf777d44", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fusion 48 (2019) 67\u201383.\n", "mimetype": "text/plain", "start_char_idx": 50032, "end_char_idx": 50056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c632910b-c42b-4034-8aaa-24fc8fdf7717", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc. ", "original_text": "[31] P. Protopapas, J.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2114337-f1a7-48c7-9500-0cddf7a687c7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "124 (2017) 188\u2013206.\n [30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron. ", "original_text": "Fusion 48 (2019) 67\u201383.\n"}, "hash": "e2c44fa9e47140a02ae89e552737b2a6313d39e8f86f2b1135042ce1f3e35fbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4efd3d4a-50f5-4421-ba26-0a74769312b5", "node_type": "1", "metadata": {"window": "Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n", "original_text": "Giammarco, L. Faccioli, M.F. "}, "hash": "0c12f0e47368bc863d6c1334ffc94ee1f18d3695b69f00c6c58326ad1e09aa8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[31] P. Protopapas, J.M. ", "mimetype": "text/plain", "start_char_idx": 50056, "end_char_idx": 50081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4efd3d4a-50f5-4421-ba26-0a74769312b5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n", "original_text": "Giammarco, L. Faccioli, M.F. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c632910b-c42b-4034-8aaa-24fc8fdf7717", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[30] H. Faris, A.M.  Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc. ", "original_text": "[31] P. Protopapas, J.M. "}, "hash": "f247e756c575ef5d0f9a1e504a88dc9c079982ad73ae48f97e8eaf90b1efbd7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21fde42a-8ecc-4937-b057-a79b9cc2a49a", "node_type": "1", "metadata": {"window": "Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. ", "original_text": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. "}, "hash": "d085b82685794972aa2139b6152e257c472e87899a3eb7fc6b4fa6e5ba55ced2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Giammarco, L. Faccioli, M.F. ", "mimetype": "text/plain", "start_char_idx": 50081, "end_char_idx": 50110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21fde42a-8ecc-4937-b057-a79b9cc2a49a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. ", "original_text": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4efd3d4a-50f5-4421-ba26-0a74769312b5", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Al-Zoubi, A.A.  Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n", "original_text": "Giammarco, L. Faccioli, M.F. "}, "hash": "bc666a8328165b9a9425ed1cb2625f5b53910096db66ce1d0e7de384326756ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "700916a5-bb39-4082-8f66-409794f2596e", "node_type": "1", "metadata": {"window": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int. ", "original_text": "Not. "}, "hash": "c9a97d9732b6ccc1a6489babc03177919f22cc3b51906ac73c443bd60f6c18ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. ", "mimetype": "text/plain", "start_char_idx": 50110, "end_char_idx": 50215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "700916a5-bb39-4082-8f66-409794f2596e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int. ", "original_text": "Not. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21fde42a-8ecc-4937-b057-a79b9cc2a49a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Heidari, I. Aljarah, M. Mafarja, M.A.  Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. ", "original_text": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon. "}, "hash": "7358024beb6a9f828af2d01276b5de2567b4d33332773b0ffbf444a2d8476e76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e617c24a-2549-46ef-af91-13988112fdd7", "node_type": "1", "metadata": {"window": "Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf. ", "original_text": "R. Astron. "}, "hash": "6d4597346ed0c2185a9ad737615a7e0b3b75c580ff61831734ad7ccfca2b96f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Not. ", "mimetype": "text/plain", "start_char_idx": 50215, "end_char_idx": 50220, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e617c24a-2549-46ef-af91-13988112fdd7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf. ", "original_text": "R. Astron. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "700916a5-bb39-4082-8f66-409794f2596e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hassonah, H. Fujita, An intelligent system for spam detection and identification of the most relevant features based on evolutionary random weight networks, Inf.  Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int. ", "original_text": "Not. "}, "hash": "e3de961b3e213ab3d7a40dd85a54c2adf7fbbd0e1f5ad09409f7d8cfd636d74f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7d4ee7d-d313-4e36-a3b9-56ec3e430ec2", "node_type": "1", "metadata": {"window": "[31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp. ", "original_text": "Soc. "}, "hash": "1f4876c9e5a43c862c16eb30c7315ecad32d83b99242070c7791e5d0da503445", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "R. Astron. ", "mimetype": "text/plain", "start_char_idx": 50220, "end_char_idx": 50231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7d4ee7d-d313-4e36-a3b9-56ec3e430ec2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp. ", "original_text": "Soc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e617c24a-2549-46ef-af91-13988112fdd7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fusion 48 (2019) 67\u201383.\n [31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf. ", "original_text": "R. Astron. "}, "hash": "22992878b16a09822baf27af0c7db4288fad05659d60ca4ffdfc92955cabeafd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b37e52b7-5879-4dc5-bb1f-0b45b1eaadd2", "node_type": "1", "metadata": {"window": "Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n", "original_text": "369 (2) (2006) 677\u2013696.\n"}, "hash": "636733878107d6bd0ed2a93b11d9a701ea7ff372751ee7e474ce585658a1f6f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Soc. ", "mimetype": "text/plain", "start_char_idx": 50231, "end_char_idx": 50236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b37e52b7-5879-4dc5-bb1f-0b45b1eaadd2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n", "original_text": "369 (2) (2006) 677\u2013696.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7d4ee7d-d313-4e36-a3b9-56ec3e430ec2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[31] P. Protopapas, J.M.  Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp. ", "original_text": "Soc. "}, "hash": "2ca37ef02e0ae426ab9179780efab429be55744a96a7ff7e26086919aacc2b12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0cc255a-dca2-45f2-8d92-3be0bad0a137", "node_type": "1", "metadata": {"window": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A. ", "original_text": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. "}, "hash": "87a354fd0d3bdc79566b65b5825c31e4afc3e84e0fabce1a6abf3a9b9ebb8396", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "369 (2) (2006) 677\u2013696.\n", "mimetype": "text/plain", "start_char_idx": 50236, "end_char_idx": 50260, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0cc255a-dca2-45f2-8d92-3be0bad0a137", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A. ", "original_text": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b37e52b7-5879-4dc5-bb1f-0b45b1eaadd2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Giammarco, L. Faccioli, M.F.  Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n", "original_text": "369 (2) (2006) 677\u2013696.\n"}, "hash": "5249945280a95cd90115cf643bab6604851ac511730cebf4fb6b917eec887cea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa46faa1-d245-4d8a-bf40-fc51fa2a20be", "node_type": "1", "metadata": {"window": "Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc. ", "original_text": "Fifth IEEE Int. "}, "hash": "c41af986689e0cb7d4d1e0f426a1eb0271a88b642849bbdf441b386d1c428496", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 50260, "end_char_idx": 50371, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fa46faa1-d245-4d8a-bf40-fc51fa2a20be", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc. ", "original_text": "Fifth IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0cc255a-dca2-45f2-8d92-3be0bad0a137", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Struble, R. Dave, C. Alcock, Finding outlier light curves in catalogues of periodic variable stars, Mon.  Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A. ", "original_text": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc. "}, "hash": "fd66c6e0dac2aa7288e9f0bbd54d5b592f27edf63a92f9d25321d368fd889544", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c38c476-6f4b-4f38-b789-49153bb7e3db", "node_type": "1", "metadata": {"window": "R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int. ", "original_text": "Conf. "}, "hash": "cfba6ec321674dc2b50c3a22ae231c3917d5fb038519ba7fa8484dddb7d1e1c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fifth IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 50371, "end_char_idx": 50387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c38c476-6f4b-4f38-b789-49153bb7e3db", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa46faa1-d245-4d8a-bf40-fc51fa2a20be", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Not.  R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc. ", "original_text": "Fifth IEEE Int. "}, "hash": "21998001cdfa116709a605d3af05288bf8159205a205c1db493a15040241ecbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8c15f29-3280-44e0-a2b3-ef4798ea1c4c", "node_type": "1", "metadata": {"window": "Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf. ", "original_text": "Data Mining, 2005, pp. "}, "hash": "2332abf61c93a0969c7c880a95cc1f3a145229d3eacd97659652404358065315", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 50387, "end_char_idx": 50393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8c15f29-3280-44e0-a2b3-ef4798ea1c4c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf. ", "original_text": "Data Mining, 2005, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c38c476-6f4b-4f38-b789-49153bb7e3db", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "R. Astron.  Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int. ", "original_text": "Conf. "}, "hash": "3e42d374df03fca5ca3747b6401fb37fd5b71134b1487f4ea8fa75081c84ac0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df1b658a-8071-454a-8c93-9f588bd21384", "node_type": "1", "metadata": {"window": "369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp. ", "original_text": "226\u2013233.\n"}, "hash": "faebbff5f75406c67672c01e56c258f7c9d10abd80346cc54b0f552e167f5a66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining, 2005, pp. ", "mimetype": "text/plain", "start_char_idx": 50393, "end_char_idx": 50416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "df1b658a-8071-454a-8c93-9f588bd21384", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp. ", "original_text": "226\u2013233.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8c15f29-3280-44e0-a2b3-ef4798ea1c4c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Soc.  369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf. ", "original_text": "Data Mining, 2005, pp. "}, "hash": "60b9b8ee4c49d96351a06bd08946b25c1329b53e4e7f75f4b88a3d5dc29f5f9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51f928ef-ad4f-42e5-ab13-91f57a9bb2b0", "node_type": "1", "metadata": {"window": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n", "original_text": "[33] E. Keogh, S. Lonardi, C.A. "}, "hash": "b3e2169a7634f845b0d753e702d2ce186a47f286d9e084b087c56d4d321977c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "226\u2013233.\n", "mimetype": "text/plain", "start_char_idx": 50416, "end_char_idx": 50425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51f928ef-ad4f-42e5-ab13-91f57a9bb2b0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n", "original_text": "[33] E. Keogh, S. Lonardi, C.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df1b658a-8071-454a-8c93-9f588bd21384", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "369 (2) (2006) 677\u2013696.\n [32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp. ", "original_text": "226\u2013233.\n"}, "hash": "48796c15e6e51666a4d446c08a00e5b7ee49324106e1eb6a0104acd93487293a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54aa650a-2c33-4513-b1cf-a52fd7fcaf29", "node_type": "1", "metadata": {"window": "Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. ", "original_text": "Ratanamahatana, Towards parameter-free data mining, in: Proc. "}, "hash": "b4c4cd791bfa7cdd9b02abecf6f26fca5d6a9e1b6302df7cec8942c943600bab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[33] E. Keogh, S. Lonardi, C.A. ", "mimetype": "text/plain", "start_char_idx": 50425, "end_char_idx": 50457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54aa650a-2c33-4513-b1cf-a52fd7fcaf29", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. ", "original_text": "Ratanamahatana, Towards parameter-free data mining, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51f928ef-ad4f-42e5-ab13-91f57a9bb2b0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[32] E. Keogh, J. Lin, A. Fu, Hot SAX: Efficiently finding the most unusual time series subsequence, in: Proc.  Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n", "original_text": "[33] E. Keogh, S. Lonardi, C.A. "}, "hash": "401ec0ddbb8d6fa61c13609c5c8baadc44131530cad39e61cf4e27b429b641ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca437125-a6f7-4ddd-81de-df2b34cb8012", "node_type": "1", "metadata": {"window": "Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int. ", "original_text": "the 10th ACM SIGKDD Int. "}, "hash": "af6934dbf5a739aa8f9a01f2c454176c704cade723a4e88c2788e682e8097a19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ratanamahatana, Towards parameter-free data mining, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 50457, "end_char_idx": 50519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca437125-a6f7-4ddd-81de-df2b34cb8012", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int. ", "original_text": "the 10th ACM SIGKDD Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54aa650a-2c33-4513-b1cf-a52fd7fcaf29", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fifth IEEE Int.  Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. ", "original_text": "Ratanamahatana, Towards parameter-free data mining, in: Proc. "}, "hash": "6ba396deeee6bb8db7378d75e8c9083902e0e3ce0e0e5c893a916c10cd201ab8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d514f3b3-d522-4838-8906-d121e47f9fd6", "node_type": "1", "metadata": {"window": "Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp. ", "original_text": "Conf. "}, "hash": "f7f0f5923fc40a91b26cb005b99ff3572627a2836b7943c880dfcc4edff896dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the 10th ACM SIGKDD Int. ", "mimetype": "text/plain", "start_char_idx": 50519, "end_char_idx": 50544, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d514f3b3-d522-4838-8906-d121e47f9fd6", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca437125-a6f7-4ddd-81de-df2b34cb8012", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int. ", "original_text": "the 10th ACM SIGKDD Int. "}, "hash": "a160141934157fc4d10ef3bc6a2a7819c9b37c33a37811b2fd7a10ca397d0dd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b72d831d-fb61-4c16-92af-8f788c4c756d", "node_type": "1", "metadata": {"window": "226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n", "original_text": "on Knowledge Discovery and Data Mining, 2004, pp. "}, "hash": "743d70119b347205f54deae6d81afe1753d15e3112816c004d65e217aca2c69a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 50544, "end_char_idx": 50550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b72d831d-fb61-4c16-92af-8f788c4c756d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n", "original_text": "on Knowledge Discovery and Data Mining, 2004, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d514f3b3-d522-4838-8906-d121e47f9fd6", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data Mining, 2005, pp.  226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp. ", "original_text": "Conf. "}, "hash": "88780bfbb5d168cd173e992198ccd251d63635d918f61dd5ba02885174ee6030", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fae13b44-bd1c-4e89-8f8b-8e52f58b2f38", "node_type": "1", "metadata": {"window": "[33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. ", "original_text": "206\u2013215.\n"}, "hash": "80d9910aa01e285053ba67231c0096b5f65b689935678f56634674d9d12c8e1d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Knowledge Discovery and Data Mining, 2004, pp. ", "mimetype": "text/plain", "start_char_idx": 50550, "end_char_idx": 50600, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fae13b44-bd1c-4e89-8f8b-8e52f58b2f38", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. ", "original_text": "206\u2013215.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b72d831d-fb61-4c16-92af-8f788c4c756d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "226\u2013233.\n [33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n", "original_text": "on Knowledge Discovery and Data Mining, 2004, pp. "}, "hash": "dbb4d4c3b741ed2082934c1548fdd02ce5a0dd5160ad9c3558a20b78b1a10380", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fab5d3d-00c9-4108-b282-4956754ee00f", "node_type": "1", "metadata": {"window": "Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput. ", "original_text": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. "}, "hash": "bf54777b21a56770af895b9c0c4054c476744bec1439850c3798dd5b34594376", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "206\u2013215.\n", "mimetype": "text/plain", "start_char_idx": 50600, "end_char_idx": 50609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fab5d3d-00c9-4108-b282-4956754ee00f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput. ", "original_text": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fae13b44-bd1c-4e89-8f8b-8e52f58b2f38", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[33] E. Keogh, S. Lonardi, C.A.  Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. ", "original_text": "206\u2013215.\n"}, "hash": "11ec9f62972f8c23fc594785db75c400477121880d7705c813e1f67cb12dd117", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1722a377-c2e5-4fd7-a07e-b7e7cc72bfab", "node_type": "1", "metadata": {"window": "the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst. ", "original_text": "Third Int. "}, "hash": "db7441166b0ee1448d36874810454602e35c72b71ddbccc9215cbe31b37a3d28", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 50609, "end_char_idx": 50723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1722a377-c2e5-4fd7-a07e-b7e7cc72bfab", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst. ", "original_text": "Third Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fab5d3d-00c9-4108-b282-4956754ee00f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ratanamahatana, Towards parameter-free data mining, in: Proc.  the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput. ", "original_text": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc. "}, "hash": "d57b747a41bbb1b1e7c06e8ff6264b4293918f21fd7cfa60c4f3587103e51827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39725e0c-04ec-4d87-bc1c-7427aebc5157", "node_type": "1", "metadata": {"window": "Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n", "original_text": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp. "}, "hash": "9cc871a074a724fe496bbc10fccf2d71837f5957b1b00f10a0550f5d3144b826", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Third Int. ", "mimetype": "text/plain", "start_char_idx": 50723, "end_char_idx": 50734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39725e0c-04ec-4d87-bc1c-7427aebc5157", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n", "original_text": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1722a377-c2e5-4fd7-a07e-b7e7cc72bfab", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "the 10th ACM SIGKDD Int.  Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst. ", "original_text": "Third Int. "}, "hash": "e39f80d8adea0eb105967a4f63f1f6c6ba66d7297f34f9797ecee37f8e8edb90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b48cf333-7a5f-425e-9ecf-f0a5b3d8f327", "node_type": "1", "metadata": {"window": "on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "original_text": "142\u2013150.\n"}, "hash": "999a9a0f0ebf89bcfd7419b5f0483706c28fc63b9fccc143f020819ccee9017d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp. ", "mimetype": "text/plain", "start_char_idx": 50734, "end_char_idx": 50794, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b48cf333-7a5f-425e-9ecf-f0a5b3d8f327", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "original_text": "142\u2013150.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39725e0c-04ec-4d87-bc1c-7427aebc5157", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n", "original_text": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp. "}, "hash": "194cc0ff5559d69816051227f3e980f58b370da81bac5f0637b2e8aaeb465cd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22e3e8b6-0ee3-407d-ad48-2b41d20b0dd4", "node_type": "1", "metadata": {"window": "206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf. ", "original_text": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. "}, "hash": "eddf614d2c55454aac0d0ab8f2d4658725a31fe5c3484dc376af98bd762eaecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "142\u2013150.\n", "mimetype": "text/plain", "start_char_idx": 50794, "end_char_idx": 50803, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22e3e8b6-0ee3-407d-ad48-2b41d20b0dd4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf. ", "original_text": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b48cf333-7a5f-425e-9ecf-f0a5b3d8f327", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Knowledge Discovery and Data Mining, 2004, pp.  206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "original_text": "142\u2013150.\n"}, "hash": "4c8a8d5891c9782aa19e44156eb3cf595137d643e8a875a44c55f166d6b84be4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6c651c8-7739-4a89-84f3-cc2cb0562ee4", "node_type": "1", "metadata": {"window": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n", "original_text": "Comput. "}, "hash": "c1fe7d3e1634033aab2588616a8773f582e2d8cec73742c2d75df126c6900779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. ", "mimetype": "text/plain", "start_char_idx": 50803, "end_char_idx": 50954, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f6c651c8-7739-4a89-84f3-cc2cb0562ee4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n", "original_text": "Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22e3e8b6-0ee3-407d-ad48-2b41d20b0dd4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "206\u2013215.\n [34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf. ", "original_text": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener. "}, "hash": "dc0f298945c35c59b2649e0723ea4f38c7ed3e12909ecd677314dc4b51b5c5d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba7180a9-0b5c-4eec-8c00-ea6779fe8295", "node_type": "1", "metadata": {"window": "Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. ", "original_text": "Syst. "}, "hash": "1b9579193f2fe3a08f8728765cc1b77e2489927c828970cd047d7be222a1fd58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Comput. ", "mimetype": "text/plain", "start_char_idx": 50954, "end_char_idx": 50962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba7180a9-0b5c-4eec-8c00-ea6779fe8295", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. ", "original_text": "Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6c651c8-7739-4a89-84f3-cc2cb0562ee4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[34] M. Das, S. Parthasarathy, Anomaly detection and spatio-temporal analysis of global climate system, in: Proc.  Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n", "original_text": "Comput. "}, "hash": "a9170dd8de3330bf40a604398817cddde6bcc62a8acca3c6baf1fc02449a4118", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80575805-d899-4595-a0a2-ef01bc797430", "node_type": "1", "metadata": {"window": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int. ", "original_text": "21 (7) (2005) 1096\u20131105.\n"}, "hash": "aadab850c95b6f50c9e3f0c643c2393c566d9a1b17739349c69f6c6bfe62f5d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Syst. ", "mimetype": "text/plain", "start_char_idx": 50962, "end_char_idx": 50968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80575805-d899-4595-a0a2-ef01bc797430", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int. ", "original_text": "21 (7) (2005) 1096\u20131105.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba7180a9-0b5c-4eec-8c00-ea6779fe8295", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Third Int.  Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. ", "original_text": "Syst. "}, "hash": "7929ea5067dcdab10b3cb83c283eff7f081443341d351554397e84fa6c695f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8c694e0-e62a-492a-88b3-1bd8ed0872cb", "node_type": "1", "metadata": {"window": "142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf. ", "original_text": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. "}, "hash": "8c06d5e757d4acc43244380020040c5e4ed2d33bcb73b42c0475009493c1966b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "21 (7) (2005) 1096\u20131105.\n", "mimetype": "text/plain", "start_char_idx": 50968, "end_char_idx": 50993, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c8c694e0-e62a-492a-88b3-1bd8ed0872cb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf. ", "original_text": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80575805-d899-4595-a0a2-ef01bc797430", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Workshop on Knowledge Discovery from Sensor Data, 2009, pp.  142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int. ", "original_text": "21 (7) (2005) 1096\u20131105.\n"}, "hash": "d3e442b8b12e0a907a10c3ccaee935751f0c0c3db8f3718ccca612c5f7aeed8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a0b785d-1acb-42ee-9493-1151e6e16c1c", "node_type": "1", "metadata": {"window": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp. ", "original_text": "Conf. "}, "hash": "3b06fd30e862fd8dc75a7493a975527aa7faf42472d707462b1fb787e698afbb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. ", "mimetype": "text/plain", "start_char_idx": 50993, "end_char_idx": 51103, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a0b785d-1acb-42ee-9493-1151e6e16c1c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8c694e0-e62a-492a-88b3-1bd8ed0872cb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "142\u2013150.\n [35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf. ", "original_text": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int. "}, "hash": "13bfa94436342b128cb8c64bf63dcf0a6ffffb135c73b392b5404e890d13799f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1e45eb4-b3a6-461f-97f2-48af41df3112", "node_type": "1", "metadata": {"window": "Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n", "original_text": "on Intelligent Syst., 1996.\n"}, "hash": "59a4eaa1c9c067473966e27194e7e1cf013357513c1e8285b448956be6e4e832", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 51103, "end_char_idx": 51109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d1e45eb4-b3a6-461f-97f2-48af41df3112", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n", "original_text": "on Intelligent Syst., 1996.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a0b785d-1acb-42ee-9493-1151e6e16c1c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[35] D. Gao, Y. Kinouchi, K. Ito, X. Zhao, Neural networks for event extraction from time series: A back propagation algorithm approach, Future Gener.  Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp. ", "original_text": "Conf. "}, "hash": "a75f299deae99c1376cf0012da61e708654ae1c4dc23e0c9a2d18f506ceafdbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a10a9d61-81a3-4c21-82d1-315ee23b8503", "node_type": "1", "metadata": {"window": "Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. ", "original_text": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. "}, "hash": "53525bea087c1e595733d582ee153c910146b0a34f92e9880c3b57acb5ec3f22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Intelligent Syst., 1996.\n", "mimetype": "text/plain", "start_char_idx": 51109, "end_char_idx": 51137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a10a9d61-81a3-4c21-82d1-315ee23b8503", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. ", "original_text": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1e45eb4-b3a6-461f-97f2-48af41df3112", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Comput.  Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n", "original_text": "on Intelligent Syst., 1996.\n"}, "hash": "8a3f6c8f226cc43910ac16ec9219fc7b9b0042ad3eecb19e094ed30878c09ea4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ccc09567-f14f-4d7c-83d5-2172909be845", "node_type": "1", "metadata": {"window": "21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n", "original_text": "Int. "}, "hash": "14e7869dc28c6b42364f7d2264044a5c93c784d1ebb58a0d456332bb6b05c0d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 51137, "end_char_idx": 51242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ccc09567-f14f-4d7c-83d5-2172909be845", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n", "original_text": "Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a10a9d61-81a3-4c21-82d1-315ee23b8503", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Syst.  21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. ", "original_text": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc. "}, "hash": "5247b42009c891ad8cb8bfe75f55d9576df21fb6ceba089ca0ac4c31db257dc0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "435102f6-4cf6-4416-b401-a784285c9a4e", "node_type": "1", "metadata": {"window": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G. ", "original_text": "Joint Conf. "}, "hash": "0a9e9edc0b8e10213ed549dad5424514da377760faf60c2b9f1476053470044d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Int. ", "mimetype": "text/plain", "start_char_idx": 51242, "end_char_idx": 51247, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "435102f6-4cf6-4416-b401-a784285c9a4e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G. ", "original_text": "Joint Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ccc09567-f14f-4d7c-83d5-2172909be845", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "21 (7) (2005) 1096\u20131105.\n [36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n", "original_text": "Int. "}, "hash": "7b2c0ccbf4bb26ac4e085ff77f36f20e01189be6c69dc44c511c26baf3c36c7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28ac0947-019c-44b1-80fd-11a28006fee7", "node_type": "1", "metadata": {"window": "Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E. ", "original_text": "Neural Networks, 2003, pp. "}, "hash": "660fa99719f444df83c52ba8236d88bfacca1859aae22a9afa4bf8aab54eb405", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Joint Conf. ", "mimetype": "text/plain", "start_char_idx": 51247, "end_char_idx": 51259, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28ac0947-019c-44b1-80fd-11a28006fee7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E. ", "original_text": "Neural Networks, 2003, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "435102f6-4cf6-4416-b401-a784285c9a4e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[36] D. Dasgupta, S. Forrest, Novelty detection in time series data using ideas from immunology, in: 5th Int.  Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G. ", "original_text": "Joint Conf. "}, "hash": "3ea73b906f98047fd04c396ef72150c65895c9b394506ea6f566c405cd379157", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "623d2e55-f419-414b-adf9-085a45332327", "node_type": "1", "metadata": {"window": "on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. ", "original_text": "1741\u20131745.\n"}, "hash": "e3923d2157aebb47f9b0813ef3dc6b17d9447c0c40aa225e5e7228fb7e9a4739", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Neural Networks, 2003, pp. ", "mimetype": "text/plain", "start_char_idx": 51259, "end_char_idx": 51286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "623d2e55-f419-414b-adf9-085a45332327", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. ", "original_text": "1741\u20131745.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28ac0947-019c-44b1-80fd-11a28006fee7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E. ", "original_text": "Neural Networks, 2003, pp. "}, "hash": "45d925a661dcce9dc9491916ade9ccc9835d5332ecf7bdbb9244f958d8cd0d47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84e20f46-3d91-4fb8-95ef-7445eaa6e9b9", "node_type": "1", "metadata": {"window": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl. ", "original_text": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. "}, "hash": "8982b171e16320ac374dd85e0eb7e957b5a3ba06204d38c8e0156ee2957214da", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1741\u20131745.\n", "mimetype": "text/plain", "start_char_idx": 51286, "end_char_idx": 51297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84e20f46-3d91-4fb8-95ef-7445eaa6e9b9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl. ", "original_text": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "623d2e55-f419-414b-adf9-085a45332327", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Intelligent Syst., 1996.\n [37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. ", "original_text": "1741\u20131745.\n"}, "hash": "000f56ad7b17bfe957c5e0605ed403ec7193cc961889622794f4328dfc7055f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfbd2403-0e9c-4266-b3ef-075cbea0e7c9", "node_type": "1", "metadata": {"window": "Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n", "original_text": "1285\u20131298.\n"}, "hash": "021186649f2255d4c628e7d5b3eaceb952617ed1ecb654166507d7d6559ab786", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. ", "mimetype": "text/plain", "start_char_idx": 51297, "end_char_idx": 51529, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bfbd2403-0e9c-4266-b3ef-075cbea0e7c9", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n", "original_text": "1285\u20131298.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84e20f46-3d91-4fb8-95ef-7445eaa6e9b9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[37] J. Ma, S. Perkins, Time-series novelty detection using one-class support vector machines, in: Proc.  Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl. ", "original_text": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. "}, "hash": "e23a6549fe973a2bd53d547336550d36efffa45425009f9d51ffced4cba58009", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0461d6b-b988-45d5-ba07-a28eaf9dacba", "node_type": "1", "metadata": {"window": "Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A. ", "original_text": "[39] S. Kanarachos, S.-R.G. "}, "hash": "20fd585a13813f769f4d5bc51c0007425046c3964154853b6fba259852f32858", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1285\u20131298.\n", "mimetype": "text/plain", "start_char_idx": 51529, "end_char_idx": 51540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a0461d6b-b988-45d5-ba07-a28eaf9dacba", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A. ", "original_text": "[39] S. Kanarachos, S.-R.G. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfbd2403-0e9c-4266-b3ef-075cbea0e7c9", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Int.  Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n", "original_text": "1285\u20131298.\n"}, "hash": "40cb4e806551dd2083c03a5b254c01bd2205c5b10366bf6854a0bb0b11446c2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0243e5c6-fbef-4c25-9608-8fcd64ad137e", "node_type": "1", "metadata": {"window": "Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n", "original_text": "Christopoulos, A. Chroneos, M.E. "}, "hash": "610ac09c3d3b3edfcf0470857a36973b5509415fb8bb6bdc515ef68eaa04d29b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[39] S. Kanarachos, S.-R.G. ", "mimetype": "text/plain", "start_char_idx": 51540, "end_char_idx": 51568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0243e5c6-fbef-4c25-9608-8fcd64ad137e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n", "original_text": "Christopoulos, A. Chroneos, M.E. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0461d6b-b988-45d5-ba07-a28eaf9dacba", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Joint Conf.  Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A. ", "original_text": "[39] S. Kanarachos, S.-R.G. "}, "hash": "e58530e4a697ee9ba4c570d57c7c825b2626c24d983b10b0dabd145f222d74d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "604cfc24-2c37-43d0-adab-4714d8c07b50", "node_type": "1", "metadata": {"window": "1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M. ", "original_text": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. "}, "hash": "e8a5dd8a77e7756cab9437e8ed72fce9bbfdd31ab692352dcd178c55e9809654", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Christopoulos, A. Chroneos, M.E. ", "mimetype": "text/plain", "start_char_idx": 51568, "end_char_idx": 51601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "604cfc24-2c37-43d0-adab-4714d8c07b50", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M. ", "original_text": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0243e5c6-fbef-4c25-9608-8fcd64ad137e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Neural Networks, 2003, pp.  1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n", "original_text": "Christopoulos, A. Chroneos, M.E. "}, "hash": "d6ca2709c1db4762bc9e5e9f193e98ab4d599301d60fa0ff783a3d7f4c6f7bd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "429f730f-6233-4e4f-a90c-a7f5323a08f4", "node_type": "1", "metadata": {"window": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C. ", "original_text": "Appl. "}, "hash": "42b40224cd78bf383c8bc5295e566cb9123784a451827e1aa4ce64f8821194c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. ", "mimetype": "text/plain", "start_char_idx": 51601, "end_char_idx": 51756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "429f730f-6233-4e4f-a90c-a7f5323a08f4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C. ", "original_text": "Appl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "604cfc24-2c37-43d0-adab-4714d8c07b50", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1741\u20131745.\n [38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M. ", "original_text": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst. "}, "hash": "ce21d58f91e566f66cae1c6ecf5ba9002180918e65911a3f051d7908168c9c74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "090f533b-91d9-46e6-863b-253920ae1e0b", "node_type": "1", "metadata": {"window": "1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M. ", "original_text": "85 (1) (2017) 292\u2013304.\n"}, "hash": "9385cd075b7a2cbcee9c746b4333974cd96cd242f812f63a5714bc4ba6b97299", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Appl. ", "mimetype": "text/plain", "start_char_idx": 51756, "end_char_idx": 51762, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "090f533b-91d9-46e6-863b-253920ae1e0b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M. ", "original_text": "85 (1) (2017) 292\u2013304.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "429f730f-6233-4e4f-a90c-a7f5323a08f4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[38] M. Du, F. Li, G. Zheng, V. Srikumar, DeepLog: Anomaly detection and diagnosis from system logs through deep learning, in: CCS \u201917 Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp.  1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C. ", "original_text": "Appl. "}, "hash": "39eeb89d599b810fd75697f1671a2d158a3b93e914d877afc794f7c503e9a51e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "186db2b1-8e5e-4e1f-bbad-b3250cf05119", "node_type": "1", "metadata": {"window": "[39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n", "original_text": "[40] M. Munir, S.A. "}, "hash": "54c0a938cc37a6414da62f2f59c6ccabac6f4ab2eec68bac4839b5bd09b4ab65", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "85 (1) (2017) 292\u2013304.\n", "mimetype": "text/plain", "start_char_idx": 51762, "end_char_idx": 51785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "186db2b1-8e5e-4e1f-bbad-b3250cf05119", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n", "original_text": "[40] M. Munir, S.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "090f533b-91d9-46e6-863b-253920ae1e0b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "1285\u20131298.\n [39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M. ", "original_text": "85 (1) (2017) 292\u2013304.\n"}, "hash": "03da0aaeb99f7dd2c2c6aa7c1a8c5ceadb54fd3041cbc15a2ebc1ac30a27c7b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "835d8d50-3870-459c-b776-7eab67f898ba", "node_type": "1", "metadata": {"window": "Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. ", "original_text": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n"}, "hash": "3c26525623c768786017edf4f107f98f8bb39b9c33eb996e99baee39065adde5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[40] M. Munir, S.A. ", "mimetype": "text/plain", "start_char_idx": 51785, "end_char_idx": 51805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "835d8d50-3870-459c-b776-7eab67f898ba", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. ", "original_text": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "186db2b1-8e5e-4e1f-bbad-b3250cf05119", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[39] S. Kanarachos, S.-R.G.  Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n", "original_text": "[40] M. Munir, S.A. "}, "hash": "10dfe8d3266d9692b5834499114088b0989e8d2fbcac3e6ee66949d539ff2dd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdcbc71c-9f3d-4d26-941d-d04df6316254", "node_type": "1", "metadata": {"window": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl. ", "original_text": "[41] G. Box, G.M. "}, "hash": "048098ddada1d77cee28485a3407b1e7dfd9c5201b02391a12aee75381686537", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n", "mimetype": "text/plain", "start_char_idx": 51805, "end_char_idx": 51953, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fdcbc71c-9f3d-4d26-941d-d04df6316254", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl. ", "original_text": "[41] G. Box, G.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "835d8d50-3870-459c-b776-7eab67f898ba", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Christopoulos, A. Chroneos, M.E.  Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. ", "original_text": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n"}, "hash": "5ec54e7855cc3842e84bc5b6b1138f8282714a34d95ec3be48152788b59227ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e50553d-152a-485d-8294-be195eef0bfb", "node_type": "1", "metadata": {"window": "Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng. ", "original_text": "Jenkins, G.C. "}, "hash": "895f123ccd885c08abd6ffff3be078f7fd73ebfcb22997423107ee43d7a63b4e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[41] G. Box, G.M. ", "mimetype": "text/plain", "start_char_idx": 51953, "end_char_idx": 51971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e50553d-152a-485d-8294-be195eef0bfb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng. ", "original_text": "Jenkins, G.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdcbc71c-9f3d-4d26-941d-d04df6316254", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Fitzpatrick, Detecting anomalies in time series data via a deep learning algorithm combining wavelets, neural networks and Hilbert transform, Expert Syst.  Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl. ", "original_text": "[41] G. Box, G.M. "}, "hash": "bf0ab7619587233d9cf9cd1e1690c4ec915a4da889631cc2701facbf4813dca8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "939a1ff9-311d-472a-bfb7-9a2d9d82c434", "node_type": "1", "metadata": {"window": "85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n", "original_text": "Reinsel, G.M. "}, "hash": "cad678b5440f9306a542f5ae6365dcc0e79e68a6c631765c861e2b2539e875a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Jenkins, G.C. ", "mimetype": "text/plain", "start_char_idx": 51971, "end_char_idx": 51985, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "939a1ff9-311d-472a-bfb7-9a2d9d82c434", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n", "original_text": "Reinsel, G.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e50553d-152a-485d-8294-be195eef0bfb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Appl.  85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng. ", "original_text": "Jenkins, G.C. "}, "hash": "641d51f4a5969a22fd7256ca5b5478e19d6fcac34892daef5dac9d1051763228", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e014f322-54ce-4602-8f6e-b587a20b128b", "node_type": "1", "metadata": {"window": "[40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. ", "original_text": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n"}, "hash": "0b98977d2c270a5361bc01f81bb76b260c5d8d68fdebb613c5378c196c6dacb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Reinsel, G.M. ", "mimetype": "text/plain", "start_char_idx": 51985, "end_char_idx": 51999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e014f322-54ce-4602-8f6e-b587a20b128b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. ", "original_text": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "939a1ff9-311d-472a-bfb7-9a2d9d82c434", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "85 (1) (2017) 292\u2013304.\n [40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n", "original_text": "Reinsel, G.M. "}, "hash": "9da66f649e4278ea5d05dccd9f96af1c49144dd336bcad530a8e1437675c28d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b1acebc-e189-48a7-a2d2-c4f692ed8e6c", "node_type": "1", "metadata": {"window": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int. ", "original_text": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. "}, "hash": "5b98755c4e79308b51b0afc2b476d6e3eabd804e86dfb4f43bb0767697fca663", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n", "mimetype": "text/plain", "start_char_idx": 51999, "end_char_idx": 52066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b1acebc-e189-48a7-a2d2-c4f692ed8e6c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int. ", "original_text": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e014f322-54ce-4602-8f6e-b587a20b128b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[40] M. Munir, S.A.  Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. ", "original_text": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n"}, "hash": "62431a8f408e59357df2128a2d00cf966296cbd58a4c668984f869663e51a2b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40974a28-f143-4971-9c68-71779e1c9ca0", "node_type": "1", "metadata": {"window": "[41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf. ", "original_text": "Knowl. "}, "hash": "1f1a1b4c5d58912bd4b085cd771feb9681593f2770cafde5ac079e965751fea6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. ", "mimetype": "text/plain", "start_char_idx": 52066, "end_char_idx": 52190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40974a28-f143-4971-9c68-71779e1c9ca0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf. ", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b1acebc-e189-48a7-a2d2-c4f692ed8e6c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Siddiqui, A. Dengel, S. Ahmed, Deepant: A deep learning approach for unsupervised anomaly detection in time series, IEEE Access 7 (2019) 1991\u20132005.\n [41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int. ", "original_text": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans. "}, "hash": "18c141c44551705e2207d780137142291dd05b4a8afa96e352695ce90838905f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2147629d-3602-46b8-81a5-846a7f3a9cde", "node_type": "1", "metadata": {"window": "Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp. ", "original_text": "Data Eng. "}, "hash": "2e05fc0cf983613bd1d5cadc7493cc80d2762b48c009ee83998cbc054b35b7ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 52190, "end_char_idx": 52197, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2147629d-3602-46b8-81a5-846a7f3a9cde", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp. ", "original_text": "Data Eng. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40974a28-f143-4971-9c68-71779e1c9ca0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[41] G. Box, G.M.  Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf. ", "original_text": "Knowl. "}, "hash": "cbf4619d72195fd819f1ca7f2cd97256e8824de08502f297350afb4759ac606d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7ac9a4a-0848-4048-b388-592517304e28", "node_type": "1", "metadata": {"window": "Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n", "original_text": "18 (4) (2006) 482\u2013489.\n"}, "hash": "533ab177eda74e40f987474780e83880511afc4bd4aaa0023fc59f5ca1b390ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Eng. ", "mimetype": "text/plain", "start_char_idx": 52197, "end_char_idx": 52207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7ac9a4a-0848-4048-b388-592517304e28", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n", "original_text": "18 (4) (2006) 482\u2013489.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2147629d-3602-46b8-81a5-846a7f3a9cde", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Jenkins, G.C.  Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp. ", "original_text": "Data Eng. "}, "hash": "d53283125546e0952e82942acb8066831051855663d9f99657fac717548779cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "deb25766-b1da-4b0b-9821-72c7c7adb640", "node_type": "1", "metadata": {"window": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. ", "original_text": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. "}, "hash": "c2c5c871b28c7e50aa41267389d314ea5bd8b9fce970d7676892ca06bbe9cf45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 (4) (2006) 482\u2013489.\n", "mimetype": "text/plain", "start_char_idx": 52207, "end_char_idx": 52230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "deb25766-b1da-4b0b-9821-72c7c7adb640", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. ", "original_text": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7ac9a4a-0848-4048-b388-592517304e28", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Reinsel, G.M.  Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n", "original_text": "18 (4) (2006) 482\u2013489.\n"}, "hash": "6e55aa1db3774baa20bb6bc2834f6217084e4f6be66ff881dd2badc5bbc6b8a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e326515b-a795-45cd-bac7-0fffab98bb63", "node_type": "1", "metadata": {"window": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. ", "original_text": "IEEE Int. "}, "hash": "ea1080dab10274cf07251c2243fea4b3fd26313e752211a9a66c6674421ea803", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 52230, "end_char_idx": 52401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e326515b-a795-45cd-bac7-0fffab98bb63", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. ", "original_text": "IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "deb25766-b1da-4b0b-9821-72c7c7adb640", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ljung, Time Series Analysis: Forecasting and Control, Wiley, 2015.\n [42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. ", "original_text": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc. "}, "hash": "f870e3fa27a0189a2b5dada453532417497a135cf27d5ec8e124c0f66356207d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba28f3b4-41c5-42c0-8085-85edcede9b91", "node_type": "1", "metadata": {"window": "Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron. ", "original_text": "Conf. "}, "hash": "262fb4d2a7a926521814bf7c13cab9a92431b2c1778e7e2f43158b972eddb256", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 52401, "end_char_idx": 52411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba28f3b4-41c5-42c0-8085-85edcede9b91", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron. ", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e326515b-a795-45cd-bac7-0fffab98bb63", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[42] J. Takeuchi, K. Yamanishi, A unifying framework for detecting outliers and change points from time series, IEEE Trans.  Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. ", "original_text": "IEEE Int. "}, "hash": "4e576d321778f2fc6e4f252849a00ca3c1f31e1580a558c7ef28e817b2c34263", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e3d7ccf-65a5-4572-99fc-12ee1d0bd39e", "node_type": "1", "metadata": {"window": "Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n", "original_text": "Data Mining Workshops, Pisa, Italy, 2008, pp. "}, "hash": "1cc9754111dc763b5f7a1c5019a17ef57a6759b7a525a319bdf0d57bb9126d98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 52411, "end_char_idx": 52417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e3d7ccf-65a5-4572-99fc-12ee1d0bd39e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n", "original_text": "Data Mining Workshops, Pisa, Italy, 2008, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba28f3b4-41c5-42c0-8085-85edcede9b91", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron. ", "original_text": "Conf. "}, "hash": "e5903bedd220be8752b92847ca212107e2915171130014522f68dd7cc68cdecc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87e83105-c556-4cb7-bae9-ef5cb7192b30", "node_type": "1", "metadata": {"window": "18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W. ", "original_text": "349\u2013358.\n"}, "hash": "4ece0edcd3fb9a450520a7c3fc7047e78eed788507d4cb9bbc27dba3251896a5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data Mining Workshops, Pisa, Italy, 2008, pp. ", "mimetype": "text/plain", "start_char_idx": 52417, "end_char_idx": 52463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "87e83105-c556-4cb7-bae9-ef5cb7192b30", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W. ", "original_text": "349\u2013358.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e3d7ccf-65a5-4572-99fc-12ee1d0bd39e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data Eng.  18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n", "original_text": "Data Mining Workshops, Pisa, Italy, 2008, pp. "}, "hash": "85dea202258fc50de599af1a931ef7fcd1f1572bc185d78f941afad00f299794", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ae31e15-a920-4b8f-adcd-2f7f6e3b80cc", "node_type": "1", "metadata": {"window": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G. ", "original_text": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. "}, "hash": "31f7890214c7cab70e82b2577c2553ba6562f52621553deb216f4a438bed6e96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "349\u2013358.\n", "mimetype": "text/plain", "start_char_idx": 52463, "end_char_idx": 52472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ae31e15-a920-4b8f-adcd-2f7f6e3b80cc", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G. ", "original_text": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87e83105-c556-4cb7-bae9-ef5cb7192b30", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "18 (4) (2006) 482\u2013489.\n [43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W. ", "original_text": "349\u2013358.\n"}, "hash": "bf6814fd53394ae142a7c5afb8cf1dff301dfb64ab2cd699f547208841371ba0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad4a506a-f336-4160-8ce5-89e07fd0f273", "node_type": "1", "metadata": {"window": "IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. ", "original_text": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. "}, "hash": "a519f1fb6d6c5a5b5c21b48af7b42e960362d785d5558fce63a494e8c07ffc00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. ", "mimetype": "text/plain", "start_char_idx": 52472, "end_char_idx": 52525, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad4a506a-f336-4160-8ce5-89e07fd0f273", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. ", "original_text": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ae31e15-a920-4b8f-adcd-2f7f6e3b80cc", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[43] H. Cheng, P. Tan, C. Potter, S. Klooster, A robust graph-based algorithm for detection and characterization of anomalies in noisy multivariate time series, in: Proc.  IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G. ", "original_text": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C. "}, "hash": "c92a25a00afd8c96624674475971f659a6e02d1f28b52f1c41dfef97948952a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cb46975-d7f3-4582-a064-43f2b7d17406", "node_type": "1", "metadata": {"window": "Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens. ", "original_text": "Mechatron. "}, "hash": "c887e516c3c56a6ef4f038027abfdb07d6fc2b581300d3b2e4d5bc97c0bceef1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. ", "mimetype": "text/plain", "start_char_idx": 52525, "end_char_idx": 52618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cb46975-d7f3-4582-a064-43f2b7d17406", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens. ", "original_text": "Mechatron. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad4a506a-f336-4160-8ce5-89e07fd0f273", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "IEEE Int.  Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. ", "original_text": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans. "}, "hash": "3c672b278597dd42b144a701265d0a8adf5dada659f024c412d69e0ac60f87f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4435cfe2-1038-4c58-8ff6-9897cccc4dd7", "node_type": "1", "metadata": {"window": "Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw. ", "original_text": "11 (4) (2006) 439\u2013447.\n"}, "hash": "3e842f1c4bb7f5f2d9129809ec949e8a77c96d3a89d897e2401938ea58bec78b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mechatron. ", "mimetype": "text/plain", "start_char_idx": 52618, "end_char_idx": 52629, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4435cfe2-1038-4c58-8ff6-9897cccc4dd7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw. ", "original_text": "11 (4) (2006) 439\u2013447.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cb46975-d7f3-4582-a064-43f2b7d17406", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens. ", "original_text": "Mechatron. "}, "hash": "7f4d436c398223843b02cdafb68130ab5f5e503b0da4434bca81cc79d417c7d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a94c724b-0074-4480-8cec-1c6f5fa2d5a2", "node_type": "1", "metadata": {"window": "349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n", "original_text": "[45] E.W. "}, "hash": "8d36a827e30d38488e5380e00784ef2e74d7e083af05d14de8e16fc5a50b88b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "11 (4) (2006) 439\u2013447.\n", "mimetype": "text/plain", "start_char_idx": 52629, "end_char_idx": 52652, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a94c724b-0074-4480-8cec-1c6f5fa2d5a2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n", "original_text": "[45] E.W. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4435cfe2-1038-4c58-8ff6-9897cccc4dd7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Data Mining Workshops, Pisa, Italy, 2008, pp.  349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw. ", "original_text": "11 (4) (2006) 439\u2013447.\n"}, "hash": "1d6d0fc9c127f73a4602874ab099a5cb8f4fff4f7350d6706b471b7fc394cd80", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc4fd902-3da8-45db-9447-3d4561b0ef6c", "node_type": "1", "metadata": {"window": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J. ", "original_text": "Dereszynski, T.G. "}, "hash": "bc6ae67b7e1537094e7960b4a70dc07c3acc6692d12170b9b14091124b54ed5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[45] E.W. ", "mimetype": "text/plain", "start_char_idx": 52652, "end_char_idx": 52662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc4fd902-3da8-45db-9447-3d4561b0ef6c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J. ", "original_text": "Dereszynski, T.G. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a94c724b-0074-4480-8cec-1c6f5fa2d5a2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "349\u2013358.\n [44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n", "original_text": "[45] E.W. "}, "hash": "20267ead3fba252157a71d53df44713b112e373873519ed45887c7dde6c23b81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "997c233a-06d8-4213-950c-d96723b11d35", "node_type": "1", "metadata": {"window": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S. ", "original_text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. "}, "hash": "796efc0e6453dd995ae47a04270e18ea8ca73930d7f021efbd5ed239fe42a4eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dereszynski, T.G. ", "mimetype": "text/plain", "start_char_idx": 52662, "end_char_idx": 52680, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "997c233a-06d8-4213-950c-d96723b11d35", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S. ", "original_text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc4fd902-3da8-45db-9447-3d4561b0ef6c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[44] A. Khatkhate, A. Ray, E. Keller, S. Gupta, S.C.  Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J. ", "original_text": "Dereszynski, T.G. "}, "hash": "e9094dc4d7727c4601c3d218eca57ae5f316e645d45041a59aeab74b0de77e01", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7abcd7e7-e0a9-4e6f-929b-e56d0f079294", "node_type": "1", "metadata": {"window": "Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "original_text": "Sens. "}, "hash": "43058459f853218ff373f844a9134292bd6a4c7859637dcfe1f57cc239efaee5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. ", "mimetype": "text/plain", "start_char_idx": 52680, "end_char_idx": 52800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7abcd7e7-e0a9-4e6f-929b-e56d0f079294", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "original_text": "Sens. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "997c233a-06d8-4213-950c-d96723b11d35", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Chin, Symbolic time series analysis for anomaly detection in mechanical systems, IEEE Trans.  Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S. ", "original_text": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans. "}, "hash": "7f18503b37f27ab440f9ca94f81bdc992881f4673e0797a5a3e2a3aee31cd707", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a38c9e5-fdba-4f1b-9bc1-e325a117574d", "node_type": "1", "metadata": {"window": "11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int. ", "original_text": "Netw. "}, "hash": "064ffeb7d3f7c218091428f94c403718447ddfe077dd7f35a2b4a48b17c6f509", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sens. ", "mimetype": "text/plain", "start_char_idx": 52800, "end_char_idx": 52806, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a38c9e5-fdba-4f1b-9bc1-e325a117574d", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int. ", "original_text": "Netw. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7abcd7e7-e0a9-4e6f-929b-e56d0f079294", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Mechatron.  11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "original_text": "Sens. "}, "hash": "fa758f27007c999c3fa3ed937f2002000e0985cc84f61b61fed02f696043c645", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0bf7637-bc2f-4811-97bb-6cdcc416dc73", "node_type": "1", "metadata": {"window": "[45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc. ", "original_text": "8 (1) (2011) 3.\n"}, "hash": "c4d5133eb5de2cd8753586f0b6f83ed4f4d15affaa468d62aa36342407001c1f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Netw. ", "mimetype": "text/plain", "start_char_idx": 52806, "end_char_idx": 52812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0bf7637-bc2f-4811-97bb-6cdcc416dc73", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc. ", "original_text": "8 (1) (2011) 3.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a38c9e5-fdba-4f1b-9bc1-e325a117574d", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "11 (4) (2006) 439\u2013447.\n [45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int. ", "original_text": "Netw. "}, "hash": "aa711ebd3356322b99669d20c09d51b77e8eeb932e8740d7088d27803c46aab2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86174c1c-96d9-4931-93d3-cfb154058fc8", "node_type": "1", "metadata": {"window": "Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng. ", "original_text": "[46] D.J. "}, "hash": "787ef60a9a4c765d91fe41b4ac60bbc63add407f6fd40078ea789c754aa31788", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "8 (1) (2011) 3.\n", "mimetype": "text/plain", "start_char_idx": 52812, "end_char_idx": 52828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86174c1c-96d9-4931-93d3-cfb154058fc8", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng. ", "original_text": "[46] D.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0bf7637-bc2f-4811-97bb-6cdcc416dc73", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[45] E.W.  Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc. ", "original_text": "8 (1) (2011) 3.\n"}, "hash": "3e373137a3d9d74a4aef02b76d56c624c718785af3ed3c4fe14b47a4455c31c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0927da60-beb4-489d-a4ea-eff9983f0051", "node_type": "1", "metadata": {"window": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n", "original_text": "Hill, B.S. "}, "hash": "d3e77592feb975fb0c38babd86e587aba60c5aa7a38b9c66229ebb311215a927", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[46] D.J. ", "mimetype": "text/plain", "start_char_idx": 52828, "end_char_idx": 52838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0927da60-beb4-489d-a4ea-eff9983f0051", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n", "original_text": "Hill, B.S. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86174c1c-96d9-4931-93d3-cfb154058fc8", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Dereszynski, T.G.  Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng. ", "original_text": "[46] D.J. "}, "hash": "234b73a81daa69a8d277663d6250798bb8be8adfce670ab7649f3177e0a562e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7e7fb65-95c1-42fe-8344-d48f224ad374", "node_type": "1", "metadata": {"window": "Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B. ", "original_text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. "}, "hash": "08b3562d45727792450000a7a3aec782210461a2e53035a2c36f35b6dff93d46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hill, B.S. ", "mimetype": "text/plain", "start_char_idx": 52838, "end_char_idx": 52849, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7e7fb65-95c1-42fe-8344-d48f224ad374", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B. ", "original_text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0927da60-beb4-489d-a4ea-eff9983f0051", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Dietterich, Spatio-temporal models for data anomaly detection in dynamic environmental monitoring campaigns, ACM Trans.  Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n", "original_text": "Hill, B.S. "}, "hash": "9dc0a6c580aa01a3a76efe1c4b123770e36a15d2f716e8927db759075671f599", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40a73b50-75d5-4c6b-b549-7dee16716e9f", "node_type": "1", "metadata": {"window": "Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. ", "original_text": "32nd Congress of the Int. "}, "hash": "0122610452e43f10fa36d3e9e7cab0c7a43672487a3728896af8aed95e1d6916", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. ", "mimetype": "text/plain", "start_char_idx": 52849, "end_char_idx": 52945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40a73b50-75d5-4c6b-b549-7dee16716e9f", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. ", "original_text": "32nd Congress of the Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7e7fb65-95c1-42fe-8344-d48f224ad374", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Sens.  Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B. ", "original_text": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc. "}, "hash": "5b127b0f064dcac2c96a1b171661114db63d2a78aa6f3e332753862a572fc8c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bd23a80-c00b-49cf-a51e-6d4aa00a28bb", "node_type": "1", "metadata": {"window": "8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast. ", "original_text": "Assoc. "}, "hash": "7291f101c34ccaeb80c98a42ada59a7e0b1a4e437e2025c2340417a9e1a6142f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "32nd Congress of the Int. ", "mimetype": "text/plain", "start_char_idx": 52945, "end_char_idx": 52971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6bd23a80-c00b-49cf-a51e-6d4aa00a28bb", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast. ", "original_text": "Assoc. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40a73b50-75d5-4c6b-b549-7dee16716e9f", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Netw.  8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. ", "original_text": "32nd Congress of the Int. "}, "hash": "3679d61a465a983f97f1977872febdb99780a9af930631737d67124a2a35c8ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4cd061c-581d-42ca-bdeb-9978f49b624b", "node_type": "1", "metadata": {"window": "[46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n", "original_text": "of Hydraulic Eng. "}, "hash": "d41507556f5391f1ce4d5cb86366e1a487af420f0e6c080c007ec54afdd2b201", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Assoc. ", "mimetype": "text/plain", "start_char_idx": 52971, "end_char_idx": 52978, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4cd061c-581d-42ca-bdeb-9978f49b624b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n", "original_text": "of Hydraulic Eng. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bd23a80-c00b-49cf-a51e-6d4aa00a28bb", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "8 (1) (2011) 3.\n [46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast. ", "original_text": "Assoc. "}, "hash": "ec60e777b09b7c876deb6e0bb561111d2e599a7dab8aa078de654be999f694c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fadb46f-df14-411f-81d1-b7296c8659ee", "node_type": "1", "metadata": {"window": "Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C. ", "original_text": "and Research, 2007.\n"}, "hash": "ad164c9279127fdc6d90e7ff4240555135644dfe98a1d3dcb2f89c0fc774bb35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "of Hydraulic Eng. ", "mimetype": "text/plain", "start_char_idx": 52978, "end_char_idx": 52996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6fadb46f-df14-411f-81d1-b7296c8659ee", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C. ", "original_text": "and Research, 2007.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4cd061c-581d-42ca-bdeb-9978f49b624b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[46] D.J.  Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n", "original_text": "of Hydraulic Eng. "}, "hash": "748802b1f509e2577347638b0fd54aa61ff3f36c5710159921c6ff119eeb5e7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2691f83-fe95-41b5-b93f-01a2be8e357a", "node_type": "1", "metadata": {"window": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. ", "original_text": "[47] D.B. "}, "hash": "3bf11000075a3c772df532f145559ba832cf897d89d457b3cc5d3d258bbd2845", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and Research, 2007.\n", "mimetype": "text/plain", "start_char_idx": 52996, "end_char_idx": 53016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2691f83-fe95-41b5-b93f-01a2be8e357a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. ", "original_text": "[47] D.B. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fadb46f-df14-411f-81d1-b7296c8659ee", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Hill, B.S.  Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C. ", "original_text": "and Research, 2007.\n"}, "hash": "80ba3c87f5e5958fcaf717efc29489a51de13733079ef3cec182c8e9c277f120", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbabd06d-de84-423c-80d3-4fa6b8a21136", "node_type": "1", "metadata": {"window": "32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci. ", "original_text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. "}, "hash": "aabf1700c18d1e1e44dc44fb765e8d0a1fe63d6082b796408cb8d0e513849946", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[47] D.B. ", "mimetype": "text/plain", "start_char_idx": 53016, "end_char_idx": 53026, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bbabd06d-de84-423c-80d3-4fa6b8a21136", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci. ", "original_text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2691f83-fe95-41b5-b93f-01a2be8e357a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Minsker, E. Amir, Real-time Bayesian anomaly detection for environmental sensor data, in: Proc.  32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. ", "original_text": "[47] D.B. "}, "hash": "13aa7aceb72281bdcb0c10e3b210cb68014e565910e7f14eb8d2dee6fa6f5e11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64aa3b42-f205-43d1-99a4-3f918cf29756", "node_type": "1", "metadata": {"window": "Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n", "original_text": "J. Forecast. "}, "hash": "e31f72c6c86690b8d1117afbf6f42c36531d86988d17db2a20d463dfe1d8fbd5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. ", "mimetype": "text/plain", "start_char_idx": 53026, "end_char_idx": 53113, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64aa3b42-f205-43d1-99a4-3f918cf29756", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n", "original_text": "J. Forecast. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbabd06d-de84-423c-80d3-4fa6b8a21136", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "32nd Congress of the Int.  Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci. ", "original_text": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int. "}, "hash": "110ca6ae00b64bb25f3b819ffaf488cd7bc4a0dcea49fb1fb12c888e60b74a31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f6ec7b7-5d35-4a99-95ba-49c018db0fda", "node_type": "1", "metadata": {"window": "of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. ", "original_text": "25 (3) (2009) 498\u2013517.\n"}, "hash": "f63ee26c0a158ada0a1ca8471a02e638557f0934e699b728c93c3687c5daee99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J. Forecast. ", "mimetype": "text/plain", "start_char_idx": 53113, "end_char_idx": 53126, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f6ec7b7-5d35-4a99-95ba-49c018db0fda", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. ", "original_text": "25 (3) (2009) 498\u2013517.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64aa3b42-f205-43d1-99a4-3f918cf29756", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Assoc.  of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n", "original_text": "J. Forecast. "}, "hash": "a3d45a9e7a2c630352a84db354a954fdf011795b860ab5117844e930cdc09a75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11aad91-354a-4c00-a3e0-60d01bf92b9c", "node_type": "1", "metadata": {"window": "and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf. ", "original_text": "[48] J.C. "}, "hash": "f5304c37a8bdc5146492cc920fc2a12dd2cfaf8df047564bc6f466f36fb7a84e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "25 (3) (2009) 498\u2013517.\n", "mimetype": "text/plain", "start_char_idx": 53126, "end_char_idx": 53149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e11aad91-354a-4c00-a3e0-60d01bf92b9c", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf. ", "original_text": "[48] J.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f6ec7b7-5d35-4a99-95ba-49c018db0fda", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "of Hydraulic Eng.  and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. ", "original_text": "25 (3) (2009) 498\u2013517.\n"}, "hash": "76df813cb7f6e07946b61ccdde67df2eca80faf34b074a475ed3a6871742bca6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa1e31d6-40a0-4603-8583-2e4101909243", "node_type": "1", "metadata": {"window": "[47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp. ", "original_text": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. "}, "hash": "4c8af3db8bbfde8c700ef0babe1bc521c87135303adfc5fcf1ca875384017e13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[48] J.C. ", "mimetype": "text/plain", "start_char_idx": 53149, "end_char_idx": 53159, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fa1e31d6-40a0-4603-8583-2e4101909243", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp. ", "original_text": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11aad91-354a-4c00-a3e0-60d01bf92b9c", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "and Research, 2007.\n [47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf. ", "original_text": "[48] J.C. "}, "hash": "4b20efc22b39d4f4a9d398c089f6c9407c0a0eedbff37f4c1fe4ea0f31e2fbe2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "133d80d8-fd0b-4b9d-809e-fb846d8f3672", "node_type": "1", "metadata": {"window": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n", "original_text": "Geosci. "}, "hash": "dca34cc34e0fc8ea1402f195d78c76f135317037efcf0baad94072342571d48a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. ", "mimetype": "text/plain", "start_char_idx": 53159, "end_char_idx": 53241, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "133d80d8-fd0b-4b9d-809e-fb846d8f3672", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n", "original_text": "Geosci. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa1e31d6-40a0-4603-8583-2e4101909243", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[47] D.B.  Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp. ", "original_text": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput. "}, "hash": "7eea9b6a7a5fb123aa9248521c1f25903d05fdc035fb4c78bd8add6af01ecd37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74ca6047-a33c-4728-9029-510e38864e4e", "node_type": "1", "metadata": {"window": "J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. ", "original_text": "10 (2\u20133) (1984) 191\u2013203.\n"}, "hash": "9ef6b070c53b2f5a76d76384561f576abe037fca1749ce19c3999fda1697d712", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Geosci. ", "mimetype": "text/plain", "start_char_idx": 53241, "end_char_idx": 53249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74ca6047-a33c-4728-9029-510e38864e4e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. ", "original_text": "10 (2\u20133) (1984) 191\u2013203.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "133d80d8-fd0b-4b9d-809e-fb846d8f3672", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Neill, Expectation-based scan statistics for monitoring spatial time series data, Int.  J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n", "original_text": "Geosci. "}, "hash": "e09f077e31bcf6dd05b9a0fdba046fef7aad55fe5aa169091efbfd21dba5da6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "994e87a1-40cd-4ef1-a3de-7dd66c96004a", "node_type": "1", "metadata": {"window": "25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci. ", "original_text": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. "}, "hash": "1d548803a3d59ca55efdfa4b7d6095b08e395f362e6f9ea082382b69a630e4e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10 (2\u20133) (1984) 191\u2013203.\n", "mimetype": "text/plain", "start_char_idx": 53249, "end_char_idx": 53274, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "994e87a1-40cd-4ef1-a3de-7dd66c96004a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci. ", "original_text": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74ca6047-a33c-4728-9029-510e38864e4e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J. Forecast.  25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. ", "original_text": "10 (2\u20133) (1984) 191\u2013203.\n"}, "hash": "0549f1393b11c0ba9ea745b6c6911a1adf5204b5e4130bad88fa9f34d7a333cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2da3b68-bf40-4e3d-b831-e512e69922d7", "node_type": "1", "metadata": {"window": "[48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n", "original_text": "Conf. "}, "hash": "801093d0e37cbd601625c1974df532dc6588d5ac1d0f42bc4ae491d064fd59bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. ", "mimetype": "text/plain", "start_char_idx": 53274, "end_char_idx": 53405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2da3b68-bf40-4e3d-b831-e512e69922d7", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n", "original_text": "Conf. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "994e87a1-40cd-4ef1-a3de-7dd66c96004a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "25 (3) (2009) 498\u2013517.\n [48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci. ", "original_text": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int. "}, "hash": "b108fa235895170e600e0b5b72fdd761bd57fc17c559630aa852a0eab49dae48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69ef3aff-bd84-4bdd-8b76-a0a21e7d00f0", "node_type": "1", "metadata": {"window": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. ", "original_text": "on Data Mining, Pisa, Italy, 2008, pp. "}, "hash": "3546f1d4e83630edeb3ccf7b83cf23dad4570c062095baa32909ea7f85919d01", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conf. ", "mimetype": "text/plain", "start_char_idx": 53405, "end_char_idx": 53411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69ef3aff-bd84-4bdd-8b76-a0a21e7d00f0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. ", "original_text": "on Data Mining, Pisa, Italy, 2008, pp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2da3b68-bf40-4e3d-b831-e512e69922d7", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[48] J.C.  Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n", "original_text": "Conf. "}, "hash": "a30267be16a90d067c2eeb6cd075c4fec4fc7c341787c9986c8958d366e132ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39da9903-e643-4d96-9cf5-f37f094aaf7b", "node_type": "1", "metadata": {"window": "Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl. ", "original_text": "743\u2013748.\n"}, "hash": "8f2bf2614933577ee314ebf0bb64b4290ce0ef31e0d351cddf6d7132cf53805f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "on Data Mining, Pisa, Italy, 2008, pp. ", "mimetype": "text/plain", "start_char_idx": 53411, "end_char_idx": 53450, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39da9903-e643-4d96-9cf5-f37f094aaf7b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl. ", "original_text": "743\u2013748.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69ef3aff-bd84-4bdd-8b76-a0a21e7d00f0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Bezdek, R. Ehrlich, W. Full, FCM: The fuzzy c-means clustering algorithm, Comput.  Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. ", "original_text": "on Data Mining, Pisa, Italy, 2008, pp. "}, "hash": "a2fdcb1e1b5aa147557b50c09f8713fd5a88c3e18c98ccc11469703d0545c6a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4fe479f-464f-4082-9d15-852c9bfa4d0e", "node_type": "1", "metadata": {"window": "10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov. ", "original_text": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. "}, "hash": "2335515a5cf6daa95f8ae6bc0b75423f686ee91a5994b5f57bea34f715daa12f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "743\u2013748.\n", "mimetype": "text/plain", "start_char_idx": 53450, "end_char_idx": 53459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4fe479f-464f-4082-9d15-852c9bfa4d0e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov. ", "original_text": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39da9903-e643-4d96-9cf5-f37f094aaf7b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Geosci.  10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl. ", "original_text": "743\u2013748.\n"}, "hash": "6dc680858769c555f24b75f7a6b72de0285783cff05fd793436108ebba923d19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60f08ea1-80a4-4fb0-beb7-9f36ab916851", "node_type": "1", "metadata": {"window": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n", "original_text": "Sci. "}, "hash": "8373d557445f2ebadb22ae045de59a85133bbe6e6392260b05261b93db65b5f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. ", "mimetype": "text/plain", "start_char_idx": 53459, "end_char_idx": 53527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60f08ea1-80a4-4fb0-beb7-9f36ab916851", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n", "original_text": "Sci. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4fe479f-464f-4082-9d15-852c9bfa4d0e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "10 (2\u20133) (1984) 191\u2013203.\n [49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov. ", "original_text": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform. "}, "hash": "16cbbfdbfbedfd191c01f75dded155f55a215feba2a514369e072fdbf961083d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63e1b9c6-8c4f-460e-85a0-a675a2e44859", "node_type": "1", "metadata": {"window": "Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. ", "original_text": "505 (2019) 513\u2013534.\n"}, "hash": "718604dfe9eca2546903fde5dc220e6040db7fe4838174f4571b19ecffb9d9f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sci. ", "mimetype": "text/plain", "start_char_idx": 53527, "end_char_idx": 53532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63e1b9c6-8c4f-460e-85a0-a675a2e44859", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. ", "original_text": "505 (2019) 513\u2013534.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60f08ea1-80a4-4fb0-beb7-9f36ab916851", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[49] V. Chandola, V. Mithal, V. Kumar, Comparative evaluation of anomaly detection techniques for sequence data, in: 8th IEEE Int.  Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n", "original_text": "Sci. "}, "hash": "9fd45902273de688c94649e13fae7cf59db8e6e6793e8d6762d276cc0303eb1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d4b93fe-bac3-475e-9184-886fba94a459", "node_type": "1", "metadata": {"window": "on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n", "original_text": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. "}, "hash": "edcab58dc61307803610fd38b014b8bb10917eb84494d777994ca59f7bd2da7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "505 (2019) 513\u2013534.\n", "mimetype": "text/plain", "start_char_idx": 53532, "end_char_idx": 53552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d4b93fe-bac3-475e-9184-886fba94a459", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n", "original_text": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63e1b9c6-8c4f-460e-85a0-a675a2e44859", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Conf.  on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. ", "original_text": "505 (2019) 513\u2013534.\n"}, "hash": "8b8b18793dd9a7186b81bf77ef9fa20b5b81c43ae62ce333468003b4a19cba99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f036118-cc86-4ac7-b559-5156492863e1", "node_type": "1", "metadata": {"window": "743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L. ", "original_text": "Knowl. "}, "hash": "55db53d919297892d9acedf622196745ff0fbd5f78585eecd5475219bd88e2b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. ", "mimetype": "text/plain", "start_char_idx": 53552, "end_char_idx": 53655, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7f036118-cc86-4ac7-b559-5156492863e1", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L. ", "original_text": "Knowl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d4b93fe-bac3-475e-9184-886fba94a459", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "on Data Mining, Pisa, Italy, 2008, pp.  743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n", "original_text": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min. "}, "hash": "cefe431a31a0d3ed792ad3fc855ae7694729ef60d7bf2921ea5d37feebfcdf81", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "921d216b-42e7-4a4c-b3c5-f24df526238a", "node_type": "1", "metadata": {"window": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom. ", "original_text": "Discov. "}, "hash": "c37da75f8047fbfaac75ef2d701b24d54dec67c596f78da066e7814dde4fc623", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Knowl. ", "mimetype": "text/plain", "start_char_idx": 53655, "end_char_idx": 53662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "921d216b-42e7-4a4c-b3c5-f24df526238a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom. ", "original_text": "Discov. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f036118-cc86-4ac7-b559-5156492863e1", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "743\u2013748.\n [50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L. ", "original_text": "Knowl. "}, "hash": "57fe41ac5822b68c66e3c25a025ef5cfcc732884d996f5da5e44c69b98c0d171", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "abab9799-129f-4af0-8f71-861cc8ac366e", "node_type": "1", "metadata": {"window": "Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n", "original_text": "29 (3) (2015) 626\u2013688.\n"}, "hash": "556ce035d58e2c2382abdacbf673b69d6cabb6e7d199ebcaa81069c71f08cb15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Discov. ", "mimetype": "text/plain", "start_char_idx": 53662, "end_char_idx": 53670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "abab9799-129f-4af0-8f71-861cc8ac366e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n", "original_text": "29 (3) (2015) 626\u2013688.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "921d216b-42e7-4a4c-b3c5-f24df526238a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[50] P. D\u2019Urso, R. Massari, Fuzzy clustering of mixed data, Inform.  Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom. ", "original_text": "Discov. "}, "hash": "efc69f6bdba26ee500f89b56acc193006e93f01bc4623d24842ec05ee4d16ed3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "042e3a8f-49fc-4002-8525-06753c228bd4", "node_type": "1", "metadata": {"window": "505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J. ", "original_text": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. "}, "hash": "82672e7afc0fb0ab185cca107cc07ba2b95aa7ed1b7da0ed581913283c93a51d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "29 (3) (2015) 626\u2013688.\n", "mimetype": "text/plain", "start_char_idx": 53670, "end_char_idx": 53693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "042e3a8f-49fc-4002-8525-06753c228bd4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J. ", "original_text": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "abab9799-129f-4af0-8f71-861cc8ac366e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Sci.  505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n", "original_text": "29 (3) (2015) 626\u2013688.\n"}, "hash": "7c724de532466c443e53ee54fc2e362535e883a68532b4ae2600d63583d5fed1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e9bbb38-b15a-4eb0-a839-2a7cb84f3834", "node_type": "1", "metadata": {"window": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L. ", "original_text": "98 (2016) 130\u2013147.\n"}, "hash": "8f4b41b50f15b71ee4e4a42752902bf4eece69736930b955a7cd5d7e5276f691", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. ", "mimetype": "text/plain", "start_char_idx": 53693, "end_char_idx": 53800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e9bbb38-b15a-4eb0-a839-2a7cb84f3834", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L. ", "original_text": "98 (2016) 130\u2013147.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "042e3a8f-49fc-4002-8525-06753c228bd4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "505 (2019) 513\u2013534.\n [51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J. ", "original_text": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst. "}, "hash": "78f6ebe6d56e6366b0668e949985daf7fd55ddd0c932a0fe559af27ddc0f30c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f78a15d-bdd6-4f37-b9e4-a22150281b46", "node_type": "1", "metadata": {"window": "Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. ", "original_text": "[53] R.L. "}, "hash": "cc3764eb333b96762104c1a4a628f6adcc4989f5d86ca9875316b6781c25e893", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "98 (2016) 130\u2013147.\n", "mimetype": "text/plain", "start_char_idx": 53800, "end_char_idx": 53819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f78a15d-bdd6-4f37-b9e4-a22150281b46", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. ", "original_text": "[53] R.L. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e9bbb38-b15a-4eb0-a839-2a7cb84f3834", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[51] L. Akoglu, H. Tong, D. Koutra, Graph based anomaly detection and description: a survey, Data Min.  Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L. ", "original_text": "98 (2016) 130\u2013147.\n"}, "hash": "b1a8321564cf1a5da8f47ceb2955907877f7352b9a7c91a2367179efb51cafd8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f248e749-688b-47f6-a4a8-5bee43b7d4f2", "node_type": "1", "metadata": {"window": "Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag. ", "original_text": "Thorndike, Who belongs in the family?, Psychom. "}, "hash": "36abdd2f4ab4556c52bda7bedb6ba45e4442b963bdb964310c1765a0aa709ba0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[53] R.L. ", "mimetype": "text/plain", "start_char_idx": 53819, "end_char_idx": 53829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f248e749-688b-47f6-a4a8-5bee43b7d4f2", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag. ", "original_text": "Thorndike, Who belongs in the family?, Psychom. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f78a15d-bdd6-4f37-b9e4-a22150281b46", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Knowl.  Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. ", "original_text": "[53] R.L. "}, "hash": "e1e65280c1af81760aecfc4561722185ab3e8ac4abf1fa855affe11a86b5d4b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b7230b7-6fd6-4a29-bf9f-9647f40c6726", "node_type": "1", "metadata": {"window": "29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J. ", "original_text": "18 (4) (1953) 267\u2013276.\n"}, "hash": "3ffeac64b1d8fe0e0f8130c6d5fd4aff9de41dbb40d20f2cce05fb860e0b06e7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thorndike, Who belongs in the family?, Psychom. ", "mimetype": "text/plain", "start_char_idx": 53829, "end_char_idx": 53877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b7230b7-6fd6-4a29-bf9f-9647f40c6726", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J. ", "original_text": "18 (4) (1953) 267\u2013276.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f248e749-688b-47f6-a4a8-5bee43b7d4f2", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Discov.  29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag. ", "original_text": "Thorndike, Who belongs in the family?, Psychom. "}, "hash": "505c360059ddca53d8271f69b23ea53c5363d38a00dd5dd45ea8dbf7f942d7be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bb37cd7-77fc-4420-ab05-35b7fd6d2101", "node_type": "1", "metadata": {"window": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n", "original_text": "[54] D.J. "}, "hash": "9a3056efe10a7f36a2035e778285ddb2fe452adea3550656fdeee3af9f9d9657", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 (4) (1953) 267\u2013276.\n", "mimetype": "text/plain", "start_char_idx": 53877, "end_char_idx": 53900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bb37cd7-77fc-4420-ab05-35b7fd6d2101", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n", "original_text": "[54] D.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b7230b7-6fd6-4a29-bf9f-9647f40c6726", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "29 (3) (2015) 626\u2013688.\n [52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J. ", "original_text": "18 (4) (1953) 267\u2013276.\n"}, "hash": "8c8a4c785b0b809453e9b91d1a72279b40aff7fc25952a88027c1fc51afae5b9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "091325ed-730b-4935-b54f-94367e2e5d71", "node_type": "1", "metadata": {"window": "98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A. ", "original_text": "Ketchen Jr, C.L. "}, "hash": "1de0dff535bc83de5988175c655440dfbfc593a83d65a9c926d7bb4e3173ae9e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[54] D.J. ", "mimetype": "text/plain", "start_char_idx": 53900, "end_char_idx": 53910, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "091325ed-730b-4935-b54f-94367e2e5d71", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A. ", "original_text": "Ketchen Jr, C.L. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bb37cd7-77fc-4420-ab05-35b7fd6d2101", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[52] T. H. Fanaee, J. Gama, Tensor-based anomaly detection: An interdisciplinary survey, Knowl-Based Syst.  98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n", "original_text": "[54] D.J. "}, "hash": "b81a9c244072c33d8258042828c28abc608d426312f1169c21b9a29e3a6537ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a8da9f5-04b6-439e-9693-cb64704ad20b", "node_type": "1", "metadata": {"window": "[53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A. ", "original_text": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. "}, "hash": "7747a5cd77de6d2418b1d2196221b1e6cdb4699931dfc939eba3a21e9aac4133", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ketchen Jr, C.L. ", "mimetype": "text/plain", "start_char_idx": 53910, "end_char_idx": 53927, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a8da9f5-04b6-439e-9693-cb64704ad20b", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A. ", "original_text": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "091325ed-730b-4935-b54f-94367e2e5d71", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "98 (2016) 130\u2013147.\n [53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A. ", "original_text": "Ketchen Jr, C.L. "}, "hash": "9aa8aa451a643c8848dc33bca20ee784867970b24186dc254d2bb913df0787a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49bada9f-b94b-4ab9-84ba-5b34362c3465", "node_type": "1", "metadata": {"window": "Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl. ", "original_text": "Manag. "}, "hash": "42a5259dbcbdcb5dded9375811976196b558542d4f0ba386673fe55bacc11568", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. ", "mimetype": "text/plain", "start_char_idx": 53927, "end_char_idx": 54039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49bada9f-b94b-4ab9-84ba-5b34362c3465", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl. ", "original_text": "Manag. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a8da9f5-04b6-439e-9693-cb64704ad20b", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[53] R.L.  Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A. ", "original_text": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg. "}, "hash": "7d12e7c16a29a677cff4b0ff823a9581acb890e149ce6b58d254a17e7051466c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d96a4023-dff6-4067-9a39-dbe4bacae946", "node_type": "1", "metadata": {"window": "18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat. ", "original_text": "J. "}, "hash": "8c111bf8a48065a875986a4b2c2451ef7d128eef2e32361220dd999d2c0d6a14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Manag. ", "mimetype": "text/plain", "start_char_idx": 54039, "end_char_idx": 54046, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d96a4023-dff6-4067-9a39-dbe4bacae946", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat. ", "original_text": "J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49bada9f-b94b-4ab9-84ba-5b34362c3465", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Thorndike, Who belongs in the family?, Psychom.  18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl. ", "original_text": "Manag. "}, "hash": "3ae341c0b8c0a5c37920dbae6124b0ffd0594eadd3db2dcb3ce9b8518ce81ffd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba53f277-db52-494f-bb7d-16ea560f456a", "node_type": "1", "metadata": {"window": "[54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n", "original_text": "17 (6) (1996) 441\u2013458.\n"}, "hash": "1c9b3812ae2f09acbf2f4385bf2861e5ded1a6075efdcfc828a728c70f9c2a4c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J. ", "mimetype": "text/plain", "start_char_idx": 54046, "end_char_idx": 54049, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba53f277-db52-494f-bb7d-16ea560f456a", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n", "original_text": "17 (6) (1996) 441\u2013458.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d96a4023-dff6-4067-9a39-dbe4bacae946", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "18 (4) (1953) 267\u2013276.\n [54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat. ", "original_text": "J. "}, "hash": "31dddc6b41b8b28249b0ced8d995d280bd25f57363ab243e27f67108606dbd65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d3db5a0-275b-4420-b2e8-c76df78c353e", "node_type": "1", "metadata": {"window": "Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "[55] J.A. "}, "hash": "da40b5e91281631290479fcee6e968d6c365645d42314215725304a8e99c80d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "17 (6) (1996) 441\u2013458.\n", "mimetype": "text/plain", "start_char_idx": 54049, "end_char_idx": 54072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d3db5a0-275b-4420-b2e8-c76df78c353e", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "[55] J.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba53f277-db52-494f-bb7d-16ea560f456a", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[54] D.J.  Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n", "original_text": "17 (6) (1996) 441\u2013458.\n"}, "hash": "0c8cb6c89878d8f918428844fc1182d81aac6d8b5cb65c8f59e9043dc258cd8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8afb69e7-e8cd-4a10-82fa-42c7bc0e11a0", "node_type": "1", "metadata": {"window": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Hartigan, M.A. "}, "hash": "88357be512997b7c41be8d122f0e7311a8225cc2d8b959abf685c1048f790a91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[55] J.A. ", "mimetype": "text/plain", "start_char_idx": 54072, "end_char_idx": 54082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8afb69e7-e8cd-4a10-82fa-42c7bc0e11a0", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Hartigan, M.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d3db5a0-275b-4420-b2e8-c76df78c353e", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Ketchen Jr, C.L.  Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "[55] J.A. "}, "hash": "1f49b6d26df2a008e96e3c595dd63f654588b27daa33635b7b5df74ef9b642c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7922208-0797-4793-8dc8-538ee8100c28", "node_type": "1", "metadata": {"window": "Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Wong, A K-Means clustering algorithm, Appl. "}, "hash": "094331a8786bf42d96bda0d5f17e03d645540ecf8a929451e8b79ad1011382f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hartigan, M.A. ", "mimetype": "text/plain", "start_char_idx": 54082, "end_char_idx": 54097, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7922208-0797-4793-8dc8-538ee8100c28", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Wong, A K-Means clustering algorithm, Appl. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8afb69e7-e8cd-4a10-82fa-42c7bc0e11a0", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Shook, The application of cluster analysis in strategic management research: An analysis and critique, Strateg.  Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Hartigan, M.A. "}, "hash": "87f8ae93cb4af9ddbef38ba16f89d8c9953ae3b31bd600ae580af1a4429356c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33cd77d2-6e59-4235-a7fb-77b2477406f4", "node_type": "1", "metadata": {"window": "J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Stat. "}, "hash": "728cf0e7f09905d0bc35e153aa45a21ab202c1a65e9c0b2981476b2ab45205b4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Wong, A K-Means clustering algorithm, Appl. ", "mimetype": "text/plain", "start_char_idx": 54097, "end_char_idx": 54141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33cd77d2-6e59-4235-a7fb-77b2477406f4", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Stat. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7922208-0797-4793-8dc8-538ee8100c28", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "Manag.  J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Wong, A K-Means clustering algorithm, Appl. "}, "hash": "ff2358f52e09ce97195475cfec9c68970bbc53ce819a9f4d782658aa0fde47ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fba92f2-6bca-4720-8dae-092e55e4fd04", "node_type": "1", "metadata": {"window": "17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "28 (1) (1979) 100\u2013108.\n"}, "hash": "8cbb074f95a1dfd77277c123b23e26bff713d85dfe18c9da92ed34f338a840b5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Stat. ", "mimetype": "text/plain", "start_char_idx": 54141, "end_char_idx": 54147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fba92f2-6bca-4720-8dae-092e55e4fd04", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "28 (1) (1979) 100\u2013108.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33cd77d2-6e59-4235-a7fb-77b2477406f4", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "J.  17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "Stat. "}, "hash": "e29904874fdbfe554e47531d83ceee86cfca0929cf32ba2cb0f8f5db20ffd0a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebadccb2-9131-48e6-b6dd-0632747efea5", "node_type": "1", "metadata": {"window": "[55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "[56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016."}, "hash": "af0b9b95500acfc940caeb37c1ba6c268e1eaac4b974622a461769c719e90b06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "28 (1) (1979) 100\u2013108.\n", "mimetype": "text/plain", "start_char_idx": 54147, "end_char_idx": 54170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebadccb2-9131-48e6-b6dd-0632747efea5", "embedding": null, "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "[55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "[56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "24deba7f-b238-4885-9ed3-e695ffea644b", "node_type": "4", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf"}, "hash": "1860e590bfc31d8a817d99dd37172c7752604493e7a7dd5d24d7a551a086fdab", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fba92f2-6bca-4720-8dae-092e55e4fd04", "node_type": "1", "metadata": {"title": "K-means-based isolation forest", "authors": "Karczmarek et al.", "year": 2020, "file_path": "ad-papers-pdf/kmeans_isolation_forest.pdf", "window": "17 (6) (1996) 441\u2013458.\n [55] J.A.  Hartigan, M.A.  Wong, A K-Means clustering algorithm, Appl.  Stat.  28 (1) (1979) 100\u2013108.\n [56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "original_text": "28 (1) (1979) 100\u2013108.\n"}, "hash": "ddf4f1ec3fc8d57f1131779b96f886e5903366a5dde55d229401bc29cbfc52f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[56] B. Donovan, D. Work, New York City Taxi Trip Data (2010\u20132013), University of Illinois at Urbana-Champaign, 2016.", "mimetype": "text/plain", "start_char_idx": 54170, "end_char_idx": 54287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]