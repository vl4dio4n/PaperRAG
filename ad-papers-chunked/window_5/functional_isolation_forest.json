[{"id_": "47002985-d183-4965-927d-276860790d41", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm. ", "original_text": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc712d52-1d0e-4893-aaa0-17088c54415c", "node_type": "1", "metadata": {"window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. ", "original_text": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest. "}, "hash": "745f86ca2a46e2420894e51e3cf82c25d92e1ab24dc52cf3cd6f9e959862689a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 508, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc712d52-1d0e-4893-aaa0-17088c54415c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. ", "original_text": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47002985-d183-4965-927d-276860790d41", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm. ", "original_text": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g. "}, "hash": "43ead53474d4a7c70c6a7e26ef2b2013d4600e698a99b5d6590ed71a01757d6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfcdcd5a-f93b-4136-9f09-28bf37a7f1d9", "node_type": "1", "metadata": {"window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. ", "original_text": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions. "}, "hash": "cdfc3c62d17a471c21df73bc54726f6c38d0274c4d18fd22bfd53850a7c0a049", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest. ", "mimetype": "text/plain", "start_char_idx": 508, "end_char_idx": 771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfcdcd5a-f93b-4136-9f09-28bf37a7f1d9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. ", "original_text": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc712d52-1d0e-4893-aaa0-17088c54415c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. ", "original_text": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest. "}, "hash": "920c3e8d1804f84def5a8368e06cb255b4bf373623f002b9b219233a9f579676", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a05dae69-fffb-42c3-9d21-a96c6f9eff42", "node_type": "1", "metadata": {"window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n", "original_text": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data. "}, "hash": "db4f0dae811d8822d969a9ada816bf7fbabe14e2f5eec318f0d3c0c1307b4a58", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions. ", "mimetype": "text/plain", "start_char_idx": 771, "end_char_idx": 888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a05dae69-fffb-42c3-9d21-a96c6f9eff42", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n", "original_text": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfcdcd5a-f93b-4136-9f09-28bf37a7f1d9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. ", "original_text": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions. "}, "hash": "3dd518eeceb90edfadb4aea62e19910a32babfa6bfe4ea8f2481b81a7ad3886a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c27dc6b-9b07-4c21-ba8a-c85c05ecafdf", "node_type": "1", "metadata": {"window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. ", "original_text": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves. "}, "hash": "429dabf700b638e1431757eaf5cba189710c0bd1a411bdf9e85f828cf2ae605f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data. ", "mimetype": "text/plain", "start_char_idx": 888, "end_char_idx": 1074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c27dc6b-9b07-4c21-ba8a-c85c05ecafdf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. ", "original_text": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a05dae69-fffb-42c3-9d21-a96c6f9eff42", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n", "original_text": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data. "}, "hash": "67fb35ba9d1ceac38defe130dce4d5a24e02f84348128f2860cdf74a9661f180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb723cce-9c1f-490a-8818-03e79a1704e1", "node_type": "1", "metadata": {"window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. ", "original_text": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm. "}, "hash": "64ad459c0c4c7a3806d8ce5bd98fe3d7306cce785ef0fe242e168a8368d6c1ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves. ", "mimetype": "text/plain", "start_char_idx": 1074, "end_char_idx": 1255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb723cce-9c1f-490a-8818-03e79a1704e1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. ", "original_text": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c27dc6b-9b07-4c21-ba8a-c85c05ecafdf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. ", "original_text": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves. "}, "hash": "174fb22c0c5692fd7bd991b155b069463d48355c8b0b06ff701323f67c240ced", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4de80fe4-5583-47bd-9b76-9d6860659f18", "node_type": "1", "metadata": {"window": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. ", "original_text": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. "}, "hash": "eee8bd77c8ad94cc7e74bb0cbb9c19cfd41ab74bdbee77968544a865a3e8853f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm. ", "mimetype": "text/plain", "start_char_idx": 1255, "end_char_idx": 1461, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4de80fe4-5583-47bd-9b76-9d6860659f18", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. ", "original_text": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb723cce-9c1f-490a-8818-03e79a1704e1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "arXiv:1904.04573v3 [stat.ML] 9 Oct 2019\nProceedings of Machine learning Research 101:1-33, 2019\n\n# Functional Isolation Forest\n\n**Guillaume Staerman**\nGUILLAUME.STAERMAN@TELECOM-PARIS.FR\n\n**Pavlo Mozharovskyi**\nPAVLO.MOZHAROVSKYI@TELECOM-PARIS.FR\n\n**Stephan Cl\u00e9men\u00e7on**\nSTEPHAN.CLEMENCON@TELECOM-PARIS.FR\n\n**Florence d\u2019Alch\u00e9-Buc**\nFLORENCE.DALCHE@TELECOM-PARIS.FR\n\nLTCI, T\u00e9l\u00e9com Paris, Institut Polytechnique de Paris\n\n## Abstract\n\nFor the purpose of monitoring the behavior of complex infrastructures (e.g.  aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. ", "original_text": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm. "}, "hash": "4d733115dd48b2431a9fadb81a9f9b70b6c9181da5a7e73da5b230cffab1c22a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2db6c1cd-e5a4-45e2-bcb0-1fa3412de2a3", "node_type": "1", "metadata": {"window": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. ", "original_text": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. "}, "hash": "02741d5ed9068967e1e1ec8b9c611f3ac750dc7209b93bf6b4062f623f3b90d5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. ", "mimetype": "text/plain", "start_char_idx": 1461, "end_char_idx": 1583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2db6c1cd-e5a4-45e2-bcb0-1fa3412de2a3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. ", "original_text": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4de80fe4-5583-47bd-9b76-9d6860659f18", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "aircrafts, transport or energy networks), high-rate sensors are deployed to capture multivariate data, generally unlabeled, in quasi continuous-time to detect quickly the occurrence of anomalies that may jeopardize the smooth operation of the system of interest.  The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. ", "original_text": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length. "}, "hash": "922f596281a0e8f06c8f9b1ba97ba3441b898bbcf637b30b924919e102141578", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc54e969-7372-47e7-8755-ad40c40b64c7", "node_type": "1", "metadata": {"window": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. ", "original_text": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n"}, "hash": "c8de0b36cf3b4cbbc42006703c77633aeaf85fe2dd5dd0b19e136935910f1c3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. ", "mimetype": "text/plain", "start_char_idx": 1583, "end_char_idx": 1833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc54e969-7372-47e7-8755-ad40c40b64c7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. ", "original_text": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2db6c1cd-e5a4-45e2-bcb0-1fa3412de2a3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The statistical analysis of such massive data of functional nature raises many challenging methodological questions.  The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. ", "original_text": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension. "}, "hash": "11de47adb19232ae1be77b5335db9cdfb0d35e8a27af69fbc38faf287cd9a2d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8acfad5-5d70-4c2a-a508-eb124ae3dd46", "node_type": "1", "metadata": {"window": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. ", "original_text": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. "}, "hash": "aac8fad3ec01a127d5143425c28bff88d00d24400fa9e8f3fbaba1fef17bc284", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n", "mimetype": "text/plain", "start_char_idx": 1833, "end_char_idx": 1933, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8acfad5-5d70-4c2a-a508-eb124ae3dd46", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. ", "original_text": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc54e969-7372-47e7-8755-ad40c40b64c7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The primary goal of this paper is to extend the popular ISOLATION FOREST (IF) approach to Anomaly Detection, originally dedicated to finite dimensional observations, to functional data.  The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. ", "original_text": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n"}, "hash": "7f7f7b06b871c8ce193c209c0ae0c9b7d109fa5d896bd3e81494006a0dbf12bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dfb7b5e2-b6b5-443a-9a80-4567acae881d", "node_type": "1", "metadata": {"window": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n", "original_text": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. "}, "hash": "f6e6f5e2d1fe71d2cfe465863398b13a71c0d0ad84c3a6ac0317a155b691e450", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. ", "mimetype": "text/plain", "start_char_idx": 1933, "end_char_idx": 2039, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dfb7b5e2-b6b5-443a-9a80-4567acae881d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n", "original_text": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8acfad5-5d70-4c2a-a508-eb124ae3dd46", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The major difficulty lies in the wide variety of topological structures that may equip a space of functions and the great variety of patterns that may characterize abnormal curves.  We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. ", "original_text": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1. "}, "hash": "112ea852adf048a17fd80505a9bcac5fcb5b21c207c81b541a80843d53da162c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "585387c4-b810-4e11-a0fe-678d53972fc4", "node_type": "1", "metadata": {"window": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. ", "original_text": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. "}, "hash": "eead56fe20efc8a734fce5b08231c47938e33a43c3fb5b24977d6a0c3165ea27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. ", "mimetype": "text/plain", "start_char_idx": 2039, "end_char_idx": 2185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "585387c4-b810-4e11-a0fe-678d53972fc4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. ", "original_text": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dfb7b5e2-b6b5-443a-9a80-4567acae881d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We address the issue of (randomly) splitting the functional space in a flexible manner in order to isolate progressively any trajectory from the others, a key ingredient to the efficiency of the algorithm.  Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n", "original_text": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g. "}, "hash": "68ccba2b9b65079b3f4ca96e25d0005964fc04d23b0d08657cfd6f778c48890d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d90aac22-91ca-430d-b786-c172f589b22e", "node_type": "1", "metadata": {"window": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n", "original_text": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. "}, "hash": "4758ea730f6e7ee8db739400a566078b0ba7b1abd08636dd19dd4772669ef934", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. ", "mimetype": "text/plain", "start_char_idx": 2185, "end_char_idx": 2276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d90aac22-91ca-430d-b786-c172f589b22e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n", "original_text": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "585387c4-b810-4e11-a0fe-678d53972fc4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Beyond a detailed description of the algorithm, computational complexity and stability issues are investigated at length.  From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. ", "original_text": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g. "}, "hash": "dcf5d5d00dfcc3d8d8084e65870d56df5157aaab977c33ad21e9f977abd21519", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a9b8c2b-073e-44aa-8c6e-9f6c9a285ed1", "node_type": "1", "metadata": {"window": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. ", "original_text": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. "}, "hash": "bdc86e8724347d1cffb8d9dd86580898623d42d034d809d4f7e72ee02b47bd5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. ", "mimetype": "text/plain", "start_char_idx": 2276, "end_char_idx": 2426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a9b8c2b-073e-44aa-8c6e-9f6c9a285ed1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. ", "original_text": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d90aac22-91ca-430d-b786-c172f589b22e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "From the scoring function measuring the degree of abnormality of an observation provided by the proposed variant of the IF algorithm, a *Functional Statistical Depth* function is defined and discussed, as well as a multivariate functional extension.  Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n", "original_text": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations. "}, "hash": "b0a016cdf2ccdd4b45a2d51faf2042e5c872a3e829f64194de1bada44e1a998e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13194170-5b3c-47bc-a70e-42ca99bffb47", "node_type": "1", "metadata": {"window": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). ", "original_text": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. "}, "hash": "371fe9ba42bebd672700f4cb6cbb195fea4f277b5e3bf8e007616ed6a88d867d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. ", "mimetype": "text/plain", "start_char_idx": 2426, "end_char_idx": 2677, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13194170-5b3c-47bc-a70e-42ca99bffb47", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). ", "original_text": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a9b8c2b-073e-44aa-8c6e-9f6c9a285ed1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Numerical experiments provide strong empirical evidence of the accuracy of the extension proposed.\n\n **Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. ", "original_text": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems. "}, "hash": "ab0f7a54b42c7122e66ae4c9543f3d644f5da5f184911dc42fc346caf5eda01c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eafa8c65-eb05-415c-8b27-1a9b0c101bcd", "node_type": "1", "metadata": {"window": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. ", "original_text": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n"}, "hash": "62930fb116aae073ac7c2f0bb90f520685c1fcfb77cb36bd4f9d12726ac104f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. ", "mimetype": "text/plain", "start_char_idx": 2677, "end_char_idx": 3074, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eafa8c65-eb05-415c-8b27-1a9b0c101bcd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. ", "original_text": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13194170-5b3c-47bc-a70e-42ca99bffb47", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Keywords:** Anomaly detection, functional data analysis, isolation forest, unsupervised learning\n\n## 1.  Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). ", "original_text": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems. "}, "hash": "7659758d690c79bdf9f07c94feb554b0457f5a6e6be5b90edcb699ec36af5a63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5cc9c3c-fe07-4722-ae54-cd686c32844b", "node_type": "1", "metadata": {"window": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n", "original_text": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. "}, "hash": "6ceaf0f7fd8a3ddaf0e241c0725c66ce79a811d99ca3a0fbfbfa5cd01c53ab85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n", "mimetype": "text/plain", "start_char_idx": 3074, "end_char_idx": 3194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f5cc9c3c-fe07-4722-ae54-cd686c32844b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n", "original_text": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eafa8c65-eb05-415c-8b27-1a9b0c101bcd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Introduction\n\nThe digital information boom, that goes hand in hand with the recent technological advances in data collection and management (e.g.  IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. ", "original_text": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n"}, "hash": "c1cdc00552cc31feab373601dda5ae6cb2e86fb7cfd66af298e31de3fa69e613", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0ba253ed-d4f3-4ecb-9fda-e8339bbd9a15", "node_type": "1", "metadata": {"window": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. ", "original_text": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n"}, "hash": "eb60050e725da5c4c0e6801cf88ed0b50537064b7f5f21e94d892795491c1c8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. ", "mimetype": "text/plain", "start_char_idx": 3194, "end_char_idx": 3690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0ba253ed-d4f3-4ecb-9fda-e8339bbd9a15", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. ", "original_text": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5cc9c3c-fe07-4722-ae54-cd686c32844b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "IoT, distributed platforms), offers new perspectives in many areas of human activity (e.g.  transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n", "original_text": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based. "}, "hash": "64e6b4c8b8a4a6ae188482ca5d283d40be0d6b24e24b8eeccd80f3bb2fed2109", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91a3dd6a-0510-4453-b8a2-ea589016bfec", "node_type": "1", "metadata": {"window": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. ", "original_text": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. "}, "hash": "967801f516acc251f46e08d9293ebe01b73276126a6208228d2ec0be0f7c409e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n", "mimetype": "text/plain", "start_char_idx": 3690, "end_char_idx": 3977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91a3dd6a-0510-4453-b8a2-ea589016bfec", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. ", "original_text": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0ba253ed-d4f3-4ecb-9fda-e8339bbd9a15", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "transportation, energy, health, commerce, insurance), and confronts these domains with major scientific challenges for exploiting these observations.  The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. ", "original_text": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n"}, "hash": "79056ee62cc52bf736165b75dbf813e4343fbdc2f4e62fe42170980fd4958ac6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbdc35ed-c747-4b78-847e-31709eec19cc", "node_type": "1", "metadata": {"window": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. ", "original_text": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). "}, "hash": "30a6396cd57ba49ac05aabc608f55b75de37886a2a9de093c6cdb1d78b0feec5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. ", "mimetype": "text/plain", "start_char_idx": 3977, "end_char_idx": 4277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cbdc35ed-c747-4b78-847e-31709eec19cc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. ", "original_text": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91a3dd6a-0510-4453-b8a2-ea589016bfec", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The ever growing availability of massive data, often collected in quasi-real time, engendered high expectations, in particular the need of increased automation and computational efficiency, with the goal to design more and more \u2018intelligent\u2019 systems.  In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. ", "original_text": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used. "}, "hash": "3a0f9fcdc8d2f7b04e684bc5dd03fe6811d5f4b5ae2275a66802da7a17b5aa0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "edd2aa17-6e58-4fc3-9c6e-6ed5d5a0177d", "node_type": "1", "metadata": {"window": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time). ", "original_text": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. "}, "hash": "77a173ef9417a00039678a07b38d1dc38d2bcc1af25cd61a84c3bced61a31c19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). ", "mimetype": "text/plain", "start_char_idx": 4277, "end_char_idx": 4536, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "edd2aa17-6e58-4fc3-9c6e-6ed5d5a0177d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time). ", "original_text": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbdc35ed-c747-4b78-847e-31709eec19cc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In particular, modern high-rate sensors enabling the continuous observation of the behavior of complex systems pave the way for the design of efficient unsupervised machine-learning approaches to anomaly detection, that may find applications in various domains ranging from fraud surveillance to distributed fleet monitoring through predictive maintenance or health monitoring of complex systems.  However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. ", "original_text": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005). "}, "hash": "c30fa898f27f7071a21d7af5a8ec49cc50d53295803c574d338bb7a470740f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cac5b59-2663-4e1c-ba8b-4540380c8d89", "node_type": "1", "metadata": {"window": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. ", "original_text": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n"}, "hash": "cf9335b5e911254d03800be5414c1714262c1cb7bde1f03fb76ba77f576360ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. ", "mimetype": "text/plain", "start_char_idx": 4536, "end_char_idx": 4875, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cac5b59-2663-4e1c-ba8b-4540380c8d89", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. ", "original_text": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "edd2aa17-6e58-4fc3-9c6e-6ed5d5a0177d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "However, although many unsupervised learning pro-\n\n\u00a9 2019 G. Staerman, P. Mozharovskyi, S. Cl\u00e9men\u00e7on & F. d\u2019Alch\u00e9-Buc.\n\n cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time). ", "original_text": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data. "}, "hash": "f7cfc7360e73145a723643ac6e9c3f009c497b2f034cc6020b698ac3208427a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7906915e-b56c-40c1-ba56-a9d7e1a60f14", "node_type": "1", "metadata": {"window": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. ", "original_text": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. "}, "hash": "6ba4b853adb670dd71ba6639165a02945aec0831f29f6eca3182187d46a8432d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n", "mimetype": "text/plain", "start_char_idx": 4875, "end_char_idx": 5171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7906915e-b56c-40c1-ba56-a9d7e1a60f14", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. ", "original_text": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cac5b59-2663-4e1c-ba8b-4540380c8d89", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "cedures for anomaly detection (AD in abbreviated form) have been proposed, analyzed and applied in a variety of practical situations (see, e.g., (Chandola et al., 2009)), the case of functional data, though of crucial importance in practice (refer to (Ramsay and Silverman, 2005; Ferraty and Vieu, 2006) for an account of Functional Data Analysis) has received much less attention in the literature, the vast majority of methods that are documented in the literature being generally model-based.  The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. ", "original_text": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n"}, "hash": "61589367f542f7d250b93ebc8af5f7870008c0641b7fd098d81ff3d6f787eb5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7387d07-1ec6-41f6-9f4e-a9eb535d2959", "node_type": "1", "metadata": {"window": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g. ", "original_text": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. "}, "hash": "2519a423b8442656562c9e4a5e67f09e382fce360c18b85733dd008a86762b53", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. ", "mimetype": "text/plain", "start_char_idx": 5171, "end_char_idx": 5356, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7387d07-1ec6-41f6-9f4e-a9eb535d2959", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g. ", "original_text": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7906915e-b56c-40c1-ba56-a9d7e1a60f14", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The main barrier to the design of nonparametric anomaly detection techniques tailored to the functional framework lies in the huge diversity of patterns that may carry the information that is relevant to discriminate between abnormal and normal observations, see (Hubert et al., 2015).\n\n It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. ", "original_text": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup. "}, "hash": "b5fecd6cafbf902ed68792749338cd4958cb774a5c94216b6122bdf18f37faa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9845bb0-9998-4eaf-88e0-027b305dc984", "node_type": "1", "metadata": {"window": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. ", "original_text": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. "}, "hash": "ec9240ee74e9c95a08e0d2fde8913a047c85eb8f18d453256355a66cd5215568", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. ", "mimetype": "text/plain", "start_char_idx": 5356, "end_char_idx": 5504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9845bb0-9998-4eaf-88e0-027b305dc984", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. ", "original_text": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7387d07-1ec6-41f6-9f4e-a9eb535d2959", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It seems indeed far from straightforward to extend machine-learning methods for anomaly detection in the finite-dimensional case such as (Scott and Nowak, 2006; Sch\u00f6lkopf et al., 2001; Steinwart et al., 2005; Vert and Vert, 2006; Park et al., 2010), unless preliminary filtering techniques are used.  The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g. ", "original_text": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure. "}, "hash": "819f0a8a4ae9273b1985733c86c81e76f6e853eb7694d8239dad6eb8f7895a8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8dfa6cc-5469-4840-8ddb-8c989817cdef", "node_type": "1", "metadata": {"window": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. ", "original_text": "the splitting direction and the splitting value both at the same time). "}, "hash": "aea39f8dfc66af13fb808ae53a1147289784ba861cc3fe2cc6e04974bd95edf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. ", "mimetype": "text/plain", "start_char_idx": 5504, "end_char_idx": 5737, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a8dfa6cc-5469-4840-8ddb-8c989817cdef", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. ", "original_text": "the splitting direction and the splitting value both at the same time). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9845bb0-9998-4eaf-88e0-027b305dc984", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The filtering approach consists in projecting the functional data onto an adequate finite dimensional function subspace and using then the coefficients describing the latter to \"feed\" next some AD algorithm for multivariate data (Ramsay and Silverman, 2005).  The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. ", "original_text": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t. "}, "hash": "01040835d2344a5d455360de1d1fd908800d569323257aea1dd6b8ab76b204a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72f91a54-7a17-4695-ad5f-4e5ba1b746d3", "node_type": "1", "metadata": {"window": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. ", "original_text": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. "}, "hash": "15ada9ffe4c590e249a09bb6b668c9a19ed676e4b23374b444d64370b6737e60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the splitting direction and the splitting value both at the same time). ", "mimetype": "text/plain", "start_char_idx": 5737, "end_char_idx": 5809, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72f91a54-7a17-4695-ad5f-4e5ba1b746d3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. ", "original_text": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8dfa6cc-5469-4840-8ddb-8c989817cdef", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The basis functions are either selected through Principal Component Analysis (they correspond in this case to elements of the Karhunen-Lo\u00e8ve basis related to the process under study, supposedly of second order), or else are chosen among a dictionary of \"time-frequency atoms\" according to their capacity to represent efficiently the data.  The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. ", "original_text": "the splitting direction and the splitting value both at the same time). "}, "hash": "e82aa1cdd94b42e829cf003ae031be2747c4db448124680b9564e86d367001ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5967d407-27ff-46c7-89d0-0f412e2411ae", "node_type": "1", "metadata": {"window": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n", "original_text": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. "}, "hash": "33aab115b364cef1eee63fdaac64cf42b981b07f01bd332f50d844f8e681819c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. ", "mimetype": "text/plain", "start_char_idx": 5809, "end_char_idx": 6071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5967d407-27ff-46c7-89d0-0f412e2411ae", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n", "original_text": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72f91a54-7a17-4695-ad5f-4e5ba1b746d3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The representation a priori chosen, which can either enhance artificially certain accessory patterns or else make totally disappear some crucial features, critically determines performance of such an approach, the type of anomalies that can be recovered being essentially shaped by this choice.\n\n The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. ", "original_text": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball. "}, "hash": "5534826002aeeaccf963d58140670bc83b4e2e12ee3f59bf4764d63219ecc92d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a272a5d7-7120-4149-92dc-b4f192b0011a", "node_type": "1", "metadata": {"window": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows. ", "original_text": "Beyond obvious advantages regarding computational cost, scalability (e.g. "}, "hash": "fd4bcefbe7d0ebdfc5e161ca934fc3e2efe7504ca3c8aaf84670566c6f5aaee7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. ", "mimetype": "text/plain", "start_char_idx": 6071, "end_char_idx": 6368, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a272a5d7-7120-4149-92dc-b4f192b0011a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows. ", "original_text": "Beyond obvious advantages regarding computational cost, scalability (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5967d407-27ff-46c7-89d0-0f412e2411ae", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The angle embraced in the present article is very different, the goal pursued being to extend the popular ISOLATION FOREST methodology (Liu et al., 2008, 2012) to the functional setup.  This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n", "original_text": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data. "}, "hash": "c152795475a3ffb88c1f80df7dffbf1df34d33cbf26f8b95ed24bf2322f529b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcf58d2d-444e-4551-8950-96994d4f6a0d", "node_type": "1", "metadata": {"window": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. ", "original_text": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. "}, "hash": "7682ffbf324f958b4f95f7a8a95b080ee59207d5a327e5e4a5b587e927ebc600", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Beyond obvious advantages regarding computational cost, scalability (e.g. ", "mimetype": "text/plain", "start_char_idx": 6368, "end_char_idx": 6442, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fcf58d2d-444e-4551-8950-96994d4f6a0d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. ", "original_text": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a272a5d7-7120-4149-92dc-b4f192b0011a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This ensemble learning algorithm builds a collection of isolation trees based on a recursive and randomized tree-structured partitioning procedure.  An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows. ", "original_text": "Beyond obvious advantages regarding computational cost, scalability (e.g. "}, "hash": "fcb3b982a9245877e5315f1e1eb5430a9140ff699b3f4c871a5cb9605660e636", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f7fbc33-aecb-4db9-88bb-5e73696a88bb", "node_type": "1", "metadata": {"window": "the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length. ", "original_text": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. "}, "hash": "759abf229a38b00bac45bbb00f6f3fdb2bea950415d65eb01e8ae337b8217911", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. ", "mimetype": "text/plain", "start_char_idx": 6442, "end_char_idx": 6748, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f7fbc33-aecb-4db9-88bb-5e73696a88bb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length. ", "original_text": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcf58d2d-444e-4551-8950-96994d4f6a0d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An isolation tree is a binary tree, representing a nested collection of partitions of the finite dimensional feature space, grown iteratively in a top-down fashion, where the cuts are axis perpendicular and random (uniformly, w.r.t.  the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. ", "original_text": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes. "}, "hash": "50a893f0685b492a4008a72e5ece7c4d807a02cab995335528c3d970f3f91f55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0419254f-a891-4cfe-9860-43a9da806847", "node_type": "1", "metadata": {"window": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. ", "original_text": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. "}, "hash": "2fdcee012d4f0e746a3634428382ad45b3f842ed6d8340ebe738722e08a2dd82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. ", "mimetype": "text/plain", "start_char_idx": 6748, "end_char_idx": 6941, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0419254f-a891-4cfe-9860-43a9da806847", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. ", "original_text": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f7fbc33-aecb-4db9-88bb-5e73696a88bb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the splitting direction and the splitting value both at the same time).  Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length. ", "original_text": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context. "}, "hash": "83522ddb6acce2ba6be308653893899c80be3babd18b18feaf493c813f294233", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b19ab7d9-581c-4467-b22b-3bd972989a1a", "node_type": "1", "metadata": {"window": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. ", "original_text": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n"}, "hash": "6770a7a1b51cf8aaafd4358eab8659aff9da522658fabbad32ccc85565793b49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. ", "mimetype": "text/plain", "start_char_idx": 6941, "end_char_idx": 7107, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b19ab7d9-581c-4467-b22b-3bd972989a1a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. ", "original_text": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0419254f-a891-4cfe-9860-43a9da806847", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Incidentally, a variant referred to as EXTENDED ISOLATION FOREST (Hariri et al., 2018), has recently been proposed in the purpose of bias reduction: rather than randomly selecting a perpendicular split, a splitting direction is randomly chosen in the unit ball.  An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. ", "original_text": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability. "}, "hash": "7b0fb5182f3f1fe7e17f7c85cc685c258666e36cab6d88fcd050828e7175380b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "670116cf-cff5-4802-832b-f5fc348b7a5d", "node_type": "1", "metadata": {"window": "Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n", "original_text": "The paper is organized as follows. "}, "hash": "837563f16f8ee25d2d13be33e446cfb96bf77ed378ec91b910ff75cac0b68a10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n", "mimetype": "text/plain", "start_char_idx": 7107, "end_char_idx": 7205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "670116cf-cff5-4802-832b-f5fc348b7a5d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n", "original_text": "The paper is organized as follows. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b19ab7d9-581c-4467-b22b-3bd972989a1a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An anomaly score is assigned to any observation, depending on the length of the path necessary to isolate it from the rest of the data points, the rationale behind this approach being that anomalies should be easier to isolate in a random manner than normal (in the sense of \u2018non-abnormal\u2019) data.  Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. ", "original_text": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n"}, "hash": "7fdcf4b9653e7653b128d411ba07530f0fc17e2928137c5926d5bfacc55dc71c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16e16c1b-3fe0-47c1-8d61-f70ac707fd0b", "node_type": "1", "metadata": {"window": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2. ", "original_text": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. "}, "hash": "002b102a31dc5f9f21b3f1ef4e91474f6d994cc0c8d3cf954820e0a7e4b51dee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The paper is organized as follows. ", "mimetype": "text/plain", "start_char_idx": 7205, "end_char_idx": 7240, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16e16c1b-3fe0-47c1-8d61-f70ac707fd0b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2. ", "original_text": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "670116cf-cff5-4802-832b-f5fc348b7a5d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Beyond obvious advantages regarding computational cost, scalability (e.g.  isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n", "original_text": "The paper is organized as follows. "}, "hash": "9db1e1b29648b2e859689bf9565855b227c30b2b60875d10a54894ba4f52357a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ade84eb4-2f29-46f4-888c-1666a6a44489", "node_type": "1", "metadata": {"window": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n", "original_text": "In Section 3, the extension to the functional case is presented and its properties are discussed at length. "}, "hash": "9d72243ce50d18493ef8f867a40048b13e176bbdcaf78873659d66b8e87fa21b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. ", "mimetype": "text/plain", "start_char_idx": 7240, "end_char_idx": 7416, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ade84eb4-2f29-46f4-888c-1666a6a44489", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n", "original_text": "In Section 3, the extension to the functional case is presented and its properties are discussed at length. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16e16c1b-3fe0-47c1-8d61-f70ac707fd0b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "isolation trees can be built from subsamples) and interpretability, the great flexibility offered by Isolation Forest regarding the splitting procedure called recursively makes it appealing when it comes to isolate (multivariate) functions/curves, possibly exhibiting a wide variety of geometrical shapes.  It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2. ", "original_text": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data. "}, "hash": "00f15209d176d616c9323e13755bc1f61292c6c4dfee3f62f8f17697e9b6f5f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "496562ad-f03c-4d97-ba1c-fedbe29cffdf", "node_type": "1", "metadata": {"window": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1. ", "original_text": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. "}, "hash": "a17afe1c98b463ceb87cfc217a284ad6e410c43ab113e420c730a21596c7a9ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 3, the extension to the functional case is presented and its properties are discussed at length. ", "mimetype": "text/plain", "start_char_idx": 7416, "end_char_idx": 7524, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "496562ad-f03c-4d97-ba1c-fedbe29cffdf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1. ", "original_text": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ade84eb4-2f29-46f4-888c-1666a6a44489", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It is precisely the goal of this paper to introduce a new generic algorithm, FUNCTIONAL ISOLATION FOREST (FIF) that generalizes (Extended) Isolation Forest to the infinite dimensional context.  Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n", "original_text": "In Section 3, the extension to the functional case is presented and its properties are discussed at length. "}, "hash": "4f88616efbbd39b75640134c43c8b7b869ff94acf64acc8e8eff206fb509d8ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a959fd3c-4c4f-439a-b76a-570f2d9e0dab", "node_type": "1", "metadata": {"window": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. ", "original_text": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. "}, "hash": "8f466438ab3edf19b9618b80df78b813a24c21012524a5a3dc9c8a0a59b51d39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. ", "mimetype": "text/plain", "start_char_idx": 7524, "end_char_idx": 7709, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a959fd3c-4c4f-439a-b76a-570f2d9e0dab", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. ", "original_text": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "496562ad-f03c-4d97-ba1c-fedbe29cffdf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Avoiding dimensionality reduction steps, this extension is shown to preserve the assets of the original algorithm concerning computational cost and interpretability.  Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1. ", "original_text": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments. "}, "hash": "8411b2436d0f9053672e44ea080c9e6fc7b265a114ea1c69683b5486562fc443", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9504462e-ff07-4515-85e3-02b5a2be0fb4", "node_type": "1", "metadata": {"window": "The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. ", "original_text": "Eventually, several concluding remarks are collected in Section 6.\n\n"}, "hash": "862ca0c660d610b6edcfda723ecfabd30726e7615941d4684b8a0f27b5d6ac96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. ", "mimetype": "text/plain", "start_char_idx": 7709, "end_char_idx": 7888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9504462e-ff07-4515-85e3-02b5a2be0fb4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. ", "original_text": "Eventually, several concluding remarks are collected in Section 6.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a959fd3c-4c4f-439a-b76a-570f2d9e0dab", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Its efficiency is supported by strong empirical evidence through a variety of numerical results.\n\n The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. ", "original_text": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting. "}, "hash": "d06eaca0849f801a024e618483a6fdc8a5c536ef0a1bb9ccdceb67d4b6106de9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f3c3ee5-8d28-4b9f-b992-69f6456e6456", "node_type": "1", "metadata": {"window": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n", "original_text": "## 2. "}, "hash": "5202147bd3350b8eff8ad4aefd6b1719343a5200031e734232b21913f4cfeb82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Eventually, several concluding remarks are collected in Section 6.\n\n", "mimetype": "text/plain", "start_char_idx": 7888, "end_char_idx": 7956, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f3c3ee5-8d28-4b9f-b992-69f6456e6456", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n", "original_text": "## 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9504462e-ff07-4515-85e3-02b5a2be0fb4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The paper is organized as follows.  Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. ", "original_text": "Eventually, several concluding remarks are collected in Section 6.\n\n"}, "hash": "d83e3a02399e68c00c903b5e4ea430679f206fd970849dc220d667bf36d89cbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ddf9f3b-d048-4e0c-9ac8-156f4282550b", "node_type": "1", "metadata": {"window": "In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. ", "original_text": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n"}, "hash": "00f1fbb8eb48aa3d8017fb28313bb5b851c7bc1d8521f04b5c85611fabd1a91c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 2. ", "mimetype": "text/plain", "start_char_idx": 7956, "end_char_idx": 7962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ddf9f3b-d048-4e0c-9ac8-156f4282550b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. ", "original_text": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f3c3ee5-8d28-4b9f-b992-69f6456e6456", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Section 2 recalls the principles under the Isolation Forest algorithm for AD in the multivariate case and introduces the framework we consider for AD based on functional data.  In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n", "original_text": "## 2. "}, "hash": "3ee6eb819cadd05c996f0b0da172e455dbfacaa7cf43b25ae1d9f3691112f231", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f31a0c83-2fab-4c05-a204-45a32ae926f3", "node_type": "1", "metadata": {"window": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. ", "original_text": "### 2.1. "}, "hash": "566a3e25535a9396f68b87a73acf0e5e3de0a5ab44613db8cbbb6bc7a408bd51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n", "mimetype": "text/plain", "start_char_idx": 7962, "end_char_idx": 8194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f31a0c83-2fab-4c05-a204-45a32ae926f3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. ", "original_text": "### 2.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ddf9f3b-d048-4e0c-9ac8-156f4282550b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Section 3, the extension to the functional case is presented and its properties are discussed at length.  In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. ", "original_text": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n"}, "hash": "ac28d55eed9b4b6c3e3b2fd02b2a475efed81995ac9605854819fa854d58e377", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc5a7ed0-2447-4e3c-8427-520f3b7cf722", "node_type": "1", "metadata": {"window": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. ", "original_text": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. "}, "hash": "d54b47471fada95718f159e2228b12e68ab5aeca7c813b5902fa346ae5c50adc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2.1. ", "mimetype": "text/plain", "start_char_idx": 8194, "end_char_idx": 8203, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc5a7ed0-2447-4e3c-8427-520f3b7cf722", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. ", "original_text": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f31a0c83-2fab-4c05-a204-45a32ae926f3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Section 4, we study the behavior of the new algorithm and compare its performance to alternative methods standing as natural competitors in the functional setup through experiments.  In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. ", "original_text": "### 2.1. "}, "hash": "f8c1d49b6d9d75562d27782b5be7ed9cc5a7c632e6f5c7ccad84f2613dbbc32d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fa17be26-d3d7-4659-8c0a-f12cc9241070", "node_type": "1", "metadata": {"window": "Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n", "original_text": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. "}, "hash": "a24a274972fb5f34ae0797eb6d0f6718f35244a4f0006c41b434e748d4a42911", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. ", "mimetype": "text/plain", "start_char_idx": 8203, "end_char_idx": 8476, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fa17be26-d3d7-4659-8c0a-f12cc9241070", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n", "original_text": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc5a7ed0-2447-4e3c-8427-520f3b7cf722", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Section 5, extension to multivariate functional data is considered, as well as relation to the data depth function and an application to the supervised classification setting.  Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. ", "original_text": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively. "}, "hash": "15cce58cd03a835c03f99997c401bf3148f8ed3bdda2a7d2409669e5c65441ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a962f6c-2239-4631-9342-91c3526ecbc3", "node_type": "1", "metadata": {"window": "## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. ", "original_text": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n"}, "hash": "9db14c13e152f539ee1e4ed87fc5b4a9353de28ed900ec52f7df526e48a7d4b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. ", "mimetype": "text/plain", "start_char_idx": 8476, "end_char_idx": 8676, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a962f6c-2239-4631-9342-91c3526ecbc3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. ", "original_text": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fa17be26-d3d7-4659-8c0a-f12cc9241070", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Eventually, several concluding remarks are collected in Section 6.\n\n ## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n", "original_text": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter. "}, "hash": "4b71cb90807d398a13791bb1eb14f315160d0c66d6f596291cc4a6d9c16e28d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b440f68-50b2-4929-9c83-8f8c7f86761a", "node_type": "1", "metadata": {"window": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. ", "original_text": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. "}, "hash": "28cb3b4d9b8a8d2cd0a0e3c690b335b530ada2bd55e259d5392b001d0e3e9412", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n", "mimetype": "text/plain", "start_char_idx": 8676, "end_char_idx": 8892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b440f68-50b2-4929-9c83-8f8c7f86761a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. ", "original_text": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a962f6c-2239-4631-9342-91c3526ecbc3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 2.  Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. ", "original_text": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n"}, "hash": "d1ef87017a570df191482323bca972af89956ea15fb58fdc3844df02818f63f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "901fc253-f90d-416c-8e20-693b0ba35cad", "node_type": "1", "metadata": {"window": "### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. ", "original_text": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. "}, "hash": "05150ed0d8f73aa02b2402e23754a8f299d80b267c07deacdb0082ed0965dc13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. ", "mimetype": "text/plain", "start_char_idx": 8892, "end_char_idx": 9053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "901fc253-f90d-416c-8e20-693b0ba35cad", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. ", "original_text": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b440f68-50b2-4929-9c83-8f8c7f86761a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Background and Preliminaries\n\nHere we briefly recall the Isolation Forest algorithm and its advantages (Section 2.1) and next introduce the framework for functional anomaly detection we consider throughout the paper (Section 2.2).\n\n ### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. ", "original_text": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d. "}, "hash": "8251226ca03d8217bc4480e88a067d2840a182c95d24946e4c0dbb28443f86c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae06c57c-8fea-4b27-99d3-15ceb0a254d4", "node_type": "1", "metadata": {"window": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. ", "original_text": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. "}, "hash": "73d62e8d10bb2a3ed82aff4c61b26493d1cadadace2e564982a5835c040007f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. ", "mimetype": "text/plain", "start_char_idx": 9053, "end_char_idx": 9306, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae06c57c-8fea-4b27-99d3-15ceb0a254d4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. ", "original_text": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "901fc253-f90d-416c-8e20-693b0ba35cad", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 2.1.  Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. ", "original_text": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d. "}, "hash": "d73b5803a14035cecf97c96b48c6309d4575d53d67928ee4edc1453c82dd526d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1d382eb-a596-4101-8c16-cbfe56725866", "node_type": "1", "metadata": {"window": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). ", "original_text": "A node (j, k) is said to be terminal if it has no children.\n\n"}, "hash": "fa67d253e83a615a48136955abbb2cd614354eb10f62a6e43a4907333496a0b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. ", "mimetype": "text/plain", "start_char_idx": 9306, "end_char_idx": 9460, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d1d382eb-a596-4101-8c16-cbfe56725866", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). ", "original_text": "A node (j, k) is said to be terminal if it has no children.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae06c57c-8fea-4b27-99d3-15ceb0a254d4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation Forest\n\nAs a first go, we describe the Isolation Forest algorithm for AD in the multivariate context in a formalized manner for clarity's sake, as well as the Extended Isolation Forest version, see (Liu et al., 2008, 2012) and (Hariri et al., 2018) respectively.  These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. ", "original_text": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}. "}, "hash": "96992e019731a90107a371b2ec2b75b146205f99a184b22025d659b79a01ae30", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90160d1c-2245-4d55-b334-c0813263a209", "node_type": "1", "metadata": {"window": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. ", "original_text": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. "}, "hash": "c25c98df5eaa14236a0a281f256ed7015326a03f0a140766ed550a95b6b0f963", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A node (j, k) is said to be terminal if it has no children.\n\n", "mimetype": "text/plain", "start_char_idx": 9460, "end_char_idx": 9521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90160d1c-2245-4d55-b334-c0813263a209", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. ", "original_text": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1d382eb-a596-4101-8c16-cbfe56725866", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These two unsupervised algorithms can be viewed as Ensemble Learning methods insofar as they build a collection of binary trees and an anomaly scoring function based on the aggregation of the latter.  Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). ", "original_text": "A node (j, k) is said to be terminal if it has no children.\n\n"}, "hash": "185f7a0efb52922192c77c6c57b233be67976d244297cb5c6a084d319d66590d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eddd5c57-6e94-4fc6-bb79-46167104680d", "node_type": "1", "metadata": {"window": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. ", "original_text": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. "}, "hash": "068dda4210f1f837ef6ca215177fdea8173d538fa0bbafd271255a49aeebe1e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. ", "mimetype": "text/plain", "start_char_idx": 9521, "end_char_idx": 9668, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eddd5c57-6e94-4fc6-bb79-46167104680d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. ", "original_text": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90160d1c-2245-4d55-b334-c0813263a209", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Let S_n = {x_1, ..., x_n} be a training sample composed of n independent realizations of a generic random variable, X, that takes its value in a finite dimensional Euclidian space, R^d say, X = (X^(1), ..., X^(d)).\n\n An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. ", "original_text": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure. "}, "hash": "b41ea4751774e57e16ee4119c974611df4a67552fc43e2914219dce338da20df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16260ed0-51da-47f3-8cf6-44bd23387a93", "node_type": "1", "metadata": {"window": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. ", "original_text": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. "}, "hash": "c72ef97e3df97fe706bad85ff37fff0471f30a53e99848589387d0d346c869b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. ", "mimetype": "text/plain", "start_char_idx": 9668, "end_char_idx": 9766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16260ed0-51da-47f3-8cf6-44bd23387a93", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. ", "original_text": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eddd5c57-6e94-4fc6-bb79-46167104680d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An isolation tree (itree in abbreviated form) T of depth J \u2265 1 is a proper binary tree that represents a nested sequence of partitions of the feature space R^d.  The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. ", "original_text": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}. "}, "hash": "f9ec569d4bc7b944e02b0e41c73e3865e6ac55e9bbc0792de511016e24b4bb1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "910e80e8-eba7-4558-a33e-bfa96ff7b809", "node_type": "1", "metadata": {"window": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. ", "original_text": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. "}, "hash": "068b9373f3e0ba03ec6058654924cf173f3c003b6f5c04889f9cd559bd79afbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. ", "mimetype": "text/plain", "start_char_idx": 9766, "end_char_idx": 10142, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "910e80e8-eba7-4558-a33e-bfa96ff7b809", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. ", "original_text": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16260ed0-51da-47f3-8cf6-44bd23387a93", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The root node corresponds to the whole space C_{0,0} = R^d, while any node of the tree, indexed by the pair (j, k) where j denotes the depth of the node with 0 \u2264 j \u2264 J and k, the node index with 0 \u2264 k \u2264 2^j \u2212 1, is associated to a subset C_{j,k} \u2282 R^d.  A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. ", "original_text": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis. "}, "hash": "e87466750d5db265b47d20464d6b8d9f691e59ea6a19428ea827602dcb68c48f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4caf5460-ebe1-4262-afd5-5b8dd751170e", "node_type": "1", "metadata": {"window": "A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n", "original_text": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). "}, "hash": "58fabb9e4811c7cb6043f591eb5aad3faa80d7077163a3fe53739c8315fac991", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. ", "mimetype": "text/plain", "start_char_idx": 10142, "end_char_idx": 10405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4caf5460-ebe1-4262-afd5-5b8dd751170e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n", "original_text": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "910e80e8-eba7-4558-a33e-bfa96ff7b809", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A non terminal node (j, k) has two children, corresponding to disjoint subsets C_{j+1,2k} and C_{j+1,2k+1} such that C_{j,k} = C_{j+1,2k} \u222a C_{j+1,2k+1}.  A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. ", "original_text": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}. "}, "hash": "30c2c9efc3e00d66669b10872b87332c76c8d5a552101369582b20d0d93db63e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41ce238a-a3d6-41aa-8899-cc50d746151f", "node_type": "1", "metadata": {"window": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction.", "original_text": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. "}, "hash": "63a5058ed88e7da2b856071fdf36fca3c40da731c4e8b49d21c628716c2dfbc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). ", "mimetype": "text/plain", "start_char_idx": 10405, "end_char_idx": 10554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41ce238a-a3d6-41aa-8899-cc50d746151f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction.", "original_text": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4caf5460-ebe1-4262-afd5-5b8dd751170e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A node (j, k) is said to be terminal if it has no children.\n\n Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n", "original_text": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained). "}, "hash": "385322e958461456ea50207d81633e539f324557f6f975eec1154bd9b52ff8e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7e55877-6298-46c5-9af4-db3135154586", "node_type": "1", "metadata": {"window": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n", "original_text": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. "}, "hash": "f1e9500fa71275866229b4e49149b9c52285e5184df831a82a821abaaffbda94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. ", "mimetype": "text/plain", "start_char_idx": 10554, "end_char_idx": 10694, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7e55877-6298-46c5-9af4-db3135154586", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n", "original_text": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41ce238a-a3d6-41aa-8899-cc50d746151f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each itree is obtained by recursively filtering a subsample of training data of size \u03c8 in a top-down fashion, by means of the following procedure.  The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction.", "original_text": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large. "}, "hash": "e120d01c1ebd5a5bc140f7f99c04cfb2219d354f0ac17cb1c8dd4f6ea29db07e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bfa0d3b-11f7-452b-8e08-1985a2d55177", "node_type": "1", "metadata": {"window": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. ", "original_text": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. "}, "hash": "b8af768cbd6742b630ad322ee334fab8ef79c425fa8795c48d23104e3ce76f55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. ", "mimetype": "text/plain", "start_char_idx": 10694, "end_char_idx": 10806, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9bfa0d3b-11f7-452b-8e08-1985a2d55177", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. ", "original_text": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7e55877-6298-46c5-9af4-db3135154586", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The dataset composed of the training observations present at a node (j, k) is denoted by S_{j,k}.  At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n", "original_text": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes. "}, "hash": "f7ae31821fdeb5d77249be313bcbf3e193ab6906e56a69136436e7d24cd66b6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3a8e64e-0131-4ba0-ace8-3851e436e412", "node_type": "1", "metadata": {"window": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n", "original_text": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. "}, "hash": "d63ac89c571481bd70514ebc792f771a85dcda134352a1bc8077f9c22c4fc93a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. ", "mimetype": "text/plain", "start_char_idx": 10806, "end_char_idx": 10998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3a8e64e-0131-4ba0-ace8-3851e436e412", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n", "original_text": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bfa0d3b-11f7-452b-8e08-1985a2d55177", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "At iteration k + 2^j of the itree growing stage, a direction m in {1, ..., d}, or equivalently a split variable X^(m), is selected uniformly at random (and independently from the previous draws) as well as a split value \u03ba in the interval [min_{x\u2208S_{j,k}} x^(m), max_{x\u2208S_{j,k}} x^(m)] corresponding to the range of the projections of the points in S_{j,k} onto the m-th axis.  The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. ", "original_text": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e. "}, "hash": "bf811ce53d98c882df3a62627f6cb6d6721a758c81e6f60b7bdd2f76c587eecb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46615509-37b1-4473-b51d-ae0199f65e60", "node_type": "1", "metadata": {"window": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest.", "original_text": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n"}, "hash": "9b6985356361d0bdbe2075b275f03fc29563878981fe920fb99810aaee65b183", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. ", "mimetype": "text/plain", "start_char_idx": 10998, "end_char_idx": 11114, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46615509-37b1-4473-b51d-ae0199f65e60", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest.", "original_text": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3a8e64e-0131-4ba0-ace8-3851e436e412", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The children subsets are then defined by C_{j+1,2k} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) \u2264 \u03ba} and C_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 R^d : x^(m) > \u03ba}, the children training datasets being defined as S_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.  An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n", "original_text": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i. "}, "hash": "62338e1f27ee77feffb6059be8f31324c8020d056b73334a7964bbbd5b373858", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d907ebc-98dd-4251-8f68-4f9810145a6b", "node_type": "1", "metadata": {"window": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. ", "original_text": "**Anomaly Score prediction."}, "hash": "02f6c4a52f8d2b2f7071193a28965edf3eda808177921dc4892a542de43ba8f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n", "mimetype": "text/plain", "start_char_idx": 11114, "end_char_idx": 11196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d907ebc-98dd-4251-8f68-4f9810145a6b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. ", "original_text": "**Anomaly Score prediction."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46615509-37b1-4473-b51d-ae0199f65e60", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An itree T is thus built by iterating this procedure until all training data points are isolated (or the depth limit J set by the user is attained).  A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest.", "original_text": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n"}, "hash": "8b4f5009595e9cfb22fb1453361bb9ad1e17811ef4a6e7c6cc3b0af0b7ff2e1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0f123c0-a361-4e8a-9f7f-f9c728c83c47", "node_type": "1", "metadata": {"window": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. ", "original_text": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n"}, "hash": "80119ea853f962e438b2b527c6c6af179f59c2c1b1fcef0ec32cdec2cf051418", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Anomaly Score prediction.", "mimetype": "text/plain", "start_char_idx": 11196, "end_char_idx": 11223, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0f123c0-a361-4e8a-9f7f-f9c728c83c47", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. ", "original_text": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d907ebc-98dd-4251-8f68-4f9810145a6b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A preliminary subsampling stage can be performed in order to avoid swamping and masking effects, when the size of the dataset is too large.  When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. ", "original_text": "**Anomaly Score prediction."}, "hash": "ee821f6272d5fc45c69b41150d9f5dc2428aa4ff5a627c5c2f7783a60935ab0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5cea48a1-8697-4d0d-97a9-f0e08cda73e7", "node_type": "1", "metadata": {"window": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. ", "original_text": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. "}, "hash": "cf8e5ec97fcbea8320a30f8d24d4ef7e0ac2ff690c110c3199f86b9955f575af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n", "mimetype": "text/plain", "start_char_idx": 11223, "end_char_idx": 11452, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5cea48a1-8697-4d0d-97a9-f0e08cda73e7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. ", "original_text": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0f123c0-a361-4e8a-9f7f-f9c728c83c47", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When it isolates any training data point, the itree contains exactly \u03c8 - 1 internal nodes and \u03c8 terminal nodes.  An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. ", "original_text": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n"}, "hash": "842f69f6180b5c33fb74e0aa7424444f24d9ae9aa67d321f6afa5dcb9b0df66c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0342e6ef-788b-41f9-923e-5ad479bc60c9", "node_type": "1", "metadata": {"window": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. ", "original_text": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n"}, "hash": "486856671e1f2de01a79990b57e810fd148b0e39fed70fe1d0aeecb917cb6a09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. ", "mimetype": "text/plain", "start_char_idx": 11452, "end_char_idx": 11656, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0342e6ef-788b-41f9-923e-5ad479bc60c9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. ", "original_text": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5cea48a1-8697-4d0d-97a9-f0e08cda73e7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An itree constructed accordingly to a training subsample allows to assign to each training datapoint x_i a path length h_T(x_i), namely the depth at which it is isolated from the others, i.e.  the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. ", "original_text": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small. "}, "hash": "f63e370dc768119fd393925a504350ab0d8c0a60af090b08b9f278ad5980daa9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7adab95d-4b9e-4c91-a6a2-6e8dafd4124c", "node_type": "1", "metadata": {"window": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t. ", "original_text": "**Extended Isolation Forest."}, "hash": "5f30727bfc3dd82824507e1e970b289b30b1f27eecbf8668bbdc36af187cb10c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n", "mimetype": "text/plain", "start_char_idx": 11656, "end_char_idx": 12019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7adab95d-4b9e-4c91-a6a2-6e8dafd4124c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t. ", "original_text": "**Extended Isolation Forest."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0342e6ef-788b-41f9-923e-5ad479bc60c9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the number of edges x_i traverses from the root node to the terminal node that contains the sole training data x_i.  More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. ", "original_text": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n"}, "hash": "b4eab5dc71112a219c99b477f331d066999369a76a471d2c0de4a77e829c733c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca478ce9-3c2f-4dc1-9692-c15e9aa691ce", "node_type": "1", "metadata": {"window": "**Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n", "original_text": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. "}, "hash": "ec3c596d57cc3a2975df2fefa3e69570727cb3ddd4c7ac06fe9e33deface2d88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Extended Isolation Forest.", "mimetype": "text/plain", "start_char_idx": 12019, "end_char_idx": 12047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca478ce9-3c2f-4dc1-9692-c15e9aa691ce", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n", "original_text": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7adab95d-4b9e-4c91-a6a2-6e8dafd4124c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "More generally, it can be used to define an anomaly score for any point x \u2208 R^d.\n\n **Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t. ", "original_text": "**Extended Isolation Forest."}, "hash": "ea940e80920ffb6438ad26f08e6b0b99ef8a9641a7488c4fcbe25449ee804a44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30bdb4c0-dab3-4fdf-9c21-27835f8b669a", "node_type": "1", "metadata": {"window": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2. ", "original_text": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. "}, "hash": "e2c68509cc29f6709f997cccf6ae88f24ae4ba317d51c07738b4279382969e06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. ", "mimetype": "text/plain", "start_char_idx": 12047, "end_char_idx": 12187, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30bdb4c0-dab3-4fdf-9c21-27835f8b669a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2. ", "original_text": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca478ce9-3c2f-4dc1-9692-c15e9aa691ce", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Anomaly Score prediction. ** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n", "original_text": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e. "}, "hash": "d62b9568e1d7f233863f78f192db63aac0b9d84d9684f097fcd25d724a533b96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c277f56-b63f-4cbc-970f-91f231d86aa4", "node_type": "1", "metadata": {"window": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. ", "original_text": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. "}, "hash": "81d64827482a6cefcb9470fcd3df2adfa6502d933c3f84d807e3319f30973137", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. ", "mimetype": "text/plain", "start_char_idx": 12187, "end_char_idx": 12376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c277f56-b63f-4cbc-970f-91f231d86aa4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. ", "original_text": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30bdb4c0-dab3-4fdf-9c21-27835f8b669a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "** As the terminal nodes of the itree T form a partition of the feature space, one may then define the piecewise constant function h_T : R^d \u2192 N by: \u2200x \u2208 R^d,\nh_T(x) = j if and only if x \u2208 C_{j,k} and (j, k) is a terminal node.\n\n This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2. ", "original_text": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction. "}, "hash": "6976052322e5e19e3a57a69a8c50d367fd12950de7c7bf48c41088f2d3a90c11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc3d0c0a-8a9c-49b3-aef6-bfa291ade174", "node_type": "1", "metadata": {"window": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). ", "original_text": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. "}, "hash": "7ee8dad27d4934c2e41c5e75dd8ebc028445639a50b4cab0caeda76c9014eda5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. ", "mimetype": "text/plain", "start_char_idx": 12376, "end_char_idx": 12530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc3d0c0a-8a9c-49b3-aef6-bfa291ade174", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). ", "original_text": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c277f56-b63f-4cbc-970f-91f231d86aa4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This random path length is viewed as an indication for its degree of abnormality in a natural manner: ideally, the more abnormal the point x, the higher the probability that the quantity h_T(x) is small.  Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. ", "original_text": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d. "}, "hash": "822a083e00d523ac352361252b617e6704c8e3bc7d247bddd46fb803ad4cb100", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1687eaf-f10e-4dab-9f47-f1d8096d4f35", "node_type": "1", "metadata": {"window": "**Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. ", "original_text": "In the case where X's distribution has a density f(x) w.r.t. "}, "hash": "b6d2c252eeeed09852f0e9e8de9bcabcae15a9a1081ad23e6fce5891dfa9d308", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. ", "mimetype": "text/plain", "start_char_idx": 12530, "end_char_idx": 12717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e1687eaf-f10e-4dab-9f47-f1d8096d4f35", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. ", "original_text": "In the case where X's distribution has a density f(x) w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc3d0c0a-8a9c-49b3-aef6-bfa291ade174", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Hence, the algorithm above can be repeated N \u2265 1 times in order to produce a collection of itrees T_1, ..., T_N, referred to as an iforest, that defines the scoring function\n\ns_n(x) = 2^[- (1 / N c(\u03c8)) \u03a3_{i=1}^N h_{T_i}(x)], (1)\n\nwhere c(\u03c8) is the average path length of unsuccessful searches in a binary search tree, see (Liu et al., 2008) for further details.\n\n **Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). ", "original_text": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region. "}, "hash": "382828d5aa36743d0216818d615ded3c7d4ed66f19aba747a838bd82ac81efbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89ffd0bf-2e20-4d2c-a94d-b9817bc2f354", "node_type": "1", "metadata": {"window": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. ", "original_text": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n"}, "hash": "dda8eb60e2a8a7cff4b60b8c47260b2eef9b0714bd0b4964ed6f8fcc5ee788e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case where X's distribution has a density f(x) w.r.t. ", "mimetype": "text/plain", "start_char_idx": 12717, "end_char_idx": 12778, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "89ffd0bf-2e20-4d2c-a94d-b9817bc2f354", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. ", "original_text": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1687eaf-f10e-4dab-9f47-f1d8096d4f35", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Extended Isolation Forest. ** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. ", "original_text": "In the case where X's distribution has a density f(x) w.r.t. "}, "hash": "caec727904d77d3b7057157f347bd94b0741cdff67125eaab325cbaf61569197", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "714271d6-33bc-4bf0-a9ab-adee5126b67f", "node_type": "1", "metadata": {"window": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n", "original_text": "### 2.2. "}, "hash": "86ed752f494d8d4d8ea32aa92ae35799912c66bddbb0491bf90635fb586e5c7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n", "mimetype": "text/plain", "start_char_idx": 12778, "end_char_idx": 13276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "714271d6-33bc-4bf0-a9ab-adee5126b67f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n", "original_text": "### 2.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89ffd0bf-2e20-4d2c-a94d-b9817bc2f354", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "** Observing that the geometry of the abnormal regions of the feature space is not necessarily well-described by perpendicular splits (i.e.  by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. ", "original_text": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n"}, "hash": "16842893121b44d801256b5ae8ef77e1837a82e9e13827373098271c9d9fdcd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d9c3aa8-7c43-4f1c-a895-db987ce75af7", "node_type": "1", "metadata": {"window": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. ", "original_text": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. "}, "hash": "060274274b36dde19615fe9c29c98adce80cd6e7f77a78ae061f9f4b7a70cf1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 2.2. ", "mimetype": "text/plain", "start_char_idx": 13276, "end_char_idx": 13285, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d9c3aa8-7c43-4f1c-a895-db987ce75af7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. ", "original_text": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "714271d6-33bc-4bf0-a9ab-adee5126b67f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "by unions of hypercubes of the cartesian product R^d), a more flexible variant of the procedure recalled above has been proposed in (Hariri et al., 2018), in the purpose of bias reduction.  Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n", "original_text": "### 2.2. "}, "hash": "5b02f2c3274c8e211587362d2caedd14c444ddd71580dac5779f60952dd1724f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb2bb2d2-4c38-4b5d-a606-74b7edb15a5c", "node_type": "1", "metadata": {"window": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. ", "original_text": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). "}, "hash": "1d5093cf06e0b93ba0e52d01a1e2b35f20c077ea55232143edfaa8b1f616aef4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. ", "mimetype": "text/plain", "start_char_idx": 13285, "end_char_idx": 13374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb2bb2d2-4c38-4b5d-a606-74b7edb15a5c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. ", "original_text": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d9c3aa8-7c43-4f1c-a895-db987ce75af7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Rather than selecting a direction in {1, ..., d}, one may choose a direction u \u2208 S_{d\u22121}, denoting by S_{d-1} the unit sphere of the euclidian space R^d.  A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. ", "original_text": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v. "}, "hash": "215868af04bd1128f26e373e9081859a73c487ebb9a9a8bec5697635755bb977", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f138a0-dce0-47a7-b6e0-b39c003e574c", "node_type": "1", "metadata": {"window": "In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. ", "original_text": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. "}, "hash": "aadebd1fcd9e4272a1c7356a9601783e29732b52eb3ba5e0934429dbdc70c7b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). ", "mimetype": "text/plain", "start_char_idx": 13374, "end_char_idx": 13458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84f138a0-dce0-47a7-b6e0-b39c003e574c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. ", "original_text": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb2bb2d2-4c38-4b5d-a606-74b7edb15a5c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A node is then cut by choosing randomly and uniformly a threshold value in the range of the projections onto this direction of the training data points lying in the corresponding region.  In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. ", "original_text": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006). "}, "hash": "3f566d1a38e351fa1c93c3d21fd29d2886341db7f8ec425a17050dfbc18752e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6508e6ce-1a2d-4a4c-bdad-ac78681554ff", "node_type": "1", "metadata": {"window": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. ", "original_text": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. "}, "hash": "1fbc9674fe2db747ecf10f0784f21301cb30ba62983515c629364cdf1d9b7811", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. ", "mimetype": "text/plain", "start_char_idx": 13458, "end_char_idx": 13530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6508e6ce-1a2d-4a4c-bdad-ac78681554ff", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. ", "original_text": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84f138a0-dce0-47a7-b6e0-b39c003e574c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the case where X's distribution has a density f(x) w.r.t.  a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. ", "original_text": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v. "}, "hash": "465319da40553285976fbbac1b7e35deb8ff07f1ba258dc00317c7e2b2909293", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "827a82ae-7ea6-48a4-9122-7de155f30755", "node_type": "1", "metadata": {"window": "### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. ", "original_text": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n"}, "hash": "57ed197ec5179b9ff3019a3bd27f657a804df6eac9a8085847f74377d0891dfa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. ", "mimetype": "text/plain", "start_char_idx": 13530, "end_char_idx": 13621, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "827a82ae-7ea6-48a4-9122-7de155f30755", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. ", "original_text": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6508e6ce-1a2d-4a4c-bdad-ac78681554ff", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "a \u03c3-finite measure \u03bb of reference, the goal of anomaly detection can be formulated as the recovery of sublevel sets {x \u2208 R^d : f(x) \u2264 q}, q \u2265 0, (under mild assumptions, they are minimum volume sets or quantile regions, see (Polonik, 1997; Scott and Nowak, 2006), when measuring the volume by \u03bb), which may be not accurately approximated by unions of hyperrectangles (in the Gaussian situation for instance, such regions are the complementary sets of ellipso\u00efds, \u03bb being Lebesgue measure on R^d).\n\n ### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. ", "original_text": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t. "}, "hash": "e579b5a9d60ee2a4f843005d9e491d6cc37163f938113a137c9a0c10ab23340d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cf09a69-500d-4c3b-9f5f-818b4ce93fe7", "node_type": "1", "metadata": {"window": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). ", "original_text": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. "}, "hash": "c7f9e97b4af93b9c5f6fa77ed6e7ffe7f98b29c8e416463e1e822f850c5d3aa2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n", "mimetype": "text/plain", "start_char_idx": 13621, "end_char_idx": 13704, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4cf09a69-500d-4c3b-9f5f-818b4ce93fe7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). ", "original_text": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "827a82ae-7ea6-48a4-9122-7de155f30755", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 2.2.  Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. ", "original_text": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n"}, "hash": "fb741cd003ba96749e9432f3ff2fc7070d4c9098c52a6c49e832263d4057bc9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48c0b8fa-dae6-4f55-9fc0-f423e2c83596", "node_type": "1", "metadata": {"window": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n", "original_text": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. "}, "hash": "a1508e4f3ef5221860c15c28c859bfbcc4f82cf5f59c7e4745a0bb25966773ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. ", "mimetype": "text/plain", "start_char_idx": 13704, "end_char_idx": 13813, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48c0b8fa-dae6-4f55-9fc0-f423e2c83596", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n", "original_text": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cf09a69-500d-4c3b-9f5f-818b4ce93fe7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Functional Data Analysis and Anomaly Detection\n\nA functional random variable X is a r.v.  that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). ", "original_text": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper. "}, "hash": "e19be6eb5bfe623ff3671254284580cd06879212a0e126a9c25aede97fa8757f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6784798b-2390-4fe0-beab-ce014b2f4e6c", "node_type": "1", "metadata": {"window": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. ", "original_text": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. "}, "hash": "dde4a4a490545f044820b9033e6e16267b57a084d368d02c911a0cf0e8c7ff75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. ", "mimetype": "text/plain", "start_char_idx": 13813, "end_char_idx": 13957, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6784798b-2390-4fe0-beab-ce014b2f4e6c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. ", "original_text": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48c0b8fa-dae6-4f55-9fc0-f423e2c83596", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "that takes its values in a space of functions, see, e.g., (Ferraty and Vieu, 2006).  To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n", "original_text": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed. "}, "hash": "9b232f69c59a21ab8ab4bddf1e1654644b3ffd4ce74d7031f1be3869812e0761", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70b2effb-829f-433a-a74b-8d167fe4adb8", "node_type": "1", "metadata": {"window": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. ", "original_text": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. "}, "hash": "d343d7f60f2a411ce177ee47698d80657a633cc1f56987546b3b15e5426e803c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. ", "mimetype": "text/plain", "start_char_idx": 13957, "end_char_idx": 14221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70b2effb-829f-433a-a74b-8d167fe4adb8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. ", "original_text": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6784798b-2390-4fe0-beab-ce014b2f4e6c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To be more specific, let I \u2282 R_+ be a time interval and consider a r.v.  taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. ", "original_text": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced. "}, "hash": "fe15b3fe7f0c7127df85eb9b9fc317ec5b65e5282f311de7d9a703c9d60e690a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db3c33de-63cc-42db-ba39-a02a3715bd49", "node_type": "1", "metadata": {"window": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al. ", "original_text": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. "}, "hash": "34f2551ed5d9a5a60a7221308b6106d113073b081e2caaea1b83f9ce8226c446", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. ", "mimetype": "text/plain", "start_char_idx": 14221, "end_char_idx": 14443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "db3c33de-63cc-42db-ba39-a02a3715bd49", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al. ", "original_text": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70b2effb-829f-433a-a74b-8d167fe4adb8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "taking its values in the Hilbert space L_2(I) of real valued and square integrable (w.r.t.  Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. ", "original_text": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy. "}, "hash": "159c38c4afb9bd3e2dca6b578dcafaadbcadb362644f7713f137c3481f919400", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a92647db-d7e1-4fb1-a8e9-b9298a9705df", "node_type": "1", "metadata": {"window": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. ", "original_text": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). "}, "hash": "bc684bf5a488abdfeeb0df34bd3615034b75f5d7703fa0815db671560af0e716", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. ", "mimetype": "text/plain", "start_char_idx": 14443, "end_char_idx": 14788, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a92647db-d7e1-4fb1-a8e9-b9298a9705df", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. ", "original_text": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db3c33de-63cc-42db-ba39-a02a3715bd49", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Lebesgue measure) functions x : I \u2192 R:\n\nX : \u03a9 \u2192 L_2(I)\n\u03c9 \u2192 X(\u03c9) = (X_t(\u03c9))_{t\u2208I}.\n\n Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al. ", "original_text": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality. "}, "hash": "258a176dfdaf47e820b8e1147496928ba0c61bf897fa1b867295fdbc37771622", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53225838-94de-4c44-9fe5-e9f3318d1f4f", "node_type": "1", "metadata": {"window": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. ", "original_text": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n"}, "hash": "607dfedb3bb7f1dc9a16a1addf14c8ddf8fc96e283db1dc8a5338d4cbba64729", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). ", "mimetype": "text/plain", "start_char_idx": 14788, "end_char_idx": 14900, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "53225838-94de-4c44-9fe5-e9f3318d1f4f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. ", "original_text": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a92647db-d7e1-4fb1-a8e9-b9298a9705df", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Without any loss of generality, we restrict ourselves with functions defined on [0, 1] throughout the paper.  In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. ", "original_text": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short). "}, "hash": "51355437a701661f66bbddaa815c7b20311de13ca49ede907c33e90490b55a5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70bb395a-a476-4d13-9d09-4b9b2edaec4a", "node_type": "1", "metadata": {"window": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. ", "original_text": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. "}, "hash": "db17b40e331ee4b55bb419eb0c018903c571be681eac5c67ba4fa1522e462975", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n", "mimetype": "text/plain", "start_char_idx": 14900, "end_char_idx": 15053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70bb395a-a476-4d13-9d09-4b9b2edaec4a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. ", "original_text": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53225838-94de-4c44-9fe5-e9f3318d1f4f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In practice, only a finite dimensional marginal (X_{t_1}, ..., X_{t_p}), t_1 < ... < t_p, p \u2265 1 and (t_1, ..., t_p) \u2208 [0, 1]^p can be observed.  However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. ", "original_text": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n"}, "hash": "3db43691c59ed02e631aec7383e49dbd2469a7c60e49a8c76e32239e4e993828", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "403e99b5-a696-4b02-b8a8-cb1e571b1003", "node_type": "1", "metadata": {"window": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point). ", "original_text": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. "}, "hash": "f34f0d963eba69a7193c09452cf1c88d1f2e37dd65913fb013bf721d62a5ed33", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. ", "mimetype": "text/plain", "start_char_idx": 15053, "end_char_idx": 15212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "403e99b5-a696-4b02-b8a8-cb1e571b1003", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point). ", "original_text": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70bb395a-a476-4d13-9d09-4b9b2edaec4a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "However, considering (X_{t_1}, ..., X_{t_p}) as a discretized curve rather than a simple random vector of dimension p permits to take into account the dependence structure between the measurements over time, especially when the time points t_i are not equispaced.  To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. ", "original_text": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available. "}, "hash": "978cb6422bcc08cdb94faebb3dcc639b941c8a64b8273e29ddde06c41a3cfafe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "783c1a73-b3a2-49b2-b6f9-461e49165ddd", "node_type": "1", "metadata": {"window": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. ", "original_text": "Following in the footsteps of Hubert et al. "}, "hash": "f74efb6a6c32d73dc51b91ac18a4c681cc7666103e3630c5bf811286c2b00673", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. ", "mimetype": "text/plain", "start_char_idx": 15212, "end_char_idx": 15404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "783c1a73-b3a2-49b2-b6f9-461e49165ddd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. ", "original_text": "Following in the footsteps of Hubert et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "403e99b5-a696-4b02-b8a8-cb1e571b1003", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To come back to a function from discrete values, interpolation procedures or approximation schemes based on appropriate dictionaries can be used, combined with a preliminary smoothing step when the observations are noisy.  From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point). ", "original_text": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves. "}, "hash": "7166a32dba467eee1f83b814f40f15781a1d31a41d9961e4473d01cce7d04206", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7e5bdbc-b724-402f-9aae-616f289cf8bb", "node_type": "1", "metadata": {"window": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n", "original_text": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. "}, "hash": "a16fcae7421d67d69eb2a2f27a990c144cb266a032df000f2c3be97a77b00ace", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Following in the footsteps of Hubert et al. ", "mimetype": "text/plain", "start_char_idx": 15404, "end_char_idx": 15448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e7e5bdbc-b724-402f-9aae-616f289cf8bb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n", "original_text": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "783c1a73-b3a2-49b2-b6f9-461e49165ddd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "From a statistical perspective, the analysis is based on a functional dataset S_n = {x_1, ..., x_n} composed of n \u2265 1 independent realizations of finite-dimensional marginals of the stochastic process X, that may be very heterogeneous in the sense that these marginals may correspond to different time points and be of different dimensionality.  One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. ", "original_text": "Following in the footsteps of Hubert et al. "}, "hash": "dd9919c970e3c870a95b8e46c01c827daf7ec99857c3faf6bc946703d1672324", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe453607-8194-4355-a73c-e4ca9d296c68", "node_type": "1", "metadata": {"window": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. ", "original_text": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. "}, "hash": "4f4fadce81199071fb3641ad040b42be570c1d4f4f63719561de384318f98c24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. ", "mimetype": "text/plain", "start_char_idx": 15448, "end_char_idx": 15654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fe453607-8194-4355-a73c-e4ca9d296c68", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. ", "original_text": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7e5bdbc-b724-402f-9aae-616f289cf8bb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One may refer to Ramsay and Silverman (2005)'s book for a deep view on Functional Data Analysis (FDA in short).  For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n", "original_text": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies. "}, "hash": "7c36847656d757e1c8232b9f691ca8b52f6bd13eac0839b64ae0509cd4ec2e4a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9376626-6ec6-4093-8200-68060b8c2947", "node_type": "1", "metadata": {"window": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. ", "original_text": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. "}, "hash": "46755e404b6001e0f812da74bba8cb4ec484fa8e67f0161ea29aaaad09207eca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. ", "mimetype": "text/plain", "start_char_idx": 15654, "end_char_idx": 15800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9376626-6ec6-4093-8200-68060b8c2947", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. ", "original_text": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe453607-8194-4355-a73c-e4ca9d296c68", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For simplicity, the functional data considered throughout the paper correspond to the observations of independent realizations of X at the same points.\n\n In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. ", "original_text": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations. "}, "hash": "8b137a04a49bcb5f8393cbc7b112cd2fa1bf5b171d64b57e357eb7a464b16f1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "901f21f2-ded5-48a3-8d15-0cccd978479f", "node_type": "1", "metadata": {"window": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. ", "original_text": "change point). "}, "hash": "bddf62f88992908fa9d58c39acd9410ec6de4601aacacda7c8d9405355e5f4ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. ", "mimetype": "text/plain", "start_char_idx": 15800, "end_char_idx": 15981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "901f21f2-ded5-48a3-8d15-0cccd978479f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. ", "original_text": "change point). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9376626-6ec6-4093-8200-68060b8c2947", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In this particular context, *functional anomaly detection* aims at detecting the curves that significantly differ from the others among the dataset available.  Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. ", "original_text": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e. "}, "hash": "7f0736525107c30c6beb3211a723cd4c0bffc18d8408ae9e299db8c60f7c07a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fc8acaf-a988-46be-b2c3-cb05ec287e93", "node_type": "1", "metadata": {"window": "Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. ", "original_text": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. "}, "hash": "8e3fdffe9e23f3f1aca3186c2bfa9c26ae75b0991bfd4213ded4b9c46b1e5c2e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "change point). ", "mimetype": "text/plain", "start_char_idx": 15981, "end_char_idx": 15996, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fc8acaf-a988-46be-b2c3-cb05ec287e93", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. ", "original_text": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "901f21f2-ded5-48a3-8d15-0cccd978479f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Given the richness of spaces of functions, the major difficulty lies in the huge diversity in the nature of the observed differences, which may not only depend on the locations of the curves.  Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. ", "original_text": "change point). "}, "hash": "43fd30601d5f16ff93861e5fde4d41f4e3c34b6a55ee19fda21b55e9ab37070f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fdf6700-81cb-4746-8f15-d0f5087abdc2", "node_type": "1", "metadata": {"window": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks. ", "original_text": "change-points).\n\n"}, "hash": "a1393017f2f34378b808c232f9d8e71530d0992446c44b556e37768f7e1cbf6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. ", "mimetype": "text/plain", "start_char_idx": 15996, "end_char_idx": 16269, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fdf6700-81cb-4746-8f15-d0f5087abdc2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks. ", "original_text": "change-points).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fc8acaf-a988-46be-b2c3-cb05ec287e93", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Following in the footsteps of Hubert et al.  (2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. ", "original_text": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g. "}, "hash": "3067fd17ad65392229fadc7fc23de2f3cba6f22d0bde0948f00d327e0c2afd5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb6e440e-8293-4d0c-84f8-710c0ae540d6", "node_type": "1", "metadata": {"window": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. ", "original_text": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. "}, "hash": "c236adbc7a2c9d56ca96a7289d0ec90d5b88bd60d6622a647f2de176114f00b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "change-points).\n\n", "mimetype": "text/plain", "start_char_idx": 16269, "end_char_idx": 16286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb6e440e-8293-4d0c-84f8-710c0ae540d6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. ", "original_text": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fdf6700-81cb-4746-8f15-d0f5087abdc2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015), one may distinguish between three types of anomalies: *shift* (the observed curve has the same shape as the majority of the sample except that it is shifted away), *amplitude* or *shape* anomalies.  All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks. ", "original_text": "change-points).\n\n"}, "hash": "1df920a10e92540899dae5a2cab5865e31977dbd863e44aeb254ecdb0bdbec10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42731e9-b92e-4372-be34-150e63920440", "node_type": "1", "metadata": {"window": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. ", "original_text": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. "}, "hash": "e1a1ef85e741e7e801a7ab9e7a23cce407f4ca22fae7bd1a0dbd9996d3c28765", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. ", "mimetype": "text/plain", "start_char_idx": 16286, "end_char_idx": 16640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e42731e9-b92e-4372-be34-150e63920440", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. ", "original_text": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb6e440e-8293-4d0c-84f8-710c0ae540d6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "All these three types of anomalies can be isolated/transient or persistent, depending on their duration with respect to that of the observations.  One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. ", "original_text": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose. "}, "hash": "c7fc5a35242308008dedd112eb93875c8cf25167c0cf05d36b3a31f0ba249918", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0d3d18e-3590-40cc-8a19-0285976e85e7", "node_type": "1", "metadata": {"window": "change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. ", "original_text": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. "}, "hash": "072da1fbb8b23048be50fa4a56634bad37bdca6b87917e5f11cbc2010ae79961", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. ", "mimetype": "text/plain", "start_char_idx": 16640, "end_char_idx": 16839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0d3d18e-3590-40cc-8a19-0285976e85e7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. ", "original_text": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42731e9-b92e-4372-be34-150e63920440", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One may easily admit that certain types of anomalies are harder to detect than others: for instance, an isolated anomaly in shape compared to an isolated anomaly in amplitude (i.e.  change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. ", "original_text": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies. "}, "hash": "f2001ff39f4834feb95d936cc8c82fad58763ea81e894ade92ab5945c1facd4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92727ff8-62e8-40e3-810f-a53cadc1fee2", "node_type": "1", "metadata": {"window": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n", "original_text": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. "}, "hash": "4cca3f73d586e2f9d833ba5a56a1fd36e46d97cf465920bc9b3757b32dbf2c55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. ", "mimetype": "text/plain", "start_char_idx": 16839, "end_char_idx": 17150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "92727ff8-62e8-40e3-810f-a53cadc1fee2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n", "original_text": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0d3d18e-3590-40cc-8a19-0285976e85e7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "change point).  Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. ", "original_text": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g. "}, "hash": "b8f926757aa31d1dd95a37d401362e5c23b79221dd694d468386a01e49fee5dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de1bebbd-c468-4bfd-ba29-504a40c49c1a", "node_type": "1", "metadata": {"window": "change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different. ", "original_text": "Such methods have obvious drawbacks. "}, "hash": "90725d9bd4bc8633d6f8a1e8ef078ca55b8c034625e77bd95f5de7b589ba5ba1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. ", "mimetype": "text/plain", "start_char_idx": 17150, "end_char_idx": 17275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de1bebbd-c468-4bfd-ba29-504a40c49c1a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different. ", "original_text": "Such methods have obvious drawbacks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92727ff8-62e8-40e3-810f-a53cadc1fee2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Although FDA has been the subject of much attention in recent years, very few generic and flexible methods tailored to functional anomaly detection are documented in the machine-learning literature to the best of our knowledge, except for specific types of anomalies (e.g.  change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n", "original_text": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation. "}, "hash": "7e2cd798caed4fdaafdc24fa9f3a29d26a82e0694107896f38a931526c366443", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a59bd9b7-60f8-480b-8c73-b2d2fb2b19f6", "node_type": "1", "metadata": {"window": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. ", "original_text": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. "}, "hash": "f47cc292c688593b4097d4efb3175bd87fed1baec297c1d040a385f68b290f06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such methods have obvious drawbacks. ", "mimetype": "text/plain", "start_char_idx": 17275, "end_char_idx": 17312, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a59bd9b7-60f8-480b-8c73-b2d2fb2b19f6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. ", "original_text": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de1bebbd-c468-4bfd-ba29-504a40c49c1a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "change-points).\n\n In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different. ", "original_text": "Such methods have obvious drawbacks. "}, "hash": "5105e6b61fac3844b0ec50fbd65ac569cebf62f34059bb743cca8291d0166af1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c1e747c-7cbf-4ae4-886a-730f893b7059", "node_type": "1", "metadata": {"window": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n", "original_text": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. "}, "hash": "3b7d33008d0bdfb63d32208b3d2227d2d1d416ecf2115dfbe2e71eede5af2cf6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. ", "mimetype": "text/plain", "start_char_idx": 17312, "end_char_idx": 17662, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c1e747c-7cbf-4ae4-886a-730f893b7059", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n", "original_text": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a59bd9b7-60f8-480b-8c73-b2d2fb2b19f6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In Statistics, although its applications are by no means restricted to AD, the concept of *functional depth* that allows to define a notion of centrality in the path space and a center-outward ordering of the curves of the functional dataset, see, e.g., (Cuevas et al., 2007; Claeskens et al., 2014; Hubert et al., 2015), has been used for this purpose.  However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. ", "original_text": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others. "}, "hash": "bf131c00b2297d0bb116d6ac08b98d8e878098638c8a3e31eac8e667fd064c3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c611cdb7-8f0d-4ed9-8b84-b2dee509ef35", "node_type": "1", "metadata": {"window": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3. ", "original_text": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. "}, "hash": "2d4ba4c43e773fcf801769ea67691483a520df111be829d016b2c9a9edebc026", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. ", "mimetype": "text/plain", "start_char_idx": 17662, "end_char_idx": 18006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c611cdb7-8f0d-4ed9-8b84-b2dee509ef35", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3. ", "original_text": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c1e747c-7cbf-4ae4-886a-730f893b7059", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "However, since the vast majority of functional depth functions introduced only describe the relative location properties of the sample curves, they generally fail to detect other types of anomalies.  Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n", "original_text": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work. "}, "hash": "8d127557a31665c1faa217755360e4887938542d77e1ba1cc469acb36f538476", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35af81cb-4670-47f1-9aa2-461d563bc013", "node_type": "1", "metadata": {"window": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. ", "original_text": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n"}, "hash": "e4d0309d67cf344adf42d816adce99fd13e63690abb64d3c8b7d6de4e2d0eb9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. ", "mimetype": "text/plain", "start_char_idx": 18006, "end_char_idx": 18155, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "35af81cb-4670-47f1-9aa2-461d563bc013", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. ", "original_text": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c611cdb7-8f0d-4ed9-8b84-b2dee509ef35", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Another popular approach, usually referred to as *filtering*, consists in bringing the AD problem to the multivariate case by means of an adequate projection using Functional Principal Component Analysis (FPCA) (Ramsay and Silverman, 2005) or a preliminary selected basis of the function space considered (e.g.  Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3. ", "original_text": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g. "}, "hash": "ff0bcaf3438934adca032251461fdb547236300ec24f2cf94c82d478ea641d16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc02fe04-5399-44d3-a4eb-d69378365074", "node_type": "1", "metadata": {"window": "Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. ", "original_text": "The angle embraced in this paper is quite different. "}, "hash": "b40137f77d0d57697160052bb87c08b4050bcf4ff14dedc31b53368bae122021", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n", "mimetype": "text/plain", "start_char_idx": 18155, "end_char_idx": 18390, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc02fe04-5399-44d3-a4eb-d69378365074", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. ", "original_text": "The angle embraced in this paper is quite different. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35af81cb-4670-47f1-9aa2-461d563bc013", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Fourier, wavelets) and apply next an AD algorithm designed for the finite-dimensional setup to the resulting representation.  Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. ", "original_text": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n"}, "hash": "a756cfb449c855dbd9207a6e1e0433b64af46e046a72f38620bb1395ca2dd756", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9c73695-3eea-4912-95ea-45422677ea4e", "node_type": "1", "metadata": {"window": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n", "original_text": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. "}, "hash": "614df98320ea291c351979a333e38a780b30c2e508bdcb75cb6ea2d174f2f2c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The angle embraced in this paper is quite different. ", "mimetype": "text/plain", "start_char_idx": 18390, "end_char_idx": 18443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9c73695-3eea-4912-95ea-45422677ea4e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n", "original_text": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc02fe04-5399-44d3-a4eb-d69378365074", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Such methods have obvious drawbacks.  In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. ", "original_text": "The angle embraced in this paper is quite different. "}, "hash": "8316e7ed5fbeec2e13309dd101f1eec5a218703cc13e2ddbe4edddf8622b5873", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22a68be4-e4b3-4b80-87b3-f208e233531b", "node_type": "1", "metadata": {"window": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1. ", "original_text": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n"}, "hash": "32dcf5787e73defbeb32e3deb8ae2597f4a2687dc9e576ea553f5278dff21b1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. ", "mimetype": "text/plain", "start_char_idx": 18443, "end_char_idx": 18591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22a68be4-e4b3-4b80-87b3-f208e233531b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1. ", "original_text": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9c73695-3eea-4912-95ea-45422677ea4e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In FPCA, estimation of the Kahrunen-Lo\u00e8ve basis can be very challenging and lead to loose approximations, jeopardizing next the AD stage, while the *a priori* representation offered by the 'atoms' of a predefined basis or frame may unsuccessfully capture the patterns carrying the relevant information to distinguish abnormal curves from the others.  Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n", "original_text": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset. "}, "hash": "b6a5b46462bdb299d4afdd4c4342f9079a03b1636c45b538faab1066127cebe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c11a1aea-f921-4a6a-b07f-aa8a61ed5027", "node_type": "1", "metadata": {"window": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . ", "original_text": "## 3. "}, "hash": "1beffefafdfd9d72028a6a2d9c0997a9860946674a6686f9e036a63c9157f569", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n", "mimetype": "text/plain", "start_char_idx": 18591, "end_char_idx": 18779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c11a1aea-f921-4a6a-b07f-aa8a61ed5027", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . ", "original_text": "## 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22a68be4-e4b3-4b80-87b3-f208e233531b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Another approach is based on the notion of *Minimum Volume sets* (MV-sets in shortened version), originally introduced in (Einmahl and Mason, 1992) and that generalizes the concept of quantile for multivariate distributions and offers a nice nonparametric framework for anomaly detection in finite dimension, see Scott and Nowak (2006)'s work.  Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1. ", "original_text": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n"}, "hash": "6d11958e3f704fed564ad9afbaa295c921a8d7483181c9f3e63a06803ed583ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5870bb49-c387-4421-87fb-1d5ba3944ddd", "node_type": "1", "metadata": {"window": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  . ", "original_text": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. "}, "hash": "dd2cff61c53cd48242a4f5ba2afddc118133907f7b2b095903508280fa601c72", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 3. ", "mimetype": "text/plain", "start_char_idx": 18779, "end_char_idx": 18785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5870bb49-c387-4421-87fb-1d5ba3944ddd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  . ", "original_text": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c11a1aea-f921-4a6a-b07f-aa8a61ed5027", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Given the fact that no analogue of Lebesgue measure on an infinite-dimensional Banach space exists and since, considering a law \u03bb of reference (e.g.  the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . ", "original_text": "## 3. "}, "hash": "08d56390f80b77c0e85d38e3b12656a62c146ccfa0825cd72f86f2464d5d9e74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8c56a4b-076b-4f62-a57b-8dc332ada2b6", "node_type": "1", "metadata": {"window": "The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  . ", "original_text": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. "}, "hash": "b338b7c6ed28d19f43900f7c80ec246d99d15e58f7b73eeae5f79c7bacb8e0b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. ", "mimetype": "text/plain", "start_char_idx": 18785, "end_char_idx": 18967, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8c56a4b-076b-4f62-a57b-8dc332ada2b6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  . ", "original_text": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5870bb49-c387-4421-87fb-1d5ba3944ddd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the Wiener or a Poisson measure) on the function space H of interest, the volume \u03bb(C) of a measurable subset C \u2282 H can be hardly computed in general, it is far from straightforward to extend MV-set estimation to the functional setup.\n\n The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  . ", "original_text": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t. "}, "hash": "9f0b2e20c63b3f142b91ce90bb97083c72bbd39ac43a94b4bb69cde699411c6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53862aa3-6bcb-40c7-960e-28dd8d565da7", "node_type": "1", "metadata": {"window": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq. ", "original_text": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n"}, "hash": "7e89c3ea24195b9e0ba4ad48f62d15801e759cdadfca915ae68790d59e99f9ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. ", "mimetype": "text/plain", "start_char_idx": 18967, "end_char_idx": 19110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "53862aa3-6bcb-40c7-960e-28dd8d565da7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq. ", "original_text": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8c56a4b-076b-4f62-a57b-8dc332ada2b6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The angle embraced in this paper is quite different.  The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  . ", "original_text": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1]. "}, "hash": "f9261437f03423d1f8ed401f125b094907432c4e827e87670f876568b8404325", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed0f5e77-6b5b-45ae-adb6-a22e3caa5c5f", "node_type": "1", "metadata": {"window": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case. ", "original_text": "### 3.1. "}, "hash": "21cecf118c2b3190bb952024c559e8d1009c10d1c1446cbff9469ed3845fb256", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n", "mimetype": "text/plain", "start_char_idx": 19110, "end_char_idx": 19236, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed0f5e77-6b5b-45ae-adb6-a22e3caa5c5f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case. ", "original_text": "### 3.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53862aa3-6bcb-40c7-960e-28dd8d565da7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The direct approach we promote here is free from any preliminary representation stage and can be straightforwardly applied to a functional dataset.  Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq. ", "original_text": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n"}, "hash": "fe3664dbb60b36081b9496f72e9207d0acd838eaff82af8c48361ed4cb8a47d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0effd00c-bcc9-40dc-9784-ceed223d9fef", "node_type": "1", "metadata": {"window": "## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. ", "original_text": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . "}, "hash": "38ca4a23d0f1ce787eb05233282f0347e243ee375f18b18c0be361eaec06bbc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.1. ", "mimetype": "text/plain", "start_char_idx": 19236, "end_char_idx": 19245, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0effd00c-bcc9-40dc-9784-ceed223d9fef", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. ", "original_text": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed0f5e77-6b5b-45ae-adb6-a22e3caa5c5f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Precisely, in the subsequent section, we propose to extend the IF algorithm to the functional data framework, in a very flexible way, so as to deal with a wide variety of anomaly shapes.\n\n ## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case. ", "original_text": "### 3.1. "}, "hash": "6e730dd5b4465269cb53c9d88b390a043f3121625a94d8a6833ffbc7814c2593", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10b58559-fa76-4dce-bda7-ee08405a036d", "node_type": "1", "metadata": {"window": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. ", "original_text": ". "}, "hash": "e7153b4d0f2c1d2695f7f686641da0882489c7612c34e9f12f102a80bfe4d580", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . ", "mimetype": "text/plain", "start_char_idx": 19245, "end_char_idx": 19687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10b58559-fa76-4dce-bda7-ee08405a036d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0effd00c-bcc9-40dc-9784-ceed223d9fef", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 3.  Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. ", "original_text": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, . "}, "hash": "2d59bdeb1817a7e32e18708d7d51edd2e00732f7dadb3a78698b54722ca31573", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d5ceb5bc-e2e5-4954-928f-ea5ee040afcc", "node_type": "1", "metadata": {"window": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. ", "original_text": ". "}, "hash": "6b2a5312d29c3590edbd3e676b0797270212bdcb2f167e4e34bec46e85038dd6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 19500, "end_char_idx": 19502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d5ceb5bc-e2e5-4954-928f-ea5ee040afcc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10b58559-fa76-4dce-bda7-ee08405a036d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Functional Isolation Forest\n\nWe consider the problem of learning a score function s : H \u2192 R that reflects the degree of anomaly of elements in an infinite dimensional space H w.r.t.  P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. ", "original_text": ". "}, "hash": "40e104d7e2f84d713868f0c3915ed2121d213f972c2d3fb2ca00d94c5945a4f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6246f59-5737-4138-9bfb-8a94a6aa22ed", "node_type": "1", "metadata": {"window": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n", "original_text": ", N as defined in Eq. "}, "hash": "4a9474ad6c5d3a20c8534ca8ac4c1b2b3b53d65d529822ba7d89bf8ab5474f78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 19685, "end_char_idx": 19687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b6246f59-5737-4138-9bfb-8a94a6aa22ed", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n", "original_text": ", N as defined in Eq. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d5ceb5bc-e2e5-4954-928f-ea5ee040afcc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "P. By H, we denote a functional Hilbert space equipped with a scalar product (\u00b7,\u00b7)_H such that any x \u2208 H is a real function defined on [0, 1].  In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. ", "original_text": ". "}, "hash": "6dd1ce9f3304f5c6cf6d3c604dcfb892c31267bb13238b2bbcac77f022886edc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45159cc4-abed-41cf-90f1-dd62d14a7d05", "node_type": "1", "metadata": {"window": "### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. ", "original_text": "1 in the multivariate case. "}, "hash": "bbc3f51b2d4e345db7d8530a769980783cf26a103d9c9d0901e795bb677101d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ", N as defined in Eq. ", "mimetype": "text/plain", "start_char_idx": 19691, "end_char_idx": 19713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45159cc4-abed-41cf-90f1-dd62d14a7d05", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. ", "original_text": "1 in the multivariate case. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6246f59-5737-4138-9bfb-8a94a6aa22ed", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the following, we describe in detail the proposed FUNCTIONAL ISOLATION FOREST (FIF) algorithm and discuss its properties.\n\n ### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n", "original_text": ", N as defined in Eq. "}, "hash": "1b73e4149d482a1cd3a18017bd7ccfb1b3477a6b7d55cf658c54c51a2d7bcb2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17bd37bc-41d6-4693-be75-ad5ec2ba8d8e", "node_type": "1", "metadata": {"window": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. ", "original_text": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. "}, "hash": "1da8536b5ebb091522bb6da0342abb60bb860dfc0e9a967af4e39f38c03659a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "1 in the multivariate case. ", "mimetype": "text/plain", "start_char_idx": 19713, "end_char_idx": 19741, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17bd37bc-41d6-4693-be75-ad5ec2ba8d8e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. ", "original_text": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45159cc4-abed-41cf-90f1-dd62d14a7d05", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 3.1.  The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. ", "original_text": "1 in the multivariate case. "}, "hash": "57f7bbad519a9cddb3b312a387ec7c93f8277a2a7981719b21f9dc4809c0126d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1226a795-b514-438a-9a26-af70f3e28351", "node_type": "1", "metadata": {"window": ".  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n", "original_text": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. "}, "hash": "735320478d3962ac48d3550850132cdd09715908e2bf7f85e2c2a279f84e67f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. ", "mimetype": "text/plain", "start_char_idx": 19741, "end_char_idx": 19970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1226a795-b514-438a-9a26-af70f3e28351", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n", "original_text": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17bd37bc-41d6-4693-be75-ad5ec2ba8d8e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The FIF algorithm\n\nA Functional Isolation Forest is a collection of Functional Isolation Trees (F-itrees) built from S = {x_1, ..., x_n}, a training sample composed of independent realizations of a functional random variable, X, that takes its values in H. Given a functional observation x, the score returned by FIF is a monotone transformation of the empirical mean of the path lengths h_{T_i}(x) computed by the F-itrees T_i, for i = 1, .  .  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. ", "original_text": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function. "}, "hash": "cddb1daef988f41071796755fa44e101ed990539482497e2a3c0571b7c8c68b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71d31a2c-1417-4243-a2ec-2181e976900c", "node_type": "1", "metadata": {"window": ".  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree.", "original_text": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. "}, "hash": "52d7cedcfd49491abc9d56bea3aa212e56feef742662acf263f1b55a78cdffa4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. ", "mimetype": "text/plain", "start_char_idx": 19970, "end_char_idx": 20123, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71d31a2c-1417-4243-a2ec-2181e976900c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree.", "original_text": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1226a795-b514-438a-9a26-af70f3e28351", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".  .  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n", "original_text": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties. "}, "hash": "5e005ada4de33eea78a49459c82a4a4c15a78b2cf90d058c712f8c3546c8b9b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "440c9991-50a4-419e-9f67-46cd65a64025", "node_type": "1", "metadata": {"window": ", N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n", "original_text": "The entire construction procedure of a F-itree is described in Figure 1.\n\n"}, "hash": "73e50189c43e20893672a1e30d1529ccfaafd59eaa206f62450e711716a21d19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. ", "mimetype": "text/plain", "start_char_idx": 20123, "end_char_idx": 20490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "440c9991-50a4-419e-9f67-46cd65a64025", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ", N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n", "original_text": "The entire construction procedure of a F-itree is described in Figure 1.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71d31a2c-1417-4243-a2ec-2181e976900c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".  , N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree.", "original_text": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*. "}, "hash": "689f0211bfcfe1eaf975e00ade61fc638fc230318805805d38073c013a0df8af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cb0b3ae-d2ed-4ad6-ae72-e36f6843caed", "node_type": "1", "metadata": {"window": "1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n", "original_text": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. "}, "hash": "b3c35844de4e61ecb79ea93ad55fcaab3a8ac1f12b1c54d11bf909c1ba97318e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The entire construction procedure of a F-itree is described in Figure 1.\n\n", "mimetype": "text/plain", "start_char_idx": 20490, "end_char_idx": 20564, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1cb0b3ae-d2ed-4ad6-ae72-e36f6843caed", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n", "original_text": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "440c9991-50a4-419e-9f67-46cd65a64025", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ", N as defined in Eq.  1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n", "original_text": "The entire construction procedure of a F-itree is described in Figure 1.\n\n"}, "hash": "b337b66e03b9151c6ebf3010e2af4c8b0063942ecff69ec05c5589707a9c7149", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea8dddd8-188e-45f9-bba5-4d547fc812bf", "node_type": "1", "metadata": {"window": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. ", "original_text": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. "}, "hash": "873e706235d3816f27ddfff218e3f84209c5777d8c87fb3fcb27ab165d4b3e1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. ", "mimetype": "text/plain", "start_char_idx": 20564, "end_char_idx": 20759, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea8dddd8-188e-45f9-bba5-4d547fc812bf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. ", "original_text": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cb0b3ae-d2ed-4ad6-ae72-e36f6843caed", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "1 in the multivariate case.  While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n", "original_text": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g. "}, "hash": "8c34dc9853a48a229b3fe8d62e30b90327edd0ff7fc949ebc86981cf25dd86b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef93ca79-4956-4978-aa5a-ab0b22dfb466", "node_type": "1", "metadata": {"window": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n", "original_text": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n"}, "hash": "4d36eef1866ee67bc36e09a39bdb258413af56b27f4116b54b494b4b31da950b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. ", "mimetype": "text/plain", "start_char_idx": 20759, "end_char_idx": 20980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef93ca79-4956-4978-aa5a-ab0b22dfb466", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n", "original_text": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea8dddd8-188e-45f9-bba5-4d547fc812bf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "While the general construction principle depicted in Section 2.1 remains the same for a F-itree, dealing with functional values raises the issue of finding an adequate feature space to represent various properties of a function.  A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. ", "original_text": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated. "}, "hash": "7938b0692c446fa9a467e01055079a3c9b31585699cbc594687354afd16f7b40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33f8a3dd-6979-4894-b518-3614a32bf687", "node_type": "1", "metadata": {"window": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. ", "original_text": "***\n**Figure 1: Construction procedure of a F-itree."}, "hash": "078547eaaa568312244a14d00d25d4ebdfd89b585ab2d85fe7c95c1d54924bf5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n", "mimetype": "text/plain", "start_char_idx": 20980, "end_char_idx": 21116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "33f8a3dd-6979-4894-b518-3614a32bf687", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. ", "original_text": "***\n**Figure 1: Construction procedure of a F-itree."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef93ca79-4956-4978-aa5a-ab0b22dfb466", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A function may be considered as abnormal according to various criteria of location and shape, and the features should permit to measure such properties.  Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n", "original_text": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n"}, "hash": "40071665e918d102a0312a3953b93cd9efccd070d78b4dee0b5e9a5e4659d32f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ca2de7f-37be-491c-8d17-e38f1f00fc6b", "node_type": "1", "metadata": {"window": "The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n", "original_text": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n"}, "hash": "85f28f3f88ead2463fd4ee78407d1955316ac9be1b30ee8fcc00e4d207fe54ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 1: Construction procedure of a F-itree.", "mimetype": "text/plain", "start_char_idx": 21116, "end_char_idx": 21168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ca2de7f-37be-491c-8d17-e38f1f00fc6b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n", "original_text": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33f8a3dd-6979-4894-b518-3614a32bf687", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Therefore four ingredients have been introduced to handle functional data in a general and flexible way: (i) a set of candidate *Split variables* and (ii) a scalar product both devoted to function representation, (iii) a probability distribution to sample from this set and select a single *Split variable*, (iv) a probability distribution to select a *Split value*.  The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. ", "original_text": "***\n**Figure 1: Construction procedure of a F-itree."}, "hash": "c6a3865cef9396f0cc244a0bc5aab727e37bb1a6e1eb52d60c30267f202253dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "642da018-d452-4099-a90f-37be06ff3d95", "node_type": "1", "metadata": {"window": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. ", "original_text": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n"}, "hash": "bca9cc18fd1a07fdf29c7a700bbd0dca9e4d8a6a346401c291494233b2110fd9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n", "mimetype": "text/plain", "start_char_idx": 21168, "end_char_idx": 21278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "642da018-d452-4099-a90f-37be06ff3d95", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. ", "original_text": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ca2de7f-37be-491c-8d17-e38f1f00fc6b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The entire construction procedure of a F-itree is described in Figure 1.\n\n **Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n", "original_text": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n"}, "hash": "18473b095e02ea4d6a8ca01b04bfa3b17f86840e78dd23b8a4a9b47f493f1a62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdffd579-9d93-4188-8639-9a0cdb9bb03b", "node_type": "1", "metadata": {"window": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n", "original_text": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. "}, "hash": "f9d37234111ed2c59ea0e3b7a3c0a623694f12c77948d8aa4dbea8a50b0dc455", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n", "mimetype": "text/plain", "start_char_idx": 21278, "end_char_idx": 21388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdffd579-9d93-4188-8639-9a0cdb9bb03b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n", "original_text": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "642da018-d452-4099-a90f-37be06ff3d95", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Function representation** To define the set of candidate *Split variables*, a direct extension of the original IF algorithm (Liu et al., 2008) would be to randomly draw an argument value (e.g.  time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. ", "original_text": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n"}, "hash": "24f2264a7f0351d7b6ed872a2e6515d79170347ad190b40957c59134b0115927", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2500a258-34b7-4eb5-9b76-6d8b6fca10e7", "node_type": "1", "metadata": {"window": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2. ", "original_text": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n"}, "hash": "13dfe199e7c697122115af8ab1bd3d64fb3cfc9d99f096be9791c2f740860233", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. ", "mimetype": "text/plain", "start_char_idx": 21388, "end_char_idx": 21586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2500a258-34b7-4eb5-9b76-6d8b6fca10e7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2. ", "original_text": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdffd579-9d93-4188-8639-9a0cdb9bb03b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "time), and use functional evaluations at this point to split a node, but this boils down to only rely on instantaneous observations of functional data to capture anomalies, which in practice will be usually interpolated.  Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n", "original_text": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d. "}, "hash": "b9bb06e10bba985d6e5ca03a11b27d652c08b6aca8820bd0b48681ecf2847f39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3ec9cf2-8ae0-4060-a8f7-2431c98555d9", "node_type": "1", "metadata": {"window": "***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. ", "original_text": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. "}, "hash": "cb2ed5cd1be5146fff6bd06f3e4946ff7473641cd6bef18d751090e05c1203ab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n", "mimetype": "text/plain", "start_char_idx": 21586, "end_char_idx": 21657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3ec9cf2-8ae0-4060-a8f7-2431c98555d9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. ", "original_text": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2500a258-34b7-4eb5-9b76-6d8b6fca10e7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Drawing a direction on a unit sphere as in (Hariri et al., 2018) is no longer possible due to the potentially excessive richness of H.\n\n ***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2. ", "original_text": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n"}, "hash": "a5f9bd972787ff374534fb2eb267711cc3a6aa3afb430c4f9b9c4a553bcdf1f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bc68ee0-bb32-49b9-a655-404ac66e6e29", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n", "original_text": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n"}, "hash": "54616ab7b220be66f0ee335bc4ed8489e0f5ec270927a6583f0b22a68170df37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. ", "mimetype": "text/plain", "start_char_idx": 21657, "end_char_idx": 21919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2bc68ee0-bb32-49b9-a655-404ac66e6e29", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n", "original_text": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3ec9cf2-8ae0-4060-a8f7-2431c98555d9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 1: Construction procedure of a F-itree. **\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. ", "original_text": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal. "}, "hash": "f107dbc32dec39654b0cb129ae1e69f7bcc7f9f883aab9813bda0736cc8ca06c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "096d12f7-0814-4c8a-a761-e0b8e4d2a8b8", "node_type": "1", "metadata": {"window": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n", "original_text": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. "}, "hash": "0086635932560ffe070bd2b1b781c8cb9b975f1cdd4471185f15201f740e00e0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n", "mimetype": "text/plain", "start_char_idx": 21919, "end_char_idx": 22008, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "096d12f7-0814-4c8a-a761-e0b8e4d2a8b8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n", "original_text": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2bc68ee0-bb32-49b9-a655-404ac66e6e29", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows the pseudocode for constructing a Functional Isolation Tree (F-itree).\n\n **Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n", "original_text": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n"}, "hash": "4bfb0f106a8c44de078f183f28a8205a8ee120c25ba9f4fbf47c1343db46063f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d43062-ae4d-4cd4-bd83-d39ac3ec1981", "node_type": "1", "metadata": {"window": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . ", "original_text": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n"}, "hash": "b859a215fc896eeadd7b017b59722ca5beb8e6b8d5f33a9e6788e402ed00e766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. ", "mimetype": "text/plain", "start_char_idx": 22008, "end_char_idx": 22110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18d43062-ae4d-4cd4-bd83-d39ac3ec1981", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . ", "original_text": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "096d12f7-0814-4c8a-a761-e0b8e4d2a8b8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Input:** A subsample {x\u2081, ..., x\u03c8}, a dictionary D, a probability measure \u03bd and a scalar product (\u00b7, \u00b7)_H.\n\n **(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n", "original_text": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1. "}, "hash": "f80b48459b4fdd2b5082250581b15f149b607dba4679045c660cc8591992664f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c9c46d5-cbf8-4d98-b140-cf7bfc3b3271", "node_type": "1", "metadata": {"window": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  . ", "original_text": "2. "}, "hash": "8e125d35f5937a6e0ff391b1a7380ea303779730e730ce92f05e8c58044ae64c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n", "mimetype": "text/plain", "start_char_idx": 22110, "end_char_idx": 22188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c9c46d5-cbf8-4d98-b140-cf7bfc3b3271", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  . ", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d43062-ae4d-4cd4-bd83-d39ac3ec1981", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(a) Initialization:** The root node indexed by (0,0) is associated with the whole input space C\u2080,\u2080 = H. The construction starts with the training dataset S\u2080,\u2080 = {x\u2081, ..., xn} composed of n i.i.d.  realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . ", "original_text": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n"}, "hash": "3d711c93fb26a2474dda97074905a7c8bd93847168558fabdc863cfa1b02597f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "283a8d4c-b729-460f-bdc2-c4d1f0ef352c", "node_type": "1", "metadata": {"window": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n", "original_text": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. "}, "hash": "684c94ccbbb2c0bf12f6510a41d8063299ef6cc5b7c9dcf2ac505ec79acc3123", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 22188, "end_char_idx": 22191, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "283a8d4c-b729-460f-bdc2-c4d1f0ef352c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n", "original_text": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c9c46d5-cbf8-4d98-b140-cf7bfc3b3271", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "realizations of the random variable X. Go to (b) with (j = 0, k = 0).\n\n **(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  . ", "original_text": "2. "}, "hash": "31f86dfc4b7a90fe539b25836ca8c4c1471d0058d3a1eb3ec696db044ad65c6e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0d8221d-03b3-4718-8d72-1b50d604421b", "node_type": "1", "metadata": {"window": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. ", "original_text": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n"}, "hash": "fdbd9e726b4e902539d8609eefca3da218ccfbba590c1ca27f31e122cbd70238", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. ", "mimetype": "text/plain", "start_char_idx": 22191, "end_char_idx": 22311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0d8221d-03b3-4718-8d72-1b50d604421b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. ", "original_text": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "283a8d4c-b729-460f-bdc2-c4d1f0ef352c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(b) Stopping criterion:** Test if the node (j, k) is terminal: a node (j, k) is declared as terminal if the intersection between the current set C_{j,k} and the current training set S_{j,k} is reduced to a single data point or to a set of predefined cardinal.  If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n", "original_text": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3. "}, "hash": "df7a5597d6f5ed4317122a121525a3f225f3ff85b95906b82f52a9a550f8cc1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83392adc-8ab3-4d65-bae5-e5906ce7cbf0", "node_type": "1", "metadata": {"window": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. ", "original_text": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n"}, "hash": "4631a1d90bd6460498a586be5b12c34b288bc47bfcd94ed9dbcb0b2328af0e22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n", "mimetype": "text/plain", "start_char_idx": 22311, "end_char_idx": 22433, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "83392adc-8ab3-4d65-bae5-e5906ce7cbf0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. ", "original_text": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0d8221d-03b3-4718-8d72-1b50d604421b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "If the node is terminal, then stop the construction for this node, otherwise go to (c).\n\n **(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. ", "original_text": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n"}, "hash": "b9fc4603b82c9ed62c4f11af785df4ca73b2d60ccc4f1a397641fb992ab20ab9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecf7e1c9-5bea-4444-9fa8-65b9948c4e6a", "node_type": "1", "metadata": {"window": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. ", "original_text": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . "}, "hash": "65d92dbe1d043c52dee476d8ae17db20dccd40e4ae78f0dcc5e9d46f4f89b0ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n", "mimetype": "text/plain", "start_char_idx": 22433, "end_char_idx": 22553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ecf7e1c9-5bea-4444-9fa8-65b9948c4e6a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. ", "original_text": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "83392adc-8ab3-4d65-bae5-e5906ce7cbf0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(c) Children node construction:** A non-terminal node (j, k) is split in three steps as follows:\n1.  Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. ", "original_text": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n"}, "hash": "50f2046503e59de3ee0f10fe0136c069e63205b3a9ef6cad43f357be71437b67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "371271cc-27b6-4983-b617-fd9649c7a14f", "node_type": "1", "metadata": {"window": "2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993). ", "original_text": ". "}, "hash": "2d9eed6dead30fb6a3e464290d388bfa0869c0fb0e67a12f0472858471eaeda9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . ", "mimetype": "text/plain", "start_char_idx": 22553, "end_char_idx": 22692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "371271cc-27b6-4983-b617-fd9649c7a14f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993). ", "original_text": ". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecf7e1c9-5bea-4444-9fa8-65b9948c4e6a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Choose a *Split variable* d according to the probability distribution \u03bd on D.\n 2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. ", "original_text": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), . "}, "hash": "02d594e0cff8439e8020b266c53dd2143a9547e33890f8451aa7e7c4742701af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2a597ead-2077-480b-bacc-67dc10ab9957", "node_type": "1", "metadata": {"window": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n", "original_text": ".)\n"}, "hash": "2f7a34098ffec1de9f77d1470653d137617bb798aa0a09cedd53dbb5c958f3d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ". ", "mimetype": "text/plain", "start_char_idx": 22690, "end_char_idx": 22692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2a597ead-2077-480b-bacc-67dc10ab9957", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n", "original_text": ".)\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "371271cc-27b6-4983-b617-fd9649c7a14f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "2.  Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993). ", "original_text": ". "}, "hash": "fb9c1cdf65f0cb15b142df7bb964258b21f21dab35417556deed80dff6eff4e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "776ff3d2-ab5c-4a9d-815a-2688c8d30b0c", "node_type": "1", "metadata": {"window": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n", "original_text": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. "}, "hash": "bed70627692b8cd09cdf8b13dafb6b474ef88f06133c9c39b861e71ba0bf79ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ".)\n", "mimetype": "text/plain", "start_char_idx": 22694, "end_char_idx": 22697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "776ff3d2-ab5c-4a9d-815a-2688c8d30b0c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n", "original_text": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2a597ead-2077-480b-bacc-67dc10ab9957", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Choose randomly and uniformly a *Split value* \u03ba in the interval\n[min_{x\u2208S_{j,k}} (x, d)_H, max_{x\u2208S_{j,k}} (x, d)_H]\n3.  Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n", "original_text": ".)\n"}, "hash": "aa6638b9b8428e21831431a64b17dce66ce400b3a2641ba22295a4d5e74922e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07a556ea-d2ec-4062-a949-0a2626707a6d", "node_type": "1", "metadata": {"window": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n", "original_text": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. "}, "hash": "960682246d5490d79b99642a07f80487d4e71d6415888ffc250703b2ead7dbe7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. ", "mimetype": "text/plain", "start_char_idx": 22697, "end_char_idx": 22945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07a556ea-d2ec-4062-a949-0a2626707a6d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n", "original_text": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "776ff3d2-ab5c-4a9d-815a-2688c8d30b0c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Form the children subsets\nC_{j+1,2k} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H \u2264 \u03ba},\nC_{j+1,2k+1} = C_{j,k} \u2229 {x \u2208 H : (x, d)_H > \u03ba}.\n as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n", "original_text": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner. "}, "hash": "22d8449d38ceec327308ae084d37091b3bfd9363ad90ca9ee48f256f38343dda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4316bd9a-9966-4e28-8fce-0f722439c085", "node_type": "1", "metadata": {"window": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. ", "original_text": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. "}, "hash": "ea865bb470b4c7e8fe55f085b4fd530a0351417d27f59f095206042f68903b55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. ", "mimetype": "text/plain", "start_char_idx": 22945, "end_char_idx": 23274, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4316bd9a-9966-4e28-8fce-0f722439c085", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. ", "original_text": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07a556ea-d2ec-4062-a949-0a2626707a6d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "as well as the children training datasets\nS_{j+1,2k} = S_{j,k} \u2229 C_{j+1,2k} and S_{j+1,2k+1} = S_{j,k} \u2229 C_{j+1,2k+1}.\n\n **(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n", "original_text": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary. "}, "hash": "befc63786de0505649cbf7087842548da67424e66f28f54f690fab0acc3f2401", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9db21fe-afea-4550-b69e-e6982c6bb15f", "node_type": "1", "metadata": {"window": ".  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. ", "original_text": "Mallat and Zhang (1993). "}, "hash": "896360f32dcbbe7a6efe142ec5cdc81ef072e12e2f21b98bce0b28907df4cc6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. ", "mimetype": "text/plain", "start_char_idx": 23274, "end_char_idx": 23398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9db21fe-afea-4550-b69e-e6982c6bb15f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. ", "original_text": "Mallat and Zhang (1993). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4316bd9a-9966-4e28-8fce-0f722439c085", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**(d) Recursion:** Apply the building procedure starting from (a) to nodes (j + 1, 2k) and (j + 1, 2k + 1)\n\n**Output:** (C(0,0), C(1,1), .  .  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. ", "original_text": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g. "}, "hash": "b40e259b3a40e25ecbd966c560c3249e56987a8a4aa80f5ba36d97926a1925bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f02d470-fd58-4046-aaa3-d3ac5637221f", "node_type": "1", "metadata": {"window": ".)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. ", "original_text": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n"}, "hash": "9cecec93d4320985f222a62c6f6de893a3b9a2f0c83f6bfee6b8b05e0a822ff8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mallat and Zhang (1993). ", "mimetype": "text/plain", "start_char_idx": 23398, "end_char_idx": 23423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f02d470-fd58-4046-aaa3-d3ac5637221f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. ", "original_text": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9db21fe-afea-4550-b69e-e6982c6bb15f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".  .)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. ", "original_text": "Mallat and Zhang (1993). "}, "hash": "06cc1892635d2fc5b12ca324e7903e0edd0890fe0c48f3a30a240cd9c0ef6bbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e997bf8e-98c0-4205-8179-862415de76ef", "node_type": "1", "metadata": {"window": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries. ", "original_text": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n"}, "hash": "938a4c1a5414deef5dedc2d1049ce9fb5c1e073bb3938c97385fe86af6dcd18b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n", "mimetype": "text/plain", "start_char_idx": 23423, "end_char_idx": 23688, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e997bf8e-98c0-4205-8179-862415de76ef", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries. ", "original_text": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f02d470-fd58-4046-aaa3-d3ac5637221f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ".)\n ***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. ", "original_text": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n"}, "hash": "11618efbff6379014009d323e7665b2095b784f26ad0ed07ff89b0ee75af2ca8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82d4afcd-c449-4ed2-b03d-ceb790325379", "node_type": "1", "metadata": {"window": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. ", "original_text": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n"}, "hash": "36d6810b014b46ef8d5013064c630559dbf516e5ceb8b53304636f642bf4a55c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n", "mimetype": "text/plain", "start_char_idx": 23688, "end_char_idx": 23995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82d4afcd-c449-4ed2-b03d-ceb790325379", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. ", "original_text": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e997bf8e-98c0-4205-8179-862415de76ef", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nTo circumvent these difficulties, we propose to project the observations on elements of a dictionary D \u2282 H that is chosen to be rich enough to explore different properties of data and well appropriate to be sampled in a representative manner.  More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries. ", "original_text": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n"}, "hash": "5d0d44356a4bdf76a1ac61d6f50e148414d53f08c86baab0a8ea1691874976b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "559ff083-3916-45aa-ab31-b5bdcc724bc5", "node_type": "1", "metadata": {"window": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n", "original_text": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. "}, "hash": "b5778a0f371fc9d0ad6f2f716f98af390dbf4e5adc075aad6b5182f1bac215d6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n", "mimetype": "text/plain", "start_char_idx": 23995, "end_char_idx": 24275, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "559ff083-3916-45aa-ab31-b5bdcc724bc5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n", "original_text": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82d4afcd-c449-4ed2-b03d-ceb790325379", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "More explicitly, given a function d \u2208 D, the projection of a function x \u2208 H on D, (x, d)_H defines a feature that partially describes x. When considering all the functions of dictionary D, one gets a set of candidate *Split variables* that provides a rich representation of function X, depending on the nature of the dictionary.  Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. ", "original_text": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n"}, "hash": "676a1ab66b6d09dfe98f9e16053948204a6fdbb2331d034aea3aa53d3b7bd5f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f6a2cd5-35f3-4293-a1d7-4661cee10c32", "node_type": "1", "metadata": {"window": "Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. ", "original_text": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. "}, "hash": "9073ad6da15c7aa2ea87675e31f2eb108768ad29375803bf6688d6d7f05fa9a4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. ", "mimetype": "text/plain", "start_char_idx": 24275, "end_char_idx": 24401, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f6a2cd5-35f3-4293-a1d7-4661cee10c32", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. ", "original_text": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "559ff083-3916-45aa-ab31-b5bdcc724bc5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Dictionaries have been throughly studied in the signal processing community to achieve *sparse coding* of signals, see e.g.  Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n", "original_text": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score. "}, "hash": "2737828432c7b4eaed9bad12176127ac6823ec87c3088d402b546d4c5d8bd5ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "255f19fb-f820-42f3-9920-f11f255ee6e7", "node_type": "1", "metadata": {"window": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. ", "original_text": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. "}, "hash": "89eb5fe52fd4291101c9ee927684e65909c64ab58fc17c92e94a40b3c9912615", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. ", "mimetype": "text/plain", "start_char_idx": 24401, "end_char_idx": 24568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "255f19fb-f820-42f3-9920-f11f255ee6e7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. ", "original_text": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f6a2cd5-35f3-4293-a1d7-4661cee10c32", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Mallat and Zhang (1993).  They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. ", "original_text": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options. "}, "hash": "ae74a7906d1a9a51838d5a2bec5b71c532744b8c52c569926d418f306048d843", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1692ec0b-39c2-4097-8e1f-245fbedef823", "node_type": "1", "metadata": {"window": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g. ", "original_text": "located at isolated points, along hyperplanes) and may provide massive dictionaries. "}, "hash": "6493cbadea31ec04c5f368df707c7888a1d1c875956bd34974edab435ae8690f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. ", "mimetype": "text/plain", "start_char_idx": 24568, "end_char_idx": 24873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1692ec0b-39c2-4097-8e1f-245fbedef823", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g. ", "original_text": "located at isolated points, along hyperplanes) and may provide massive dictionaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "255f19fb-f820-42f3-9920-f11f255ee6e7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "They also provide a way to incorporate a *priori* information about the nature of the data, a property very useful in an industrial context in which functional data often come from the observation of a well known device and thus can benefit from expert knowledge.\n\n **Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. ", "original_text": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g. "}, "hash": "b3b6dbf05315ee968a63905ee91b5450a4c455411200a1b8b566d9a65bb2cb1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ab2c652-7870-42ba-9ab9-d4f07d9eafdb", "node_type": "1", "metadata": {"window": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al. ", "original_text": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. "}, "hash": "7ca4e9b81487555d0925a7d6b46c8bfd0892ed1611e237d0491bdde6f0039088", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "located at isolated points, along hyperplanes) and may provide massive dictionaries. ", "mimetype": "text/plain", "start_char_idx": 24873, "end_char_idx": 24958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ab2c652-7870-42ba-9ab9-d4f07d9eafdb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al. ", "original_text": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1692ec0b-39c2-4097-8e1f-245fbedef823", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Sampling a Split variable** Once a dictionary is chosen, a probability distribution \u03bd on D is defined to draw a *Split variable* d. Note that the choice of the sampling distribution \u03bd gives an additional flexibility to orientate the algorithm towards the search for specific properties of the functions.\n\n **Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g. ", "original_text": "located at isolated points, along hyperplanes) and may provide massive dictionaries. "}, "hash": "091726ed4fba5edf7e7dc338f929365834790ac2027def92465f26b2f607f32d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e411d989-816f-4ad9-ad67-7bb24015a947", "node_type": "1", "metadata": {"window": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. ", "original_text": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n"}, "hash": "7813b5064078190a5a82a9d177c3c6b27fac9dba3c44d16f014be0f83841b699", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. ", "mimetype": "text/plain", "start_char_idx": 24958, "end_char_idx": 25292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e411d989-816f-4ad9-ad67-7bb24015a947", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. ", "original_text": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ab2c652-7870-42ba-9ab9-d4f07d9eafdb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Sampling a Split value** Given a chosen *Split variable* d and a current training dataset S_{j,k}, a *Split value* is uniformly drawn in the real interval defined by the smallest and largest values of the projections on d when considering the observations present in the node.\n\n **Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al. ", "original_text": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself. "}, "hash": "9d90f64c22d57f11d36a5aa8902e6188a90462f3eb0beee5dbfa454b3edb605e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5554a14d-2472-4b31-bbde-e6225f9d45fc", "node_type": "1", "metadata": {"window": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n", "original_text": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. "}, "hash": "0c5f65d1c2daaf9fd3571fe773b0774ae3d99666417b7d989fe8477a9a6f2759", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n", "mimetype": "text/plain", "start_char_idx": 25292, "end_char_idx": 25437, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5554a14d-2472-4b31-bbde-e6225f9d45fc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n", "original_text": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e411d989-816f-4ad9-ad67-7bb24015a947", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Discussion on the dictionary** The choice of a suited dictionary plays a key role in construction of the FIF anomaly score.  The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. ", "original_text": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n"}, "hash": "b95b5bde500afa48fa45daf8ba35993d3ae682908906efda671008a04eeeb251", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50809a63-baeb-4c15-a357-e8a3c989a1d7", "node_type": "1", "metadata": {"window": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2. ", "original_text": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. "}, "hash": "d94df222a82b8784db8a33a2db4963eb7f02be0dfc29d5b9be08959b76f4f5b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. ", "mimetype": "text/plain", "start_char_idx": 25437, "end_char_idx": 25603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50809a63-baeb-4c15-a357-e8a3c989a1d7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2. ", "original_text": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5554a14d-2472-4b31-bbde-e6225f9d45fc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The dictionary can consist of deterministic functions, incorporate stochastic elements, contain the observations from S, or be a mixture of several mentioned options.  In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n", "original_text": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly. "}, "hash": "478c33c31b4478b170e9cb2630fd774870b5b087021da8975c9f6c4b1d7f73cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f950b4bf-5152-4fce-84ae-2d907731efba", "node_type": "1", "metadata": {"window": "located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. ", "original_text": "This last type of anomalies can be challenging; e.g. "}, "hash": "f5a5849bfca85f137e7fd686501d5dd9affdbb37413a745c4af1e0d42f985bc3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. ", "mimetype": "text/plain", "start_char_idx": 25603, "end_char_idx": 25771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f950b4bf-5152-4fce-84ae-2d907731efba", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. ", "original_text": "This last type of anomalies can be challenging; e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50809a63-baeb-4c15-a357-e8a3c989a1d7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Computational Harmonic Analysis*, a wide variety of bases or frames, such as wavelets, ridgelets, cosine packets, brushlets and so on, have been developed in the last decades in order to represent efficiently/parsimoniously functions, signals or images exhibiting specific form of singularities (e.g.  located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2. ", "original_text": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*. "}, "hash": "c93e7bc6eaefd5aaeeb02a7fdd431ecda6a74690ea8e57d4333a2540068bb015", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4fcb9f57-bb06-4645-bc41-7ea65eccf589", "node_type": "1", "metadata": {"window": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. ", "original_text": "Hubert et al. "}, "hash": "37f4206b9c311f93cde788a6e544f904b566e808d633e82783be98091edc5c89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This last type of anomalies can be challenging; e.g. ", "mimetype": "text/plain", "start_char_idx": 25771, "end_char_idx": 25824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4fcb9f57-bb06-4645-bc41-7ea65eccf589", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. ", "original_text": "Hubert et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f950b4bf-5152-4fce-84ae-2d907731efba", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "located at isolated points, along hyperplanes) and may provide massive dictionaries.  The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. ", "original_text": "This last type of anomalies can be challenging; e.g. "}, "hash": "8d48788c6347830a60ed5014b6047c465c8b6bc8c03f6328a6c8e9a66daf14cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8e39a0e-b775-48ce-ab5a-fa6d5f4fe002", "node_type": "1", "metadata": {"window": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. ", "original_text": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. "}, "hash": "4c7d12dfc4a66944ecf2bf88ce2177ff9f28b61ec36dcad9aeccaf9c825f9463", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Hubert et al. ", "mimetype": "text/plain", "start_char_idx": 25824, "end_char_idx": 25838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8e39a0e-b775-48ce-ab5a-fa6d5f4fe002", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. ", "original_text": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4fcb9f57-bb06-4645-bc41-7ea65eccf589", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The following ones will be used throughout the article: *mexican hat wavelet dictionary* (MHW), *Brownian motion dictionary* (B), *Brownian bridge dictionary* (BB), *cosine dictionary* (Cos), *uniform indicator dictionary* (UI), *dyadic indicator dictionary* (DI), and the *self-data dictionary* (Self) containing the dataset itself.  See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. ", "original_text": "Hubert et al. "}, "hash": "6d5cb262f2fea9003ff72ad55416a550d55efb54c4b3870f07ba748ad7d54bd3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca9b139e-ffb9-4c4e-bbdc-949bfa986fb8", "node_type": "1", "metadata": {"window": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. ", "original_text": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n"}, "hash": "e7823247409f5a82e15d0172147d1d3fe485d88277532c3f971610abdcfb9984", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. ", "mimetype": "text/plain", "start_char_idx": 25838, "end_char_idx": 26044, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca9b139e-ffb9-4c4e-bbdc-949bfa986fb8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. ", "original_text": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8e39a0e-b775-48ce-ab5a-fa6d5f4fe002", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "See Section B and C of the Supplementary Materials for detailed definitions of these dictionaries and further discussion on them, respectively.\n\n **Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. ", "original_text": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves. "}, "hash": "1609f5bb1680bf684b86081b0e9950a93b2ac89601942de6c2664b6b87752398", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34eebea6-3733-41ff-bc29-c317e744fa7e", "node_type": "1", "metadata": {"window": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. ", "original_text": "### 3.2. "}, "hash": "8a5a52f1508275706e18ae7114052dd32c18e6e5b6bfde524a89a86a6086da60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n", "mimetype": "text/plain", "start_char_idx": 26044, "end_char_idx": 26289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "34eebea6-3733-41ff-bc29-c317e744fa7e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. ", "original_text": "### 3.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca9b139e-ffb9-4c4e-bbdc-949bfa986fb8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Discussion on the scalar product** Besides the dictionary, the scalar product defined on H brings some additional flexibility to measure different type of anomaly.  While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. ", "original_text": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n"}, "hash": "a6039ddbc93640869937be44de6e1019ebea4c5a0ded1c9b3d91d5275c713972", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b72f551-c7bf-43cf-bb6b-8ce0686da706", "node_type": "1", "metadata": {"window": "This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. ", "original_text": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. "}, "hash": "82c0ec2ed52bc1d1cd2efa971bb871fd2a7bcc8529f2841ec036d3ab14b0e374", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 3.2. ", "mimetype": "text/plain", "start_char_idx": 26289, "end_char_idx": 26298, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b72f551-c7bf-43cf-bb6b-8ce0686da706", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. ", "original_text": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34eebea6-3733-41ff-bc29-c317e744fa7e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "While L_2 scalar product allows for detection of *location anomalies*, L_2 scalar product of derivatives (or slopes) would allow to detect anomalies regarding *shape*.  This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. ", "original_text": "### 3.2. "}, "hash": "eb5d6413ffc859f72f7f63fb3c583e051fcf08b2d4c121cde536e610d966f8d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e6e6038-e382-4bdf-99f4-e9c1442b6b2d", "node_type": "1", "metadata": {"window": "Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n", "original_text": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. "}, "hash": "b934fcbc772c167a669151ad185e72d219c691f91d36fdf6cc02e645d0a67c9f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. ", "mimetype": "text/plain", "start_char_idx": 26298, "end_char_idx": 26523, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e6e6038-e382-4bdf-99f4-e9c1442b6b2d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n", "original_text": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b72f551-c7bf-43cf-bb6b-8ce0686da706", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This last type of anomalies can be challenging; e.g.  Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. ", "original_text": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior. "}, "hash": "a8f41240abd7117f5ab5571765aedcb569faad675b15bcc98bad89c0eec8ed7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40bb597a-88dc-4774-9518-a7c181e410fc", "node_type": "1", "metadata": {"window": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). ", "original_text": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. "}, "hash": "d2e28a147005de800f002ef18f3e852abfce01f43528205a4a97639bb8f0349b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. ", "mimetype": "text/plain", "start_char_idx": 26523, "end_char_idx": 26664, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40bb597a-88dc-4774-9518-a7c181e410fc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). ", "original_text": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e6e6038-e382-4bdf-99f4-e9c1442b6b2d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Hubert et al.  (2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n", "original_text": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary. "}, "hash": "4fd25318399612d78edf9ffcd132d74d6ea58664c138ec5a359c02d812bcca82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ddc4f0d-f938-4821-b59f-a1e7ae75c29c", "node_type": "1", "metadata": {"window": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies. ", "original_text": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. "}, "hash": "97152b99c1736a47fa6818218c360a71d6cac527a33051cf90ea2684662b6a05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. ", "mimetype": "text/plain", "start_char_idx": 26664, "end_char_idx": 26805, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7ddc4f0d-f938-4821-b59f-a1e7ae75c29c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies. ", "original_text": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40bb597a-88dc-4774-9518-a7c181e410fc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015) mention that shape anomalies are more difficult to detect, and Mosler and Mozharovskyi (2017) argue that one should consider both location and slope simultaneously for distinguishing complex curves.  Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). ", "original_text": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data. "}, "hash": "d985c347854980dd5606f46ff4b8dd1aa2ee33491025e7902f8c26a22988ac71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c9e9cd5-3474-4eba-be8e-4689e9c2fd95", "node_type": "1", "metadata": {"window": "### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right).", "original_text": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. "}, "hash": "c1de1d50cf3b8458db91b93211d028290519b99ae7c12fdf590e7b1ca9bddde1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. ", "mimetype": "text/plain", "start_char_idx": 26805, "end_char_idx": 27084, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c9e9cd5-3474-4eba-be8e-4689e9c2fd95", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right).", "original_text": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ddc4f0d-f938-4821-b59f-a1e7ae75c29c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Beyond these two, a wide diversity of scalar products can be used, involving a variety of L_2-scalar products related to derivatives of certain orders, like in the definition of Banach spaces such as weighted Sobolev spaces, see Maz\u2019ya (2011).\n\n ### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies. ", "original_text": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below. "}, "hash": "e918d015d7a908451c3cefd40ec21287986a920bf00930f296cfca20c42cd40a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28eecd3d-858a-4fbf-bc9d-c6b461138366", "node_type": "1", "metadata": {"window": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots. ", "original_text": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. "}, "hash": "9da3a0ea30176a1f9e8caee60fad4f4a51de8ecd5f06c49aa73a0189f730b9b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. ", "mimetype": "text/plain", "start_char_idx": 27084, "end_char_idx": 27255, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28eecd3d-858a-4fbf-bc9d-c6b461138366", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots. ", "original_text": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c9e9cd5-3474-4eba-be8e-4689e9c2fd95", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 3.2.  Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right).", "original_text": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product. "}, "hash": "edf3052e088e4fb596cc5b785759438035c6fb971f4e625d365b2748aedb95ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91c25e6e-32ab-432b-b9f3-ef14d6bb71dc", "node_type": "1", "metadata": {"window": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. ", "original_text": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n"}, "hash": "bd56dba476b06a99fcbf9b8fa0fffd6a2aaee13e42508b2838a222740d99a909", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. ", "mimetype": "text/plain", "start_char_idx": 27255, "end_char_idx": 27545, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91c25e6e-32ab-432b-b9f3-ef14d6bb71dc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. ", "original_text": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28eecd3d-858a-4fbf-bc9d-c6b461138366", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Ability of FIF to detect a variety of anomalies\n\nAs discussed in Section 2.2, most of state-of-the-art methods have a focus on a certain type of anomalies and are unable to detect various deviations from the normal behavior.  The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots. ", "original_text": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig. "}, "hash": "7eb824c9a1a671206e4582442f545e4b24d3914eab0643d128933a4c8cd54da0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7053b434-fd62-4813-9f5c-8febbacc5be9", "node_type": "1", "metadata": {"window": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". ", "original_text": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). "}, "hash": "4ad6e3540a390dcd52e7563efd551ca70dfb5b63faefdbd9c843626424c2399f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n", "mimetype": "text/plain", "start_char_idx": 27545, "end_char_idx": 27972, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7053b434-fd62-4813-9f5c-8febbacc5be9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". ", "original_text": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91c25e6e-32ab-432b-b9f3-ef14d6bb71dc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The flexibility of the FIF algorithm allows for choosing the scope of the detection by selecting both the scalar product and the dictionary.  Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. ", "original_text": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n"}, "hash": "eadb6063a0d543a7332097f19789006fd3d6548dcc5990b51804942d50c4bf9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62cf9827-e648-4ca0-80e5-e710bb6c76ad", "node_type": "1", "metadata": {"window": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. ", "original_text": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies. "}, "hash": "92d85a31377057a8d273a1e8bffe29427d6b366c2a39878f512cd1f63472fb9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). ", "mimetype": "text/plain", "start_char_idx": 27972, "end_char_idx": 28053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62cf9827-e648-4ca0-80e5-e710bb6c76ad", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. ", "original_text": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7053b434-fd62-4813-9f5c-8febbacc5be9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, by choosing appropriate scalar product and dictionary, FIF is able to detect a great diversity of deviations from normal data.  First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". ", "original_text": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left). "}, "hash": "898c234b8051ceb3e891d4aa1586cd9599be2f3341f74a33005407432835707e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d522668e-3c26-4822-87c3-90666a63b785", "node_type": "1", "metadata": {"window": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores. ", "original_text": "The sorted anomaly score of the dataset (right)."}, "hash": "c53d8728122b6e839c405518da552f92e8458b20864761015987b8517cb4d991", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies. ", "mimetype": "text/plain", "start_char_idx": 28053, "end_char_idx": 28150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d522668e-3c26-4822-87c3-90666a63b785", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores. ", "original_text": "The sorted anomaly score of the dataset (right)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62cf9827-e648-4ca0-80e5-e710bb6c76ad", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, to account for both location and shape anomalies, we suggest the following scalar product that provides a compromise between the both\n\n(f, g) := \u03b1 \u00d7 (f,g)_{L_2} / (||f|| ||g||) + (1 \u2212 \u03b1) \u00d7 (f', g')_{L_2} / (||f'|| ||g'||), \u03b1 \u2208 [0, 1],\n\nand illustrate its use right below.  Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. ", "original_text": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies. "}, "hash": "d3103571bc8c8bef5687ad5688787bed4a4363316c86245321e9db26301cf959", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5080dd1-0254-4dba-9415-adc69c69db8b", "node_type": "1", "metadata": {"window": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. ", "original_text": "**\n\n**Description:** The figure contains three plots. "}, "hash": "72ea5c1a1b10e91093f74ec0539e2f4439f2e285090a2281fb1aa21ae5648d8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sorted anomaly score of the dataset (right).", "mimetype": "text/plain", "start_char_idx": 28150, "end_char_idx": 28198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5080dd1-0254-4dba-9415-adc69c69db8b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. ", "original_text": "**\n\n**Description:** The figure contains three plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d522668e-3c26-4822-87c3-90666a63b785", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, setting \u03b1 = 1 yields the classical L_2 scalar product, \u03b1 = 0 corresponds to the L_2 scalar product of derivative, and \u03b1 = 0.5 is the Sobolev W_{1,2} scalar product.  To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores. ", "original_text": "The sorted anomaly score of the dataset (right)."}, "hash": "9d7c506b35dbbf83729df94267bf520aee16b85ba7b8160afd408d39d786124b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0224f72c-1cdd-4b6a-9d41-054ff1ac4941", "node_type": "1", "metadata": {"window": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n", "original_text": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. "}, "hash": "ff2729caaf0348f2010e5dd2d71b368d67474530505329198ee0815fc75ab48b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure contains three plots. ", "mimetype": "text/plain", "start_char_idx": 28198, "end_char_idx": 28252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0224f72c-1cdd-4b6a-9d41-054ff1ac4941", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n", "original_text": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5080dd1-0254-4dba-9415-adc69c69db8b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate the FIF's ability to detect a wide variety of anomalies at a time, we calculate the FIF anomaly scores with the Sobolev scalar product and the *gaussian wavelets dictionary* for a sample consisting of 105 curves defined as follows (inspired by (Cuevas et al., 2007), see Fig.  2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. ", "original_text": "**\n\n**Description:** The figure contains three plots. "}, "hash": "dc6466f587c2bc6f183098836698fdc3ea868fdf762fdf40435107ffb5a86b90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2f49f88-13c2-44c9-a0c6-1365ce22350c", "node_type": "1", "metadata": {"window": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". "}, "hash": "0ebc88e65ae12124beb4f22cd866700d6fbbab62b2faffe6586bd90b659d1ba1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. ", "mimetype": "text/plain", "start_char_idx": 28252, "end_char_idx": 28440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2f49f88-13c2-44c9-a0c6-1365ce22350c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0224f72c-1cdd-4b6a-9d41-054ff1ac4941", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "2):\n\n- 100 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 5 abnormal curves composed by one isolated anomaly x\u2080(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 with a jump in t = 0.7, one magnitude anomaly x\u2081(t) = 30(1 \u2212 t)\u00b9\u00b7\u2076t\u00b9\u00b7\u2076 and three kind of shape anomalies x\u2082(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + sin(2\u03c0t), x\u2083(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8] and x\u2084(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 + 1/2 sin(10\u03c0t).\n\n ***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n", "original_text": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves. "}, "hash": "12430de3f66af699e05bbaeb6c1b88d884c79ee613c85656930804c4cefb3d04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e8c7643-3045-4cbd-b062-b9513b2ee500", "node_type": "1", "metadata": {"window": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4. ", "original_text": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. "}, "hash": "5a4d810a67bc67736d2c97cb459fcaa890c2b4badbb01a040e89c25403c5140c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". ", "mimetype": "text/plain", "start_char_idx": 28440, "end_char_idx": 28504, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e8c7643-3045-4cbd-b062-b9513b2ee500", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4. ", "original_text": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2f49f88-13c2-44c9-a0c6-1365ce22350c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 2: The simulated dataset with the five introduced anomalies (left).  The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\". "}, "hash": "13929803452ef4663dded0d2a517b3de8ef3fd6b2b05cfb3311afb1129b9bfdb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4e46dba-bb1b-4197-84d1-c1dd8de45049", "node_type": "1", "metadata": {"window": "The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. ", "original_text": "The rightmost plot is a scatter plot showing the sorted anomaly scores. "}, "hash": "edd5b5ca5bc4278a66d629e95d82f7d1f7fc3d94cfd5d4ff803e0b91ed848e3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. ", "mimetype": "text/plain", "start_char_idx": 28504, "end_char_idx": 28661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4e46dba-bb1b-4197-84d1-c1dd8de45049", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. ", "original_text": "The rightmost plot is a scatter plot showing the sorted anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e8c7643-3045-4cbd-b062-b9513b2ee500", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The scored dataset (middle), the darker the color, the more the curves are considered anomalies.  The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4. ", "original_text": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly. "}, "hash": "72d2e95c8b3d1f3a245addc9e26afd58e17d777e0a20170f8f07f01ee432b35f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba4804da-8104-43c2-8cee-e3910d5ad931", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. ", "original_text": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. "}, "hash": "5df6a41a3d17138050458f3295fdd0ed9e743b6999023cd71d35c657d4a0b48d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The rightmost plot is a scatter plot showing the sorted anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 28661, "end_char_idx": 28733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ba4804da-8104-43c2-8cee-e3910d5ad931", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. ", "original_text": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4e46dba-bb1b-4197-84d1-c1dd8de45049", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The sorted anomaly score of the dataset (right). **\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. ", "original_text": "The rightmost plot is a scatter plot showing the sorted anomaly scores. "}, "hash": "184d0dd6e63a7f93194f05c443183f3c6506658c91aef61da8c9b8b9dfa2d136", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5245d1e7-ed44-45d6-8841-43c71b106d65", "node_type": "1", "metadata": {"window": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v. ", "original_text": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n"}, "hash": "4289b887c0b5867683c80a92bec95f356ed58c382927a00449a17eb8a663d843", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. ", "mimetype": "text/plain", "start_char_idx": 28733, "end_char_idx": 28830, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5245d1e7-ed44-45d6-8841-43c71b106d65", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v. ", "original_text": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba4804da-8104-43c2-8cee-e3910d5ad931", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure contains three plots.  The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. ", "original_text": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8. "}, "hash": "6c5b200aef4832bb36da01fd9d114c6bfb1ffe4d086591c83606940e02ad871f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c298759-e74a-47a4-add1-0024bdbe414c", "node_type": "1", "metadata": {"window": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size. ", "original_text": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n"}, "hash": "4598b3230a6ae10f56d846e9a9a1e49471c629af2cff5e6b19efee7e4006b31b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n", "mimetype": "text/plain", "start_char_idx": 28830, "end_char_idx": 28912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c298759-e74a-47a4-add1-0024bdbe414c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size. ", "original_text": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5245d1e7-ed44-45d6-8841-43c71b106d65", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The leftmost plot shows a set of curves, with five specific anomalous curves (x\u2080 to x\u2084) highlighted in different colors and plotted over a gray shaded area representing the normal curves.  The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v. ", "original_text": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n"}, "hash": "9bbf27079f77ef8fd48b782e7b53706604ed3b9f1fd2386328c434c4e45ee2aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26027a8e-6488-4d92-b680-300ffcbc2b65", "node_type": "1", "metadata": {"window": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. ", "original_text": "## 4. "}, "hash": "0f968b86c5e4b1d44722fa2c6b690a7d1ad626cacbc849abe898c485acc1ec7a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n", "mimetype": "text/plain", "start_char_idx": 28912, "end_char_idx": 29042, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26027a8e-6488-4d92-b680-300ffcbc2b65", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. ", "original_text": "## 4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c298759-e74a-47a4-add1-0024bdbe414c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Time\" from 0.0 to 1.0, and the y-axis is \"x(t)\".  The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size. ", "original_text": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n"}, "hash": "402ccbe615fab3b20ea771b6f54b2ddd8fb1ff6f293d3467734f16ceab3fdc17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea49b042-4487-44c1-a418-f9de36de2566", "node_type": "1", "metadata": {"window": "The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. ", "original_text": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. "}, "hash": "3a2069e2979df23884ff259a56d4915ec378194a7933663a8676518977eafc4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 4. ", "mimetype": "text/plain", "start_char_idx": 29042, "end_char_idx": 29048, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea49b042-4487-44c1-a418-f9de36de2566", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. ", "original_text": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26027a8e-6488-4d92-b680-300ffcbc2b65", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The middle plot shows the same set of curves, but this time they are color-coded based on their anomaly score, with darker colors indicating higher anomaly.  The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. ", "original_text": "## 4. "}, "hash": "dfaced9d5bbc31ff63760b65dcb73bb53acf5763ba45d7abb0cee172f5fef751", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35185bdc-02ef-4422-ab3c-c1f065665a99", "node_type": "1", "metadata": {"window": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n", "original_text": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. "}, "hash": "cb2a0d88fb2e0708056e79463452f61831192015d424169230c02fbbb55aa7e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. ", "mimetype": "text/plain", "start_char_idx": 29048, "end_char_idx": 29141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "35185bdc-02ef-4422-ab3c-c1f065665a99", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n", "original_text": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea49b042-4487-44c1-a418-f9de36de2566", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The rightmost plot is a scatter plot showing the sorted anomaly scores.  The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. ", "original_text": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm. "}, "hash": "c89da2be67814a0391f8c5180320e874a1297a788d549df6cd7fe1fb08fd7162", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90675665-6b3c-41cf-815d-7ffaac032d22", "node_type": "1", "metadata": {"window": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1. ", "original_text": "the probability distribution of a r.v. "}, "hash": "cd83358e78fbb9b1cbb1816fbc272b326e3c3a1db5f8924f3765d2be8e77dcd0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. ", "mimetype": "text/plain", "start_char_idx": 29141, "end_char_idx": 29233, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90675665-6b3c-41cf-815d-7ffaac032d22", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1. ", "original_text": "the probability distribution of a r.v. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35185bdc-02ef-4422-ab3c-c1f065665a99", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Index of sorted curves\" from 0 to 100, and the y-axis is \"Score\" from 0.4 to 0.8.  The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n", "original_text": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t. "}, "hash": "7f19c4a3151308acaa28a00a86cd1a1f8c50a4078e51f0cabfa25be5a793141b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9649d415-1a5b-4173-a409-b1ff68f249c4", "node_type": "1", "metadata": {"window": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. ", "original_text": "X and the sample size. "}, "hash": "4eebea59e201a77431c306d13be2dda6ac3937b66c56ec4c2cfb546a7ffd6d0c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the probability distribution of a r.v. ", "mimetype": "text/plain", "start_char_idx": 29233, "end_char_idx": 29272, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9649d415-1a5b-4173-a409-b1ff68f249c4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. ", "original_text": "X and the sample size. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90675665-6b3c-41cf-815d-7ffaac032d22", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The five anomalous points (x\u2080 to x\u2084) are highlighted and have the highest scores.\n ***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1. ", "original_text": "the probability distribution of a r.v. "}, "hash": "77c835efd4e8dcf3be651979e441d09215d82167d3fb8a58113129bdb5dc9b47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab8865d6-ce02-40f8-b86a-f390e22ba3cc", "node_type": "1", "metadata": {"window": "## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature. ", "original_text": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. "}, "hash": "6935f5f0f96abecc56f64b9850924c9ab144ed1582983d8b2a56edd8b676d1c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "X and the sample size. ", "mimetype": "text/plain", "start_char_idx": 29272, "end_char_idx": 29295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab8865d6-ce02-40f8-b86a-f390e22ba3cc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature. ", "original_text": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9649d415-1a5b-4173-a409-b1ff68f249c4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nOne can see that the five anomalies, although very different, are all detected by FIF with a significantly different score.\n\n ## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. ", "original_text": "X and the sample size. "}, "hash": "02325fb29c684d3cb0c2763d2fd4d4951701cc2347ca4c7737996226d3fac96a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c57501a-5d1f-41e9-b460-a69f5fe3c3a3", "node_type": "1", "metadata": {"window": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig. ", "original_text": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. "}, "hash": "10562afe6a1299a965cda953f79beb8ed4fc5752210da4ddfbcb7af709ba028d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. ", "mimetype": "text/plain", "start_char_idx": 29295, "end_char_idx": 29438, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c57501a-5d1f-41e9-b460-a69f5fe3c3a3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig. ", "original_text": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab8865d6-ce02-40f8-b86a-f390e22ba3cc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 4.  Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature. ", "original_text": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods. "}, "hash": "ff861a35735bd5befc49a56634386998125042e6676b9886e688b3b251f5374d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e95e17f-33a7-486d-8c69-769cda49d93f", "node_type": "1", "metadata": {"window": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. ", "original_text": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n"}, "hash": "1969af3bfda2d0d0f1806a8d476f768cdee54a5d19c3c2a09f55434871ac12a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. ", "mimetype": "text/plain", "start_char_idx": 29438, "end_char_idx": 29604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e95e17f-33a7-486d-8c69-769cda49d93f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. ", "original_text": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c57501a-5d1f-41e9-b460-a69f5fe3c3a3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Numerical Results\n\nIn this section, we provide an empirical study of the proposed algorithm.  First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig. ", "original_text": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set. "}, "hash": "3dfd505fb6049d923bde02eef50b12b022335ef8d0553bc035d978a4159fbf48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "635c6ed1-0f4a-4462-bb5a-d883f3571102", "node_type": "1", "metadata": {"window": "the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al. ", "original_text": "### 4.1. "}, "hash": "e7bcb6c904f0867debd2ea9768c6589c5a0f0e2741eb34511081c21addee90c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n", "mimetype": "text/plain", "start_char_idx": 29604, "end_char_idx": 29710, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "635c6ed1-0f4a-4462-bb5a-d883f3571102", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al. ", "original_text": "### 4.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e95e17f-33a7-486d-8c69-769cda49d93f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, in Section 4.1 we explore the stability and consistency of the score function w.r.t.  the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. ", "original_text": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n"}, "hash": "10ee7217e21a7d7c303a2b1d1868e62a65d547e453d5b4de04940ac92ee9fe3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0fec519-d6d6-42b6-af51-beb23e8b8d01", "node_type": "1", "metadata": {"window": "X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths. ", "original_text": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. "}, "hash": "ec1fb0f92f41426b4507bd4cf891fd19410877acb2b345e19afdfd4320025d3e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### 4.1. ", "mimetype": "text/plain", "start_char_idx": 29710, "end_char_idx": 29719, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0fec519-d6d6-42b6-af51-beb23e8b8d01", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths. ", "original_text": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "635c6ed1-0f4a-4462-bb5a-d883f3571102", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "the probability distribution of a r.v.  X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al. ", "original_text": "### 4.1. "}, "hash": "30cf67a4742ec89487090ffb31c07060e6276c5d3e367acf03954f49132f6288", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d38d59c5-98b6-4d39-841c-6c917d133849", "node_type": "1", "metadata": {"window": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. ", "original_text": "This issue is even more important because of the absence of theoretical developments due to their challenging nature. "}, "hash": "57d319e387cff7d3891ae2d3814860d76faf5491e5b9c79580715e3be8eabcf0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. ", "mimetype": "text/plain", "start_char_idx": 29719, "end_char_idx": 29971, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d38d59c5-98b6-4d39-841c-6c917d133849", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. ", "original_text": "This issue is even more important because of the absence of theoretical developments due to their challenging nature. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0fec519-d6d6-42b6-af51-beb23e8b8d01", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "X and the sample size.  Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths. ", "original_text": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest. "}, "hash": "a0a2b894546642a9f2d3332d9a86c468d1343dbcc30e3132f61bf75c3315c01c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19add24c-c25e-44b9-a25a-a1ef58556f3a", "node_type": "1", "metadata": {"window": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n", "original_text": "The empirical study is conducted on two simulated functional datasets presented in Fig. "}, "hash": "4bf2b1d9abb7b74dd39756f104ad8292d65ef55964249d8573d1843ada692aab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This issue is even more important because of the absence of theoretical developments due to their challenging nature. ", "mimetype": "text/plain", "start_char_idx": 29971, "end_char_idx": 30089, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19add24c-c25e-44b9-a25a-a1ef58556f3a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n", "original_text": "The empirical study is conducted on two simulated functional datasets presented in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d38d59c5-98b6-4d39-841c-6c917d133849", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Furthermore, we examine the influence of proposed dictionaries on the score function and bring performance comparisons with benchmark methods.  Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. ", "original_text": "This issue is even more important because of the absence of theoretical developments due to their challenging nature. "}, "hash": "8a4c87f625656256c807121caaca74faabcaa2e9204eefd422e57cf86ac9b9eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "996734d0-053c-4ab6-905b-473f1019987c", "node_type": "1", "metadata": {"window": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score. ", "original_text": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. "}, "hash": "95257384af181c8bd483e819ddab5d07f4e1673556f6a497012f9fab63ce7252", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The empirical study is conducted on two simulated functional datasets presented in Fig. ", "mimetype": "text/plain", "start_char_idx": 30089, "end_char_idx": 30177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "996734d0-053c-4ab6-905b-473f1019987c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score. ", "original_text": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19add24c-c25e-44b9-a25a-a1ef58556f3a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Second, in Section 4.2, we benchmark the performance of FIF on several real labeled datasets by measuring its ability to recover an \"abnormal\" class on the test set.  In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n", "original_text": "The empirical study is conducted on two simulated functional datasets presented in Fig. "}, "hash": "c0ea6217a77cb6f650ede214409cd58e332c0cba43f19d93b41b362b7905995e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f689ea2-6ea3-4933-9b3c-617cfc285471", "node_type": "1", "metadata": {"window": "### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. ", "original_text": "Dataset (b) has been used by Claeskens et al. "}, "hash": "300487d696969dcf3c134a7864654698ef58fabd7c91c7dcd058c75477b5433c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. ", "mimetype": "text/plain", "start_char_idx": 30177, "end_char_idx": 30292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f689ea2-6ea3-4933-9b3c-617cfc285471", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. ", "original_text": "Dataset (b) has been used by Claeskens et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "996734d0-053c-4ab6-905b-473f1019987c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In all experiments, N the number of F-itrees is fixed to 100 and the height limit is fixed to [log\u2082(\u03c8)].\n\n ### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score. ", "original_text": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature. "}, "hash": "9a442884b0b96f9e6c8519a135da999314124df52599e97cf5c563d1119869c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6244b59b-4143-4d92-8d50-b9092b8d56c7", "node_type": "1", "metadata": {"window": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4. ", "original_text": "(2014) and has smooth paths. "}, "hash": "0733a26045bee1f22dae59537898d05d5036d745dd760861e5645d40aa5c95b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dataset (b) has been used by Claeskens et al. ", "mimetype": "text/plain", "start_char_idx": 30292, "end_char_idx": 30338, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6244b59b-4143-4d92-8d50-b9092b8d56c7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4. ", "original_text": "(2014) and has smooth paths. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f689ea2-6ea3-4933-9b3c-617cfc285471", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### 4.1.  Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. ", "original_text": "Dataset (b) has been used by Claeskens et al. "}, "hash": "b3bf62b1e60415c414a230d9b3ce7455fd9e79c6bcac74ee815f65f3e107f723", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aa3669bb-9514-4a10-8631-5d36b6d9e3f9", "node_type": "1", "metadata": {"window": "This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. ", "original_text": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. "}, "hash": "6003f9369e78f21c3866904854c656019810c1e2262e957f16942307899efd37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2014) and has smooth paths. ", "mimetype": "text/plain", "start_char_idx": 30338, "end_char_idx": 30367, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aa3669bb-9514-4a10-8631-5d36b6d9e3f9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. ", "original_text": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6244b59b-4143-4d92-8d50-b9092b8d56c7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Impact of the Hyperparameters on Stability\n\nSince functional data are more complex than multivariate data, and the dictionary constitutes an additional source of variance, a question of stability of the FIF anomaly score estimates is of high interest.  This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4. ", "original_text": "(2014) and has smooth paths. "}, "hash": "1f1142d95b788623f6dac5acd18ef0a85257bbda488688e941175bc5eb402c46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "614619c6-fabf-40a7-b1d1-d3bd37bbadad", "node_type": "1", "metadata": {"window": "The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. ", "original_text": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n"}, "hash": "acee96d638ebf1b8f0b309549f312e79ddf88f88e957008f8a7c17c462e10d0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. ", "mimetype": "text/plain", "start_char_idx": 30367, "end_char_idx": 30558, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "614619c6-fabf-40a7-b1d1-d3bd37bbadad", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. ", "original_text": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aa3669bb-9514-4a10-8631-5d36b6d9e3f9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This issue is even more important because of the absence of theoretical developments due to their challenging nature.  The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. ", "original_text": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083. "}, "hash": "41117fa09f21d8916d941827fa1be0cbdb4435fc05a881c92f82829b30b2aceb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aefc931e-1d9a-4340-8af0-987737bf4468", "node_type": "1", "metadata": {"window": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). ", "original_text": "Further, we provide an illustration of the empirical convergence of the score. "}, "hash": "f31c75fceb1ede836cbe4ef9f8050ebec6c307e459cef7c270d06f359effbc15", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 30558, "end_char_idx": 30674, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aefc931e-1d9a-4340-8af0-987737bf4468", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). ", "original_text": "Further, we provide an illustration of the empirical convergence of the score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "614619c6-fabf-40a7-b1d1-d3bd37bbadad", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The empirical study is conducted on two simulated functional datasets presented in Fig.  3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. ", "original_text": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n"}, "hash": "e6d49a9d0c51236b9e6833720ea76cbffacecf1110d213efd7d358cff8920b04", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8bb0808f-101c-4d31-881b-1517dc6b58be", "node_type": "1", "metadata": {"window": "Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080. ", "original_text": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. "}, "hash": "f7949e86ca9361f28eaccb590441bdd39343fae419351524b351e32f352a8a8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Further, we provide an illustration of the empirical convergence of the score. ", "mimetype": "text/plain", "start_char_idx": 30674, "end_char_idx": 30753, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8bb0808f-101c-4d31-881b-1517dc6b58be", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080. ", "original_text": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aefc931e-1d9a-4340-8af0-987737bf4468", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "3: Dataset (a) is the standard Brownian motion being a classical stochastic process widely used in the literature.  Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). ", "original_text": "Further, we provide an illustration of the empirical convergence of the score. "}, "hash": "161ffbd8abbe02d7009f3ac20a849cfe76e8041021fc405f6e2adf13a685397a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3b70fba-a985-4ca8-9410-58c2709f0286", "node_type": "1", "metadata": {"window": "(2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. ", "original_text": "4. "}, "hash": "4f1f2993605c5462faa859829c0ce43298461fb2ed6df0c26d5f535d8939ef91", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. ", "mimetype": "text/plain", "start_char_idx": 30753, "end_char_idx": 31007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3b70fba-a985-4ca8-9410-58c2709f0286", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. ", "original_text": "4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8bb0808f-101c-4d31-881b-1517dc6b58be", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Dataset (b) has been used by Claeskens et al.  (2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080. ", "original_text": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig. "}, "hash": "a72bd4bdcdc929f99518e7310d47a02b070816da03f7ea8cbb5240ccd2e9eb77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e43c086-30f1-4fe9-8602-0c7a6932a180", "node_type": "1", "metadata": {"window": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig. ", "original_text": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. "}, "hash": "53cc3f634b8f56a21da77e39f25ce1c09a8a34c847df602fc05a8d1323cf8f60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "4. ", "mimetype": "text/plain", "start_char_idx": 31007, "end_char_idx": 31010, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e43c086-30f1-4fe9-8602-0c7a6932a180", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig. ", "original_text": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3b70fba-a985-4ca8-9410-58c2709f0286", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2014) and has smooth paths.  For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. ", "original_text": "4. "}, "hash": "494d29b7a4957f50c1edff181fbd0772e55cddece9fbb61199268709722afce6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12cd1fda-cbc8-4b6e-9667-6f90bc673016", "node_type": "1", "metadata": {"window": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations.", "original_text": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. "}, "hash": "98152169403b85fcf729ac1cbbcbd46923273b55d7652ba9b218d51d4186a569", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. ", "mimetype": "text/plain", "start_char_idx": 31010, "end_char_idx": 31164, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "12cd1fda-cbc8-4b6e-9667-6f90bc673016", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations.", "original_text": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e43c086-30f1-4fe9-8602-0c7a6932a180", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each dataset, we choose/add four observations for which the FIF anomaly score is computed after training: a normal observation x\u2080, two anomalies x\u2081 and x\u2082, and a more extreme anomaly x\u2083.  We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig. ", "original_text": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083. "}, "hash": "ad8d63abc9ace060d78886dd8a2fa9637b81fe9f295f2608f0580aef205b5986", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdeb2c5d-f62d-456f-9cff-043d063c2038", "node_type": "1", "metadata": {"window": "Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots. ", "original_text": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). "}, "hash": "bee80c634c797f49df60691f58aeaba8a199e45e5df455c6ae0b8b9b0807323a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. ", "mimetype": "text/plain", "start_char_idx": 31164, "end_char_idx": 31681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fdeb2c5d-f62d-456f-9cff-043d063c2038", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots. ", "original_text": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12cd1fda-cbc8-4b6e-9667-6f90bc673016", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We therefore expect the following ranking of the scores: s_n(x\u2080) < s_n(x\u2081) \u2264 s_n(x\u2082) < s_n(x\u2083), for both datasets.\n\n Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations.", "original_text": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500. "}, "hash": "234d4e2a7f726845a37d952c6fdf3dfb4c8f6f0662268a9de7e122074d717b10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90a510a8-eac9-4682-a76e-74d9f5161ce8", "node_type": "1", "metadata": {"window": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. ", "original_text": "The behavior is reciprocal for the typical observation x\u2080. "}, "hash": "ef46826ace773f56344374385165a97b5116db49998c7b9b38b0e690fd40b5c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). ", "mimetype": "text/plain", "start_char_idx": 31681, "end_char_idx": 31787, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90a510a8-eac9-4682-a76e-74d9f5161ce8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. ", "original_text": "The behavior is reciprocal for the typical observation x\u2080. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdeb2c5d-f62d-456f-9cff-043d063c2038", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Further, we provide an illustration of the empirical convergence of the score.  All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots. ", "original_text": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008). "}, "hash": "63f95510fcd7cd62c9c043a7db94d0dc8b3ca8602e6b33a6f615dfabae6e5188", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "39810d01-fbb8-471f-aec6-c3f9ad282d00", "node_type": "1", "metadata": {"window": "4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. ", "original_text": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. "}, "hash": "0b9067fc3d8c0e47496e98dab3e97b12799d1cc60d8c49f34eef169c970b765f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The behavior is reciprocal for the typical observation x\u2080. ", "mimetype": "text/plain", "start_char_idx": 31787, "end_char_idx": 31846, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "39810d01-fbb8-471f-aec6-c3f9ad282d00", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. ", "original_text": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90a510a8-eac9-4682-a76e-74d9f5161ce8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "All other parameters being fixed, we increase the number of observations n when calculating the scores of the four selected observations; the empirical median and the boxplots of the scores computed over 100 random draws of the dataset are shown in Fig.  4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. ", "original_text": "The behavior is reciprocal for the typical observation x\u2080. "}, "hash": "12d3440c0fcef6bd5324d5aded929e7b42c7ca1647cd99352295f85b3926a461", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b23042f5-29e5-44f2-85a5-aaea95a441af", "node_type": "1", "metadata": {"window": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths. ", "original_text": "A second experiment illustrated in Fig. "}, "hash": "749ee238993e313f3701ce6842dd2a7d7c4afc7e5d47c590f045c05edd9ef4ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. ", "mimetype": "text/plain", "start_char_idx": 31846, "end_char_idx": 32169, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b23042f5-29e5-44f2-85a5-aaea95a441af", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths. ", "original_text": "A second experiment illustrated in Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39810d01-fbb8-471f-aec6-c3f9ad282d00", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "4.  First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. ", "original_text": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes. "}, "hash": "f854550a98632e0dc45912bd5cc26ffbd588b46443c99d7997bcf3d46cdb79a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2451d419-859f-4b99-b1c4-c1988c837227", "node_type": "1", "metadata": {"window": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. ", "original_text": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations."}, "hash": "7e9ca183b7de6c427685fe8d7e20871c449834a0325d2f6ca1a2dd635562b32b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A second experiment illustrated in Fig. ", "mimetype": "text/plain", "start_char_idx": 32169, "end_char_idx": 32209, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2451d419-859f-4b99-b1c4-c1988c837227", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. ", "original_text": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b23042f5-29e5-44f2-85a5-aaea95a441af", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, one observes score convergence and variance decrease in n. Further, let us take a closer look at the score tendencies on the example of x\u2080 and x\u2083.  The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths. ", "original_text": "A second experiment illustrated in Fig. "}, "hash": "03e9ea78112e9eec6b4fbd2dc9e08876968f1b4d1ce1f93aee85605eb4d6784b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f713e63-b686-45bf-af6d-3accdf2de3ae", "node_type": "1", "metadata": {"window": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** The figure shows two plots. "}, "hash": "3b3d468da6c22c94ac3f5a87acf1c5b467b2669c6bf314f97553c8869d4d392c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations.", "mimetype": "text/plain", "start_char_idx": 32209, "end_char_idx": 32454, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f713e63-b686-45bf-af6d-3accdf2de3ae", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** The figure shows two plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2451d419-859f-4b99-b1c4-c1988c837227", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The score of x\u2083 first increases (for dataset (a)) and slightly decreases (for dataset (b)) with growing n until n reaches \u03c8 = 64, which happens because this abnormal observation is isolated quite fast (and thus has short path length) but the c(\u03c8) in the denominator of the exponent of (1) increases in \u03c8. For n > 64, the score of x\u2083 decreases in n since h\u1d62(x\u2083) overestimates the real path length of x\u2083 for subsamples in which it is absent; frequency of such subsamples grows in n and equals, e.g., 0.872 for n = 500.  On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. ", "original_text": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations."}, "hash": "f8a6e8d8a65593bb768ff67bf978615b9a06a9014563af624750a647bfe7fca5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1316ee88-c035-4c1d-b468-f700f7adf524", "node_type": "1", "metadata": {"window": "The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. "}, "hash": "664ddf2006e584005789bac38b3c0f2130965520238d146afb9ae27f56b2ce29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure shows two plots. ", "mimetype": "text/plain", "start_char_idx": 32454, "end_char_idx": 32503, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1316ee88-c035-4c1d-b468-f700f7adf524", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f713e63-b686-45bf-af6d-3accdf2de3ae", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "On the other hand, this phenomenon allows to unmask grouped anomalies as mentioned in (Liu et al., 2008).  The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** The figure shows two plots. "}, "hash": "cbcbce93bcbf87e0c2ef1a52e9d6b074cf5c144b4c77e06280baf2d2ea35680d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3fc6b84-c18c-4b3d-aa1c-1d49aa1c78eb", "node_type": "1", "metadata": {"window": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. "}, "hash": "9ea9fac2c2e094f1ee1e41f14babb53f27ff6043b6014d00cdfece61d9f3a41e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. ", "mimetype": "text/plain", "start_char_idx": 32503, "end_char_idx": 32600, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f3fc6b84-c18c-4b3d-aa1c-1d49aa1c78eb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1316ee88-c035-4c1d-b468-f700f7adf524", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The behavior is reciprocal for the typical observation x\u2080.  Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion. "}, "hash": "5482e0bac74053d4641c04d7def1f8925496f9af38035278d4c277887b2aff90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21d2a0e2-3917-4f26-ae0c-05cdb556e664", "node_type": "1", "metadata": {"window": "A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "original_text": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths. "}, "hash": "0c6198588e0142089dab9ad5a5376a48296a7df804eb0aeb5b8a93c06032743e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. ", "mimetype": "text/plain", "start_char_idx": 32600, "end_char_idx": 32730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21d2a0e2-3917-4f26-ae0c-05cdb556e664", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "original_text": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3fc6b84-c18c-4b3d-aa1c-1d49aa1c78eb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Its FIF anomaly score starts by decreasing in n since x\u2080 tends to belong to the deepest branches of the trees and is always selected while \u03c8 < n. For larger n, the path length of x\u2080 is underestimated for subsamples where it is absent when growing the tree, which explains slight increase in the score before it stabilizes.  A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors. "}, "hash": "cbf2fc7edcaae051bce568d6e0aefe70e67a17af2d004cbcf4930b24af84c10c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "379d676a-48d1-41e8-941a-f55b16dedc16", "node_type": "1", "metadata": {"window": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots. ", "original_text": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. "}, "hash": "9541b394c76e8d7122fffa7b98eff995706cf33ff56eda5094ad03a0de313776", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths. ", "mimetype": "text/plain", "start_char_idx": 32730, "end_char_idx": 32808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "379d676a-48d1-41e8-941a-f55b16dedc16", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots. ", "original_text": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21d2a0e2-3917-4f26-ae0c-05cdb556e664", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A second experiment illustrated in Fig.  5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "original_text": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths. "}, "hash": "f27f4663a395dd52051ce4487e345eecc7349a8d6d5498bb047b0997040252ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a140c194-b96b-4f14-b617-3a4d77405abb", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). ", "original_text": "Both plots have a horizontal axis from 0.0 to 1.0.\n"}, "hash": "30045f061cbe276475f2f902eccbaeb6b7d124b835b766ee9b3b209f43a9b1c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. ", "mimetype": "text/plain", "start_char_idx": 32808, "end_char_idx": 32908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a140c194-b96b-4f14-b617-3a4d77405abb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). ", "original_text": "Both plots have a horizontal axis from 0.0 to 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "379d676a-48d1-41e8-941a-f55b16dedc16", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "5 is conducted to measure the impact of various dictionaries shortly cited in Section 3 and more thoroughly\n\n***\n**Figure 3: Datasets (a) (left) and (b) (right) containing, respectively, 500 and 200 functional paths with 4 selected observations. **\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots. ", "original_text": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted. "}, "hash": "401fa9caca09ac2becfbe5029fc62c2848f0511a3d516df49eeef0c03e1a9f48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8711d9e7-4bd0-410e-a75f-698759199e0a", "node_type": "1", "metadata": {"window": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\". ", "original_text": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "e4289925c34eb66875198c16090284739e4482384c004eff63612a37d248b784", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both plots have a horizontal axis from 0.0 to 1.0.\n", "mimetype": "text/plain", "start_char_idx": 32908, "end_char_idx": 32959, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8711d9e7-4bd0-410e-a75f-698759199e0a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\". ", "original_text": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a140c194-b96b-4f14-b617-3a4d77405abb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows two plots.  The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). ", "original_text": "Both plots have a horizontal axis from 0.0 to 1.0.\n"}, "hash": "3bcb79b4cd7f6bbf99a465c9200d0fe686e994b088e87f3c49356959ddee6b29", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6264f1b2-015d-4a1b-8fd3-b9cadf86fc1f", "node_type": "1", "metadata": {"window": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "db629a4a13155f774229b39796d01f1720390cd29c992d5ef87d526a25bd777c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "mimetype": "text/plain", "start_char_idx": 32959, "end_char_idx": 33092, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6264f1b2-015d-4a1b-8fd3-b9cadf86fc1f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8711d9e7-4bd0-410e-a75f-698759199e0a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot displays dataset (a), which consists of multiple paths resembling Brownian motion.  A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\". ", "original_text": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "557539fd2c6b2b51baa6ef12b5c924516a9f1f29b055ccd3c4ce35ebfb821aa5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "698a394e-df9e-4d47-93dd-0f77e41520d3", "node_type": "1", "metadata": {"window": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n", "original_text": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). "}, "hash": "5412b82a25baa112afa0beecbb97037b119e699cb5ec450e907edc6c35cba65b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 33092, "end_char_idx": 33190, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "698a394e-df9e-4d47-93dd-0f77e41520d3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n", "original_text": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6264f1b2-015d-4a1b-8fd3-b9cadf86fc1f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A gray shaded area represents the bulk of the data, and four specific paths (x\u2080, x\u2081, x\u2082, x\u2083) are highlighted in different colors.  The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "81776c053ca8128b9c88268d6108af688862f994a033eca7f77a98013ed93663", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6513e90c-cc22-458c-a250-e44d32d2a9c6", "node_type": "1", "metadata": {"window": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. ", "original_text": "Each panel contains a series of horizontal boxplots. "}, "hash": "bf1d61888328b7829b24f74dd58408e6aa898c033435b7a9c601d1c8de9b1a21", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "mimetype": "text/plain", "start_char_idx": 33190, "end_char_idx": 33290, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6513e90c-cc22-458c-a250-e44d32d2a9c6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. ", "original_text": "Each panel contains a series of horizontal boxplots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "698a394e-df9e-4d47-93dd-0f77e41520d3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot displays dataset (b), showing smoother, sinusoidal-like paths.  Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n", "original_text": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). "}, "hash": "3f731110ebbb7a3f7a7d0b25ff0da1318542ac9435fdb299453520778a2e991f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d63804e4-4aea-4b73-97ae-0f6ebd4c28cc", "node_type": "1", "metadata": {"window": "Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. ", "original_text": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). "}, "hash": "fb6865d3045226999ea90793c0e4b7389e94316f9f4966282ed104ee902a4bcf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each panel contains a series of horizontal boxplots. ", "mimetype": "text/plain", "start_char_idx": 33290, "end_char_idx": 33343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d63804e4-4aea-4b73-97ae-0f6ebd4c28cc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. ", "original_text": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6513e90c-cc22-458c-a250-e44d32d2a9c6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Again, a gray shaded area represents the bulk of the data, and four specific paths are highlighted.  Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. ", "original_text": "Each panel contains a series of horizontal boxplots. "}, "hash": "d8d6c8f6d98a509afb98708f560a0330a7793ff631d5b9f098bdcc8c227ab442", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfbb5c65-6b95-46f5-ad4c-01b939bf4fd0", "node_type": "1", "metadata": {"window": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. ", "original_text": "The x-axis represents the \"Score\". "}, "hash": "be33b236d89755fc7043c874e4e7e31ce0f8f7e6ba65827327136b6ac8378d0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). ", "mimetype": "text/plain", "start_char_idx": 33343, "end_char_idx": 33448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bfbb5c65-6b95-46f5-ad4c-01b939bf4fd0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. ", "original_text": "The x-axis represents the \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d63804e4-4aea-4b73-97ae-0f6ebd4c28cc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Both plots have a horizontal axis from 0.0 to 1.0.\n ***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. ", "original_text": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000). "}, "hash": "ef75f84807a7ceed6ad1545901b43c77d648e0ce4e02f9b4ac6a02263b7e4e12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fda7fe0b-9944-4777-89e0-4779dd8e6ffd", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. ", "original_text": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). "}, "hash": "cfb36f935945ae03b33b79a8b69373ef8183eb01376499076901e0aabcfdbd2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis represents the \"Score\". ", "mimetype": "text/plain", "start_char_idx": 33448, "end_char_idx": 33483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fda7fe0b-9944-4777-89e0-4779dd8e6ffd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. ", "original_text": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfbb5c65-6b95-46f5-ad4c-01b939bf4fd0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n***\n**Figure 4: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. ", "original_text": "The x-axis represents the \"Score\". "}, "hash": "77d54a2a1578f52b33b98686b9419dfd99fd00888df08c006f72d41d73bf4f96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e12c5ad-21d6-40ee-9178-a403887cab31", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). ", "original_text": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n"}, "hash": "ed5ade411950e92f21c192a0d378faff6271361782cad55302166138efd37bd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). ", "mimetype": "text/plain", "start_char_idx": 33483, "end_char_idx": 33593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e12c5ad-21d6-40ee-9178-a403887cab31", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). ", "original_text": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fda7fe0b-9944-4777-89e0-4779dd8e6ffd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. ", "original_text": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b). "}, "hash": "7787e6bb8ed3267d01601e71c08b8963b444fc5d885a0ca8633bffa0fef0cd9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2361965-c648-4600-8bd2-35e63a861720", "node_type": "1", "metadata": {"window": "Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig. ", "original_text": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. "}, "hash": "e60884da0e09109bf47b8357adcfce8c2f2a66badaa8bed5410653045a048073", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n", "mimetype": "text/plain", "start_char_idx": 33593, "end_char_idx": 33742, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2361965-c648-4600-8bd2-35e63a861720", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig. ", "original_text": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e12c5ad-21d6-40ee-9178-a403887cab31", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure consists of four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). ", "original_text": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n"}, "hash": "5768fb75526056979c912814695350f052c90a08d4b48d2c0cb3b6c2088c1c10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcaedc7c-3914-4d35-ab37-078e4957ab05", "node_type": "1", "metadata": {"window": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. ", "original_text": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. "}, "hash": "631e0a2d6c0cc4dc31ad5cbf691f5a81e348f7273253b8291f838c1085b0959f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. ", "mimetype": "text/plain", "start_char_idx": 33742, "end_char_idx": 33829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fcaedc7c-3914-4d35-ab37-078e4957ab05", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. ", "original_text": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2361965-c648-4600-8bd2-35e63a861720", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each panel contains a series of horizontal boxplots.  The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig. ", "original_text": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used. "}, "hash": "7d8e0fc40091c8a848d30afb716a493d211eae90d7efc3594ad334a913d58675", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3025bca8-8243-46c4-a878-f163da67518f", "node_type": "1", "metadata": {"window": "The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n", "original_text": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. "}, "hash": "5f3afd625f37849af16c399d652fd5b9f00c5e251fdc39c97621595ba2721b83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. ", "mimetype": "text/plain", "start_char_idx": 33829, "end_char_idx": 33939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3025bca8-8243-46c4-a878-f163da67518f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n", "original_text": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcaedc7c-3914-4d35-ab37-078e4957ab05", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis, labeled \"Sample size\", shows different sample sizes (10, 20, 50, 100, 200, 500, 1000, 2000).  The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. ", "original_text": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets. "}, "hash": "18d3c9558aeee0f0de79bbbffa1c6702f7612adc3937430bdc784cb7b93bac21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58276f16-12e4-43a9-997b-f296c7666bb9", "node_type": "1", "metadata": {"window": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n", "original_text": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. "}, "hash": "f4ed93028939dbfbac21da8c73d5dae29c1e23b59f1c6d2acbffa9df472698a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. ", "mimetype": "text/plain", "start_char_idx": 33939, "end_char_idx": 34073, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58276f16-12e4-43a9-997b-f296c7666bb9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n", "original_text": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3025bca8-8243-46c4-a878-f163da67518f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis represents the \"Score\".  For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n", "original_text": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score. "}, "hash": "069cb47184f311533122d754ecfeb4696f81feda1ac2e67abd4377c9a32e25f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a541cfcf-1b24-47ad-8a0e-788a3fcbddd4", "node_type": "1", "metadata": {"window": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. ", "original_text": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). "}, "hash": "47dd3ed3fea5052a6ef00b26cf67a7a6553890c371262cf51d1cab9b58d7c9f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. ", "mimetype": "text/plain", "start_char_idx": 34073, "end_char_idx": 34263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a541cfcf-1b24-47ad-8a0e-788a3fcbddd4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. ", "original_text": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58276f16-12e4-43a9-997b-f296c7666bb9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each sample size, there are two boxplots: an orange one for dataset (a) and a purple one for dataset (b).  The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n", "original_text": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081. "}, "hash": "db63d602fad08eb263917b4924bad3f642d672457de411e7c59d48c81ea44b07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3685e17d-3916-4322-a54f-6b518503c358", "node_type": "1", "metadata": {"window": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "For the scalar product L\u2082 on derivatives (see Fig. "}, "hash": "39b128ee795bdd58b643e6925f27aaf58e0fe5ba6ee272d19491f5158e3ae17f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). ", "mimetype": "text/plain", "start_char_idx": 34263, "end_char_idx": 34579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3685e17d-3916-4322-a54f-6b518503c358", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "For the scalar product L\u2082 on derivatives (see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a541cfcf-1b24-47ad-8a0e-788a3fcbddd4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The plots illustrate how the distribution of the anomaly score changes with the sample size for normal (X\u2080) and anomalous (X\u2081, X\u2082, X\u2083) observations.\n ***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. ", "original_text": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary). "}, "hash": "32eb3643633272ceb223e24adbdb6b55f9a24a3050a27113a5b48b77baa33476", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88abd7a1-3f82-4e5a-ba5d-9545a61909e7", "node_type": "1", "metadata": {"window": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. "}, "hash": "bec3b4948a245da630bce3bcdd5fdb1432b1af5c418d46683f379dd6039ba95f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For the scalar product L\u2082 on derivatives (see Fig. ", "mimetype": "text/plain", "start_char_idx": 34579, "end_char_idx": 34630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88abd7a1-3f82-4e5a-ba5d-9545a61909e7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3685e17d-3916-4322-a54f-6b518503c358", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\ndescribed in Section B of the Supplementary Materials; L\u2082 scalar product is used.  One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "For the scalar product L\u2082 on derivatives (see Fig. "}, "hash": "f16f3d016d7990f6fd19004f67085850793437f744894b5d17c77716b4d63f8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b45579b-6567-4f66-b5b5-dd2da4a0ace0", "node_type": "1", "metadata": {"window": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. ", "original_text": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n"}, "hash": "64afd5ed91275ac9354295502d3948f1e825e334df5ac1ce35deaebd396519a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. ", "mimetype": "text/plain", "start_char_idx": 34630, "end_char_idx": 34857, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1b45579b-6567-4f66-b5b5-dd2da4a0ace0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. ", "original_text": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88abd7a1-3f82-4e5a-ba5d-9545a61909e7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One observes that the variance of the score seems to be mostly stable across dictionaries, for both datasets.  Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product. "}, "hash": "ff32aa14d1cb3aca8d2f3739fb7bff2db264294f3381979b50b1e02a0c99341f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "372820f5-31b4-4f08-aad6-76cc073a8c3d", "node_type": "1", "metadata": {"window": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\". ", "original_text": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n"}, "hash": "7285000c26e72b1fad57634779813d7e96b474be3b8e97668eddff4ef764a14f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n", "mimetype": "text/plain", "start_char_idx": 34857, "end_char_idx": 35094, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "372820f5-31b4-4f08-aad6-76cc073a8c3d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\". ", "original_text": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b45579b-6567-4f66-b5b5-dd2da4a0ace0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, random dictionaries like uniform indicator (UI) or Brownian motion (B) do not introduce additional variance into the FIF score.  Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. ", "original_text": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n"}, "hash": "ce43be1b1f3a52cc7ce6b647a145863b2cfa7f78b0baea6887f46110a834ca27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d5e32a7-8938-4fe0-a3a2-23831beff134", "node_type": "1", "metadata": {"window": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "original_text": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. "}, "hash": "3efcc566309818dd7aa38c63a42d7b72cd2965d2239899f389f883a6c32ccd46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n", "mimetype": "text/plain", "start_char_idx": 35094, "end_char_idx": 35248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d5e32a7-8938-4fe0-a3a2-23831beff134", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "original_text": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "372820f5-31b4-4f08-aad6-76cc073a8c3d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Since we know the expected ranking of the scores, we can observe that FIF relying on the Self, UI, and dyadic indicator (DI) dictionaries fail to make a strong difference between x\u2080 and x\u2081.  Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\". ", "original_text": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n"}, "hash": "6a3f29715c6b23f478f6e6740fe6ef4b47405cca1021a0a5f8fdf007259f814e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbf66b9e-3a94-45c3-bf6e-827c6664cc8a", "node_type": "1", "metadata": {"window": "For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "ab416aea9f3b36d69081737e4bbf5f91960f2ea64b84b308cf07fd386b604ba0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. ", "mimetype": "text/plain", "start_char_idx": 35248, "end_char_idx": 35404, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dbf66b9e-3a94-45c3-bf6e-827c6664cc8a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d5e32a7-8938-4fe0-a3a2-23831beff134", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Since x\u2081 differs only slightly in the amplitude from the general pattern, these dictionaries seem insufficient to capture this fine dissimilarity: while Self and DI dictionaries simply do not contain enough elements, UI dictionary is to simple to capture this difference (it shares this feature with DI dictionary).  For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "original_text": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product. "}, "hash": "4b3fa55295c0efc95e2e69632f66593036ebe0cacb74583536afeb4806be55be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dada062-791b-42b7-9eb1-1fd9206bfa09", "node_type": "1", "metadata": {"window": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2. ", "original_text": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "f6ebfcfc53aeab27c56840401f2ebc6f18ebfec6412495fddfea86fb362a908e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 35404, "end_char_idx": 35502, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1dada062-791b-42b7-9eb1-1fd9206bfa09", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2. ", "original_text": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbf66b9e-3a94-45c3-bf6e-827c6664cc8a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For the scalar product L\u2082 on derivatives (see Fig.  18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "d0d78d8d804048edfc2b5637e456ef6455913000b0ccd81adbdd72a2e19057de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e57c7d9-aad0-4d32-8fc9-29944e16546b", "node_type": "1", "metadata": {"window": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). ", "original_text": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. "}, "hash": "d05bb007177cc52a7ea8b345fa8d1a77a7c45ee732438730d2460ff734cfc6ad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. ", "mimetype": "text/plain", "start_char_idx": 35502, "end_char_idx": 35593, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e57c7d9-aad0-4d32-8fc9-29944e16546b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). ", "original_text": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dada062-791b-42b7-9eb1-1fd9206bfa09", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "18 in the Supplementary Materials), distinguishing anomalies for the Brownian motion becomes difficult since they differ mainly in location, while for a sine function the scores resemble those with the usual L\u2082 scalar product.  Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2. ", "original_text": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "85e7fd1c7dbde45bb92fa1e50dad0b43e897822cfb4be4d95f9d7f98fb92e388", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca810e1e-9abc-4cdb-bf0e-afeed76ea915", "node_type": "1", "metadata": {"window": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details). ", "original_text": "The x-axis is the \"Score\". "}, "hash": "61e48e4c08125564e53915ecd5894000b575ef3732286f6e346bf70d7f711595", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. ", "mimetype": "text/plain", "start_char_idx": 35593, "end_char_idx": 35743, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca810e1e-9abc-4cdb-bf0e-afeed76ea915", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details). ", "original_text": "The x-axis is the \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e57c7d9-aad0-4d32-8fc9-29944e16546b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, even though \u2013 as seen in Section 3.2 \u2013 capturing different types of anomalies is one of the general strengths of the FIF algorithm, the dictionary may still have an impact on detection of functional anomalies in particular cases.\n\n More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). ", "original_text": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis. "}, "hash": "9c56ce93a8016cdb01bf9139086c1aa63c202662835005ad42324e0112c2dc53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a04a58e-bb86-4cca-952d-c40694b641c4", "node_type": "1", "metadata": {"window": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. ", "original_text": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). "}, "hash": "3673bcfcf1473771995af8418f47c2de49cbfcb5fa042c15075005df2d17988b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is the \"Score\". ", "mimetype": "text/plain", "start_char_idx": 35743, "end_char_idx": 35770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1a04a58e-bb86-4cca-952d-c40694b641c4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. ", "original_text": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca810e1e-9abc-4cdb-bf0e-afeed76ea915", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "More experiments were run regarding the stability of the algorithm, but for sake of space, we describe them in Section C of the Supplementary Materials.\n\n ***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details). ", "original_text": "The x-axis is the \"Score\". "}, "hash": "57a86da14cf32a461a5c0a78e6db3c79c917eab8472741c04f847311d7969e74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad3ed7de-dbfc-4671-a096-2d53f1cfbc5f", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. ", "original_text": "This allows for a visual comparison of the performance and stability of different dictionaries.\n"}, "hash": "d7734c7baf0f5b614d834194637f2212cd82df2ea0496e02bbb3f5e7a12243eb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "mimetype": "text/plain", "start_char_idx": 35770, "end_char_idx": 35866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad3ed7de-dbfc-4671-a096-2d53f1cfbc5f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. ", "original_text": "This allows for a visual comparison of the performance and stability of different dictionaries.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a04a58e-bb86-4cca-952d-c40694b641c4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 5: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. ", "original_text": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b). "}, "hash": "76a2dccba134ba64566b6c450734399bfe7954cbb7c3c3a1771b50e7c2c013a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "983b9874-ac27-499c-82bb-1675978cffba", "node_type": "1", "metadata": {"window": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. ", "original_text": "***\n\n### 4.2. "}, "hash": "c158508c879054935d63ee81628545739e407c407282a10f7a5efebde4deb0aa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This allows for a visual comparison of the performance and stability of different dictionaries.\n", "mimetype": "text/plain", "start_char_idx": 35866, "end_char_idx": 35962, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "983b9874-ac27-499c-82bb-1675978cffba", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. ", "original_text": "***\n\n### 4.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad3ed7de-dbfc-4671-a096-2d53f1cfbc5f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. ", "original_text": "This allows for a visual comparison of the performance and stability of different dictionaries.\n"}, "hash": "101f2d99db655126e9bad3fa2ef637789392fdf0ec2708750811764caceb6754", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de406e97-687f-4b75-9bf6-b2cb660a92cb", "node_type": "1", "metadata": {"window": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. ", "original_text": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). "}, "hash": "b0d8d8f9738a47bbb17d823faa96899ca77b582bc488c0ab4af6a0b6c51afb2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n### 4.2. ", "mimetype": "text/plain", "start_char_idx": 35962, "end_char_idx": 35976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de406e97-687f-4b75-9bf6-b2cb660a92cb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. ", "original_text": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "983b9874-ac27-499c-82bb-1675978cffba", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure also has four panels for observations X\u2080, X\u2081, X\u2082, and X\u2083.  Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. ", "original_text": "***\n\n### 4.2. "}, "hash": "6364ab42e5593970703e05d881833ea257006b98eb9fb8a4dccba54b0cef8cde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07f2ef43-3ec6-42cc-9453-e4eb325b9489", "node_type": "1", "metadata": {"window": "The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n", "original_text": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details). "}, "hash": "e7e5514ad400210604f2234ab94f5b2d2f6186e3160d8b607a9f7dbb251dfa8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). ", "mimetype": "text/plain", "start_char_idx": 35976, "end_char_idx": 36167, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07f2ef43-3ec6-42cc-9453-e4eb325b9489", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n", "original_text": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de406e97-687f-4b75-9bf6-b2cb660a92cb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each panel shows horizontal boxplots comparing the anomaly scores obtained with different dictionaries (Self, BB, UI, DI, Cos, B, MHW) on the y-axis.  The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. ", "original_text": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015). "}, "hash": "260e667e0cfc09288f9d6beaf4c525325b3c5cddfb2206fd149565aa1360ea5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4b64fd9-ef1a-496e-8cb8-7559f09ac3c4", "node_type": "1", "metadata": {"window": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). ", "original_text": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. "}, "hash": "121b7966d54b3649f6b1deb2c7ce3a60c51484c145727484c4a18f9633a79273", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details). ", "mimetype": "text/plain", "start_char_idx": 36167, "end_char_idx": 36265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4b64fd9-ef1a-496e-8cb8-7559f09ac3c4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). ", "original_text": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07f2ef43-3ec6-42cc-9453-e4eb325b9489", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is the \"Score\".  For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n", "original_text": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details). "}, "hash": "67a332d892f96e9ac795c5152017f5aea1e08054fa1bf99b85640983aafa87a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9fb1f79f-6c8e-4b18-a7c4-218fbefdd208", "node_type": "1", "metadata": {"window": "This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. ", "original_text": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. "}, "hash": "ba8d567e4245d4ae0e345c5993c90aa982cce2758ebb5fe14ade68d8d5b05dce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. ", "mimetype": "text/plain", "start_char_idx": 36265, "end_char_idx": 36472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9fb1f79f-6c8e-4b18-a7c4-218fbefdd208", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. ", "original_text": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4b64fd9-ef1a-496e-8cb8-7559f09ac3c4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each dictionary, there are two boxplots: orange for dataset (a) and purple for dataset (b).  This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). ", "original_text": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison. "}, "hash": "5a613170fc88ac32dadfb4b8635881106903f67c94ba786df4c0f6981a5e823e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e132402f-37d8-479f-94e7-bcb49387ef30", "node_type": "1", "metadata": {"window": "***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. ", "original_text": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. "}, "hash": "a682a4ffc804fa1ad9553dc6bf202b0922a1aa4005a0581b7483e82c55e71bf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. ", "mimetype": "text/plain", "start_char_idx": 36472, "end_char_idx": 36626, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e132402f-37d8-479f-94e7-bcb49387ef30", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. ", "original_text": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9fb1f79f-6c8e-4b18-a7c4-218fbefdd208", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This allows for a visual comparison of the performance and stability of different dictionaries.\n ***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. ", "original_text": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set. "}, "hash": "f181eb6e0cc3dbed43650bf88ef12281d313a0c9786c2444dd5a98a29393bf17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd15ed0-4c0a-4ca0-b31d-15b1cbff450f", "node_type": "1", "metadata": {"window": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n", "original_text": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. "}, "hash": "2f8533f5667f8e8c0070cbeab806711d7d8b866b19d648c322f65493af88022b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. ", "mimetype": "text/plain", "start_char_idx": 36626, "end_char_idx": 36763, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fd15ed0-4c0a-4ca0-b31d-15b1cbff450f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n", "original_text": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e132402f-37d8-479f-94e7-bcb49387ef30", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n### 4.2.  Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. ", "original_text": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set. "}, "hash": "049032ac615614f0c6383829a27acd7715266a238e40bf559a2023f6ae81ba62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "207471aa-77d2-4367-97cd-3766ad690edd", "node_type": "1", "metadata": {"window": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. ", "original_text": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n"}, "hash": "f9a1b4433385f5d7590fd625a4ceaaa4c0c6f6f0789fd73c4934f3202bf058b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. ", "mimetype": "text/plain", "start_char_idx": 36763, "end_char_idx": 36897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "207471aa-77d2-4367-97cd-3766ad690edd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. ", "original_text": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fd15ed0-4c0a-4ca0-b31d-15b1cbff450f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Real Data Benchmarking\n\nTo explore the performance of the proposed FIF algorithm, we conduct a comparative study using 13 classification datasets from the UCR repository (Chen et al., 2015).  We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n", "original_text": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model. "}, "hash": "8b144d13aecce8b041873fcea89d817f91c766f624b2b5fc557669d447c0cf13", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "015a09c0-b1fc-4585-bc87-76f8ba8d2cfc", "node_type": "1", "metadata": {"window": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. ", "original_text": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). "}, "hash": "7e77f910c40eb667216b51d88ec8888aac214e8b50e14ca826d1fb0a99e05b02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n", "mimetype": "text/plain", "start_char_idx": 36897, "end_char_idx": 37024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "015a09c0-b1fc-4585-bc87-76f8ba8d2cfc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. ", "original_text": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "207471aa-77d2-4367-97cd-3766ad690edd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We consider the larger class as normal and some of others as anomalies (see Table 1 for details).  When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. ", "original_text": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n"}, "hash": "00cc59cb08f9b43de47f796d81c9f9ed47cc039d4c841b5121b3d81c6850ee7a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6f02dc69-e268-4f22-a261-0033e028e568", "node_type": "1", "metadata": {"window": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. ", "original_text": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. "}, "hash": "2eb61dcb7ea7b7382fcb8ca7edc6db77c1fb63568a69c44cdea373e57ddead4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). ", "mimetype": "text/plain", "start_char_idx": 37024, "end_char_idx": 37281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6f02dc69-e268-4f22-a261-0033e028e568", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. ", "original_text": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "015a09c0-b1fc-4585-bc87-76f8ba8d2cfc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When classes are balanced, i.e for 9 datasets out of 13, we keep only part of the anomaly class to reduce its size, always taking the same observations (at the beginning of the table) for a fair comparison.  Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. ", "original_text": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]). "}, "hash": "fd8d36e7e041d0708ad75905cff445ae1701f50c9665a155816573d6b0467f0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7af4933-32ac-463c-a171-4a1af06ccb4a", "node_type": "1", "metadata": {"window": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. ", "original_text": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. "}, "hash": "00244a1d5e7f8a14ba0fc5603c67b1cdfa16f5933350d992da63c41c0189f586", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. ", "mimetype": "text/plain", "start_char_idx": 37281, "end_char_idx": 37421, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7af4933-32ac-463c-a171-4a1af06ccb4a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. ", "original_text": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6f02dc69-e268-4f22-a261-0033e028e568", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Since the datasets are already split into train/test sets, we use the train part (without labels) to build the FIF and compute the score on the test set.  We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. ", "original_text": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings. "}, "hash": "2f06b5f45b09e00c4b52a55ba8833fdad9d5babd1bd73fdf0e4d532f3ecaa0c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c0e7036-dd0b-4cb9-b58e-9b495fe97fb4", "node_type": "1", "metadata": {"window": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. ", "original_text": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n"}, "hash": "be70b3dac60f63b4ae246f47d9bcebef8a985eb818f2fd1becfaae1936c28847", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. ", "mimetype": "text/plain", "start_char_idx": 37421, "end_char_idx": 37781, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c0e7036-dd0b-4cb9-b58e-9b495fe97fb4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. ", "original_text": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7af4933-32ac-463c-a171-4a1af06ccb4a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We assess the performance of the algorithm by measuring an Area Under the Receiver Operation Characteristic curve (AUC) on the test set.  Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. ", "original_text": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis. "}, "hash": "c5b097d3b13da70adf7530570b897693735f9aeef8f1ef598aba555bc378943c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c32ddb4-0b47-4a6d-ac19-df83195d3d66", "node_type": "1", "metadata": {"window": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n", "original_text": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. "}, "hash": "cb09b7e2fef3b02ec065a5d3232ff4e40f8b2c62fceb2f744200c58789132994", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n", "mimetype": "text/plain", "start_char_idx": 37781, "end_char_idx": 37926, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5c32ddb4-0b47-4a6d-ac19-df83195d3d66", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n", "original_text": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c0e7036-dd0b-4cb9-b58e-9b495fe97fb4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Both train and test sets are rarely used during learning in unsupervised setting since labels are unavailable when fitting the model.  Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. ", "original_text": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n"}, "hash": "ffbf8c842dddf5a19b2152fd953031a5fa730a0bce708bd71ceb143b6bda5cd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90c53563-ac93-4dda-8b37-073e6dac86be", "node_type": "1", "metadata": {"window": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. ", "original_text": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. "}, "hash": "88f93df84e54d93098b73067626fe34e7b1004cd7883bf34174c456713ed0d68", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. ", "mimetype": "text/plain", "start_char_idx": 37926, "end_char_idx": 38071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90c53563-ac93-4dda-8b37-073e6dac86be", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. ", "original_text": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c32ddb4-0b47-4a6d-ac19-df83195d3d66", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, when fitting the models on unlabeled training data, good performances on the test set show a good generalization power.\n\n **Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n", "original_text": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best. "}, "hash": "fdd0318ea442eb1bebd074f4dbcd4ea207a4775b7732615d8e63b7bac43c43a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9b9dc97-7bbc-4f9c-874f-6a636489805e", "node_type": "1", "metadata": {"window": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points.", "original_text": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. "}, "hash": "ab8f893a2fa48fddd9fb57f2bc634d5f63a76ba62fd6d6fbf9172829b0f27bd3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. ", "mimetype": "text/plain", "start_char_idx": 38071, "end_char_idx": 38193, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9b9dc97-7bbc-4f9c-874f-6a636489805e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points.", "original_text": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90c53563-ac93-4dda-8b37-073e6dac86be", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Competitors** FIF is considered with two finite size dictionaries *dyadic indicator*, the *self-data* and the infinite size dictionary *cosines* (with \u03b1 = 1 and \u03b1 = 0); its parameters are set N = 100, \u03c8 = min(256, n) and the height limit to = [log\u2082(\u03c8)]).  We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. ", "original_text": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets. "}, "hash": "5f8ddead8b394d8e3a46845da60f8e1443b4ddebc4b19eaff05208d6247181cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72dd9eb5-ed39-4884-a49b-5848d46f4292", "node_type": "1", "metadata": {"window": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. ", "original_text": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. "}, "hash": "7799514cf6b802bac6ff516f382fc63a6ce353e6fdfd39d5961503f3171c48d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. ", "mimetype": "text/plain", "start_char_idx": 38193, "end_char_idx": 38329, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "72dd9eb5-ed39-4884-a49b-5848d46f4292", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. ", "original_text": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9b9dc97-7bbc-4f9c-874f-6a636489805e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We contrast the FIF method with three most used multivariate anomaly detection techniques and two functional depths, with default settings.  The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points.", "original_text": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t. "}, "hash": "168a7d5300f70dc6ca75635f1571a41b5fa736a4e0ff7f805b94bd73c85142fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc7c5f2c-cfd9-4b4a-b66f-23607331af16", "node_type": "1", "metadata": {"window": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best.", "original_text": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. "}, "hash": "32e9ac14404fb86e37bd7721044a7423999a2250bbf152f420b5eb265f42f78b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. ", "mimetype": "text/plain", "start_char_idx": 38329, "end_char_idx": 38465, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc7c5f2c-cfd9-4b4a-b66f-23607331af16", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best.", "original_text": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72dd9eb5-ed39-4884-a49b-5848d46f4292", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The multivariate methods\u2014isolation forest (IF) (Liu et al., 2008), local outlier factor (LOF) (Breunig et al., 2000), and one-class support vector machine (OCSVM) (Sch\u00f6lkopf et al., 2001) \u2014 are employed after dimension reduction by Functional PCA keeping 20 principal components with largest eigenvalues after a preliminary step of filtering using Haar basis.  The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. ", "original_text": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary. "}, "hash": "cd531edd150b3b99a45522eb2f6645888aa47af3148a04e1531bba7684741819", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "613a06d3-aee4-4b58-b799-16814b4367af", "node_type": "1", "metadata": {"window": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. ", "original_text": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n"}, "hash": "a4f61a4bca42ac21fc99a275b851659292a6dda6444c286af96c55bc223dd573", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. ", "mimetype": "text/plain", "start_char_idx": 38465, "end_char_idx": 38563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "613a06d3-aee4-4b58-b799-16814b4367af", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. ", "original_text": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc7c5f2c-cfd9-4b4a-b66f-23607331af16", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The depths are the random projection halfspace depth (Cuevas et al., 2007) and the functional Stahel-Donoho outlyingness (Hubert et al., 2015).\n\n **Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best.", "original_text": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets. "}, "hash": "588e7ed9c8f1c2e64ed7fee9de16c96c02e0c57a7b3bc142a984f4a8e7a9fbc7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01f38fa1-af02-4208-87e0-d6a184b7b1c2", "node_type": "1", "metadata": {"window": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. ", "original_text": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. "}, "hash": "54c62cd90ea5e7d188bd8608717e8235f5ba4c4c28ed7ff4b24c2de1dfedf4b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n", "mimetype": "text/plain", "start_char_idx": 38563, "end_char_idx": 38716, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01f38fa1-af02-4208-87e0-d6a184b7b1c2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. ", "original_text": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "613a06d3-aee4-4b58-b799-16814b4367af", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Analysis of the results** Taking into account the complexity of the functional data, as expected there is no method performing generally best.  Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. ", "original_text": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n"}, "hash": "b8c5b34f9d2b3b675b05bcd4aaa19b234d291b0231879accf85f88d81853ca5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a14e3cda-107e-4e66-94d9-00215813559d", "node_type": "1", "metadata": {"window": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time. ", "original_text": "p is the number of discretization points."}, "hash": "d421f400aea2f87c11cee2139815370f44b944376b68f9594c22ee572bf46f32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. ", "mimetype": "text/plain", "start_char_idx": 38716, "end_char_idx": 38835, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a14e3cda-107e-4e66-94d9-00215813559d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time. ", "original_text": "p is the number of discretization points."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01f38fa1-af02-4208-87e0-d6a184b7b1c2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, FIF performs well in most of the cases, giving best results for 10 datasets and second best for 6 datasets.  It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. ", "original_text": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies. "}, "hash": "f26189707f0a86624638b548efe921b9237063cc1b3657721bd64f4eb6cb080d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09438b08-7c6f-4aa8-bac9-b30bddb03272", "node_type": "1", "metadata": {"window": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. ", "original_text": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. "}, "hash": "8e438fe71b9c7e7b1652b2b40e1246b1baebd8e3c986129922c2138c828d8eb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "p is the number of discretization points.", "mimetype": "text/plain", "start_char_idx": 38835, "end_char_idx": 38876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09438b08-7c6f-4aa8-bac9-b30bddb03272", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. ", "original_text": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a14e3cda-107e-4e66-94d9-00215813559d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It is worth to mention that the dictionary plays an important role in identifying anomalies, while FIF seems to be rather robust w.r.t.  other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time. ", "original_text": "p is the number of discretization points."}, "hash": "9be69e00d84854b22d8e42e761476f4d8242bb39e44ebf5f6c33fbbfa8a9760a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e64737cb-c24c-482c-8e93-d6bb8630f4a3", "node_type": "1", "metadata": {"window": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. ", "original_text": "Bold numbers correspond to the best result while italics to the second best."}, "hash": "edb9bb815668393cbe5934d7944c7f0d28055c475f09df8fd6ef07f93a68f522", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. ", "mimetype": "text/plain", "start_char_idx": 38876, "end_char_idx": 39832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e64737cb-c24c-482c-8e93-d6bb8630f4a3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. ", "original_text": "Bold numbers correspond to the best result while italics to the second best."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09438b08-7c6f-4aa8-bac9-b30bddb03272", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "other parameters: The \u201cCinECGTorso\u201d dataset contains anomalies differing in location shift which are captured by the cosine dictionary.  Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. ", "original_text": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set. "}, "hash": "66ac16c71c08c163353a6c4b7ceac3a0b96cda2a5c4228bd3ad176aa5bc424c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cedd297-dce6-43a0-a8b9-f031f372e1b1", "node_type": "1", "metadata": {"window": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n", "original_text": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. "}, "hash": "088652d7343d8c4def870df7bcbcf6611dca2e47ecfa813e019b6423d238f73a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bold numbers correspond to the best result while italics to the second best.", "mimetype": "text/plain", "start_char_idx": 39832, "end_char_idx": 39908, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3cedd297-dce6-43a0-a8b9-f031f372e1b1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n", "original_text": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e64737cb-c24c-482c-8e93-d6bb8630f4a3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Dyadic indicator dictionary allows to detect local anomalies in \"TwoLeadECG\" and \"Yoga\" datasets.  Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. ", "original_text": "Bold numbers correspond to the best result while italics to the second best."}, "hash": "c86a01debe1f34e4dabdbe35052944597118c66a36f5de54c42c8e6920c9784a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63826386-0175-47c8-84dd-96578dccb1bf", "node_type": "1", "metadata": {"window": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). ", "original_text": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. "}, "hash": "e2523cb0db111ebb35090e3f3dc13169dadbf9a43819c87f203948fe1db92ab6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. ", "mimetype": "text/plain", "start_char_idx": 39908, "end_char_idx": 41185, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "63826386-0175-47c8-84dd-96578dccb1bf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). ", "original_text": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cedd297-dce6-43a0-a8b9-f031f372e1b1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Self-data dictionary seems suited for Datasets \u201cSonyRobotAI2\" and \"Starlight Curves\" whose challenge is to cope with many different types of anomalies.\n\n **Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n", "original_text": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5. "}, "hash": "e3488eb17999a441473c629439610065abf765ef2e21577c37deeff2783ed3ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97fb765a-90f8-412e-a788-9357f4c9ea83", "node_type": "1", "metadata": {"window": "p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). ", "original_text": "when the quantity of interest lies in R^d for each moment of time. "}, "hash": "c08e59769bda25edd3886af0cef6df6d2feafd1e0725b1ea0a5743b9661f7cc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. ", "mimetype": "text/plain", "start_char_idx": 41185, "end_char_idx": 41313, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97fb765a-90f8-412e-a788-9357f4c9ea83", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). ", "original_text": "when the quantity of interest lies in R^d for each moment of time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63826386-0175-47c8-84dd-96578dccb1bf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Table 1: Datasets considered in performance comparison: n is the number of instances, na is the number of anomalies.  p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). ", "original_text": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e. "}, "hash": "ef0a97b016a5d22d3786672607fd6bcf7aa9d5f8acbff7c690b3394b61d6de3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79c12a4d-4e97-4c91-91fc-f5241a08f37e", "node_type": "1", "metadata": {"window": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. ", "original_text": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. "}, "hash": "28d72356c7034b110bf4d3908d7a6a0d6569fb4908718205e2019025ae3521f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "when the quantity of interest lies in R^d for each moment of time. ", "mimetype": "text/plain", "start_char_idx": 41313, "end_char_idx": 41380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "79c12a4d-4e97-4c91-91fc-f5241a08f37e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. ", "original_text": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97fb765a-90f8-412e-a788-9357f4c9ea83", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "p is the number of discretization points. **\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). ", "original_text": "when the quantity of interest lies in R^d for each moment of time. "}, "hash": "0cddcfb9d4800e56b1a223c9eccdb6c20e30a8026df11fd2a1ea9220cdde9072", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b7ab046-6609-4f45-8a77-ab69ad1c9050", "node_type": "1", "metadata": {"window": "Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al. ", "original_text": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. "}, "hash": "d4c77276c3720cfaafd70ed4a1a9fd1b846329931baa9f96931985751c88c2bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. ", "mimetype": "text/plain", "start_char_idx": 41380, "end_char_idx": 41560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b7ab046-6609-4f45-8a77-ab69ad1c9050", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al. ", "original_text": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79c12a4d-4e97-4c91-91fc-f5241a08f37e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n| | p | training: n_a / n | testing: n_a / n | normal lab | anomaly lab |\n| :--- | :-: | :--- | :--- | :--- | :--- |\n| Chinatown | 24 | 4 / 14 (29%) | 95 / 345 | 2 | 1 |\n| Coffee | 286 | 5 / 19 (26%) | 6 / 19 | 1 | 0 |\n| ECGFiveDays | 136 | 2 / 16 (12%) | 53 / 481 | 1 | 2 |\n| ECG200 | 96 | 31 / 100 (31%) | 36 / 100 | 1 | -1 |\n| Handoutlines | 2709 | 362 / 1000 (36%) | 133 / 370 | 1 | 0 |\n| SonyRobotAI1 | 70 | 6 / 20 (30%) | 343 / 601 | 2 | 1 |\n| SonyRobotAI2 | 65 | 4 / 20 (20%) | 365 / 953 | 2 | 1 |\n| StarLightCurves | 1024 | 100 / 673 (15%) | 3482 / 8236 | 3 | 1 and 2 |\n| TwoLeadECG | 82 | 2 / 14 (14%) | 570 / 1139 | 1 | 2 |\n| Yoga | 426 | 10 / 173 (06%) | 1393 / 3000 | 2 | 1 |\n| EOGHorizontal | 1250 | 10 / 40 (25%) | 30 / 61 | 5 | 6 |\n| CinECGTorso | 1639 | 4 / 16 (25%) | 345 / 688 | 3 | 4 |\n| ECG5000 | 140 | 31 / 323 (10%) | 283 / 2910 | 1 | 3,4 and 5 |\n\n**Table 2: AUC of different anomaly detection methods calculated on the test set.  Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. ", "original_text": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H. "}, "hash": "a679abef36ece1e8582fb2db2871428261370064c94bee515624495839c9151c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fff2cebd-1507-48e5-bda6-909de281fb90", "node_type": "1", "metadata": {"window": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. ", "original_text": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n"}, "hash": "4b71ac09d6ab99cfbe5f33b32ff25e1b80e5f34145c22fbd5d5b66ca263af703", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. ", "mimetype": "text/plain", "start_char_idx": 41560, "end_char_idx": 41705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fff2cebd-1507-48e5-bda6-909de281fb90", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. ", "original_text": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b7ab046-6609-4f45-8a77-ab69ad1c9050", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Bold numbers correspond to the best result while italics to the second best. **\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al. ", "original_text": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3. "}, "hash": "fe9efddea7826280ad6c351fdda405629a2984c8dcd105a7a6717bef350b6052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c1aeea0-6e74-44c8-8616-c0d9be824cf9", "node_type": "1", "metadata": {"window": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. ", "original_text": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). "}, "hash": "1ae2250cf848b6ff8c4714e5494f247de1b7b9de37b374728af80d6013fd758f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n", "mimetype": "text/plain", "start_char_idx": 41705, "end_char_idx": 41886, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c1aeea0-6e74-44c8-8616-c0d9be824cf9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. ", "original_text": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fff2cebd-1507-48e5-bda6-909de281fb90", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n| Methods: | DI_L2 | Cos_Sob | Cos_L2 | Self_L2 | IF | LOF | OCSVM | fHD_RP | fS_DO |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Chinatown | *0.93* | 0.82 | 0.74 | 0.77 | 0.69 | 0.68 | 0.70 | 0.76 | **0.98** |\n| Coffee | 0.76 | **0.87** | 0.73 | *0.77* | 0.60 | 0.51 | 0.59 | 0.74 | 0.67 |\n| ECGFiveDays | 0.78 | 0.75 | 0.81 | 0.56 | 0.81 | *0.89* | **0.90** | 0.60 | 0.81 |\n| ECG200 | *0.86* | **0.88** | **0.88** | *0.87* | 0.80 | 0.80 | 0.79 | 0.85 | *0.86* |\n| Handoutlines | 0.73 | **0.76** | 0.73 | 0.72 | 0.68 | 0.61 | 0.71 | 0.73 | **0.76** |\n| SonyRobotAI1 | **0.89** | 0.80 | 0.85 | 0.83 | 0.79 | 0.69 | 0.74 | 0.83 | *0.94* |\n| SonyRobotAI2 | 0.77 | 0.75 | 0.79 | **0.92** | *0.86* | 0.78 | 0.80 | *0.86* | 0.81 |\n| StarLightCurves | 0.82 | 0.81 | 0.86 | **0.86** | 0.76 | 0.72 | 0.77 | 0.77 | *0.85* |\n| TwoLeadECG | **0.71** | 0.61 | 0.56 | 0.71 | 0.63 | 0.61 | 0.71 | 0.65 | *0.69* |\n| Yoga | **0.62** | 0.54 | 0.60 | 0.58 | 0.57 | 0.52 | *0.59* | 0.55 | 0.55 |\n| EOGHorizontal | 0.72 | *0.76* | **0.81** | 0.74 | 0.70 | 0.69 | 0.74 | 0.73 | 0.75 |\n| CinECGTorso | 0.70 | **0.92** | *0.86* | 0.43 | 0.51 | 0.46 | 0.41 | 0.64 | 0.80 |\n| ECG5000 | *0.93* | **0.98** | **0.98** | 0.95 | 0.96 | 0.93 | 0.95 | 0.91 | *0.93* |\n\n## 5.  Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. ", "original_text": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n"}, "hash": "31b18ea8f9a17757e51c88854073931ac8e302064165835bef9b736e6fa28049", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44f70d09-5e38-4fc3-b901-c80dcf11bb52", "node_type": "1", "metadata": {"window": "when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n", "original_text": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). "}, "hash": "02889827a7e4dfac87d5129f986e344c6d94bf77ab3f253a49b8e4f28a678039", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). ", "mimetype": "text/plain", "start_char_idx": 41886, "end_char_idx": 42156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44f70d09-5e38-4fc3-b901-c80dcf11bb52", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n", "original_text": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c1aeea0-6e74-44c8-8616-c0d9be824cf9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Extensions of FIF\n\n**Extension to multivariate functions** FIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. ", "original_text": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)). "}, "hash": "e73a84d7e2b37005ce3531c9e30f8dfa7006143fa516c7b7385329d744a578d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a076d81-5ace-4db3-a8df-625674142798", "node_type": "1", "metadata": {"window": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). ", "original_text": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. "}, "hash": "36b050bea154ac3373928617f7ab49d2982f3e6e3c0644fd8d75d2a96d2940b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). ", "mimetype": "text/plain", "start_char_idx": 42156, "end_char_idx": 42447, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4a076d81-5ace-4db3-a8df-625674142798", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). ", "original_text": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44f70d09-5e38-4fc3-b901-c80dcf11bb52", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "when the quantity of interest lies in R^d for each moment of time.  For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n", "original_text": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S). "}, "hash": "7d607e2d1faf28563956897bfec6cf299dc5633d60c2bfeb9a039031dc7dfa68", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b2f8d15-68ac-4489-99a4-84a4913f0647", "node_type": "1", "metadata": {"window": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. ", "original_text": "Using this property, Li et al. "}, "hash": "cab2a050efa56385d8d9ecc29a5bc5c46f8341d216e4ec189a1363ec1dc3038c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. ", "mimetype": "text/plain", "start_char_idx": 42447, "end_char_idx": 42553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b2f8d15-68ac-4489-99a4-84a4913f0647", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. ", "original_text": "Using this property, Li et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a076d81-5ace-4db3-a8df-625674142798", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element: (f, g)_{H^{\u2297d}} := \u2211_{i=1}^d \u27e8f\u207d\u2071\u207e, g\u207d\u2071\u207e\u27e9_H.  The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). ", "original_text": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*. "}, "hash": "e53283aa105167de37a93e2734536bd2cab494c4da440bed9788e09786fc5895", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d029aea-84e0-4910-88e4-a5a113ceec64", "node_type": "1", "metadata": {"window": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n", "original_text": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. "}, "hash": "70fed3663f386d0ad62947a48dd6c344aedb8e0a22d69685f7c19e3ee425b531", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Using this property, Li et al. ", "mimetype": "text/plain", "start_char_idx": 42553, "end_char_idx": 42584, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d029aea-84e0-4910-88e4-a5a113ceec64", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n", "original_text": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b2f8d15-68ac-4489-99a4-84a4913f0647", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The dictionary is then defined in (H([0, 1]))^{\u2297d}, e.g., by componentwise application of one or several univariate dictionaries, see Section 3.  In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. ", "original_text": "Using this property, Li et al. "}, "hash": "7fa1eff584cad8327489c7c74f6b408f86f7a3b8c18187a4097b87c0765f8a47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b03a254a-a5fa-4f9a-a283-c5cbed791692", "node_type": "1", "metadata": {"window": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6. ", "original_text": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. "}, "hash": "f2e75e2d4fdac8fb4ca718eab14715f108eea7370a11c37461d2668c8a2552a7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. ", "mimetype": "text/plain", "start_char_idx": 42584, "end_char_idx": 42731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b03a254a-a5fa-4f9a-a283-c5cbed791692", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6. ", "original_text": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d029aea-84e0-4910-88e4-a5a113ceec64", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the Supplementary Materials we give an illustration of multivariate functional anomaly detection on the MNIST (Lecun et al., 1998) dataset, each digit being seen as a 2D-curve.\n\n **Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n", "original_text": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map. "}, "hash": "3ecbc9bd7170551cb9a5a4810209c4749573b15b72032191e0dd65267112a735", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f371a66-61ff-45f1-a41a-9d10af893edd", "node_type": "1", "metadata": {"window": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. ", "original_text": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n"}, "hash": "80b759afc7a0c4533c75d71a16d9078b17367034067bb6f2332137c4e2818633", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. ", "mimetype": "text/plain", "start_char_idx": 42731, "end_char_idx": 42880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f371a66-61ff-45f1-a41a-9d10af893edd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. ", "original_text": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b03a254a-a5fa-4f9a-a283-c5cbed791692", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Connection to data depth** Regarding FIF score as an anomaly ranking yields a connection to the notion of the statistical depth function (see (Mosler, 2013) for an overview), which has been successfully applied in outlier detection (see, e.g., (Hubert et al., 2015)).  Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6. ", "original_text": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth. "}, "hash": "04d69d4730a50ae917d783a37b3cfcc5d278585860936a527bce22a758825e05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28f189f2-530c-45a4-a039-8c9f976bbd1b", "node_type": "1", "metadata": {"window": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. ", "original_text": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). "}, "hash": "778f909f72ed7fd792b49640c417b054579db72728bca596f9c16a8d01d5ba0d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n", "mimetype": "text/plain", "start_char_idx": 42880, "end_char_idx": 43118, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28f189f2-530c-45a4-a039-8c9f976bbd1b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. ", "original_text": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f371a66-61ff-45f1-a41a-9d10af893edd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Statistical data depth has been introduced as a measure of centrality (or depth) of an arbitrary observation x \u2208 (H([0, 1]))^{\u2297d} with respect to the data at hand S. A data depth measure based on FIF score can be defined for (multivariate) functional data as: D_{FIF}(x; S) = 1 \u2212 s_n(x; S).  Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. ", "original_text": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n"}, "hash": "5ae363bab6f9ed3ada6f56a01c78058470cc58b2e6df209e07975c7dc607855e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b068d4d0-7471-46bb-a9e5-b4e27055c9cf", "node_type": "1", "metadata": {"window": "Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data. ", "original_text": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. "}, "hash": "5d6c7e88c406a58d27f649757b4d59651c52868b9e47a3f8866986684ac4da98", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). ", "mimetype": "text/plain", "start_char_idx": 43118, "end_char_idx": 43365, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b068d4d0-7471-46bb-a9e5-b4e27055c9cf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data. ", "original_text": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28f189f2-530c-45a4-a039-8c9f976bbd1b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Data depth proves to be a useful tool for a low-dimensional data representation called *depth-based map*.  Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. ", "original_text": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6). "}, "hash": "da5db0ada6c3efdb9d33229d48fd2d8ddd9c76dddea8ce1bbc245cf0f028cfea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48430a18-8c03-436f-a836-3f52af3e3d92", "node_type": "1", "metadata": {"window": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. ", "original_text": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n"}, "hash": "47bdf46d365971202425898104f07babafb8e705843b9189cdabc588706edafb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. ", "mimetype": "text/plain", "start_char_idx": 43365, "end_char_idx": 43560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48430a18-8c03-436f-a836-3f52af3e3d92", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. ", "original_text": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b068d4d0-7471-46bb-a9e5-b4e27055c9cf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Using this property, Li et al.  (2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data. ", "original_text": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes. "}, "hash": "19f07613afc133fdeacb70829c99ceb198b8ce4e31892edf3ca4341b2a5caac7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6616a094-d5fd-4ce8-856e-699bc02f4c65", "node_type": "1", "metadata": {"window": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n", "original_text": "## 6. "}, "hash": "dfc491e036b336718e55b9535a040c54091e551523a400dce0e54041079e68ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n", "mimetype": "text/plain", "start_char_idx": 43560, "end_char_idx": 43711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6616a094-d5fd-4ce8-856e-699bc02f4c65", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n", "original_text": "## 6. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48430a18-8c03-436f-a836-3f52af3e3d92", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2012) and Mosler and Mozharovskyi (2017) define a DD-plot classifier which consists in applying a multivariate classifier to the depth-based map.  Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. ", "original_text": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n"}, "hash": "f3f9cd22d8ab8723dbeecaaf843a2df6fd46242f8995deb11e7134d9d4da7aa6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b440f2b-74b8-46fc-8c41-f283ee27e444", "node_type": "1", "metadata": {"window": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset.", "original_text": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. "}, "hash": "070e921e4b610571c015e745cd506b5f8b042b35a3f4236496bca6570e92411b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 6. ", "mimetype": "text/plain", "start_char_idx": 43711, "end_char_idx": 43717, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b440f2b-74b8-46fc-8c41-f283ee27e444", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset.", "original_text": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6616a094-d5fd-4ce8-856e-699bc02f4c65", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Low-dimensional representation is of particular interest for functional data and a DD-plot classifier can be defined using the FIF-based data depth.  Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n", "original_text": "## 6. "}, "hash": "b70a8aa651b7dc46ecb11ab00650653d5ec5c98827dc4a9d9924eeb40eef2b5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75bc2bfa-b15c-4c0e-a0fe-5edb1138d3f5", "node_type": "1", "metadata": {"window": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot. ", "original_text": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. "}, "hash": "4688cb14ba8881d207bc452f93b4c5ae73968fb4a9b67f442baeaac260d5f392", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. ", "mimetype": "text/plain", "start_char_idx": 43717, "end_char_idx": 43852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "75bc2bfa-b15c-4c0e-a0fe-5edb1138d3f5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot. ", "original_text": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b440f2b-74b8-46fc-8c41-f283ee27e444", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Let S^{trn} = S\u00b9 \u222a ... \u222a S^q be a training set for supervised classification containing q classes, each subset S^j standing for class j. The depth map is defined as follows:\n\nx \u21a6 \u03c6(x) = (D_{FIF}(x; S\u00b9), ..., D_{FIF}(x; S^q)) \u2208 [0, 1]^q.\n\n As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset.", "original_text": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data. "}, "hash": "60ae78394610757a7ac557b34e8417251018c18bc862a1820310ce9859143871", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76916fd2-83a3-44e0-84e3-351d48eccb40", "node_type": "1", "metadata": {"window": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t. ", "original_text": "FIF is extendable to multivariate functional data. "}, "hash": "038852f86ae942e73bdc89cc0f128ca434bd671df1798688ae98aaa4dca36ef2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. ", "mimetype": "text/plain", "start_char_idx": 43852, "end_char_idx": 44096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "76916fd2-83a3-44e0-84e3-351d48eccb40", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t. ", "original_text": "FIF is extendable to multivariate functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75bc2bfa-b15c-4c0e-a0fe-5edb1138d3f5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As an illustration, we apply the depth map to 3 digits (1, 5 and 7, 100 observations per digit for training and 100 testing) of the MNIST dataset after their transformation to two-variate functions using **skimage** python library (see Figure 6).  One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot. ", "original_text": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks. "}, "hash": "c22150cc973821de9f5c2522a9e95cc5a23557e19b286928f0451a1cddf4409c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d40208d3-a0be-4347-94b6-5ea4c7737902", "node_type": "1", "metadata": {"window": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t. ", "original_text": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. "}, "hash": "65070f8e1b08909e4b4c6b146099b75ace08644cf0cc567cacb139203439901a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "FIF is extendable to multivariate functional data. ", "mimetype": "text/plain", "start_char_idx": 44096, "end_char_idx": 44147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d40208d3-a0be-4347-94b6-5ea4c7737902", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t. ", "original_text": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76916fd2-83a3-44e0-84e3-351d48eccb40", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One observes appealing geometrical interpretation (observe, e.g., the location of the abnormally distant \u2013 from their corresponding classes \u2013 observations) and a clear separation of the classes.  To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t. ", "original_text": "FIF is extendable to multivariate functional data. "}, "hash": "facef1daeeca55f063c523c2fa7fd375f9d69cc9a29c36dd5406c9e9a8fe455f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00351152-b947-41e4-9ef5-fac768d4f2da", "node_type": "1", "metadata": {"window": "## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t. ", "original_text": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n"}, "hash": "1556710315d252759db04996b0b917836a22cb5b4c17ed2e58dc33fe92f660ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. ", "mimetype": "text/plain", "start_char_idx": 44147, "end_char_idx": 44294, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "00351152-b947-41e4-9ef5-fac768d4f2da", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t. ", "original_text": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d40208d3-a0be-4347-94b6-5ea4c7737902", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate separability, we apply linear multiclass (one-against-all) SVM in the depth space, which delivers the accuracy of 99% on the test data.\n\n ## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t. ", "original_text": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space. "}, "hash": "19a8cb55129e62fd8abd3a60d745642437e3c7f560c3842939f75af787172d8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d7a21be-cb1f-4633-86c2-d33beb2993a5", "node_type": "1", "metadata": {"window": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\". ", "original_text": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset."}, "hash": "d9fc30b0bc1a99abb9f2eb4cbe47de9cb747f4bb224978e8c13eef4163ef4284", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n", "mimetype": "text/plain", "start_char_idx": 44294, "end_char_idx": 44430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d7a21be-cb1f-4633-86c2-d33beb2993a5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\". ", "original_text": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00351152-b947-41e4-9ef5-fac768d4f2da", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## 6.  Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t. ", "original_text": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n"}, "hash": "7685e3caa74f54a50d7452646f6d02c758b8a976dfa913223ed9c65e56eb31cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dcbf187-8a5f-4a8e-99a2-015a120ff655", "node_type": "1", "metadata": {"window": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. ", "original_text": "**\n\n**Description:** The figure shows a 3D scatter plot. "}, "hash": "d8c99061bac7fa74b518c3ef4f131e21c1472f1342cd1acc82270c9ab3987463", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset.", "mimetype": "text/plain", "start_char_idx": 44430, "end_char_idx": 44522, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2dcbf187-8a5f-4a8e-99a2-015a120ff655", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. ", "original_text": "**\n\n**Description:** The figure shows a 3D scatter plot. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d7a21be-cb1f-4633-86c2-d33beb2993a5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Conclusion\n\nThe Functional Isolation Forest algorithm has been proposed, which is an extension of Isolation Forest to functional data.  The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\". ", "original_text": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset."}, "hash": "3f6e6ab664905d0624e292ab4594ebcbf9700be36c16aafcf1a879170cf69305", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f72f929-547e-49ed-878f-7cfbb476584d", "node_type": "1", "metadata": {"window": "FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n", "original_text": "The three axes are \"Depth w.r.t. "}, "hash": "a8173fb62f164b27fb9cbf0624314dc51dd34696704f9d7b9a0eae9c97d5d748", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure shows a 3D scatter plot. ", "mimetype": "text/plain", "start_char_idx": 44522, "end_char_idx": 44579, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f72f929-547e-49ed-878f-7cfbb476584d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n", "original_text": "The three axes are \"Depth w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dcbf187-8a5f-4a8e-99a2-015a120ff655", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The combined choice of the dictionary itself, the probability distribution used to pick a *Split variable* and the scalar product used for the projection enables FIF to exhibit a great flexibility in detecting anomalies for a variety of tasks.  FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. ", "original_text": "**\n\n**Description:** The figure shows a 3D scatter plot. "}, "hash": "9889c3043e860664972d674b3e7ebf5967e003c959e5ab49c5f3523dd6a7f466", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1214527d-b5ef-4a47-aee7-9bf1af0a1ac8", "node_type": "1", "metadata": {"window": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman. ", "original_text": "S\u00b9\", \"Depth w.r.t. "}, "hash": "0d4772a8dd1809ab2ef06e992dbe7c0bccc441ccfab8fcdf0848f8dbd6c9d091", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The three axes are \"Depth w.r.t. ", "mimetype": "text/plain", "start_char_idx": 44579, "end_char_idx": 44612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1214527d-b5ef-4a47-aee7-9bf1af0a1ac8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman. ", "original_text": "S\u00b9\", \"Depth w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f72f929-547e-49ed-878f-7cfbb476584d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "FIF is extendable to multivariate functional data.  When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n", "original_text": "The three axes are \"Depth w.r.t. "}, "hash": "7d304ce295e296a694f265857e11a33875f2c2a734598ba4a963402dc9caa478", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69092d48-ee99-4d92-aa2f-7409ac892a85", "node_type": "1", "metadata": {"window": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests. ", "original_text": "S\u2075\", and \"Depth w.r.t. "}, "hash": "8bf670fc8d0998ade02398f8a37d3dfb873dd725848892cc54a306a08ad816f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S\u00b9\", \"Depth w.r.t. ", "mimetype": "text/plain", "start_char_idx": 44612, "end_char_idx": 44631, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "69092d48-ee99-4d92-aa2f-7409ac892a85", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests. ", "original_text": "S\u2075\", and \"Depth w.r.t. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1214527d-b5ef-4a47-aee7-9bf1af0a1ac8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When transformed in a data depth definition, FIF can be used for supervised classification via a low-dimensional representation \u2013 the depth space.  The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman. ", "original_text": "S\u00b9\", \"Depth w.r.t. "}, "hash": "0c53a26965eb1f345bbe1c6c51b2dc03301875526f67ddbce483f62c543ba7de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "268bb4b8-bce2-49b1-b99d-5842ddd4a129", "node_type": "1", "metadata": {"window": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n", "original_text": "S\u2077\". "}, "hash": "19bb7cdbab0bd8033b471ce135215e82e44e35de3fcabb951028f9774168bd20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S\u2075\", and \"Depth w.r.t. ", "mimetype": "text/plain", "start_char_idx": 44631, "end_char_idx": 44654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "268bb4b8-bce2-49b1-b99d-5842ddd4a129", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n", "original_text": "S\u2077\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69092d48-ee99-4d92-aa2f-7409ac892a85", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The open-source implementation of the method, along with all reproducing scripts, can be accessed at https://github.com/Gstaerman/FIF.\n\n ***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests. ", "original_text": "S\u2075\", and \"Depth w.r.t. "}, "hash": "c32cf645e291ef2d2cfddcf7106b0696f174efd462fb30097c584e30c23e9b3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fac3ab8-7b5c-43ba-9086-129d8ed6fc09", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M. ", "original_text": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. "}, "hash": "0b5be888c2794c3d0974c8e8769c11100f75850bdb87ae4a0f2f90da6138cfb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S\u2077\". ", "mimetype": "text/plain", "start_char_idx": 44654, "end_char_idx": 44659, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6fac3ab8-7b5c-43ba-9086-129d8ed6fc09", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M. ", "original_text": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "268bb4b8-bce2-49b1-b99d-5842ddd4a129", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 6: Depth space embedding of the three digits (1, 5 and 7) of the MNIST dataset. **\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n", "original_text": "S\u2077\". "}, "hash": "a7db6b11a1f93c4a202532118924cff3e09b66099ece83205ffbe08b99df0d86", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c08450ca-b0ae-4c5a-a561-4d65ef73af32", "node_type": "1", "metadata": {"window": "The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P. ", "original_text": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n"}, "hash": "c11d6db44752b710267291504be21d9cdb0d783d0535763b6e3998ee717e8b6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. ", "mimetype": "text/plain", "start_char_idx": 44659, "end_char_idx": 44775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c08450ca-b0ae-4c5a-a561-4d65ef73af32", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P. ", "original_text": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fac3ab8-7b5c-43ba-9086-129d8ed6fc09", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows a 3D scatter plot.  The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M. ", "original_text": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7. "}, "hash": "ce2c5267c2277ad2db194f4859f7e23672bd6d311c52fdebf7c98d84ae48550c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f81e9baf-6e10-4a90-9b24-2267b4394fde", "node_type": "1", "metadata": {"window": "S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T. ", "original_text": "***\n\n## References\n\nL. Breiman. "}, "hash": "9670ded3f29da13d4b0c6f080dbd2b0152a86a9ba68bde32aa1443bf2e5cd21c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n", "mimetype": "text/plain", "start_char_idx": 44775, "end_char_idx": 44912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f81e9baf-6e10-4a90-9b24-2267b4394fde", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T. ", "original_text": "***\n\n## References\n\nL. Breiman. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c08450ca-b0ae-4c5a-a561-4d65ef73af32", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The three axes are \"Depth w.r.t.  S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P. ", "original_text": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n"}, "hash": "942a19279959d9ee44ad483e61dd7acdfc7220673e267e4520b3247a23f05869", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bffcc0ad-feb9-4c5e-af2c-6741af955673", "node_type": "1", "metadata": {"window": "S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander. ", "original_text": "Random forests. "}, "hash": "cae469e9f44177996f12f3f54d17c1a2decdee995cc2512e8270c8ea9d5dd054", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n## References\n\nL. Breiman. ", "mimetype": "text/plain", "start_char_idx": 44912, "end_char_idx": 44944, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bffcc0ad-feb9-4c5e-af2c-6741af955673", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander. ", "original_text": "Random forests. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f81e9baf-6e10-4a90-9b24-2267b4394fde", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S\u00b9\", \"Depth w.r.t.  S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T. ", "original_text": "***\n\n## References\n\nL. Breiman. "}, "hash": "d87c511e3cfccd5e9c9748130a5efce217243fd9a868d08ecede1cfbe2402dd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3ae470b-7350-4b9a-9628-da21d9c134ea", "node_type": "1", "metadata": {"window": "S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers. ", "original_text": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n"}, "hash": "996d15549efe8a094e345338fe0862fab9c2182fef1ea0d8e2ce3f831b111e69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Random forests. ", "mimetype": "text/plain", "start_char_idx": 44944, "end_char_idx": 44960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e3ae470b-7350-4b9a-9628-da21d9c134ea", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers. ", "original_text": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bffcc0ad-feb9-4c5e-af2c-6741af955673", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S\u2075\", and \"Depth w.r.t.  S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander. ", "original_text": "Random forests. "}, "hash": "13ec92e6c52a8a73f2eda347c39b06336b3ec6c8ddf144d5a2d3f1205b078f11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c93229e3-a5b8-4eed-975b-dd3ed064e7bc", "node_type": "1", "metadata": {"window": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. ", "original_text": "M.M. "}, "hash": "6ff0853ea131a3f11d59e63aadd1c42be698628aa6181ff089fec9ac68005e3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n", "mimetype": "text/plain", "start_char_idx": 44960, "end_char_idx": 44999, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c93229e3-a5b8-4eed-975b-dd3ed064e7bc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. ", "original_text": "M.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3ae470b-7350-4b9a-9628-da21d9c134ea", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S\u2077\".  There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers. ", "original_text": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n"}, "hash": "6334fa22c518871c2dce702b6f307e861bf0e90165eb10affc4ebc2eccf964a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d45f019-4ece-4dd7-b01c-213a373b1b78", "node_type": "1", "metadata": {"window": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n", "original_text": "Breunig, H.-P. "}, "hash": "1887ee91e251a1d535cc086580ef72d54e17c5edbf4a53ac51117b8b91f20701", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "M.M. ", "mimetype": "text/plain", "start_char_idx": 44999, "end_char_idx": 45004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d45f019-4ece-4dd7-b01c-213a373b1b78", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n", "original_text": "Breunig, H.-P. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c93229e3-a5b8-4eed-975b-dd3ed064e7bc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "There are three distinct clusters of points, colored red, blue, and green, corresponding to the digits 1, 5, and 7.  The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. ", "original_text": "M.M. "}, "hash": "de3b82d9df9ca302e2f97dcd63389462c5ec8268d64ec8f3aeef317eb86d70e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8064917f-711f-4a33-9d05-52fa337e7fc3", "node_type": "1", "metadata": {"window": "***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar. ", "original_text": "Kriegel, R.T. "}, "hash": "c6fda262eb6ec48b9cf40a2bc7a7e6c8ea7fe02e7a015b53e9e4d51e2d9dddbc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Breunig, H.-P. ", "mimetype": "text/plain", "start_char_idx": 45004, "end_char_idx": 45019, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8064917f-711f-4a33-9d05-52fa337e7fc3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar. ", "original_text": "Kriegel, R.T. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d45f019-4ece-4dd7-b01c-213a373b1b78", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The clusters are well-separated in this 3D depth space, indicating that the depth map provides a good representation for classification.\n ***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n", "original_text": "Breunig, H.-P. "}, "hash": "ed533f09ae6cd6356d1198cb4d0a49bafd536336a799c758c0c9a5c08ed02c79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b365abeb-880c-459f-8cb4-d38d366ac2d2", "node_type": "1", "metadata": {"window": "Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey. ", "original_text": "Ng, and J. Sander. "}, "hash": "1ac1972790579679c9bcedd485ab5ec064a66812acb21936eff91355ec7217f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Kriegel, R.T. ", "mimetype": "text/plain", "start_char_idx": 45019, "end_char_idx": 45033, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b365abeb-880c-459f-8cb4-d38d366ac2d2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey. ", "original_text": "Ng, and J. Sander. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8064917f-711f-4a33-9d05-52fa337e7fc3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n## References\n\nL. Breiman.  Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar. ", "original_text": "Kriegel, R.T. "}, "hash": "73ef1d1962ebce3b42c66ca2c26eb63c0a887993831d455f240633911349fb15", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28779648-59bc-406a-9805-4d890d0c303d", "node_type": "1", "metadata": {"window": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n", "original_text": "LOF: Identifying density-based local outliers. "}, "hash": "7098bca957da01b6ba24bd119d4a842497620ebc95c66f0924d4c3af183c8f2c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ng, and J. Sander. ", "mimetype": "text/plain", "start_char_idx": 45033, "end_char_idx": 45052, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "28779648-59bc-406a-9805-4d890d0c303d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n", "original_text": "LOF: Identifying density-based local outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b365abeb-880c-459f-8cb4-d38d366ac2d2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Random forests.  *Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey. ", "original_text": "Ng, and J. Sander. "}, "hash": "8828d8136093078ed98c2031080d6cd41992c3c6e50b8602942035cb1a9fe38e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0a2b342-9123-4121-8726-559863b84473", "node_type": "1", "metadata": {"window": "M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. ", "original_text": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. "}, "hash": "3c4510308ae2954e08509fd544002f62786fb572c431bfbc15620a942bc48fda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "LOF: Identifying density-based local outliers. ", "mimetype": "text/plain", "start_char_idx": 45052, "end_char_idx": 45099, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0a2b342-9123-4121-8726-559863b84473", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. ", "original_text": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28779648-59bc-406a-9805-4d890d0c303d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Machine Learning*, 45(1):5\u201332, 2001.\n\n M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n", "original_text": "LOF: Identifying density-based local outliers. "}, "hash": "5f272b652d6c4234deb3f0f23e2c36ba2c4d50ce098298a1878092e21292eb96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a658bca-06e7-4e63-8fe4-ef68da9e52d4", "node_type": "1", "metadata": {"window": "Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015. ", "original_text": "ACM, 2000.\n\n"}, "hash": "171caba3db40e8e3fe276c328e2e81b3bfc3b1a8617e1b84eab336a1782de0a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. ", "mimetype": "text/plain", "start_char_idx": 45099, "end_char_idx": 45212, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0a658bca-06e7-4e63-8fe4-ef68da9e52d4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015. ", "original_text": "ACM, 2000.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0a2b342-9123-4121-8726-559863b84473", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "M.M.  Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. ", "original_text": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104. "}, "hash": "c489e4a4547cd70f8ed9c1a329c093c0e36c3ad2edb31b5a2c56caa7cb0dcda9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67eccc59-0faf-49a5-8c22-352bc14a25e4", "node_type": "1", "metadata": {"window": "Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n", "original_text": "V. Chandola, A. Banerjee, and V. Kumar. "}, "hash": "af3d38acb8778ac0c8318eaad17edb99e0b0152d2cd77e568414b069d298d663", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "ACM, 2000.\n\n", "mimetype": "text/plain", "start_char_idx": 45212, "end_char_idx": 45224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67eccc59-0faf-49a5-8c22-352bc14a25e4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n", "original_text": "V. Chandola, A. Banerjee, and V. Kumar. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a658bca-06e7-4e63-8fe4-ef68da9e52d4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Breunig, H.-P.  Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015. ", "original_text": "ACM, 2000.\n\n"}, "hash": "c40184751e44afe084952cf021cfcd01927d4618ca39fdae75572cdd641482d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78f71373-774c-4130-a7b3-ea86dd291582", "node_type": "1", "metadata": {"window": "Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. ", "original_text": "Anomaly detection: A survey. "}, "hash": "411404649bcd3da40d34f48607fb99d0430e000ff2b3786d06632b836c4cd07d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "V. Chandola, A. Banerjee, and V. Kumar. ", "mimetype": "text/plain", "start_char_idx": 45224, "end_char_idx": 45264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "78f71373-774c-4130-a7b3-ea86dd291582", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. ", "original_text": "Anomaly detection: A survey. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67eccc59-0faf-49a5-8c22-352bc14a25e4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Kriegel, R.T.  Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n", "original_text": "V. Chandola, A. Banerjee, and V. Kumar. "}, "hash": "00b95a89d3962448b468ac6c4aecbb23d31019f4fea99a538a52658c562678e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d0b84c3-17cf-4a12-b441-69f300183304", "node_type": "1", "metadata": {"window": "LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth. ", "original_text": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n"}, "hash": "83142edb7f42c8047ce61b2109ad7b9d432f035d28140df0c1d5b14569bf6023", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly detection: A survey. ", "mimetype": "text/plain", "start_char_idx": 45264, "end_char_idx": 45293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d0b84c3-17cf-4a12-b441-69f300183304", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth. ", "original_text": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78f71373-774c-4130-a7b3-ea86dd291582", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Ng, and J. Sander.  LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. ", "original_text": "Anomaly detection: A survey. "}, "hash": "f10a0c6ac7b5f84d60f902b8d027e1f57927f085d307f95ad3c6d673a19e4463", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de6246ee-dc69-4d79-bdd3-f7a367936351", "node_type": "1", "metadata": {"window": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n", "original_text": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. "}, "hash": "70f9ef96e62e710495cfef65b20375fdc1375b78003d42fb79960c316dfb2604", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n", "mimetype": "text/plain", "start_char_idx": 45293, "end_char_idx": 45350, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de6246ee-dc69-4d79-bdd3-f7a367936351", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n", "original_text": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d0b84c3-17cf-4a12-b441-69f300183304", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "LOF: Identifying density-based local outliers.  In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth. ", "original_text": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n"}, "hash": "8f450921b021fea815694c929d6275f6403b999073d0d2df9744c4c954c809a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f66b81ed-349f-4134-972b-bb7defbb2b75", "node_type": "1", "metadata": {"window": "ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman. ", "original_text": "The UCR time series classification archive, July 2015. "}, "hash": "382dd7957b77e619d6f755698e481591d031641a9347e95d31240bc2e9d97baf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. ", "mimetype": "text/plain", "start_char_idx": 45350, "end_char_idx": 45424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f66b81ed-349f-4134-972b-bb7defbb2b75", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman. ", "original_text": "The UCR time series classification archive, July 2015. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de6246ee-dc69-4d79-bdd3-f7a367936351", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data*, volume 29, pages 93\u2013104.  ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n", "original_text": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista. "}, "hash": "37b5a4083a3c1bb5d9c67a0cbbaec47a6b9c3c5bc0788dde098e8e8657085415", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2de265a-66a8-4f2e-b885-4782d69e8e30", "node_type": "1", "metadata": {"window": "V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions. ", "original_text": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n"}, "hash": "6dfe844bd47aa5e2cb556fe2e7417ef8854547adb6fbfc17f2794fd35f234a37", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The UCR time series classification archive, July 2015. ", "mimetype": "text/plain", "start_char_idx": 45424, "end_char_idx": 45479, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2de265a-66a8-4f2e-b885-4782d69e8e30", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions. ", "original_text": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f66b81ed-349f-4134-972b-bb7defbb2b75", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "ACM, 2000.\n\n V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman. ", "original_text": "The UCR time series classification archive, July 2015. "}, "hash": "c1907fff90798cb4b0f2da76fdbe44c596b21b87cd841fc7e188e72d9222fc07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61851d76-09cc-49ac-89d6-024778b7c1f0", "node_type": "1", "metadata": {"window": "Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n", "original_text": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. "}, "hash": "16912e1d23f65705f7d00aa4f2ba6cc1c2928617d21ab050d1f331f09f7fa87f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n", "mimetype": "text/plain", "start_char_idx": 45479, "end_char_idx": 45526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61851d76-09cc-49ac-89d6-024778b7c1f0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n", "original_text": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2de265a-66a8-4f2e-b885-4782d69e8e30", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "V. Chandola, A. Banerjee, and V. Kumar.  Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions. ", "original_text": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n"}, "hash": "cb2b64aa549eade6177ce568b219a0ed652ec5cd4a2470f26faea2bd407e6f98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "505126da-30f3-45cc-9966-319e63fe2846", "node_type": "1", "metadata": {"window": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J. ", "original_text": "Multivariate functional halfspace depth. "}, "hash": "403d14222970fa173d41553a073e25d01e9ce510ce7f9b9b1c0f08dafb41939e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. ", "mimetype": "text/plain", "start_char_idx": 45526, "end_char_idx": 45577, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "505126da-30f3-45cc-9966-319e63fe2846", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J. ", "original_text": "Multivariate functional halfspace depth. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61851d76-09cc-49ac-89d6-024778b7c1f0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Anomaly detection: A survey.  *ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n", "original_text": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili. "}, "hash": "58b9a925148049018bb9b11e362ca354cd9936b4e329c59590033ee637008cd0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4ff5b41-2e2c-4abd-97ca-2b59169c9554", "node_type": "1", "metadata": {"window": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M. ", "original_text": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n"}, "hash": "7e436b3e446c48bf08289b151c19e2fe9832fda1171462a9ab6d2c1641733a4c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multivariate functional halfspace depth. ", "mimetype": "text/plain", "start_char_idx": 45577, "end_char_idx": 45618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4ff5b41-2e2c-4abd-97ca-2b59169c9554", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M. ", "original_text": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "505126da-30f3-45cc-9966-319e63fe2846", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*ACM Computing Surveys (CSUR)*, 41(3):15:1\u201315:58, 2009.\n\n Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J. ", "original_text": "Multivariate functional halfspace depth. "}, "hash": "747496c1719d28c5090c68a41e2d143a7922e16fb3964c5863419f37e5e94624", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4c1256e-c759-4d93-a7fb-09ec919ed573", "node_type": "1", "metadata": {"window": "The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason. ", "original_text": "A. Cuevas, M. Febrero, and R. Fraiman. "}, "hash": "9be1d13504c50d8e3f875d406200692a1d41bacc3bd45e751d59fa002a16f3c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n", "mimetype": "text/plain", "start_char_idx": 45618, "end_char_idx": 45690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4c1256e-c759-4d93-a7fb-09ec919ed573", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason. ", "original_text": "A. Cuevas, M. Febrero, and R. Fraiman. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4ff5b41-2e2c-4abd-97ca-2b59169c9554", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Y. Chen, E. Keogh, B. Hu, N. Begum, A. Bagnall, A. Mueen, and G. Batista.  The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M. ", "original_text": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n"}, "hash": "bed19d2a6d5c9e3a24ec5acc2d158eac496be2998e2edb33472a6b2a2390dbbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "353160d7-a7f5-43c2-8ea6-af72ed4eeb96", "node_type": "1", "metadata": {"window": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes. ", "original_text": "Robust estimation and classification for functional data via projection-based depth notions. "}, "hash": "826d8e20709e460ffc6fb4cee9d35351ff77032dd60086b5469f11b8fcd32c10", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A. Cuevas, M. Febrero, and R. Fraiman. ", "mimetype": "text/plain", "start_char_idx": 45690, "end_char_idx": 45729, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "353160d7-a7f5-43c2-8ea6-af72ed4eeb96", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes. ", "original_text": "Robust estimation and classification for functional data via projection-based depth notions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4c1256e-c759-4d93-a7fb-09ec919ed573", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The UCR time series classification archive, July 2015.  URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason. ", "original_text": "A. Cuevas, M. Febrero, and R. Fraiman. "}, "hash": "30e4f97a28de6ff84cc038192775691162985540f10592a2eb2ed5b8086ccb79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5007dbf-195c-463f-ad64-34728638398c", "node_type": "1", "metadata": {"window": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n", "original_text": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n"}, "hash": "2ca71b1dbd0f87961fd3239242d955719234a82d7dfc15f59f1097e1f4f762cd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Robust estimation and classification for functional data via projection-based depth notions. ", "mimetype": "text/plain", "start_char_idx": 45729, "end_char_idx": 45822, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5007dbf-195c-463f-ad64-34728638398c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n", "original_text": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "353160d7-a7f5-43c2-8ea6-af72ed4eeb96", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "URL www.cs.ucr.edu/~eamonn/time_series_data/.\n\n G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes. ", "original_text": "Robust estimation and classification for functional data via projection-based depth notions. "}, "hash": "df114b766d4dbabead6e0cc77a4927ce11a74e0cca674b02322c237fe463e4a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4f6f9431-5247-4416-b19e-2e0b57fd3fc9", "node_type": "1", "metadata": {"window": "Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu. ", "original_text": "J.H.J. "}, "hash": "7f3352cb1d28699d7423d83433d981c9dbce3f5436b693dab504679b030cdb5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n", "mimetype": "text/plain", "start_char_idx": 45822, "end_char_idx": 45872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4f6f9431-5247-4416-b19e-2e0b57fd3fc9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu. ", "original_text": "J.H.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5007dbf-195c-463f-ad64-34728638398c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "G. Claeskens, M. Hubert, L. Slaets, and K. Vakili.  Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n", "original_text": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n"}, "hash": "d9beb3f30da54abd0958354e8d84ef402f65ba9e63139ab16575a8492684b067", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0043bf0e-8146-49bb-966c-6294469fb39c", "node_type": "1", "metadata": {"window": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*. ", "original_text": "Einmahl and D.M. "}, "hash": "b0ca86db103faeda3ece9d1708db801b0036e3ffc763fa58454635deb986dd14", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J.H.J. ", "mimetype": "text/plain", "start_char_idx": 45872, "end_char_idx": 45879, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0043bf0e-8146-49bb-966c-6294469fb39c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*. ", "original_text": "Einmahl and D.M. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f6f9431-5247-4416-b19e-2e0b57fd3fc9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Multivariate functional halfspace depth.  *Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu. ", "original_text": "J.H.J. "}, "hash": "e1fcf98366c126d50b6a04c56bb1dc178a64fb1516ca36491bae7a7454cb34f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c787119-dc0f-4c69-9789-3f35e16458c2", "node_type": "1", "metadata": {"window": "A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n", "original_text": "Mason. "}, "hash": "74303153df872b608f70cbf559d5d33a5dc4b376c2669585ee3e77261d575a78", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Einmahl and D.M. ", "mimetype": "text/plain", "start_char_idx": 45879, "end_char_idx": 45896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c787119-dc0f-4c69-9789-3f35e16458c2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n", "original_text": "Mason. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0043bf0e-8146-49bb-966c-6294469fb39c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of American Statistical Association*, 109(505):411\u2013423, 2014.\n\n A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*. ", "original_text": "Einmahl and D.M. "}, "hash": "9f758d9b098af06d49473515467459ee770c350af8f7aa3e6a08f371cf7cd903", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "156452e4-bfd3-4d9a-9e4e-ec64ca66c407", "node_type": "1", "metadata": {"window": "Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz. ", "original_text": "Generalized quantile processes. "}, "hash": "9fbc9fec59da7472f3dd71f359a6470577d452e9646bce750b0ba6504aff016d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mason. ", "mimetype": "text/plain", "start_char_idx": 45896, "end_char_idx": 45903, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "156452e4-bfd3-4d9a-9e4e-ec64ca66c407", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz. ", "original_text": "Generalized quantile processes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c787119-dc0f-4c69-9789-3f35e16458c2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A. Cuevas, M. Febrero, and R. Fraiman.  Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n", "original_text": "Mason. "}, "hash": "b434a6b4b3255d69fc1299c1374fc98d1acdc89a8d13fb148b536ce104b67fc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff61a516-a972-4d9a-99a8-294dba6fefdf", "node_type": "1", "metadata": {"window": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data. ", "original_text": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n"}, "hash": "2bc6e2340660404c234011096ff23039f58b9307dbfac553817efbfa90a4010f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Generalized quantile processes. ", "mimetype": "text/plain", "start_char_idx": 45903, "end_char_idx": 45935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff61a516-a972-4d9a-99a8-294dba6fefdf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data. ", "original_text": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "156452e4-bfd3-4d9a-9e4e-ec64ca66c407", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Robust estimation and classification for functional data via projection-based depth notions.  *Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz. ", "original_text": "Generalized quantile processes. "}, "hash": "f158bf066b67b84c39ac8aebb70d3edc3fce4a182ce0a7edb7ddf8f44bb2a3ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21a40a2c-4284-4615-a873-568fa0ffe6a3", "node_type": "1", "metadata": {"window": "J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n", "original_text": "F. Ferraty and P. Vieu. "}, "hash": "520f11adb2b7c88e5918605d4745b5212c4f463bb0f80c1aad5378bc57f9393b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n", "mimetype": "text/plain", "start_char_idx": 45935, "end_char_idx": 45987, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21a40a2c-4284-4615-a873-568fa0ffe6a3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n", "original_text": "F. Ferraty and P. Vieu. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff61a516-a972-4d9a-99a8-294dba6fefdf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Computational Statistics*, 22(3):481\u2013496, 2007.\n\n J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data. ", "original_text": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n"}, "hash": "75a7125798d5dda717f1597e516bd0c256f8c59749b5f48a573ed401e700891f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48eeaec0-f7a4-4374-ab45-a5aa092333ce", "node_type": "1", "metadata": {"window": "Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel. ", "original_text": "*Nonparametric Functional Data Analysis*. "}, "hash": "3c981488c7241da34f43ad5069479e6e91eb4270510d1bae2ce01c4c0b039b60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "F. Ferraty and P. Vieu. ", "mimetype": "text/plain", "start_char_idx": 45987, "end_char_idx": 46011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48eeaec0-f7a4-4374-ab45-a5aa092333ce", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel. ", "original_text": "*Nonparametric Functional Data Analysis*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21a40a2c-4284-4615-a873-568fa0ffe6a3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "J.H.J.  Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n", "original_text": "F. Ferraty and P. Vieu. "}, "hash": "4402f258c28278a25e818116ee078193aea82db9263edba59a8f3066c48cbb26", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8e48cfbc-56c8-481d-a859-742bc88c74d4", "node_type": "1", "metadata": {"window": "Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees. ", "original_text": "Springer-Verlag, New York, 2006.\n\n"}, "hash": "1c2a87eee72a8a4d0c8f72a1a333b465ae21e154d0b8162a28d69aced0cd67dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Nonparametric Functional Data Analysis*. ", "mimetype": "text/plain", "start_char_idx": 46011, "end_char_idx": 46053, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8e48cfbc-56c8-481d-a859-742bc88c74d4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees. ", "original_text": "Springer-Verlag, New York, 2006.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48eeaec0-f7a4-4374-ab45-a5aa092333ce", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Einmahl and D.M.  Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel. ", "original_text": "*Nonparametric Functional Data Analysis*. "}, "hash": "e1257accbf8ddaa8fe91beb53cca037aa1251b6dca9ebdc6ecf90f661b40dce5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5865d083-3649-4496-8008-f00f4a30afb4", "node_type": "1", "metadata": {"window": "Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n", "original_text": "R Fraiman and G Muniz. "}, "hash": "96364fdd891d2445f12a8dcd4f48849064bb5ff32b3e5d11c164409f73aadb83", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer-Verlag, New York, 2006.\n\n", "mimetype": "text/plain", "start_char_idx": 46053, "end_char_idx": 46087, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5865d083-3649-4496-8008-f00f4a30afb4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n", "original_text": "R Fraiman and G Muniz. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8e48cfbc-56c8-481d-a859-742bc88c74d4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Mason.  Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees. ", "original_text": "Springer-Verlag, New York, 2006.\n\n"}, "hash": "8a337f4d07cb6880313d7536b0736b19f12bdb79a716a063b6c39849d464c393", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cf3d006-1190-456a-aedf-a67fe1e88d08", "node_type": "1", "metadata": {"window": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner. ", "original_text": "Trimmed means for functional data. "}, "hash": "fe1d3f904ff0085e572d4868bfcae23003f1d797439868aad9f3dffd868c9ef4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "R Fraiman and G Muniz. ", "mimetype": "text/plain", "start_char_idx": 46087, "end_char_idx": 46110, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8cf3d006-1190-456a-aedf-a67fe1e88d08", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner. ", "original_text": "Trimmed means for functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5865d083-3649-4496-8008-f00f4a30afb4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Generalized quantile processes.  *The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n", "original_text": "R Fraiman and G Muniz. "}, "hash": "6cf7ca2083e30af9c53b9d0cf5ca0ff0d4d25fab46a742e5026405ffbaf97622", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66c39acf-dfb0-4d10-9c57-ae520386ebc7", "node_type": "1", "metadata": {"window": "F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest. ", "original_text": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n"}, "hash": "e6fa1c44ad568f7034551205e1b78b111169b1acb283b3edb4fe7f0131bd8945", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Trimmed means for functional data. ", "mimetype": "text/plain", "start_char_idx": 46110, "end_char_idx": 46145, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "66c39acf-dfb0-4d10-9c57-ae520386ebc7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest. ", "original_text": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8cf3d006-1190-456a-aedf-a67fe1e88d08", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*The Annals of Statistics*, 20(2):1062\u20131078, 1992.\n\n F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner. ", "original_text": "Trimmed means for functional data. "}, "hash": "1ff6110db136b49fcc240eb16e8e90fc2836dc8d1c14df9ed7f8a3b552342eda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4b654ea-4b7e-407e-9e81-0a4e62c3dcdc", "node_type": "1", "metadata": {"window": "*Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018. ", "original_text": "P Geurts, D Ernst, and L Wehenkel. "}, "hash": "b172e5fcd514de796260fe78131438d0d98e2e7acfd7f8a7c5f565964aeaa8a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n", "mimetype": "text/plain", "start_char_idx": 46145, "end_char_idx": 46179, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4b654ea-4b7e-407e-9e81-0a4e62c3dcdc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018. ", "original_text": "P Geurts, D Ernst, and L Wehenkel. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66c39acf-dfb0-4d10-9c57-ae520386ebc7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "F. Ferraty and P. Vieu.  *Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest. ", "original_text": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n"}, "hash": "d59187983e0fc846b56f8a49cddfd856e2e950d20bf869a650609523c927e7d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "308b487b-da0a-43e9-b6d3-91fb0dd640c7", "node_type": "1", "metadata": {"window": "Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n", "original_text": "Extremely randomized trees. "}, "hash": "f3ff6d5d63275b95a625d5ddb433af7dfcde5ff64e87bd2236fc0a8ae328f9c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "P Geurts, D Ernst, and L Wehenkel. ", "mimetype": "text/plain", "start_char_idx": 46179, "end_char_idx": 46214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "308b487b-da0a-43e9-b6d3-91fb0dd640c7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n", "original_text": "Extremely randomized trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4b654ea-4b7e-407e-9e81-0a4e62c3dcdc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Nonparametric Functional Data Analysis*.  Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018. ", "original_text": "P Geurts, D Ernst, and L Wehenkel. "}, "hash": "ec9ed3353b1f8cbad31f7249d1e32164e9b7fbc8a7805248706384a254d27a84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e302ae3-ef0e-4f0d-86b7-38fecf75f14e", "node_type": "1", "metadata": {"window": "R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J. ", "original_text": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n"}, "hash": "d3c595a10bdb38f8ade942b1c64ad9c70bde71cffec21f0ad46daa3839b5645c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extremely randomized trees. ", "mimetype": "text/plain", "start_char_idx": 46214, "end_char_idx": 46242, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e302ae3-ef0e-4f0d-86b7-38fecf75f14e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J. ", "original_text": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "308b487b-da0a-43e9-b6d3-91fb0dd640c7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer-Verlag, New York, 2006.\n\n R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n", "original_text": "Extremely randomized trees. "}, "hash": "f3eaf2624b0d63b051fa54848a8d5457ff1ec1589fa4f0546b2f269622f2dbc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c177b0ba-5b99-4c91-b66c-82cef10acb2e", "node_type": "1", "metadata": {"window": "Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert. ", "original_text": "S. Hariri, M. Carrasco Kind, and R. J. Brunner. "}, "hash": "f873f214223ba7fa1f6d4ada94a56490ad40174914274239356dd4509f184493", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n", "mimetype": "text/plain", "start_char_idx": 46242, "end_char_idx": 46286, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c177b0ba-5b99-4c91-b66c-82cef10acb2e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert. ", "original_text": "S. Hariri, M. Carrasco Kind, and R. J. Brunner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e302ae3-ef0e-4f0d-86b7-38fecf75f14e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "R Fraiman and G Muniz.  Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J. ", "original_text": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n"}, "hash": "acaf0ee4d53f1fccac7c004284479633bb4c540cd65584a58c1a3ae2f9a7cab5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14a92bf2-9a33-4919-9692-03a3d6ddc585", "node_type": "1", "metadata": {"window": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection. ", "original_text": "Extended isolation forest. "}, "hash": "964035d5606ac8e492337cdcc48a84441a9c9d2697809e5995fcddd52c7ab55c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S. Hariri, M. Carrasco Kind, and R. J. Brunner. ", "mimetype": "text/plain", "start_char_idx": 46286, "end_char_idx": 46334, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14a92bf2-9a33-4919-9692-03a3d6ddc585", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection. ", "original_text": "Extended isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c177b0ba-5b99-4c91-b66c-82cef10acb2e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Trimmed means for functional data.  *Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert. ", "original_text": "S. Hariri, M. Carrasco Kind, and R. J. Brunner. "}, "hash": "0bdf9ad4f7ca9802dd316719ba8aa162b2dfe8dab6865323168d0938294e8603", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90c5965e-66c8-4541-aff1-e614fc269500", "node_type": "1", "metadata": {"window": "P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n", "original_text": "*ArXiv e-prints*, 2018. "}, "hash": "24fa3b1953a9fa5fe62a179d215edbc0cb7a40434795ad35b468fec9a90b4f51", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extended isolation forest. ", "mimetype": "text/plain", "start_char_idx": 46334, "end_char_idx": 46361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90c5965e-66c8-4541-aff1-e614fc269500", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n", "original_text": "*ArXiv e-prints*, 2018. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14a92bf2-9a33-4919-9692-03a3d6ddc585", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Test*, 10(2):419\u2013440, Dec 2001.\n\n P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection. ", "original_text": "Extended isolation forest. "}, "hash": "d95ebf6eafca451c843ed4967ced499d3fe444138a39c8adbc914ea964ec2063", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09c5e38f-d8e1-4cac-9324-2cdd100531fd", "node_type": "1", "metadata": {"window": "Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. ", "original_text": "URL https://arxiv.org/abs/1811.02141.\n\n"}, "hash": "868c3505f09a2ff5bf8344ffbf78d0c395b003bd68fdfb443a09eccc0fd938a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*ArXiv e-prints*, 2018. ", "mimetype": "text/plain", "start_char_idx": 46361, "end_char_idx": 46385, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09c5e38f-d8e1-4cac-9324-2cdd100531fd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. ", "original_text": "URL https://arxiv.org/abs/1811.02141.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90c5965e-66c8-4541-aff1-e614fc269500", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "P Geurts, D Ernst, and L Wehenkel.  Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n", "original_text": "*ArXiv e-prints*, 2018. "}, "hash": "4775342261cd05b07fbcdb07accb801d6c191297510b468369f2665dd7c77bda", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8145de9-dffb-453f-a954-4c5d517e3edf", "node_type": "1", "metadata": {"window": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition. ", "original_text": "M. Hubert, P.J. "}, "hash": "d996b531e4f5542b95ead2143cca2a24d7d591cbc8ffc12c85a1ce56d54fa0f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "URL https://arxiv.org/abs/1811.02141.\n\n", "mimetype": "text/plain", "start_char_idx": 46385, "end_char_idx": 46424, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8145de9-dffb-453f-a954-4c5d517e3edf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition. ", "original_text": "M. Hubert, P.J. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09c5e38f-d8e1-4cac-9324-2cdd100531fd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Extremely randomized trees.  *Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. ", "original_text": "URL https://arxiv.org/abs/1811.02141.\n\n"}, "hash": "34b7083b26231c0beb8be0baacadde733f5b67a078f64becd1634eaa2554c8cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea00f9ac-7430-4378-ba54-90561682a7ff", "node_type": "1", "metadata": {"window": "S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n", "original_text": "Rousseeuw, and P. Segaert. "}, "hash": "6e8b45bef45dee01add78923a5d7427c22acdb2127b9806992e2baa96d4f58c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "M. Hubert, P.J. ", "mimetype": "text/plain", "start_char_idx": 46424, "end_char_idx": 46440, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea00f9ac-7430-4378-ba54-90561682a7ff", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n", "original_text": "Rousseeuw, and P. Segaert. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8145de9-dffb-453f-a954-4c5d517e3edf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Machine Learning*, 63 (1):3\u201342, Apr 2006.\n\n S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition. ", "original_text": "M. Hubert, P.J. "}, "hash": "458f1cd84960b6fc0ffef23c1d757b35b7684a16da1caddf51b336848f0ef491", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5103cde9-28d0-4682-b271-aef99ab82b99", "node_type": "1", "metadata": {"window": "Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A. ", "original_text": "Multivariate functional outlier detection. "}, "hash": "30442972967f129c68857527500bb6711624f2e8fed151028c09e528a30105a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rousseeuw, and P. Segaert. ", "mimetype": "text/plain", "start_char_idx": 46440, "end_char_idx": 46467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5103cde9-28d0-4682-b271-aef99ab82b99", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A. ", "original_text": "Multivariate functional outlier detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea00f9ac-7430-4378-ba54-90561682a7ff", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S. Hariri, M. Carrasco Kind, and R. J. Brunner.  Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n", "original_text": "Rousseeuw, and P. Segaert. "}, "hash": "ba7bda7af544b05c3921316c4eeaec2eb81b76bf83153bd67e61472ebd05b0d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "febb1b07-71ff-4e49-901c-a999e2010cd3", "node_type": "1", "metadata": {"window": "*ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y. ", "original_text": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n"}, "hash": "2d644d5c793d2efbb7e89fddf90598c99847a5f582b20361fed9f99778a4d422", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multivariate functional outlier detection. ", "mimetype": "text/plain", "start_char_idx": 46467, "end_char_idx": 46510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "febb1b07-71ff-4e49-901c-a999e2010cd3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y. ", "original_text": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5103cde9-28d0-4682-b271-aef99ab82b99", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Extended isolation forest.  *ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A. ", "original_text": "Multivariate functional outlier detection. "}, "hash": "e2a6de7c2f61b60f3a53c464377e0a632534f9082da014befa43476736784f5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e21c30ce-337f-414e-8091-e04b8fee74fb", "node_type": "1", "metadata": {"window": "URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu. ", "original_text": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. "}, "hash": "af3afc40f98fb5947835af0a4d62c632bf3f27071ca7f74f0c5b4d9076a149b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n", "mimetype": "text/plain", "start_char_idx": 46510, "end_char_idx": 46570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e21c30ce-337f-414e-8091-e04b8fee74fb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu. ", "original_text": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "febb1b07-71ff-4e49-901c-a999e2010cd3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*ArXiv e-prints*, 2018.  URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y. ", "original_text": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n"}, "hash": "5fb963eb56b8a4cf101995c82e3f526dcebb9ab097313a0079b5eca9e8b7ec3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "719c280a-cd86-463d-9e92-2ebd802dd72b", "node_type": "1", "metadata": {"window": "M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot. ", "original_text": "Gradient-based learning applied to document recognition. "}, "hash": "18a9b3ab47ff8035334ef9831ca99f85d6a96ed1370c860e36d8503b2aefb661", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. ", "mimetype": "text/plain", "start_char_idx": 46570, "end_char_idx": 46618, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "719c280a-cd86-463d-9e92-2ebd802dd72b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot. ", "original_text": "Gradient-based learning applied to document recognition. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e21c30ce-337f-414e-8091-e04b8fee74fb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "URL https://arxiv.org/abs/1811.02141.\n\n M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu. ", "original_text": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. "}, "hash": "ddcae1d41be513518bfd1cd09e45eb38a7794c773f1c6c7dc04dcf89b43c13a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09e1821c-184b-4907-bd1a-a936e66cf46f", "node_type": "1", "metadata": {"window": "Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n", "original_text": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n"}, "hash": "4ffde2e816758e9834b04f22316766fdf2e342b5abf21fa3338801310a014ba4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Gradient-based learning applied to document recognition. ", "mimetype": "text/plain", "start_char_idx": 46618, "end_char_idx": 46675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09e1821c-184b-4907-bd1a-a936e66cf46f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n", "original_text": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "719c280a-cd86-463d-9e92-2ebd802dd72b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "M. Hubert, P.J.  Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot. ", "original_text": "Gradient-based learning applied to document recognition. "}, "hash": "b24266427c3e00417bf92ac9a97e27cdeae0a60f6f7970042aec131fcc3e83f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd5b0856-09fc-4a78-8257-a19fd3cc93c2", "node_type": "1", "metadata": {"window": "Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou. ", "original_text": "J. Li, J.A. "}, "hash": "606f14408d4a36df96a3d920daa1f4bf1bd2d93f9cf6b0adc8b5a68f17138109", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n", "mimetype": "text/plain", "start_char_idx": 46675, "end_char_idx": 46727, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd5b0856-09fc-4a78-8257-a19fd3cc93c2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou. ", "original_text": "J. Li, J.A. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09e1821c-184b-4907-bd1a-a936e66cf46f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Rousseeuw, and P. Segaert.  Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n", "original_text": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n"}, "hash": "84852546af72e7f637a01ea4f07900760f3b5bb302730ec66bf0c46ec7900a63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef6e1bd4-3778-456a-9f38-880aceb6d5b5", "node_type": "1", "metadata": {"window": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest. ", "original_text": "Cuesta-Albertos, and R.Y. "}, "hash": "71415c248f62788346e80010d1dbbab1c060db4b6200befa077d64233929b36c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J. Li, J.A. ", "mimetype": "text/plain", "start_char_idx": 46727, "end_char_idx": 46739, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef6e1bd4-3778-456a-9f38-880aceb6d5b5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest. ", "original_text": "Cuesta-Albertos, and R.Y. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd5b0856-09fc-4a78-8257-a19fd3cc93c2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Multivariate functional outlier detection.  *Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou. ", "original_text": "J. Li, J.A. "}, "hash": "9a96165d72a4077e9c5f58e5cc9127d13215f7151e04b61f160b74e637d2c057", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac3a0452-cef2-48db-9863-0b98be390957", "node_type": "1", "metadata": {"window": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. ", "original_text": "Liu. "}, "hash": "d10f1ae34b2552bbea6eb9f451f446cefe1d7e1e0d0b775ed1b5d0bf19422698", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Cuesta-Albertos, and R.Y. ", "mimetype": "text/plain", "start_char_idx": 46739, "end_char_idx": 46765, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac3a0452-cef2-48db-9863-0b98be390957", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. ", "original_text": "Liu. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef6e1bd4-3778-456a-9f38-880aceb6d5b5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Statistical Methods & Applications*, 24(2):177\u2013202, 2015.\n\n Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest. ", "original_text": "Cuesta-Albertos, and R.Y. "}, "hash": "71bce5367d24583f0b23bfa579f2ea540080cd5ba99f672f666a2f719c2138bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8aa4c24-2bdc-4219-84f8-181a1893f587", "node_type": "1", "metadata": {"window": "Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n", "original_text": "DD-classifier: Nonparametric classification procedure based on DD-plot. "}, "hash": "89531175ea9f6d5adcb3c87a9e1d3897457e50858c475e6545587c8c0afefc0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Liu. ", "mimetype": "text/plain", "start_char_idx": 46765, "end_char_idx": 46770, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8aa4c24-2bdc-4219-84f8-181a1893f587", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n", "original_text": "DD-classifier: Nonparametric classification procedure based on DD-plot. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac3a0452-cef2-48db-9863-0b98be390957", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner.  Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. ", "original_text": "Liu. "}, "hash": "5419b4b2f8277f3d134960aeff8b7887be9c10709900414e8eac561028960e10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7be9b198-b727-4a3c-98c2-9e392c00cc28", "node_type": "1", "metadata": {"window": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou. ", "original_text": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n"}, "hash": "0ce3b8f4d9f1bc57664f3b3dc9d44e169113aa24459c00423ddf28a17c171e66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DD-classifier: Nonparametric classification procedure based on DD-plot. ", "mimetype": "text/plain", "start_char_idx": 46770, "end_char_idx": 46842, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7be9b198-b727-4a3c-98c2-9e392c00cc28", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou. ", "original_text": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8aa4c24-2bdc-4219-84f8-181a1893f587", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Gradient-based learning applied to document recognition.  *Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n", "original_text": "DD-classifier: Nonparametric classification procedure based on DD-plot. "}, "hash": "6cdcad76f6b61558e19101b0511be2ba7bf2402f825d530e9f7173cd885e866b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b5c9866-3d8f-454a-a7cd-8d55af3efb93", "node_type": "1", "metadata": {"window": "J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection. ", "original_text": "F. T. Liu, K. M. Ting, and Z. Zhou. "}, "hash": "e068115b7a43738c5ab572c60d3ddfcf9a5a468d9c7ebb8a863a669a8e3d112d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n", "mimetype": "text/plain", "start_char_idx": 46842, "end_char_idx": 46919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b5c9866-3d8f-454a-a7cd-8d55af3efb93", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection. ", "original_text": "F. T. Liu, K. M. Ting, and Z. Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7be9b198-b727-4a3c-98c2-9e392c00cc28", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Proceedings of the IEEE*, 86(11):2278\u20132324, 1998.\n\n J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou. ", "original_text": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n"}, "hash": "8f1b6a98c16bb61f149fbb33c8d29e608fc962db6ee840df770f580a15671071", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16a1a1df-0bdb-4491-88f1-b4be0aa172f4", "node_type": "1", "metadata": {"window": "Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n", "original_text": "Isolation forest. "}, "hash": "14f8abac3cf278dff4c11c934e5c9ce613359b4d8d277de54f0c5784bd0a4906", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "F. T. Liu, K. M. Ting, and Z. Zhou. ", "mimetype": "text/plain", "start_char_idx": 46919, "end_char_idx": 46955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "16a1a1df-0bdb-4491-88f1-b4be0aa172f4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n", "original_text": "Isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b5c9866-3d8f-454a-a7cd-8d55af3efb93", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "J. Li, J.A.  Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection. ", "original_text": "F. T. Liu, K. M. Ting, and Z. Zhou. "}, "hash": "374fb1690f21efd2f4ecf9b8ba05fe758d3caaca85cbc12118fa4406ee59d18e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07683600-5476-430d-8b07-6c1e6500895c", "node_type": "1", "metadata": {"window": "Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G. ", "original_text": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. "}, "hash": "44dc77f9f8790d82ec5772f47b98c9b70db7c9740045c1f8aea8803047eb3547", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation forest. ", "mimetype": "text/plain", "start_char_idx": 46955, "end_char_idx": 46973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07683600-5476-430d-8b07-6c1e6500895c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G. ", "original_text": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16a1a1df-0bdb-4491-88f1-b4be0aa172f4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Cuesta-Albertos, and R.Y.  Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n", "original_text": "Isolation forest. "}, "hash": "359198ea3e17f3f3e8729935c4d2f2192b15263d88c63caba04cac51b29ba66a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b4079df-9682-4be2-9516-3a4f795906c4", "node_type": "1", "metadata": {"window": "DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang. ", "original_text": "IEEE Computer Society, 2008.\n\n"}, "hash": "fb1b2c675189977c870c727c28787f0a72829a6b23b52050fdb06a4a2e0db459", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. ", "mimetype": "text/plain", "start_char_idx": 46973, "end_char_idx": 47070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b4079df-9682-4be2-9516-3a4f795906c4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang. ", "original_text": "IEEE Computer Society, 2008.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07683600-5476-430d-8b07-6c1e6500895c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Liu.  DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G. ", "original_text": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422. "}, "hash": "f89168551c2e53456985bcd0e314c8eb6cbb4c3fb68e030384f2e6e2bd745af2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ddacb665-72be-4b1a-ba5d-552931b22f27", "node_type": "1", "metadata": {"window": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries. ", "original_text": "F. T. Liu, K. M. Ting, and Z. Zhou. "}, "hash": "57e6fcc59bd2ea97499bd257aaf9ed446affd678d6f96623001ebcd39ccce96a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE Computer Society, 2008.\n\n", "mimetype": "text/plain", "start_char_idx": 47070, "end_char_idx": 47100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ddacb665-72be-4b1a-ba5d-552931b22f27", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries. ", "original_text": "F. T. Liu, K. M. Ting, and Z. Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b4079df-9682-4be2-9516-3a4f795906c4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "DD-classifier: Nonparametric classification procedure based on DD-plot.  *Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang. ", "original_text": "IEEE Computer Society, 2008.\n\n"}, "hash": "ca4a4e6c4bb932dcdeae1575f18b8193b6c3e4a99b159ae192ca74ebfedaf2f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8961bfb4-ef87-4c92-9883-86bd12830a78", "node_type": "1", "metadata": {"window": "F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n", "original_text": "Isolation-based anomaly detection. "}, "hash": "e32edaa32656a83824709d348db63bb78ce119da3729439d13fe3d4d829cc98b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "F. T. Liu, K. M. Ting, and Z. Zhou. ", "mimetype": "text/plain", "start_char_idx": 47100, "end_char_idx": 47136, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8961bfb4-ef87-4c92-9883-86bd12830a78", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n", "original_text": "Isolation-based anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ddacb665-72be-4b1a-ba5d-552931b22f27", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of the American Statistical Association*, 107(498): 737\u2013753, 2012.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries. ", "original_text": "F. T. Liu, K. M. Ting, and Z. Zhou. "}, "hash": "b1895b9ce64897469ecf935355e2e49c134fe4a871576d7c254e4226050a3b48", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "735b4549-1f35-4d98-b320-25f520d9244c", "node_type": "1", "metadata": {"window": "Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya. ", "original_text": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n"}, "hash": "0049fe72c7cb9bd8bd6609fdc426e15a15bc988185360eb72b4c52745fb3b6d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation-based anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 47136, "end_char_idx": 47171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "735b4549-1f35-4d98-b320-25f520d9244c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya. ", "original_text": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8961bfb4-ef87-4c92-9883-86bd12830a78", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n", "original_text": "Isolation-based anomaly detection. "}, "hash": "d9ba4760d67d65178c611ddf6cf45d1824a72320681bbb0995fb91b7464ed737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31bba59b-1c9b-464b-89a7-d46644279523", "node_type": "1", "metadata": {"window": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. ", "original_text": "S.G. "}, "hash": "a87c08baaccc68d31a3749d56ac0f7b42bc61e1452cefd07403cadfe1b38f072", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n", "mimetype": "text/plain", "start_char_idx": 47171, "end_char_idx": 47263, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31bba59b-1c9b-464b-89a7-d46644279523", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. ", "original_text": "S.G. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "735b4549-1f35-4d98-b320-25f520d9244c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation forest.  In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya. ", "original_text": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n"}, "hash": "b03cf4d39cd7cfb622239e701fbe78df5551f7a69bbdc07db12eb90e27154462", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d293bd0-dea3-44ef-97f3-c3708af26b97", "node_type": "1", "metadata": {"window": "IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n", "original_text": "Mallat and Z. Zhang. "}, "hash": "f9717d120d14698c25cfc3638bbb6170c10c10ac7d2379d2c0af24d856b7ef70", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S.G. ", "mimetype": "text/plain", "start_char_idx": 47263, "end_char_idx": 47268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d293bd0-dea3-44ef-97f3-c3708af26b97", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n", "original_text": "Mallat and Z. Zhang. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31bba59b-1c9b-464b-89a7-d46644279523", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Proceedings of the 2008 Eighth IEEE International Conference on Data Mining*, pages 413\u2013422.  IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. ", "original_text": "S.G. "}, "hash": "86164e356fdd9ac9f12ce6aa30935ca425088734a6104914115323d96e7a0ee8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7770fc67-032f-4052-96fc-a592f0e221b2", "node_type": "1", "metadata": {"window": "F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler. ", "original_text": "Matching pursuits with time-frequency dictionaries. "}, "hash": "2376e7252fc95723eb86d4fa9579ee68cfef526cc00679ec3277668c028817bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Mallat and Z. Zhang. ", "mimetype": "text/plain", "start_char_idx": 47268, "end_char_idx": 47289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7770fc67-032f-4052-96fc-a592f0e221b2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler. ", "original_text": "Matching pursuits with time-frequency dictionaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d293bd0-dea3-44ef-97f3-c3708af26b97", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "IEEE Computer Society, 2008.\n\n F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n", "original_text": "Mallat and Z. Zhang. "}, "hash": "d6fd9a71c793d29cd61e77c8574775d2c9ec88fa904e7445dea97451cebbe1b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b91ebee6-664c-4b44-8847-cff01c921bba", "node_type": "1", "metadata": {"window": "Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics. ", "original_text": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n"}, "hash": "9e8f10f24bf0c4c28183710dc0072d98e675d44ba30a2395567a59e380113432", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Matching pursuits with time-frequency dictionaries. ", "mimetype": "text/plain", "start_char_idx": 47289, "end_char_idx": 47341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b91ebee6-664c-4b44-8847-cff01c921bba", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics. ", "original_text": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7770fc67-032f-4052-96fc-a592f0e221b2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "F. T. Liu, K. M. Ting, and Z. Zhou.  Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler. ", "original_text": "Matching pursuits with time-frequency dictionaries. "}, "hash": "584f48d6a9518bcf033aa6e33926bde9d5cfa1261b408bbab62e59495ab538d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebfd57cc-f54e-4ac8-abb6-344b73042232", "node_type": "1", "metadata": {"window": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. ", "original_text": "V. Maz\u2019ya. "}, "hash": "f6e940dbe5ef5b2c8143a1614fae8362c5af9abe17b31e614153b1376b1ee7fa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n", "mimetype": "text/plain", "start_char_idx": 47341, "end_char_idx": 47408, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebfd57cc-f54e-4ac8-abb6-344b73042232", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. ", "original_text": "V. Maz\u2019ya. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b91ebee6-664c-4b44-8847-cff01c921bba", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation-based anomaly detection.  In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics. ", "original_text": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n"}, "hash": "cd71aea1a1d16c73dfb954c77f1e19115f79e7355a555d5f393ea2d3e89ea8e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2f113ec-666a-41f8-be8a-8bcecc07d800", "node_type": "1", "metadata": {"window": "S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n", "original_text": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. "}, "hash": "492a4146c176077606938bb0dc677cc12962fc79dc3302e6c296113c64725d34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "V. Maz\u2019ya. ", "mimetype": "text/plain", "start_char_idx": 47408, "end_char_idx": 47419, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2f113ec-666a-41f8-be8a-8bcecc07d800", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n", "original_text": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebfd57cc-f54e-4ac8-abb6-344b73042232", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *ACM Transactions on Knowledge Discovery from Data (TKDD)*, volume 6, pages 1\u201339, 2012.\n\n S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. ", "original_text": "V. Maz\u2019ya. "}, "hash": "592f5700e93cbaebafbf9dcb8ebb57ffe97159fd5734b4fa1e0a3a14570b32f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b9283ca-1365-40bb-8cab-28fb4e1a18c3", "node_type": "1", "metadata": {"window": "Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi. ", "original_text": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n"}, "hash": "06151e8dc0021ebe876fd7effaf31e94f7f6515c8505d39cae55f4ee3b42de89", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. ", "mimetype": "text/plain", "start_char_idx": 47419, "end_char_idx": 47499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b9283ca-1365-40bb-8cab-28fb4e1a18c3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi. ", "original_text": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2f113ec-666a-41f8-be8a-8bcecc07d800", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S.G.  Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n", "original_text": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*. "}, "hash": "12ea8523158a30afbfb9a34ab6767d6902a0f20d88a4874bd1446ac477bf60cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a61bc096-e48f-46d4-9666-c1c291e8e94e", "node_type": "1", "metadata": {"window": "Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data. ", "original_text": "K. Mosler. "}, "hash": "eaaf5935f360d40e5bba4c28c75565805259bfa0a5c79dc09e0dbdc1e5b76755", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n", "mimetype": "text/plain", "start_char_idx": 47499, "end_char_idx": 47542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a61bc096-e48f-46d4-9666-c1c291e8e94e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data. ", "original_text": "K. Mosler. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b9283ca-1365-40bb-8cab-28fb4e1a18c3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Mallat and Z. Zhang.  Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi. ", "original_text": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n"}, "hash": "ed2903a1c40396c6eef300bbbccf9bb28397c7a1c03f176cbbcd04dc8e3b9119", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a60c29bb-a6ac-4dc2-9d7f-ba108e4ecac6", "node_type": "1", "metadata": {"window": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n", "original_text": "Depth statistics. "}, "hash": "8e884c54c968a4264d7524274720eb8878b21509bcda961f81974a152dbc2115", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K. Mosler. ", "mimetype": "text/plain", "start_char_idx": 47542, "end_char_idx": 47553, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a60c29bb-a6ac-4dc2-9d7f-ba108e4ecac6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n", "original_text": "Depth statistics. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a61bc096-e48f-46d4-9666-c1c291e8e94e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Matching pursuits with time-frequency dictionaries.  *IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data. ", "original_text": "K. Mosler. "}, "hash": "0e1600caf79a2d4f4973ea9ad23ca95f5b9942af9bba8fe2e38d8cafc611f4cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62161a4c-f3e0-448d-8d0e-2863572a7f98", "node_type": "1", "metadata": {"window": "V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z. ", "original_text": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. "}, "hash": "4f1466ebd4d8f5746eb0a23df6adc77ac089a7ed2f51fba372a5d27181458958", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Depth statistics. ", "mimetype": "text/plain", "start_char_idx": 47553, "end_char_idx": 47571, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62161a4c-f3e0-448d-8d0e-2863572a7f98", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z. ", "original_text": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a60c29bb-a6ac-4dc2-9d7f-ba108e4ecac6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*IEEE Transactions on signal processing*, 41(12):3397\u20133415, 1993.\n\n V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n", "original_text": "Depth statistics. "}, "hash": "d77c2b3f8ef1fc0b6f1e47678b0b8e3f30c06db70c8eb1de67bdc6ca48332879", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0307296-5800-4f35-ad32-cf21e1caecf2", "node_type": "1", "metadata": {"window": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding. ", "original_text": "Springer, Berlin Heidelberg, 2013.\n\n"}, "hash": "c1d17c82bb956f2d2017c0d295036166ef0f77bda2838207059d79904ef096cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. ", "mimetype": "text/plain", "start_char_idx": 47571, "end_char_idx": 47669, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a0307296-5800-4f35-ad32-cf21e1caecf2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding. ", "original_text": "Springer, Berlin Heidelberg, 2013.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62161a4c-f3e0-448d-8d0e-2863572a7f98", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "V. Maz\u2019ya.  *Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z. ", "original_text": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334. "}, "hash": "14a2ca0f5ced9790e5161e89fbe5aab1d46fd525a9b6d3c5428a1236da096d9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8af598d8-9855-46c9-8b36-7a56960624d1", "node_type": "1", "metadata": {"window": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection. ", "original_text": "K. Mosler and P. Mozharovskyi. "}, "hash": "d00dbb49d3c0d65430a18eb444bbd8752e55c4118eb389c01252848019621b2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer, Berlin Heidelberg, 2013.\n\n", "mimetype": "text/plain", "start_char_idx": 47669, "end_char_idx": 47705, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8af598d8-9855-46c9-8b36-7a56960624d1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection. ", "original_text": "K. Mosler and P. Mozharovskyi. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0307296-5800-4f35-ad32-cf21e1caecf2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Sobolev Spaces: with Applications to Elliptic Partial Differential Equations*.  Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding. ", "original_text": "Springer, Berlin Heidelberg, 2013.\n\n"}, "hash": "7ba6a2bb54de03a5cd7d64a2db1d0e015ed1d5cec4a040ef5ef28b934e78a6e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c7ad337-c5e9-4377-aec2-d3535460494d", "node_type": "1", "metadata": {"window": "K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n", "original_text": "Fast DD-classification of functional data. "}, "hash": "085f2ac231fcb04544d56f1ba1e40cb655f249a3c801b67270e10b4c7a865445", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "K. Mosler and P. Mozharovskyi. ", "mimetype": "text/plain", "start_char_idx": 47705, "end_char_idx": 47736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c7ad337-c5e9-4377-aec2-d3535460494d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n", "original_text": "Fast DD-classification of functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8af598d8-9855-46c9-8b36-7a56960624d1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer-Verlag, Berlin Heidelberg, 2011.\n\n K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection. ", "original_text": "K. Mosler and P. Mozharovskyi. "}, "hash": "103079bff610437477d4518308f130b5a61e4c4ee732ccea92300461915ed117", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95df865b-e356-4f1a-bcc8-a3271b313875", "node_type": "1", "metadata": {"window": "Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik. ", "original_text": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n"}, "hash": "53fdc6950fce6113472f116123349e0f300430c9977a5c32a0176f68a8371961", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fast DD-classification of functional data. ", "mimetype": "text/plain", "start_char_idx": 47736, "end_char_idx": 47779, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95df865b-e356-4f1a-bcc8-a3271b313875", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik. ", "original_text": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c7ad337-c5e9-4377-aec2-d3535460494d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "K. Mosler.  Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n", "original_text": "Fast DD-classification of functional data. "}, "hash": "852c91add9bcdf6b7afda90c9b8790e43052861da49ff2d56bdf44be9cda33ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95905765-3b07-4637-a08e-f66d37633d64", "node_type": "1", "metadata": {"window": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes. ", "original_text": "C. Park, J.Z. "}, "hash": "10c5dd343a24022abdda0bee5b8434d89895003f8e159b157ab3e874a4c1ed52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n", "mimetype": "text/plain", "start_char_idx": 47779, "end_char_idx": 47825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "95905765-3b07-4637-a08e-f66d37633d64", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes. ", "original_text": "C. Park, J.Z. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95df865b-e356-4f1a-bcc8-a3271b313875", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Depth statistics.  In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik. ", "original_text": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n"}, "hash": "427d741564868d51279a8769f2538c5618923b96d37c5ff513ec583fcd990e96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "777b24d1-b10d-4b92-a0d6-83d3824927ed", "node_type": "1", "metadata": {"window": "Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n", "original_text": "Huang, and Y. Ding. "}, "hash": "eebd8fb0aca28330df0989f82903d7621ce0433f78867a2e0ead6ecfd192cc8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C. Park, J.Z. ", "mimetype": "text/plain", "start_char_idx": 47825, "end_char_idx": 47839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "777b24d1-b10d-4b92-a0d6-83d3824927ed", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n", "original_text": "Huang, and Y. Ding. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95905765-3b07-4637-a08e-f66d37633d64", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In *Robustness and Complex Data Structures: Festschrift in Honour of Ursula Gather*, pages 17\u201334.  Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes. ", "original_text": "C. Park, J.Z. "}, "hash": "05112d23f9974cadfa4c42c2729566da3a97b1e68d61d4baaefd7cef712192c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee9bec99-9f64-4b6a-941d-fa352d2f6910", "node_type": "1", "metadata": {"window": "K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O. ", "original_text": "A computable plug-in estimator of minimum volume sets for novelty detection. "}, "hash": "5c71f73dafed0f6bf90e10e683eca4b452ca193d69e9ad849da9708d8f0a020d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Huang, and Y. Ding. ", "mimetype": "text/plain", "start_char_idx": 47839, "end_char_idx": 47859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ee9bec99-9f64-4b6a-941d-fa352d2f6910", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O. ", "original_text": "A computable plug-in estimator of minimum volume sets for novelty detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "777b24d1-b10d-4b92-a0d6-83d3824927ed", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer, Berlin Heidelberg, 2013.\n\n K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n", "original_text": "Huang, and Y. Ding. "}, "hash": "d65d40676e86785e076a64103d933ed9893084af5cbe1ad6e53541d34eb2a717", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ae695f0-5075-4cfb-b091-e7790b6cc1cb", "node_type": "1", "metadata": {"window": "Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W. ", "original_text": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n"}, "hash": "c34ca3a4d762f3f228cce7405295ba902dee465c8c309d9debf007ad3c6ba7ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A computable plug-in estimator of minimum volume sets for novelty detection. ", "mimetype": "text/plain", "start_char_idx": 47859, "end_char_idx": 47936, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3ae695f0-5075-4cfb-b091-e7790b6cc1cb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W. ", "original_text": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee9bec99-9f64-4b6a-941d-fa352d2f6910", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "K. Mosler and P. Mozharovskyi.  Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O. ", "original_text": "A computable plug-in estimator of minimum volume sets for novelty detection. "}, "hash": "d1ea8e0b12e96ca924a6e7a793f85ac13509eb9feb45aed8848611b94de810ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5a1f288-7ac0-4e12-9d84-4cdc62a3f86f", "node_type": "1", "metadata": {"window": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman. ", "original_text": "W. Polonik. "}, "hash": "d767f15da75bbb4852d85b52a09a19a3e9ef413ba43e0443dc02fec009693eab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n", "mimetype": "text/plain", "start_char_idx": 47936, "end_char_idx": 47983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5a1f288-7ac0-4e12-9d84-4cdc62a3f86f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman. ", "original_text": "W. Polonik. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ae695f0-5075-4cfb-b091-e7790b6cc1cb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Fast DD-classification of functional data.  *Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W. ", "original_text": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n"}, "hash": "fbbe804baedec5ebcb8bd6dda92023c977885c66bb897fb5cf725c59c121eb8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce672a7e-de3d-4c1a-a6ab-f36140831812", "node_type": "1", "metadata": {"window": "C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*. ", "original_text": "Minimum volume sets and generalized quantile processes. "}, "hash": "6343c7b42cf467765561bd029e4cfaa4982ce7e67a4a60016872ccd61cf11d35", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "W. Polonik. ", "mimetype": "text/plain", "start_char_idx": 47983, "end_char_idx": 47995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ce672a7e-de3d-4c1a-a6ab-f36140831812", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*. ", "original_text": "Minimum volume sets and generalized quantile processes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5a1f288-7ac0-4e12-9d84-4cdc62a3f86f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Statistical Papers*, 58(4):1055\u20131089, 2017.\n\n C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman. ", "original_text": "W. Polonik. "}, "hash": "17accd77e17106f018617bcbb693187beb7c7fd96561ece100c2243a3dae4999", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "645997be-6868-4679-b02d-7891b049760c", "node_type": "1", "metadata": {"window": "Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n", "original_text": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n"}, "hash": "960ca23359f18de971946825207efdd220777b6c3b4f96323473225cdaa86f49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Minimum volume sets and generalized quantile processes. ", "mimetype": "text/plain", "start_char_idx": 47995, "end_char_idx": 48051, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "645997be-6868-4679-b02d-7891b049760c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n", "original_text": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce672a7e-de3d-4c1a-a6ab-f36140831812", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C. Park, J.Z.  Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*. ", "original_text": "Minimum volume sets and generalized quantile processes. "}, "hash": "509e17b4e014c2e7e288c01ce90dcc015b28d533bc5fe18a189621f031f14338", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcc5d55d-90f5-4936-9d6e-e197fe8ccaf0", "node_type": "1", "metadata": {"window": "A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C. ", "original_text": "J.O. "}, "hash": "678b0f0645c3884af4023444f04c8e8c4d5289c296b6c3f50a07c8c13daf27c5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n", "mimetype": "text/plain", "start_char_idx": 48051, "end_char_idx": 48117, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bcc5d55d-90f5-4936-9d6e-e197fe8ccaf0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C. ", "original_text": "J.O. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "645997be-6868-4679-b02d-7891b049760c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Huang, and Y. Ding.  A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n", "original_text": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n"}, "hash": "b9b2226aa45326e69c3481e462ba244149cfccc4b7fcab7efa2f87fa79389d79", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e672d129-cce9-439a-8896-036606b0420c", "node_type": "1", "metadata": {"window": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. ", "original_text": "Ramsay and B.W. "}, "hash": "c24d7b1a94205e200d69a023fef396d5526256129b4cb0d58202005e6c0b1c34", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "J.O. ", "mimetype": "text/plain", "start_char_idx": 48117, "end_char_idx": 48122, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e672d129-cce9-439a-8896-036606b0420c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. ", "original_text": "Ramsay and B.W. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcc5d55d-90f5-4936-9d6e-e197fe8ccaf0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A computable plug-in estimator of minimum volume sets for novelty detection.  *Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C. ", "original_text": "J.O. "}, "hash": "7b54d4874dd499f34ecd7fe76829ba3ab9c73d1f1168f04780abcc70ffbeb7b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a4810eb-4f49-4a75-bdc3-1b8ee8a2b9ee", "node_type": "1", "metadata": {"window": "W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution. ", "original_text": "Silverman. "}, "hash": "42fe9514ed0babda6564b7ff7be49e5c3e2bc174718cd10a3961a104e70dea3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Ramsay and B.W. ", "mimetype": "text/plain", "start_char_idx": 48122, "end_char_idx": 48138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a4810eb-4f49-4a75-bdc3-1b8ee8a2b9ee", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution. ", "original_text": "Silverman. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e672d129-cce9-439a-8896-036606b0420c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Operations Research*, 58(5):1469\u20131480, 2010.\n\n W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. ", "original_text": "Ramsay and B.W. "}, "hash": "8538ebe401b6c19e7ee250a339cebf8273b804d50574867f190728eb9a65ac63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9cce22e-015f-4d94-86e1-26d614c515e0", "node_type": "1", "metadata": {"window": "Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n", "original_text": "*Functional Data Analysis*. "}, "hash": "2f76c19a3bd06f5aab2d06d8443ce477df988a7bc3377769c370db03f37363a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Silverman. ", "mimetype": "text/plain", "start_char_idx": 48138, "end_char_idx": 48149, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9cce22e-015f-4d94-86e1-26d614c515e0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n", "original_text": "*Functional Data Analysis*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a4810eb-4f49-4a75-bdc3-1b8ee8a2b9ee", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "W. Polonik.  Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution. ", "original_text": "Silverman. "}, "hash": "af42b43019dede3580ee9e4e8c32619fdca19a264b16903f0efc9b5bbd377033", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84f1d4bc-d1b7-4549-bb19-067b5814d53b", "node_type": "1", "metadata": {"window": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak. ", "original_text": "Springer-Verlag, New-York, 2005.\n\n"}, "hash": "2e8b842968a7fb59005e34a12004483e8575428bc4e579838b00807bb8cb6700", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Functional Data Analysis*. ", "mimetype": "text/plain", "start_char_idx": 48149, "end_char_idx": 48177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84f1d4bc-d1b7-4549-bb19-067b5814d53b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak. ", "original_text": "Springer-Verlag, New-York, 2005.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9cce22e-015f-4d94-86e1-26d614c515e0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Minimum volume sets and generalized quantile processes.  *Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n", "original_text": "*Functional Data Analysis*. "}, "hash": "c3b300413bd959513862047c330bf6345febefa29ad0425bc2e4410af0cee806", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88e2f395-db82-4d77-b2d4-fbfb5653f79d", "node_type": "1", "metadata": {"window": "J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets. ", "original_text": "B. Sch\u00f6lkopf, J.C. "}, "hash": "0f59833a3a1b47e8061c826a3c0cb43b36b712de7b95beac260a8f0447a11008", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer-Verlag, New-York, 2005.\n\n", "mimetype": "text/plain", "start_char_idx": 48177, "end_char_idx": 48211, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88e2f395-db82-4d77-b2d4-fbfb5653f79d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets. ", "original_text": "B. Sch\u00f6lkopf, J.C. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84f1d4bc-d1b7-4549-bb19-067b5814d53b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Stochastic Processes and their Applications*, 69(1):1\u201324, 1997.\n\n J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak. ", "original_text": "Springer-Verlag, New-York, 2005.\n\n"}, "hash": "d89afb6ecc65dbe062892d493447d940a16d3d711c79241e9337af325c6247b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1cea5a50-1d29-49a2-9db8-8c211aab5743", "node_type": "1", "metadata": {"window": "Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n", "original_text": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. "}, "hash": "b291426d22b9e676db7f69c3b12b19a0b4d867089ea7e85b16e72bbfc07ff295", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "B. Sch\u00f6lkopf, J.C. ", "mimetype": "text/plain", "start_char_idx": 48211, "end_char_idx": 48230, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1cea5a50-1d29-49a2-9db8-8c211aab5743", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n", "original_text": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88e2f395-db82-4d77-b2d4-fbfb5653f79d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "J.O.  Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets. ", "original_text": "B. Sch\u00f6lkopf, J.C. "}, "hash": "bd32f6b77d0c539bbfab011794e70ac1dad02a0b7b6f8e0bbdb1b690b065a596", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4dd0e20f-f47f-4439-8cc2-b48a66710ae0", "node_type": "1", "metadata": {"window": "Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel. ", "original_text": "Estimating the support of a high-dimensional distribution. "}, "hash": "b7689809c7845282487057e38e9cbb4ee1d49aa6b5160db31759e83de0d808b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. ", "mimetype": "text/plain", "start_char_idx": 48230, "end_char_idx": 48283, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4dd0e20f-f47f-4439-8cc2-b48a66710ae0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel. ", "original_text": "Estimating the support of a high-dimensional distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cea5a50-1d29-49a2-9db8-8c211aab5743", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Ramsay and B.W.  Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n", "original_text": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson. "}, "hash": "bac56d03692ff25bbbf21bee6d2f11d557c688c9f9094483087ca9d880a4aa14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "88cf18c9-18a0-47ed-bda4-ca4c28286faf", "node_type": "1", "metadata": {"window": "*Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection. ", "original_text": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n"}, "hash": "973276490039ce2295e6816ff793a81fe3635d667ef4b4d8e681fe4a4b7f00c7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Estimating the support of a high-dimensional distribution. ", "mimetype": "text/plain", "start_char_idx": 48283, "end_char_idx": 48342, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "88cf18c9-18a0-47ed-bda4-ca4c28286faf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection. ", "original_text": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4dd0e20f-f47f-4439-8cc2-b48a66710ae0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Silverman.  *Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel. ", "original_text": "Estimating the support of a high-dimensional distribution. "}, "hash": "7934474fa963826868241096b97c79a77825ee8f133a304c606b26975858fd77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41757e94-a0c5-4898-92da-2b9720b4f9bd", "node_type": "1", "metadata": {"window": "Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n", "original_text": "C. Scott and R. Nowak. "}, "hash": "4fd6d3873e471cf9d917efa7bb8dbc89838f3c8ee678d1d28bb77d994807c564", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n", "mimetype": "text/plain", "start_char_idx": 48342, "end_char_idx": 48388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41757e94-a0c5-4898-92da-2b9720b4f9bd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n", "original_text": "C. Scott and R. Nowak. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "88cf18c9-18a0-47ed-bda4-ca4c28286faf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Functional Data Analysis*.  Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection. ", "original_text": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n"}, "hash": "39f50074437cf228eaa6e6102dba90c719af859bd1563dfd43235e5ba2fceb8e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9a2297c-ba82-45e3-be26-02b00ddfea4b", "node_type": "1", "metadata": {"window": "B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L. ", "original_text": "Learning minimum volume sets. "}, "hash": "ed3f0b66ef3711e8d1fadccdf2a3c967beb5bbd497cc1611a15c242e2ff48c81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C. Scott and R. Nowak. ", "mimetype": "text/plain", "start_char_idx": 48388, "end_char_idx": 48411, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c9a2297c-ba82-45e3-be26-02b00ddfea4b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L. ", "original_text": "Learning minimum volume sets. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41757e94-a0c5-4898-92da-2b9720b4f9bd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Springer-Verlag, New-York, 2005.\n\n B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n", "original_text": "C. Scott and R. Nowak. "}, "hash": "ca182fbeb2f51c24c08249d8ea7e239c2b1760bb64f01f3a00eb352bfd86cebf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ef79c56-568d-4d2f-8625-2119ae26e12e", "node_type": "1", "metadata": {"window": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. ", "original_text": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n"}, "hash": "e7b7b85bd89b51b6f88aba7fe165b879ee1d601fe04386ed35c7358232233f36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Learning minimum volume sets. ", "mimetype": "text/plain", "start_char_idx": 48411, "end_char_idx": 48441, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6ef79c56-568d-4d2f-8625-2119ae26e12e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. ", "original_text": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9a2297c-ba82-45e3-be26-02b00ddfea4b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "B. Sch\u00f6lkopf, J.C.  Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L. ", "original_text": "Learning minimum volume sets. "}, "hash": "0625ccf819fd8d394fde49f17d94c3ced2dbfdb5fa25dd8fb52e4b9d259e0eb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7824602c-dacf-44f5-9e79-27199f1c45d9", "node_type": "1", "metadata": {"window": "Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu. ", "original_text": "I. Steinwart, D. Hush, and C. Scovel. "}, "hash": "77d26427ce1e79ed76e75128f018c1547b7ea5665e1f03068af8ebc663996105", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n", "mimetype": "text/plain", "start_char_idx": 48441, "end_char_idx": 48499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7824602c-dacf-44f5-9e79-27199f1c45d9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu. ", "original_text": "I. Steinwart, D. Hush, and C. Scovel. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ef79c56-568d-4d2f-8625-2119ae26e12e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Platt, J. Shawe-Taylor, A. Smola, and R. Williamson.  Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. ", "original_text": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n"}, "hash": "3f778f7ab8aa9749cf4c33f7527964bb170d225db54eb73c45e9446f0f4fb910", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d4155d2-3a08-4df6-b8b4-6ea6413b7fd9", "node_type": "1", "metadata": {"window": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python. ", "original_text": "A classification framework for anomaly detection. "}, "hash": "d54c45d0989535a82a74be7907a6854acdd9955e7cb6a9ea0195a765398af8e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "I. Steinwart, D. Hush, and C. Scovel. ", "mimetype": "text/plain", "start_char_idx": 48499, "end_char_idx": 48537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4d4155d2-3a08-4df6-b8b4-6ea6413b7fd9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python. ", "original_text": "A classification framework for anomaly detection. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7824602c-dacf-44f5-9e79-27199f1c45d9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Estimating the support of a high-dimensional distribution.  *Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu. ", "original_text": "I. Steinwart, D. Hush, and C. Scovel. "}, "hash": "89ac730f7b6ef3a258728b47c32ba6c54e629ac16f857dfe7e1cf4bb08a6470c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41c4f2a7-da3f-4fcd-bc1b-b6843a6b4073", "node_type": "1", "metadata": {"window": "C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n", "original_text": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n"}, "hash": "306e6d21ff67e8c2ad15f257b31ffef3ed494deb640f7367e994ddec0a7e4e8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A classification framework for anomaly detection. ", "mimetype": "text/plain", "start_char_idx": 48537, "end_char_idx": 48587, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41c4f2a7-da3f-4fcd-bc1b-b6843a6b4073", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n", "original_text": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d4155d2-3a08-4df6-b8b4-6ea6413b7fd9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Neural Computation*, 13(7):1443\u20131471, 2001.\n\n C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python. ", "original_text": "A classification framework for anomaly detection. "}, "hash": "78db04709093ce3b02c2ecd9cfcfe0102a1e6c693d7514ed50dd2aa7b2f6d3c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8ad8614-b450-4726-b4cc-2af8e3d98c43", "node_type": "1", "metadata": {"window": "Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P. ", "original_text": "S van der Walt, J.L. "}, "hash": "852816a6efbae7ba8d01149a26f6280392fe7cb500bc00ae7f08568dd33ce8ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n", "mimetype": "text/plain", "start_char_idx": 48587, "end_char_idx": 48645, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a8ad8614-b450-4726-b4cc-2af8e3d98c43", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P. ", "original_text": "S van der Walt, J.L. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41c4f2a7-da3f-4fcd-bc1b-b6843a6b4073", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C. Scott and R. Nowak.  Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n", "original_text": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n"}, "hash": "82f812c28aae393b98e5160d5ae73cd6d930c9155ac1ee393b366ebde4f22c70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "890118c3-b1e8-4f64-9cd8-2e0cf522f889", "node_type": "1", "metadata": {"window": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert. ", "original_text": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. "}, "hash": "8133b4c30a94b953bbaa39bace12c80c90cbd5d4c8c3a59accfa02197339996d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "S van der Walt, J.L. ", "mimetype": "text/plain", "start_char_idx": 48645, "end_char_idx": 48666, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "890118c3-b1e8-4f64-9cd8-2e0cf522f889", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert. ", "original_text": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8ad8614-b450-4726-b4cc-2af8e3d98c43", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Learning minimum volume sets.  *Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P. ", "original_text": "S van der Walt, J.L. "}, "hash": "54fb7395342090e438d9a5a1c4bff597ba214cfd294dae7a4bc5fec50578d283", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "811d2991-049b-4951-adec-de01b65a0bec", "node_type": "1", "metadata": {"window": "I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms. ", "original_text": "Warner, N Yager, E Gouillart, and T Yu. "}, "hash": "099c6bcb9ec1290d9e2a6fdbf08c09b26dae4a803c6751309dc160f14d6e2763", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. ", "mimetype": "text/plain", "start_char_idx": 48666, "end_char_idx": 48714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "811d2991-049b-4951-adec-de01b65a0bec", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms. ", "original_text": "Warner, N Yager, E Gouillart, and T Yu. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "890118c3-b1e8-4f64-9cd8-2e0cf522f889", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of Machine Learning Research*, 7:665\u2013704, 2006.\n\n I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert. ", "original_text": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D. "}, "hash": "437a7c5cfc5c199a9fc33966a33ba8bd627564ae62dc0e3258c780b2820472c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b663e9b-b937-469a-a3ae-21392e748134", "node_type": "1", "metadata": {"window": "A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n", "original_text": "scikit-image: image processing in python. "}, "hash": "27c57e2e35fcc767f94d1a2e4cf1b1635614ad26dde28d7018f3b36e8bbf430d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Warner, N Yager, E Gouillart, and T Yu. ", "mimetype": "text/plain", "start_char_idx": 48714, "end_char_idx": 48754, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b663e9b-b937-469a-a3ae-21392e748134", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n", "original_text": "scikit-image: image processing in python. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "811d2991-049b-4951-adec-de01b65a0bec", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "I. Steinwart, D. Hush, and C. Scovel.  A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms. ", "original_text": "Warner, N Yager, E Gouillart, and T Yu. "}, "hash": "7286eb4c888cd2281579195a6d60f907ff1d8f10eb4d0c6a6f15f1c5192cddb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5151af2-5ef3-448d-a979-e9d92a2a46d9", "node_type": "1", "metadata": {"window": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. ", "original_text": "*PeerJ*, 2, 2014.\n\n"}, "hash": "6cb86e3d1273c3ff9529e7733cd9a1d96745b68e1044bf9de9c48a7caad8c56e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "scikit-image: image processing in python. ", "mimetype": "text/plain", "start_char_idx": 48754, "end_char_idx": 48796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5151af2-5ef3-448d-a979-e9d92a2a46d9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. ", "original_text": "*PeerJ*, 2, 2014.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b663e9b-b937-469a-a3ae-21392e748134", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A classification framework for anomaly detection.  *Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n", "original_text": "scikit-image: image processing in python. "}, "hash": "00b128c901bb817116c9a56fc89d7337d5958a999d9a3aebaf2b677ad1106bb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "338181c7-246a-4cad-82ea-633a4eb22bb1", "node_type": "1", "metadata": {"window": "S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7. ", "original_text": "R. Vert and J.-P. "}, "hash": "902e5c6fdf7a0d2035a80c985755e0c3624943f7e70d70815e25769041237389", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*PeerJ*, 2, 2014.\n\n", "mimetype": "text/plain", "start_char_idx": 48796, "end_char_idx": 48815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "338181c7-246a-4cad-82ea-633a4eb22bb1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7. ", "original_text": "R. Vert and J.-P. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5151af2-5ef3-448d-a979-e9d92a2a46d9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of Machine Learning Research*, 6:211\u2013232, 2005.\n\n S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. ", "original_text": "*PeerJ*, 2, 2014.\n\n"}, "hash": "78e39ec40caa0ba9e29f01d5bcec6ff25e1cc9b4b5ef70f099f0fbf288f86767", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2b830b1-43e6-4b70-9f5b-740b053d2427", "node_type": "1", "metadata": {"window": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n", "original_text": "Vert. "}, "hash": "cfdde9d39d39eb928d2e167e201c118c7d3eed7167328d70b48a270338a3e3a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "R. Vert and J.-P. ", "mimetype": "text/plain", "start_char_idx": 48815, "end_char_idx": 48833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2b830b1-43e6-4b70-9f5b-740b053d2427", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n", "original_text": "Vert. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "338181c7-246a-4cad-82ea-633a4eb22bb1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "S van der Walt, J.L.  Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7. ", "original_text": "R. Vert and J.-P. "}, "hash": "55689b7054147f7552bec89a225828c45d16a9910d7f38c3c42b25eaa31eec6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "313508bd-173e-47e6-8307-ef4605dbbb1a", "node_type": "1", "metadata": {"window": "Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset. ", "original_text": "Consistency and convergence rates of one-class SVMs and related algorithms. "}, "hash": "93d9588adde96f0e3b1ae552738944af7c8acc2ef1ad4e555914470f777c2ccb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Vert. ", "mimetype": "text/plain", "start_char_idx": 48833, "end_char_idx": 48839, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "313508bd-173e-47e6-8307-ef4605dbbb1a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset. ", "original_text": "Consistency and convergence rates of one-class SVMs and related algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2b830b1-43e6-4b70-9f5b-740b053d2427", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Sch\u00e4nberger, J Nunez-Iglesias, F Boulogne, J.D.  Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n", "original_text": "Vert. "}, "hash": "72ccaa246c816e436cd2291cc2fb5b618788ff05c2ec5c8efa803802ff6afa0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31582c3b-f7c2-481d-9791-7330377ba8df", "node_type": "1", "metadata": {"window": "scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them.", "original_text": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n"}, "hash": "b936582731a05472a8217b362767e79e2cb20f0937d155114c002a97044617ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Consistency and convergence rates of one-class SVMs and related algorithms. ", "mimetype": "text/plain", "start_char_idx": 48839, "end_char_idx": 48915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31582c3b-f7c2-481d-9791-7330377ba8df", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them.", "original_text": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "313508bd-173e-47e6-8307-ef4605dbbb1a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Warner, N Yager, E Gouillart, and T Yu.  scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset. ", "original_text": "Consistency and convergence rates of one-class SVMs and related algorithms. "}, "hash": "d4a0b32858a0a0271d38bb6706ca6ed2260c3cf6c5c6f155f0559a603bfe28c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7c13798-972a-488a-bb78-25b47ec548aa", "node_type": "1", "metadata": {"window": "*PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. ", "original_text": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. "}, "hash": "9003d72058121e79477ee8cba6401e530d6c103a0c9fb68d9ad06551b0fe225f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n", "mimetype": "text/plain", "start_char_idx": 48915, "end_char_idx": 48973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f7c13798-972a-488a-bb78-25b47ec548aa", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. ", "original_text": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31582c3b-f7c2-481d-9791-7330377ba8df", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "scikit-image: image processing in python.  *PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them.", "original_text": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n"}, "hash": "0e24d2d80bb2794fb8d7a73398411635bf0c902a29dfcdcdc40e05daa677f96d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da9ebf1c-94b1-4997-b7dd-aa4db75ef3c5", "node_type": "1", "metadata": {"window": "R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis. ", "original_text": "(2015) repository\u2014and evaluation points is plotted in Figure 7. "}, "hash": "7293a85fc071aee3cdbd5b059a13ed7e4f501978e39da1209212f2095c97edba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. ", "mimetype": "text/plain", "start_char_idx": 48973, "end_char_idx": 49116, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da9ebf1c-94b1-4997-b7dd-aa4db75ef3c5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis. ", "original_text": "(2015) repository\u2014and evaluation points is plotted in Figure 7. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7c13798-972a-488a-bb78-25b47ec548aa", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*PeerJ*, 2, 2014.\n\n R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. ", "original_text": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al. "}, "hash": "6a9cbdce85d344ba7b52700a50cf6f2e61552d1586733ee769728875f9a6068f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8706fe13-743f-4c2e-8cd9-c48fdd0a6b95", "node_type": "1", "metadata": {"window": "Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed. ", "original_text": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n"}, "hash": "9d29c65785a25b83bcf569e356d2ba2c64e4ce660b13406719f232febd428cb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2015) repository\u2014and evaluation points is plotted in Figure 7. ", "mimetype": "text/plain", "start_char_idx": 49116, "end_char_idx": 49180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8706fe13-743f-4c2e-8cd9-c48fdd0a6b95", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed. ", "original_text": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da9ebf1c-94b1-4997-b7dd-aa4db75ef3c5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "R. Vert and J.-P.  Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis. ", "original_text": "(2015) repository\u2014and evaluation points is plotted in Figure 7. "}, "hash": "080e7700d6477f1b5bf47c65c0ec217316f3e5b88f0a3f478409e3c82c3c71f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dbc2dae9-3fc9-4c7d-bcc5-4e9affb2a07e", "node_type": "1", "metadata": {"window": "Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n", "original_text": "***\n**Figure 7: Example of a functional dataset. "}, "hash": "0073410712cdd365f003b36c870a5dc56be58c6f935a3d7728167ce40e5e240f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n", "mimetype": "text/plain", "start_char_idx": 49180, "end_char_idx": 49253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dbc2dae9-3fc9-4c7d-bcc5-4e9affb2a07e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n", "original_text": "***\n**Figure 7: Example of a functional dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8706fe13-743f-4c2e-8cd9-c48fdd0a6b95", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Vert.  Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed. ", "original_text": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n"}, "hash": "c21397473486d681b87a862214f30f94735d6479a73cd7bdf91464bd9e98a61f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ebfc444-da2b-49b4-abab-547244343b4e", "node_type": "1", "metadata": {"window": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node.", "original_text": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them."}, "hash": "b54fe7f6b28c0746667dfa70fa07912ffef9d708d1458d0713f3df0d9f0234a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 7: Example of a functional dataset. ", "mimetype": "text/plain", "start_char_idx": 49253, "end_char_idx": 49302, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ebfc444-da2b-49b4-abab-547244343b4e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node.", "original_text": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dbc2dae9-3fc9-4c7d-bcc5-4e9affb2a07e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Consistency and convergence rates of one-class SVMs and related algorithms.  *Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n", "original_text": "***\n**Figure 7: Example of a functional dataset. "}, "hash": "2bfecd0d19db3b7c6140a4c775010bdcaa9e70f795adacb44dfd7a99e4d7eb7e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0a0a0cb-6e1e-4c47-9e45-a875023e57fe", "node_type": "1", "metadata": {"window": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure. ", "original_text": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. "}, "hash": "2efab56e11199e8ff4bb8583e04788b638be36c203d966c40e4b5929927efb0e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them.", "mimetype": "text/plain", "start_char_idx": 49302, "end_char_idx": 49445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0a0a0cb-6e1e-4c47-9e45-a875023e57fe", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure. ", "original_text": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ebfc444-da2b-49b4-abab-547244343b4e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Journal of Machine Learning Research*, 7:817\u2013854, 2006.\n\n ---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node.", "original_text": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them."}, "hash": "a1ba4a7bd5eb88aa92ef2f32f2b54b4bdb9d1614689063e1b2206eeda4fd6187", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62962894-76eb-4145-a713-ea9adb5963c7", "node_type": "1", "metadata": {"window": "(2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080. ", "original_text": "The curves are plotted over a time axis. "}, "hash": "566c21bf74b5cdee4c36b0433b3098993b4c30279094ba9ccfd45d4a858146f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. ", "mimetype": "text/plain", "start_char_idx": 49445, "end_char_idx": 49568, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62962894-76eb-4145-a713-ea9adb5963c7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080. ", "original_text": "The curves are plotted over a time axis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0a0a0cb-6e1e-4c47-9e45-a875023e57fe", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "---\n\n# Supplementary material\n\n## A. Illustrative figures\n\nAn example of a functional dataset\u2014the \"Chinatown\" dataset form the UCR Chen et al.  (2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure. ", "original_text": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset. "}, "hash": "c088b6f65b58ac3b1d8aa3bd83af70e4b51ebd685e2034a857c46fb7d0f7eb4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de22787b-7800-430d-a7bc-6cbd54c44195", "node_type": "1", "metadata": {"window": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081. ", "original_text": "Red stars on the curves indicate the discrete points at which data was observed. "}, "hash": "b28bbbe80713b7bfc06c1ef85c545c1f9eb16dbb01a365bdbc8bf08168e3e56d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The curves are plotted over a time axis. ", "mimetype": "text/plain", "start_char_idx": 49568, "end_char_idx": 49609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "de22787b-7800-430d-a7bc-6cbd54c44195", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081. ", "original_text": "Red stars on the curves indicate the discrete points at which data was observed. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62962894-76eb-4145-a713-ea9adb5963c7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015) repository\u2014and evaluation points is plotted in Figure 7.  Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080. ", "original_text": "The curves are plotted over a time axis. "}, "hash": "0f5817173ff072559a2c622f1e6faceb26b703e359a2a15636c335bb71641cbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1869ee2c-5906-40c6-9592-ec22b9c5d8aa", "node_type": "1", "metadata": {"window": "***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. ", "original_text": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n"}, "hash": "d026f354aa2edd5339ee9d63cd2271fcda8c217c36314bbf8094281b6237d14d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Red stars on the curves indicate the discrete points at which data was observed. ", "mimetype": "text/plain", "start_char_idx": 49609, "end_char_idx": 49690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1869ee2c-5906-40c6-9592-ec22b9c5d8aa", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. ", "original_text": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "de22787b-7800-430d-a7bc-6cbd54c44195", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Figure 8 depicts a sample isolation tree with a selected terminal node.\n\n ***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081. ", "original_text": "Red stars on the curves indicate the discrete points at which data was observed. "}, "hash": "ab3e46e5c44cfe7a12830fcf9894141575dd08b28afe214e2633e7ed42687e1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1fa370b-d81d-40a3-8db3-84969dccd95f", "node_type": "1", "metadata": {"window": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). ", "original_text": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node."}, "hash": "28cef6e52c623df417c4fd38a78bb6a38ee35ede58bd3144c55e732139046e13", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n", "mimetype": "text/plain", "start_char_idx": 49690, "end_char_idx": 49772, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c1fa370b-d81d-40a3-8db3-84969dccd95f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). ", "original_text": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1869ee2c-5906-40c6-9592-ec22b9c5d8aa", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 7: Example of a functional dataset.  Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. ", "original_text": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n"}, "hash": "c2c8af38818447ed9593bb3f3da4b9353786933f79e9ea8ffc429304f4e307c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdddd8f4-a8ce-49d8-87bf-35bb35b55c37", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. ", "original_text": "**\n\n**Description:** The figure shows a binary tree structure. "}, "hash": "3d79164d324e6860da8bc3fdfe829fa489a07709c53ea418903dadf9f7e97283", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node.", "mimetype": "text/plain", "start_char_idx": 49772, "end_char_idx": 49912, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdddd8f4-a8ce-49d8-87bf-35bb35b55c37", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. ", "original_text": "**\n\n**Description:** The figure shows a binary tree structure. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c1fa370b-d81d-40a3-8db3-84969dccd95f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Red stars indicate the observed values (at the same equispaced time points), while the blue curves are obtained by linearly interpolating them. **\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). ", "original_text": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node."}, "hash": "dbb6aab765a5b126be8a88483923081d3b8a082818f1578526d9b225518376a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4973c290-c352-456a-944c-d8e98b7bc1a4", "node_type": "1", "metadata": {"window": "The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. ", "original_text": "The root node is C\u2080,\u2080. "}, "hash": "0e01aae48b3a9e2f2e0a070885f940379149fbb1ef5b0f71ccf472d4f0be845c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure shows a binary tree structure. ", "mimetype": "text/plain", "start_char_idx": 49912, "end_char_idx": 49975, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4973c290-c352-456a-944c-d8e98b7bc1a4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. ", "original_text": "The root node is C\u2080,\u2080. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdddd8f4-a8ce-49d8-87bf-35bb35b55c37", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The plot shows multiple time series curves (in blue and purple) representing the \"Chinatown\" dataset.  The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. ", "original_text": "**\n\n**Description:** The figure shows a binary tree structure. "}, "hash": "6bd9e6f1da530a028b7fb39267dc34bbefeb5cd47c7002060a2983a732fdf737", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b22ad62b-ae49-49d8-bce3-719f6c36a543", "node_type": "1", "metadata": {"window": "Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". ", "original_text": "It splits into C\u2081,\u2080 and C\u2081,\u2081. "}, "hash": "dda9ef273063e628a864917e1ebcaa874cb914d894f9a711b5db3c2dcf9e8c1b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The root node is C\u2080,\u2080. ", "mimetype": "text/plain", "start_char_idx": 49975, "end_char_idx": 49998, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b22ad62b-ae49-49d8-bce3-719f6c36a543", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". ", "original_text": "It splits into C\u2081,\u2080 and C\u2081,\u2081. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4973c290-c352-456a-944c-d8e98b7bc1a4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The curves are plotted over a time axis.  Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. ", "original_text": "The root node is C\u2080,\u2080. "}, "hash": "074091562c77c9ffc8a346316e7b41080cf5e5edd47ff51e69cd2dc6ac53e766", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b44a771-8628-4753-bf5d-fc814baadaf8", "node_type": "1", "metadata": {"window": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. ", "original_text": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. "}, "hash": "7a4a9a2084f2130f6e5ddb1901c417ecb85d33ec32b313de0bbbf0d9419444c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It splits into C\u2081,\u2080 and C\u2081,\u2081. ", "mimetype": "text/plain", "start_char_idx": 49998, "end_char_idx": 50028, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b44a771-8628-4753-bf5d-fc814baadaf8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. ", "original_text": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b22ad62b-ae49-49d8-bce3-719f6c36a543", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Red stars on the curves indicate the discrete points at which data was observed.  The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". ", "original_text": "It splits into C\u2081,\u2080 and C\u2081,\u2081. "}, "hash": "dea4a259ce9280fb28c63317e8c6c46ea8059d32368098cf451d0ebcf444f682", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bad0ed5b-9d53-4466-9fbf-6fb1988e162e", "node_type": "1", "metadata": {"window": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n", "original_text": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). "}, "hash": "504e29c27aeefc6b1683fd233740e82595e7eeb193db8a5e1ad8cc34b0f888d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. ", "mimetype": "text/plain", "start_char_idx": 50028, "end_char_idx": 50060, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bad0ed5b-9d53-4466-9fbf-6fb1988e162e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n", "original_text": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b44a771-8628-4753-bf5d-fc814baadaf8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The curves appear to have a bimodal shape, with two peaks over the time interval.\n ***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. ", "original_text": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081. "}, "hash": "2a993d5e87a7d21cf8615eab22f8d15212b5792691b527dc5611e1df2cf5bcc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b80122f1-7609-409a-82ee-0ae8a4b7844e", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n", "original_text": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. "}, "hash": "37e49e27a0a8ea58612890da009d74b43898e2c61b4a962e0a4cb1dfd71ebfb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). ", "mimetype": "text/plain", "start_char_idx": 50060, "end_char_idx": 50150, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b80122f1-7609-409a-82ee-0ae8a4b7844e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n", "original_text": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bad0ed5b-9d53-4466-9fbf-6fb1988e162e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n***\n**Figure 8: An example of a functional isolation tree structure denoted by T. Here, C\u2082,\u2081 is a cell associated with a terminal node. **\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n", "original_text": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure). "}, "hash": "abd0908b1988398445a2979bc7c6c47cf1020fa95343ae10e1b6edbca21d5dd5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c51b4c37-c5ab-48fa-a1c2-9ed61509ef10", "node_type": "1", "metadata": {"window": "The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n", "original_text": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. "}, "hash": "141eac9d5907bd39a8981ba571ade27d6044aa649990df6b012c49bad458a789", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. ", "mimetype": "text/plain", "start_char_idx": 50150, "end_char_idx": 50182, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c51b4c37-c5ab-48fa-a1c2-9ed61509ef10", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n", "original_text": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b80122f1-7609-409a-82ee-0ae8a4b7844e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure shows a binary tree structure.  The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n", "original_text": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081. "}, "hash": "425601139c9b88bf956b8b959af3e70f3768fd17b7159406abd1990fc5eff10c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c884bc95-6e91-48c2-a434-d41beb705afa", "node_type": "1", "metadata": {"window": "It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n", "original_text": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". "}, "hash": "a6383481dad08de69558d658b0f4f1d70dc0609b46591873abe56c66b2480ed0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. ", "mimetype": "text/plain", "start_char_idx": 50182, "end_char_idx": 50253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c884bc95-6e91-48c2-a434-d41beb705afa", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n", "original_text": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c51b4c37-c5ab-48fa-a1c2-9ed61509ef10", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The root node is C\u2080,\u2080.  It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n", "original_text": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node. "}, "hash": "cf08989a2a2ce207fb91365180828f5ae1633542628e9cf428f149773c90e2ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b5e8b92-f716-48a5-be84-259262cebd74", "node_type": "1", "metadata": {"window": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n", "original_text": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. "}, "hash": "f5464f1d708aa448ec218cc2bc9ce975a835c7c5ef58928804b41a8f07a88063", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". ", "mimetype": "text/plain", "start_char_idx": 50253, "end_char_idx": 50311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b5e8b92-f716-48a5-be84-259262cebd74", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n", "original_text": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c884bc95-6e91-48c2-a434-d41beb705afa", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It splits into C\u2081,\u2080 and C\u2081,\u2081.  C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n", "original_text": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\". "}, "hash": "5298bbdff2ff411cb2ca0d3fe772f3d34b0a826257a70375917e1067fb9dee59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e3131dc-8fe2-41db-883c-9d6bdc7382ad", "node_type": "1", "metadata": {"window": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n", "original_text": "This diagram illustrates how the feature space is recursively partitioned.\n"}, "hash": "e0e00df6c76013f0b928af997522e3dbff00640df34b5a698abdcb349c7bb56e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. ", "mimetype": "text/plain", "start_char_idx": 50311, "end_char_idx": 50398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7e3131dc-8fe2-41db-883c-9d6bdc7382ad", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n", "original_text": "This diagram illustrates how the feature space is recursively partitioned.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b5e8b92-f716-48a5-be84-259262cebd74", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C\u2081,\u2080 splits into C\u2082,\u2080 and C\u2082,\u2081.  C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n", "original_text": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2. "}, "hash": "de5523719f30fa1645165792d24c3d8cd3e7780e228e1e4c89b5a8507d3fe4cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "413309a9-3654-402b-a65e-9c01f9314fff", "node_type": "1", "metadata": {"window": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n", "original_text": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n"}, "hash": "4bffae3a743a25e2b1a299c8cf6dea6d9a94de2cd3b13136d4d0a5efb5e6becd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This diagram illustrates how the feature space is recursively partitioned.\n", "mimetype": "text/plain", "start_char_idx": 50398, "end_char_idx": 50473, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "413309a9-3654-402b-a65e-9c01f9314fff", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n", "original_text": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7e3131dc-8fe2-41db-883c-9d6bdc7382ad", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C\u2081,\u2081 splits into two nodes both labeled C\u2082,\u2083 (this may be a typo in the original figure).  C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n", "original_text": "This diagram illustrates how the feature space is recursively partitioned.\n"}, "hash": "8b38fa38a6d5388781276929a808b4b5157f39e706442690d8b490662e515f5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebbd7724-78a3-4d14-ac09-620f1cbed860", "node_type": "1", "metadata": {"window": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "original_text": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n"}, "hash": "273bf1b25d5666bcc75d5fd13df03059f388de35e0bc2e47c41ae8145c041c96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n", "mimetype": "text/plain", "start_char_idx": 50473, "end_char_idx": 50591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebbd7724-78a3-4d14-ac09-620f1cbed860", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "original_text": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "413309a9-3654-402b-a65e-9c01f9314fff", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "C\u2082,\u2080 splits into C\u2083,\u2080 and C\u2083,\u2081.  The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n", "original_text": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n"}, "hash": "fd0e37b1e76685b6665aeda31b4c02206572de95fe22a51b4caac9e67b532215", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aabc1aae-aac0-4da6-9052-644d278d9b1e", "node_type": "1", "metadata": {"window": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "original_text": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n"}, "hash": "b0a508bbaaf67c476318ac55d5036e95f5e2238db1f1194d0b6f72d4fd179f0a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n", "mimetype": "text/plain", "start_char_idx": 50591, "end_char_idx": 50665, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aabc1aae-aac0-4da6-9052-644d278d9b1e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "original_text": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebbd7724-78a3-4d14-ac09-620f1cbed860", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The node C\u2082,\u2081 is highlighted with a red circle and is a terminal node.  A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "original_text": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n"}, "hash": "0df165cc3488949eaf4be089177a0dabb4f4c9c60684728549a08d947e50d1c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca142004-fdbb-40fc-b8b5-bdcc6b8153a1", "node_type": "1", "metadata": {"window": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "original_text": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n"}, "hash": "f1b8eee9c3683ddd235d89646029199075dad3019d544974ac48396b126b4a9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n", "mimetype": "text/plain", "start_char_idx": 50665, "end_char_idx": 50797, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca142004-fdbb-40fc-b8b5-bdcc6b8153a1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "original_text": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aabc1aae-aac0-4da6-9052-644d278d9b1e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A text box points to it, stating \"x \u2208 C\u2082,\u2081 \u21d2 h_T(x) = 2\".  Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "original_text": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n"}, "hash": "a7897c6dfe062720e8d9384f7d598b7223c79b5c9603bc140fc002403f429396", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58f4a54f-2114-486f-b069-5ec7954f692a", "node_type": "1", "metadata": {"window": "This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "original_text": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n"}, "hash": "4c6f92123ef6c11c760e33c8868b8790496f6447db56429c179b679bac21e87e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n", "mimetype": "text/plain", "start_char_idx": 50797, "end_char_idx": 50939, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58f4a54f-2114-486f-b069-5ec7954f692a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "original_text": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca142004-fdbb-40fc-b8b5-bdcc6b8153a1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Other nodes C\u2083,\u2085, C\u2083,\u2086, C\u2083,\u2087, C\u2083,\u2088 are also shown as children of the nodes at level 2.  This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "original_text": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n"}, "hash": "27e1ab6a43fe9f8e147f309615a6975e0a1d7d610800ad0c724a7c3be95ec579", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26c4521d-03ed-436b-8a85-45352d2874b6", "node_type": "1", "metadata": {"window": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. ", "original_text": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n"}, "hash": "52c9d710218d72511b07c384920174abc6f41dd089c2727bbca8cad94e3e6e3a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n", "mimetype": "text/plain", "start_char_idx": 50939, "end_char_idx": 51071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26c4521d-03ed-436b-8a85-45352d2874b6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. ", "original_text": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58f4a54f-2114-486f-b069-5ec7954f692a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This diagram illustrates how the feature space is recursively partitioned.\n ***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "original_text": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n"}, "hash": "d84a56cfb9e1b16e5d79ed72a3ad34adb094a3475e745f4772dad15429652658", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3ea96aa-6586-4d9b-a3f0-c70697d2c958", "node_type": "1", "metadata": {"window": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. ", "original_text": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n"}, "hash": "4248306759c9924d48ec2869cec08bba0ac9b7a685814fb58366201ea9d4621a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n", "mimetype": "text/plain", "start_char_idx": 51071, "end_char_idx": 51343, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3ea96aa-6586-4d9b-a3f0-c70697d2c958", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. ", "original_text": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26c4521d-03ed-436b-8a85-45352d2874b6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n## B. Presentation of used dictionaries\n\nIn this part, we define properly every dictionaries used in the paper.\n\n *Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. ", "original_text": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n"}, "hash": "0d9a40db3d7124936d470c3ac787514eaa8f7d1060b4c7cb2781393019fd4a8f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd942e63-d630-46c9-92a1-2db62cf2c463", "node_type": "1", "metadata": {"window": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). ", "original_text": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n"}, "hash": "fe00328695d9b86f725ec332e66e1a7cbe7ec8fb77c29cddffddeb12b42f3984", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "mimetype": "text/plain", "start_char_idx": 51343, "end_char_idx": 51661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd942e63-d630-46c9-92a1-2db62cf2c463", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). ", "original_text": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3ea96aa-6586-4d9b-a3f0-c70697d2c958", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Self-data dictionary* (Self) consisting of the training dataset itself.\n\n *Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. ", "original_text": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n"}, "hash": "e904e7c4dc388f032125be0d9611dd146a5590240ea01c3b0468a1668d0966ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32914867-200a-4b9d-80bf-f25442bbbde8", "node_type": "1", "metadata": {"window": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. ", "original_text": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n"}, "hash": "1377d01cc6b240c79e54598042046bd88f37491a1b1f7de03e5077ef305c759d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "mimetype": "text/plain", "start_char_idx": 51661, "end_char_idx": 51802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "32914867-200a-4b9d-80bf-f25442bbbde8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. ", "original_text": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd942e63-d630-46c9-92a1-2db62cf2c463", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Brownian motion dictionary* (B) is a combination of the space of continuous function D = C([0, 1]) and the Wiener measure W on D.\n\n *Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). ", "original_text": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n"}, "hash": "8e0289433bb562e82fd4ab751bfb9a23db4fa4ec29783aeedd4806737fe143d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fdf9da98-05c3-4148-8377-2cbfb3d8d642", "node_type": "1", "metadata": {"window": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig. ", "original_text": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n"}, "hash": "f82201a2a7c5539caf71097f8df88d5f941e5a53486f9cc275ad04a80d8306d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n", "mimetype": "text/plain", "start_char_idx": 51802, "end_char_idx": 52125, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fdf9da98-05c3-4148-8377-2cbfb3d8d642", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig. ", "original_text": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32914867-200a-4b9d-80bf-f25442bbbde8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Brownian bridge dictionary* (BB) is a combination of the space of continuous function D = C([0, 1]) and the Brownian bridge measure G on D.\n\n *Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. ", "original_text": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n"}, "hash": "3dd85dc48603600c17f4fe3e8337007feda94bd1ff9b3d3014eba871f851b839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b33e1d5e-4380-41dc-b2a9-70d0a5826bbf", "node_type": "1", "metadata": {"window": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n", "original_text": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. "}, "hash": "516b7b56073c9815b635e3881262414b5865501e50a30d205944298bbc2496a2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n", "mimetype": "text/plain", "start_char_idx": 52125, "end_char_idx": 52264, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b33e1d5e-4380-41dc-b2a9-70d0a5826bbf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n", "original_text": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fdf9da98-05c3-4148-8377-2cbfb3d8d642", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Cosine dictionary* (Cos) consisting of curves with the following forms:\nx_{a,\u03c9}(t) = a cos(2\u03c0\u03c9t)\nwith a \u2208 [0, 1] and \u03c9 \u2208 [0, 10].\n\n *Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig. ", "original_text": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n"}, "hash": "73c27e5db77581c2241ba10f7a89e6106287b4664b5044e61d7b69cb37c6e3b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31577f7f-0d04-4373-aab1-92da0bbf88d9", "node_type": "1", "metadata": {"window": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n", "original_text": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. "}, "hash": "dfafb63c290fec6fea9170491016abf217521c445cbaa301a6fae0ec4104272b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. ", "mimetype": "text/plain", "start_char_idx": 52264, "end_char_idx": 52510, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31577f7f-0d04-4373-aab1-92da0bbf88d9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n", "original_text": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b33e1d5e-4380-41dc-b2a9-70d0a5826bbf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Mexican hat wavelet dictionary* (MHW) consists of the negative second derivatives of the normal density, shifted and scaled in a appropriate fashion:\nx_{\u03b8,\u03c3}(t) = (2 / (\u221a3\u03c3\u03c0^(1/4))) * (1 - ((t-\u03b8)/\u03c3)\u00b2) * exp(-((t-\u03b8)\u00b2 / (2\u03c3\u00b2)))\nwith \u03b8 \u2208 [\u22120.8, 0.8] and \u03c3 \u2208 ([0.04, 0.2]).\n\n *Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n", "original_text": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary. "}, "hash": "d533026bb3b5b1f33e739f57d98746dd1719b8b4410bcffe7ccbfe01256d2ca2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4bc1409-1743-4dca-a165-fa8b45ed6a66", "node_type": "1", "metadata": {"window": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. ", "original_text": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). "}, "hash": "51bee3d20bdbfabba98631bd3ce6dc4f5ab51f533eac86bd25e50a1effe3a073", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. ", "mimetype": "text/plain", "start_char_idx": 52510, "end_char_idx": 52624, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e4bc1409-1743-4dca-a165-fa8b45ed6a66", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. ", "original_text": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31577f7f-0d04-4373-aab1-92da0bbf88d9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Dyadic indicator dictionary* (DI) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n", "original_text": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space. "}, "hash": "ea134a69eed3fcc6eeb6fc52db9e165eb500f15806a5fd8edf107f7515a78b85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7cbf5f02-78d6-4410-bff8-4d5f25b6ff5f", "node_type": "1", "metadata": {"window": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. ", "original_text": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. "}, "hash": "49ed3d821a691c3de7d595b45af79dc035aa0089b0cd3ed9e5fcfa6a0668a96a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). ", "mimetype": "text/plain", "start_char_idx": 52624, "end_char_idx": 52734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7cbf5f02-78d6-4410-bff8-4d5f25b6ff5f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. ", "original_text": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4bc1409-1743-4dca-a165-fa8b45ed6a66", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Uniform indicator dictionary* (UI) consists of indicator function on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n *Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. ", "original_text": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]). "}, "hash": "e216a90b8f286a81ac96ba6431bf32b913dc43eef5eaf88e359008b25db059b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3628efe-f462-40aa-85f8-734538a91a39", "node_type": "1", "metadata": {"window": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n", "original_text": "On Fig. "}, "hash": "625e832440a6244a9ac416cc8d39c94f27793e3fe49c800927f10b7424a097bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. ", "mimetype": "text/plain", "start_char_idx": 52734, "end_char_idx": 52896, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b3628efe-f462-40aa-85f8-734538a91a39", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n", "original_text": "On Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7cbf5f02-78d6-4410-bff8-4d5f25b6ff5f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Dyadic indicator derivative* (DId) consisting of a set of indicator functions on the elements of binary partitioning, for a given J (chosen according to the granularity to be captured or from the discretisation considerations) having as elements {(x_{k,j})_{0\u2264k<2^j}}_{1\u2264j\u2264J}:\nx_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)).\n\n *Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. ", "original_text": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*. "}, "hash": "fa31de61bd48b87bed06e134f0593271fc44920eb7269c917706e74572696a95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc75ecaf-9448-48c8-bb3f-becf8e499a3b", "node_type": "1", "metadata": {"window": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g. ", "original_text": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n"}, "hash": "02bb46647892d9d813e3e782feb7a9e52f97cc12f2186ecad7e227b5ebd860ce", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On Fig. ", "mimetype": "text/plain", "start_char_idx": 52896, "end_char_idx": 52904, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc75ecaf-9448-48c8-bb3f-becf8e499a3b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g. ", "original_text": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3628efe-f462-40aa-85f8-734538a91a39", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Uniform indicator derivative* (UId) consists of functions t \u21a6 t on [a, b] where a and b are choosen uniformly on [0, 1] such that a < b.\n\n ## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n", "original_text": "On Fig. "}, "hash": "8aef03069f9c2c2d2dfab806ed09dbda9c1a956512ebd010a219fe12ae579d72", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35ed504a-9209-46ba-aa94-4c8aa7d15e23", "node_type": "1", "metadata": {"window": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. ", "original_text": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n"}, "hash": "b61d9c5aaa6833b300a875983707ae9eae0df7e3feea90eb2daad8a61b88275f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n", "mimetype": "text/plain", "start_char_idx": 52904, "end_char_idx": 53102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "35ed504a-9209-46ba-aa94-4c8aa7d15e23", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. ", "original_text": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc75ecaf-9448-48c8-bb3f-becf8e499a3b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## C. Further discussion on the choice of dictionary\n\nTo illustrate the dicussion on dictionaries, especially the incorporation of stochastic elements and external informations, we bring an example of the use of the *Brownian motion* dictionary.  Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g. ", "original_text": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n"}, "hash": "86ac4d57cffb9dc17300904202ca22a312e948698399081b22b6dad102e8c6df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09d7f375-ee98-427c-b300-ce5f4493c9ea", "node_type": "1", "metadata": {"window": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. ", "original_text": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. "}, "hash": "09fc63df8670b06961c9c3936cf33848a554f8d9d45f39be60f38a9a533c0398", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n", "mimetype": "text/plain", "start_char_idx": 53102, "end_char_idx": 53238, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "09d7f375-ee98-427c-b300-ce5f4493c9ea", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. ", "original_text": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35ed504a-9209-46ba-aa94-4c8aa7d15e23", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Let W be the Wiener measure defined on C([0, 1]) the space of continuous function on [0,1] and H be the L\u2082 space.  We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. ", "original_text": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n"}, "hash": "696f0f5a687323aeaf9e459c1d03f3f96397a51f419db5f6cf8a0d7ec78918fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5988bfae-1384-4c99-96e6-64d14aa9c60a", "node_type": "1", "metadata": {"window": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al. ", "original_text": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. "}, "hash": "cec24f588aaef736c7b6ca4eaa2d90eede3990dccdd6ba3f2e016bb3814213c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. ", "mimetype": "text/plain", "start_char_idx": 53238, "end_char_idx": 53393, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5988bfae-1384-4c99-96e6-64d14aa9c60a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al. ", "original_text": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09d7f375-ee98-427c-b300-ce5f4493c9ea", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We define by *Brownian motion dictionary* (B) the *Split variables* space induced by \u03bd = W and D = C([0, 1]).  Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. ", "original_text": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though. "}, "hash": "71582652585ae5392510095a4f47d851a6db68b164d0873e4b6896523c8425e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7458973c-8cd8-4eef-87e2-be2b7ed9952e", "node_type": "1", "metadata": {"window": "On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. ", "original_text": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n"}, "hash": "e51faae87abce3ac16cfe49ec0f57ed5a02394c3ef2d58871eef91dcec4eba2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. ", "mimetype": "text/plain", "start_char_idx": 53393, "end_char_idx": 53701, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7458973c-8cd8-4eef-87e2-be2b7ed9952e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. ", "original_text": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5988bfae-1384-4c99-96e6-64d14aa9c60a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Although seeming universal, this dictionary explores almost the entire argument space equivalently, and in practice can be unable to detect *isolated anomalies*.  On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al. ", "original_text": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation. "}, "hash": "c86228dd92484a6810033bc46fe8bdf03565ac89f20c172ab14dff190de7a385", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c8f6174-621a-45d8-bb28-eadcc4c6bc09", "node_type": "1", "metadata": {"window": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). ", "original_text": "When having not enough prior knowledge, e.g. "}, "hash": "2ae753c3c0732710f28b872e11584c09676866f186278d3f26b77ab65a30ab55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n", "mimetype": "text/plain", "start_char_idx": 53701, "end_char_idx": 53782, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c8f6174-621a-45d8-bb28-eadcc4c6bc09", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). ", "original_text": "When having not enough prior knowledge, e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7458973c-8cd8-4eef-87e2-be2b7ed9952e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "On Fig.  9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. ", "original_text": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n"}, "hash": "ab6cd32ff4dd95a161cd82e17200a9fab8025b54911a97ced28e1fcb7f2bb5ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e663417e-ce0e-4c1f-9007-4c3fbf838151", "node_type": "1", "metadata": {"window": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n", "original_text": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. "}, "hash": "2e82d9f3226f20b54f8fd95169454e85273676673e9425441c2a92b0e724bbb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When having not enough prior knowledge, e.g. ", "mimetype": "text/plain", "start_char_idx": 53782, "end_char_idx": 53827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e663417e-ce0e-4c1f-9007-4c3fbf838151", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n", "original_text": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c8f6174-621a-45d8-bb28-eadcc4c6bc09", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "9 we plot the following synthetic dataset:\n\n- 30 curves defined by x(t) = 30(1 - t)^q t^q on t \u2208 [0, 0.2] and x(t) = 30(0.8)^q 0.2^q + N(0, 0.3\u00b2) on t \u2208 [0.2, 0.7] with q equispaced in [0.5, 0.55].\n - 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). ", "original_text": "When having not enough prior knowledge, e.g. "}, "hash": "bc39e2f8167254245aeafba302265a28e7969947a7804dda234b5fada113a01e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "898c23aa-bfa6-45da-b756-5447c8b1c060", "node_type": "1", "metadata": {"window": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right).", "original_text": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. "}, "hash": "cedfd1ce0e6bb18c24421a7adbd78ca6cad0fc78489fbc34095e13ecab148bda", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. ", "mimetype": "text/plain", "start_char_idx": 53827, "end_char_idx": 53983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "898c23aa-bfa6-45da-b756-5447c8b1c060", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right).", "original_text": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e663417e-ce0e-4c1f-9007-4c3fbf838151", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "- 1 abnormal curve with the same shape but that is shifted at the beginning and whose continuation is deep in the 30 preceding curves.\n\n One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n", "original_text": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities. "}, "hash": "8ebf392a18c811da029f0f4dfdb55f90dfd6cb26a5ea581ce449691d3e9184b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d48d059-d252-40fb-9f2a-9f076cdd663b", "node_type": "1", "metadata": {"window": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots. ", "original_text": "Regard the \"Chinatown\" dataset Chen et al. "}, "hash": "223693e2a1912273e9c7ede838f89cb418a9aca1076e09b252ddeab3d95e2980", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. ", "mimetype": "text/plain", "start_char_idx": 53983, "end_char_idx": 54100, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3d48d059-d252-40fb-9f2a-9f076cdd663b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots. ", "original_text": "Regard the \"Chinatown\" dataset Chen et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "898c23aa-bfa6-45da-b756-5447c8b1c060", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One can see that the anomaly is not detected, that indicated as anomaly curve (the one with highest anomaly score) is on the fringe of the dataset though.  Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right).", "original_text": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries. "}, "hash": "39ec76a325e340494a53ba7ced8de75c7fe5dd80a4c8d18e58ceff7ee4db9810", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4445ed75-f6bf-4110-bbf3-86361e8b086a", "node_type": "1", "metadata": {"window": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. ", "original_text": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. "}, "hash": "910b6f9fb1a6763c91688379795dbb36b4b3abe17775f12457ee5c7d553f8ea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Regard the \"Chinatown\" dataset Chen et al. ", "mimetype": "text/plain", "start_char_idx": 54100, "end_char_idx": 54143, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4445ed75-f6bf-4110-bbf3-86361e8b086a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. ", "original_text": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d48d059-d252-40fb-9f2a-9f076cdd663b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Illustrative incorporation of the prior knowledge, in its simplified version, can consist, e.g., in adding to the measure W, a Dirac of the indicator function on the interval of interest x(t) = 1(t \u2264 0.25) with weights: W' := 0.2W + 0.8\u03b4_x; this assigns the highest anomaly score to the desired observation.  In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots. ", "original_text": "Regard the \"Chinatown\" dataset Chen et al. "}, "hash": "f90c30e57e5345cc7d7117dde754a74257e7d864c5213b388a2071da85a0b7f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac365a44-c5c2-41f9-9512-07255f67a78a", "node_type": "1", "metadata": {"window": "When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. ", "original_text": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). "}, "hash": "a351860dd1aad680ac42d8818b8788080734aa9ba04eeb33959c02bb61382d55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. ", "mimetype": "text/plain", "start_char_idx": 54143, "end_char_idx": 54248, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac365a44-c5c2-41f9-9512-07255f67a78a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. ", "original_text": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4445ed75-f6bf-4110-bbf3-86361e8b086a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In the sequel, \u03bd follows a uniform distribution if is not explicitly mentioned.\n\n When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. ", "original_text": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017. "}, "hash": "fbc83b2a2ab88e151c1d2e63ba1e7c60b4359945febedc38914fbb885f742666", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fe4c949-1bb1-4e23-a588-4a05e463a583", "node_type": "1", "metadata": {"window": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. ", "original_text": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n"}, "hash": "7d357e8207ebd2042a6916073c427a37e8d57da4ed44d660ac296ad4b7937c31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). ", "mimetype": "text/plain", "start_char_idx": 54248, "end_char_idx": 54374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5fe4c949-1bb1-4e23-a588-4a05e463a583", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. ", "original_text": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac365a44-c5c2-41f9-9512-07255f67a78a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "When having not enough prior knowledge, e.g.  just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. ", "original_text": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10). "}, "hash": "3147243f55c4ec017b282f683fb2a4d566c76e267a46f1d581a7c97b439d270b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "48179cfa-44fb-4a37-aeca-5e3eecfb919f", "node_type": "1", "metadata": {"window": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n", "original_text": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right)."}, "hash": "8ce639a90d365a948cb14edf25ba4f42fc12dd57164dc6a42c115b0a530ea8b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n", "mimetype": "text/plain", "start_char_idx": 54374, "end_char_idx": 54592, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "48179cfa-44fb-4a37-aeca-5e3eecfb919f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n", "original_text": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fe4c949-1bb1-4e23-a588-4a05e463a583", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "just knowing to stick to local features of functional data but not the precise interval, one would like to use a dictionary exploring different localities.  To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. ", "original_text": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n"}, "hash": "f888bd0b83e335c65ac1bf0a34ee103acd3267e346c53a179463150186815190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "844cd453-2f9a-4e4d-8363-d4a15616ab4c", "node_type": "1", "metadata": {"window": "Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. ", "original_text": "**\n\n**Description:** The figure contains two plots. "}, "hash": "40736bc4a685e7d64bc0043710216b0c6e4b58d3738dce322fd94321c6031cc9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right).", "mimetype": "text/plain", "start_char_idx": 54592, "end_char_idx": 54865, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "844cd453-2f9a-4e4d-8363-d4a15616ab4c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. ", "original_text": "**\n\n**Description:** The figure contains two plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "48179cfa-44fb-4a37-aeca-5e3eecfb919f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate possible advantage of this approach, we use *Mexican hat wavelet* and *Dyadic indicator* dictionaries.  Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n", "original_text": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right)."}, "hash": "ba735dd50c6a38afc67845e508ab4d7cd44a63f7479866fa3338a9c828336f71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf3ae527-808d-453b-ad94-1c5a4625379d", "node_type": "1", "metadata": {"window": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative. ", "original_text": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. "}, "hash": "f12eb3c604fdd4cadaca5025dd5f598c95287be4e6ec70f8dbda90ef1f8f0ba5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure contains two plots. ", "mimetype": "text/plain", "start_char_idx": 54865, "end_char_idx": 54917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf3ae527-808d-453b-ad94-1c5a4625379d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative. ", "original_text": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "844cd453-2f9a-4e4d-8363-d4a15616ab4c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Regard the \"Chinatown\" dataset Chen et al.  (2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. ", "original_text": "**\n\n**Description:** The figure contains two plots. "}, "hash": "8b124b6d79d5217aeab966d2223377f87aa69a2386007b335b6c6c02f2bc697b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f9a722c1-66a1-4656-8050-76a910d83614", "node_type": "1", "metadata": {"window": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. ", "original_text": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. "}, "hash": "26485fd09f990323f2583103e8d872dc2eecd2043eb6ea36526f1a1a6f8db891", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. ", "mimetype": "text/plain", "start_char_idx": 54917, "end_char_idx": 55027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f9a722c1-66a1-4656-8050-76a910d83614", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. ", "original_text": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf3ae527-808d-453b-ad94-1c5a4625379d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015), which represents pedestrian count in Chinatown-Swanston St North for 12 months during year 2017.  With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative. ", "original_text": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly. "}, "hash": "2c95f5824c977e08e95bfc77251a1c377357c132c173bf92bf81a15b897596d6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "820b417f-de49-42b7-8dcb-84910f9a0777", "node_type": "1", "metadata": {"window": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions). ", "original_text": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. "}, "hash": "5a7c3b984038c243970d0b08eda70a7cad5f00d404672a82fb8ce74c3ae50241", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. ", "mimetype": "text/plain", "start_char_idx": 55027, "end_char_idx": 55168, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "820b417f-de49-42b7-8dcb-84910f9a0777", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions). ", "original_text": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f9a722c1-66a1-4656-8050-76a910d83614", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "With 14 functions (working days) representing normal observations and taking 4 functions (weekends) as anomalies (Figure 10).  One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. ", "original_text": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous. "}, "hash": "1f930a8043d6a3134015ef940bad9563a25632559d81a7ff9f666d2098251468", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d3c717a-f174-42df-8ad5-fa5d4c27082d", "node_type": "1", "metadata": {"window": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n", "original_text": "The x-axis ranges from 0.0 to 1.0.\n"}, "hash": "3139f8342e8b9ac5ca330b47b37270410befe2f672884d4bd9109bfeadbf7a12", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. ", "mimetype": "text/plain", "start_char_idx": 55168, "end_char_idx": 55317, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d3c717a-f174-42df-8ad5-fa5d4c27082d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n", "original_text": "The x-axis ranges from 0.0 to 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "820b417f-de49-42b7-8dcb-84910f9a0777", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One observes that while the Mexican hat wavelet dictionary correctly detects part of the anomalies, due to its smooth nature it is distracted by two normal curves with high deviation on the second half of the domain.\n\n ***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions). ", "original_text": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score. "}, "hash": "222fd8ab1f54715ea8bf03c95a9a0727d4d5b43fbaa6543c3bdcaf6701512d21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3bda7ca7-d746-42bd-b7d7-989767e2b5e4", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right).", "original_text": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. "}, "hash": "3876418ab27e90d6785423fea16b97d6b23bc41fcf7abcb73eae41e9a045cca2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis ranges from 0.0 to 1.0.\n", "mimetype": "text/plain", "start_char_idx": 55317, "end_char_idx": 55352, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3bda7ca7-d746-42bd-b7d7-989767e2b5e4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right).", "original_text": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d3c717a-f174-42df-8ad5-fa5d4c27082d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 9: Synthetic data containing an isolated anomaly and the observation having the highest anomaly score, with dictionary being pure Brownian motion (left) and Brownian motion mixed with an indicator function in the area of interest in proportions 4 to 1 (right). **\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n", "original_text": "The x-axis ranges from 0.0 to 1.0.\n"}, "hash": "5fc82d90824bd47b3c41893f92b488a9ab00f8d456bcd2e9f71db5b3f4c45755", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3ed6abe-2497-450d-ac44-185dc9452241", "node_type": "1", "metadata": {"window": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. ", "original_text": "Nevertheless, it is not adapted to scalar product that involves derivative. "}, "hash": "e5eeb1a29035eee6b43183b75f2f97834390adfca7ac8dbec2119952d365a430", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. ", "mimetype": "text/plain", "start_char_idx": 55352, "end_char_idx": 55507, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3ed6abe-2497-450d-ac44-185dc9452241", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. ", "original_text": "Nevertheless, it is not adapted to scalar product that involves derivative. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3bda7ca7-d746-42bd-b7d7-989767e2b5e4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure contains two plots.  Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right).", "original_text": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations. "}, "hash": "550b17cfd653b611546bd3fb8724258c057f3105f1e9d45067d802495f144bc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "810c88f8-a149-41ab-bd2a-1445a4b1a41a", "node_type": "1", "metadata": {"window": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines). ", "original_text": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. "}, "hash": "90cc89df238bb259725002cc48a9a6e295fd94430c45c0e910d4f3e9a101d651", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, it is not adapted to scalar product that involves derivative. ", "mimetype": "text/plain", "start_char_idx": 55507, "end_char_idx": 55583, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "810c88f8-a149-41ab-bd2a-1445a4b1a41a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines). ", "original_text": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3ed6abe-2497-450d-ac44-185dc9452241", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Both plots show a set of red curves representing normal data and a single blue curve representing an anomaly.  The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. ", "original_text": "Nevertheless, it is not adapted to scalar product that involves derivative. "}, "hash": "a99153758fc0229bb77225723fb37007cb4c64492bd436332b906b2c05e2d9fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f85d96e3-f9c1-4ebc-a9e2-01cfeaaad4e4", "node_type": "1", "metadata": {"window": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves. ", "original_text": "their derivatives become indicator functions). "}, "hash": "815c8ffc433614521ab65f4dc4955b9404b701ef4f477d9b35bf6b393cfd3c59", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. ", "mimetype": "text/plain", "start_char_idx": 55583, "end_char_idx": 55873, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f85d96e3-f9c1-4ebc-a9e2-01cfeaaad4e4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves. ", "original_text": "their derivatives become indicator functions). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "810c88f8-a149-41ab-bd2a-1445a4b1a41a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot shows the result with a pure Brownian motion dictionary; the anomalous blue curve is not the one identified as most anomalous.  The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines). ", "original_text": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e. "}, "hash": "9fec8e2927bf8a701c1a7d177cd6fc0920ba7cd62e9d6499e5e4801ad9a7d54d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71e2d5bf-e5f4-4d06-8ba8-9c88697c05d4", "node_type": "1", "metadata": {"window": "The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. ", "original_text": "Clearly, this list can be extended with further task-specific dictionaries.\n\n"}, "hash": "252ddfa17d954fee623de7b590fb2e88c9e433bc0f6c3d734d4c7074dfb66e67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "their derivatives become indicator functions). ", "mimetype": "text/plain", "start_char_idx": 55873, "end_char_idx": 55920, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71e2d5bf-e5f4-4d06-8ba8-9c88697c05d4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. ", "original_text": "Clearly, this list can be extended with further task-specific dictionaries.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f85d96e3-f9c1-4ebc-a9e2-01cfeaaad4e4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot shows the result with a mixed dictionary; here, the anomalous blue curve is correctly identified as having the highest anomaly score.  The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves. ", "original_text": "their derivatives become indicator functions). "}, "hash": "3d0a5af4c3ef86f882f7814ac87e0138b8b697819a7062b5fe025f72cb8e40e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6faa9a8-8248-4f10-b9a4-c72713b9e179", "node_type": "1", "metadata": {"window": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. ", "original_text": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right)."}, "hash": "c877426a9e8b659c797cbf9b90d8824e8a99373927ebbeecd6739bb9dc289a8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Clearly, this list can be extended with further task-specific dictionaries.\n\n", "mimetype": "text/plain", "start_char_idx": 55920, "end_char_idx": 55997, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e6faa9a8-8248-4f10-b9a4-c72713b9e179", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. ", "original_text": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71e2d5bf-e5f4-4d06-8ba8-9c88697c05d4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis ranges from 0.0 to 1.0.\n ***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. ", "original_text": "Clearly, this list can be extended with further task-specific dictionaries.\n\n"}, "hash": "7f4a984b6c3099201ff139d0013a3fbb83ac0d9b537ed6f6f9186c1a4c2d9743", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "715b1854-5aae-443f-88d7-b5dbcf8c766a", "node_type": "1", "metadata": {"window": "Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. "}, "hash": "01930e194ca6b41e7b83b8a10f1cebb3d39c57145364563d6fa05f48815fac38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right).", "mimetype": "text/plain", "start_char_idx": 55997, "end_char_idx": 56273, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "715b1854-5aae-443f-88d7-b5dbcf8c766a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6faa9a8-8248-4f10-b9a4-c72713b9e179", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nHaving straight fronts and begin non-zero only in a small part of the domain, the dyadic indicator dictionary detects all four abnormal observations.  Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. ", "original_text": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right)."}, "hash": "70148888b0a360ceb0642ef8b2e3a3db2970b36aebfd376a4662220e5d8ffc62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9d5a821-8da2-4a40-a09e-47ca7b6f9ce9", "node_type": "1", "metadata": {"window": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data. ", "original_text": "Normal curves are in red, and anomalous curves are in blue (dotted lines). "}, "hash": "ba5927745f14770ba5f8d38ea4b6e85e405f5585108064ef7667004da17f28fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. ", "mimetype": "text/plain", "start_char_idx": 56273, "end_char_idx": 56360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9d5a821-8da2-4a40-a09e-47ca7b6f9ce9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data. ", "original_text": "Normal curves are in red, and anomalous curves are in blue (dotted lines). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "715b1854-5aae-443f-88d7-b5dbcf8c766a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, it is not adapted to scalar product that involves derivative.  To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset. "}, "hash": "45f6da3748a4f57afbf7591675f240cbef23e9ecc676d4c29195b472a953d46f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c3a6ab4-b790-4b3e-b878-79c5f02ebc8c", "node_type": "1", "metadata": {"window": "their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). ", "original_text": "The left plot shows the ground truth, clearly separating normal and anomalous curves. "}, "hash": "523e16b9171129cc887b9b5bc801858633045c164e28089f4af632d8ff068acd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Normal curves are in red, and anomalous curves are in blue (dotted lines). ", "mimetype": "text/plain", "start_char_idx": 56360, "end_char_idx": 56435, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c3a6ab4-b790-4b3e-b878-79c5f02ebc8c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). ", "original_text": "The left plot shows the ground truth, clearly separating normal and anomalous curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9d5a821-8da2-4a40-a09e-47ca7b6f9ce9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To adapt the dyadic indicator dictionary and the uniform indicator dictionary for the scalar product involving derivatives, we define their *slope* versions being x_{k,j}(t) = t * 1(t \u2208 [k/2^j, (k+1)/2^j)) and x(t) = t * 1(t \u2208 [t\u2081, t\u2082]), respectively, with the notation defined above (i.e.  their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data. ", "original_text": "Normal curves are in red, and anomalous curves are in blue (dotted lines). "}, "hash": "2c8889836bacb65d5c7ba18861f29da9f0943cff2863500d2b9b3f27420b3cbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26263c54-e8d7-415e-9e5b-177ac0105ff4", "node_type": "1", "metadata": {"window": "Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. ", "original_text": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. "}, "hash": "e5914f930eb7865d5c57a12e7ce7e69274b40b4e68f83559f3f862bf4fa8d13f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot shows the ground truth, clearly separating normal and anomalous curves. ", "mimetype": "text/plain", "start_char_idx": 56435, "end_char_idx": 56521, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26263c54-e8d7-415e-9e5b-177ac0105ff4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. ", "original_text": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c3a6ab4-b790-4b3e-b878-79c5f02ebc8c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "their derivatives become indicator functions).  Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). ", "original_text": "The left plot shows the ground truth, clearly separating normal and anomalous curves. "}, "hash": "0afa673219fbc6c793bfefa06f0356e2ab30260876e968d939a7b4a5a35182e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08f4e4b9-d220-48e5-8732-2c55edef99f8", "node_type": "1", "metadata": {"window": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n", "original_text": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. "}, "hash": "c446b0104f20af408e7e13939032e7f473c0ef634a00e8d7d30aaf14245f5ae6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. ", "mimetype": "text/plain", "start_char_idx": 56521, "end_char_idx": 56657, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "08f4e4b9-d220-48e5-8732-2c55edef99f8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n", "original_text": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26263c54-e8d7-415e-9e5b-177ac0105ff4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Clearly, this list can be extended with further task-specific dictionaries.\n\n ***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. ", "original_text": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves. "}, "hash": "5039fa8a82051378a82319c2117966e259cd6e69dca4cc7aab88992284d2bc84", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad0a2310-28cd-4513-99ee-276734073805", "node_type": "1", "metadata": {"window": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right).", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0.\n"}, "hash": "6c0f519b33470e65fa284bac02755804dffb542128117c61fd6c20e2944d163d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. ", "mimetype": "text/plain", "start_char_idx": 56657, "end_char_idx": 56785, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad0a2310-28cd-4513-99ee-276734073805", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right).", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08f4e4b9-d220-48e5-8732-2c55edef99f8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 10: The \"Chinatown\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the Mexican hat wavelet dictionary (middle), and anomalies detected using the dyadic indicator dictionary (right). **\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n", "original_text": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves. "}, "hash": "f519976902a892eb02b31c4458c81b0fc536128e7788102d6eada56bacea9ac8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a19307e1-17e1-4c14-a414-ccb93fdaf512", "node_type": "1", "metadata": {"window": "Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. ", "original_text": "***\n\nBefore, we were considering dictionaries that are independent of data. "}, "hash": "71b1e7ed8b8c6fef9a899664f479ea028b24fb1dd91cb6f3888de333bd279bb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Time\" from 0.0 to 1.0.\n", "mimetype": "text/plain", "start_char_idx": 56785, "end_char_idx": 56823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a19307e1-17e1-4c14-a414-ccb93fdaf512", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. ", "original_text": "***\n\nBefore, we were considering dictionaries that are independent of data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad0a2310-28cd-4513-99ee-276734073805", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure has three plots, all showing the \"Chinatown\" dataset.  Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right).", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0.\n"}, "hash": "10920f682db27ccf78a69c011381fbb710263073b09ac8d6a121cc2763b404d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8468534-72d7-4aa2-bb23-3d42550145be", "node_type": "1", "metadata": {"window": "The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots. ", "original_text": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). "}, "hash": "96a2186c7b20f33881d4c1512d8861a748339c3c48bb3a049e59d0390ac47824", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nBefore, we were considering dictionaries that are independent of data. ", "mimetype": "text/plain", "start_char_idx": 56823, "end_char_idx": 56899, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8468534-72d7-4aa2-bb23-3d42550145be", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots. ", "original_text": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a19307e1-17e1-4c14-a414-ccb93fdaf512", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Normal curves are in red, and anomalous curves are in blue (dotted lines).  The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. ", "original_text": "***\n\nBefore, we were considering dictionaries that are independent of data. "}, "hash": "929add550a999d0adcd05b52232d5c44ba2ef97c16011e04c4b48bae0f9b316a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "506190b8-b91e-4d8b-ab71-491a451e723f", "node_type": "1", "metadata": {"window": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. ", "original_text": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. "}, "hash": "b1f91446e451c524e9a546d5f206014dae8580d525de8649e71ed7949115974b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). ", "mimetype": "text/plain", "start_char_idx": 56899, "end_char_idx": 57138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "506190b8-b91e-4d8b-ab71-491a451e723f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. ", "original_text": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8468534-72d7-4aa2-bb23-3d42550145be", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot shows the ground truth, clearly separating normal and anomalous curves.  The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots. ", "original_text": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self). "}, "hash": "792225f9f76261bad09ad7a6c1c564f5e4b3eb4c6573494b80fce663457e71a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c7acfe6-1952-475b-b0b7-46e14f941c8b", "node_type": "1", "metadata": {"window": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies. ", "original_text": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n"}, "hash": "67b0187f144c1c5f759da312a0dc610ac828a1021fb2eec487eafca2b891da79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. ", "mimetype": "text/plain", "start_char_idx": 57138, "end_char_idx": 57292, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c7acfe6-1952-475b-b0b7-46e14f941c8b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies. ", "original_text": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "506190b8-b91e-4d8b-ab71-491a451e723f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The middle plot shows which curves were detected as anomalous by a Mexican hat wavelet dictionary; it misclassifies some normal curves.  The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. ", "original_text": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary. "}, "hash": "0b8b8ef5a5ab3b4dd0f440b927d3c43448e70ee843f237dc76be64e1c6395687", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c3d58f8-47e7-40f2-a18c-b0ce1b7d6feb", "node_type": "1", "metadata": {"window": "The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. ", "original_text": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right)."}, "hash": "2c890a5f5e100931282616f8a42cece7061bb2803b4a1d697f5ebc0cd7539c5e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n", "mimetype": "text/plain", "start_char_idx": 57292, "end_char_idx": 57459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c3d58f8-47e7-40f2-a18c-b0ce1b7d6feb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. ", "original_text": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c7acfe6-1952-475b-b0b7-46e14f941c8b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot shows the detections by a dyadic indicator dictionary, which appears to correctly identify all anomalous curves.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies. ", "original_text": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n"}, "hash": "6f2a383e509fc400a8d138c4627268bda5a63b9d969a8d7e0f8289f930f319de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46c3f10e-b3e2-4c97-8f12-dc5dd3bfd34a", "node_type": "1", "metadata": {"window": "***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. "}, "hash": "65c2b1baf83a7c80b740aa60ba883346cdb8f3c44aab8bd125f9b7ba7a3ce510", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right).", "mimetype": "text/plain", "start_char_idx": 57459, "end_char_idx": 57713, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46c3f10e-b3e2-4c97-8f12-dc5dd3bfd34a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c3d58f8-47e7-40f2-a18c-b0ce1b7d6feb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. ", "original_text": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right)."}, "hash": "fae517b466276886e447abbfbbe57a0fa72b5557f4d946af2a8b6885262814e0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "90c9d424-7fa0-47bf-bb82-41ad18db4bb7", "node_type": "1", "metadata": {"window": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). ", "original_text": "It has three plots. "}, "hash": "221403d8559921edfae1f76796d95d7180464b2d8d01ead3c4b1c1c77355148c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. ", "mimetype": "text/plain", "start_char_idx": 57713, "end_char_idx": 57808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "90c9d424-7fa0-47bf-bb82-41ad18db4bb7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). ", "original_text": "It has three plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46c3f10e-b3e2-4c97-8f12-dc5dd3bfd34a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nBefore, we were considering dictionaries that are independent of data.  Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n", "original_text": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset. "}, "hash": "8a25bb6e389e3d61f7c1291056dc08a17ffd87de39debe154252701ad3445b03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b750070-afc3-41c7-9455-abb8f251950b", "node_type": "1", "metadata": {"window": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot.", "original_text": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. "}, "hash": "563d1c47163a640397580ba32a855b877bb10666f68993ae9d43aaf2d229ae40", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It has three plots. ", "mimetype": "text/plain", "start_char_idx": 57808, "end_char_idx": 57828, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b750070-afc3-41c7-9455-abb8f251950b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot.", "original_text": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "90c9d424-7fa0-47bf-bb82-41ad18db4bb7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless one can use observations or their certain transform as a dictionary itself: projections on both normal and abnormal observations shall differ for normal ones and for anomalies; this suggests the *self-data dictionary* (Self).  This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). ", "original_text": "It has three plots. "}, "hash": "ea94f7771fc30f8d67068865ebc47baf0990dd01cb70f5a34b79959270961eac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "667ea9a7-0aa5-41e3-8b47-654ff9cc31fa", "node_type": "1", "metadata": {"window": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data. ", "original_text": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies. "}, "hash": "f76d05e84b7f4ccf47a1c55a7a29c55a80db0dcd02e6fc84f1364e39796460f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. ", "mimetype": "text/plain", "start_char_idx": 57828, "end_char_idx": 57925, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "667ea9a7-0aa5-41e3-8b47-654ff9cc31fa", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data. ", "original_text": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b750070-afc3-41c7-9455-abb8f251950b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This can be extended to the *local self-data dictionary* which consists of the product of the self-data dictionary with the uniform indicator dictionary.  As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot.", "original_text": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals. "}, "hash": "a1de02d7dfcb413612df68c42b14a381fa78695accee388b4c20e8add3dc351e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebdbbca1-317a-47dc-b7d7-adbfdeeb684f", "node_type": "1", "metadata": {"window": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score. ", "original_text": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. "}, "hash": "5d0e878c079886033ade62cf1a86f5c448a72f7ba473ad3bc374c663ab5486b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies. ", "mimetype": "text/plain", "start_char_idx": 57925, "end_char_idx": 58018, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebdbbca1-317a-47dc-b7d7-adbfdeeb684f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score. ", "original_text": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "667ea9a7-0aa5-41e3-8b47-654ff9cc31fa", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As an example, we apply this to the \"ECG5000\" dataset plotted in Figure 11, where, different to the cosine dicitonary, it allows to detect all abnormal observations.\n\n ***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data. ", "original_text": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies. "}, "hash": "9f739458c5c3b985aceaec6bd288e6cd435ec74e21c490cf7906bd5ba591b1eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbba1850-3364-4f8e-b42d-08987aec78a8", "node_type": "1", "metadata": {"window": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. ", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0.\n"}, "hash": "1ab62ebee87c68a8d34e8307a5b177aceee7dabf49b8307d82fecc55f77ddbb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. ", "mimetype": "text/plain", "start_char_idx": 58018, "end_char_idx": 58133, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fbba1850-3364-4f8e-b42d-08987aec78a8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. ", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebdbbca1-317a-47dc-b7d7-adbfdeeb684f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 11: The \"ECG5000\" dataset, with normal observations in red and anomalies in blue: The data and the true anomalies (left), anomalies detected using the cosine dictionary (middle), and anomalies detected using the self data dictionary (right). **\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score. ", "original_text": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals. "}, "hash": "7cf8336e5ec403e14fb3adb6b69e088982556a73e6ada3fd38baf1d260f76ce1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9a73a15-c915-4698-857b-26a2dca1b158", "node_type": "1", "metadata": {"window": "It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. ", "original_text": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). "}, "hash": "7c61f9da2a5803b75c3da4cfa7fce97057b294de01ab5153505eb7eb835f6008", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Time\" from 0.0 to 1.0.\n", "mimetype": "text/plain", "start_char_idx": 58133, "end_char_idx": 58171, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e9a73a15-c915-4698-857b-26a2dca1b158", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. ", "original_text": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbba1850-3364-4f8e-b42d-08987aec78a8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure is similar to the previous one but for the \"ECG5000\" dataset.  It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. ", "original_text": "The x-axis is \"Time\" from 0.0 to 1.0.\n"}, "hash": "21cbfeaf3d70c95892b2ece2e056bdd3f8ec3ec0a7d9c0c0e4d29f09a3c783cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45eda4af-7847-452b-b289-bc7aa393529f", "node_type": "1", "metadata": {"window": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n", "original_text": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot."}, "hash": "14c9e4bb1b054b28007b1865b5f84352ccde775991450295bb4d223e460d6a85", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). ", "mimetype": "text/plain", "start_char_idx": 58171, "end_char_idx": 58276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45eda4af-7847-452b-b289-bc7aa393529f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n", "original_text": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9a73a15-c915-4698-857b-26a2dca1b158", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It has three plots.  The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. ", "original_text": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right). "}, "hash": "3912f63204aed35eed01f994fe3a488fa14cba4f08e2d1a8b0975decd681cca5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "26cfdb23-1b38-477d-96b5-8a48f43e19d2", "node_type": "1", "metadata": {"window": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig. ", "original_text": "**\n\n**Description:** The figure displays two plots of functional data. "}, "hash": "06ecdd8a803cb3ddb323464f787d55a227e7dbb9191fabe9c105d56370b5ddb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot.", "mimetype": "text/plain", "start_char_idx": 58276, "end_char_idx": 58372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "26cfdb23-1b38-477d-96b5-8a48f43e19d2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig. ", "original_text": "**\n\n**Description:** The figure displays two plots of functional data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45eda4af-7847-452b-b289-bc7aa393529f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot shows the ground truth with normal (red) and anomalous (blue, dotted) ECG signals.  The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n", "original_text": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot."}, "hash": "6d89431a694dfd3e229d18328f703db5de8003dad3f46d4b9e1f91a2d9be542d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4077fdaf-59ee-45aa-a975-ff52b212f6eb", "node_type": "1", "metadata": {"window": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice. ", "original_text": "In both plots, the curves are colored based on their anomaly score. "}, "hash": "b909563c985e37f856a961e2b6bab3ae187dc70364e73204c572b78221d044f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure displays two plots of functional data. ", "mimetype": "text/plain", "start_char_idx": 58372, "end_char_idx": 58443, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4077fdaf-59ee-45aa-a975-ff52b212f6eb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice. ", "original_text": "In both plots, the curves are colored based on their anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "26cfdb23-1b38-477d-96b5-8a48f43e19d2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The middle plot shows the detections using a cosine dictionary, which misses some anomalies.  The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig. ", "original_text": "**\n\n**Description:** The figure displays two plots of functional data. "}, "hash": "65341f4ece19c1c08aa2efce20f356835b55386122717732c3c0c1f718e3e8d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "118a4025-8e0f-4101-a709-b4bfdd65d702", "node_type": "1", "metadata": {"window": "The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. ", "original_text": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. "}, "hash": "740189db974871b312439539d1db4e49040bee71f8ace56f065631ec17d91c7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In both plots, the curves are colored based on their anomaly score. ", "mimetype": "text/plain", "start_char_idx": 58443, "end_char_idx": 58511, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "118a4025-8e0f-4101-a709-b4bfdd65d702", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. ", "original_text": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4077fdaf-59ee-45aa-a975-ff52b212f6eb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot shows detections using a self-data dictionary, which successfully identifies all anomalous signals.  The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice. ", "original_text": "In both plots, the curves are colored based on their anomaly score. "}, "hash": "f4e58347c35492103816744d850f729b2933d9a156263a14d23e467273bbf548", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c20effd-2758-4408-8517-e29fe2321a1a", "node_type": "1", "metadata": {"window": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig. ", "original_text": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. "}, "hash": "89aec339470594e385e8ee3ecbf9ff58024149dbc10d42d951a15f8147e5de20", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. ", "mimetype": "text/plain", "start_char_idx": 58511, "end_char_idx": 58714, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c20effd-2758-4408-8517-e29fe2321a1a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig. ", "original_text": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "118a4025-8e0f-4101-a709-b4bfdd65d702", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Time\" from 0.0 to 1.0.\n ***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. ", "original_text": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves. "}, "hash": "536ff7014a593e1a5613a1ddea242fecbe13d5c34108bc734b03ab3a8938110b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4be2782-85bd-458e-bd7f-f367a769ebe2", "node_type": "1", "metadata": {"window": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n", "original_text": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n"}, "hash": "de831dca87d9a0ce53c0c66d81f799fc89256ac8ce730ed1261ec9261e5031d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. ", "mimetype": "text/plain", "start_char_idx": 58714, "end_char_idx": 58921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b4be2782-85bd-458e-bd7f-f367a769ebe2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n", "original_text": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c20effd-2758-4408-8517-e29fe2321a1a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n***\n**Figure 12: FIF anomaly scores for a sample of 100 curves with \u03b1 = 1 (left) and \u03b1 = 0 (right).  Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig. ", "original_text": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves. "}, "hash": "22718631f823247bcf9399f36d27defe0be9e27aca249146b6796446e61ff53e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b677b8b9-543a-4262-8714-c59761706891", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. ", "original_text": "***\n\nTo conclude this, we provide a last example (see Fig. "}, "hash": "87fc23d0a90e996614463f8dae199f8aa7cda88cd9fc4f3365ef23616b475ab4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n", "mimetype": "text/plain", "start_char_idx": 58921, "end_char_idx": 59024, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b677b8b9-543a-4262-8714-c59761706891", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. ", "original_text": "***\n\nTo conclude this, we provide a last example (see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4be2782-85bd-458e-bd7f-f367a769ebe2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Anomaly score increases from magenta to yellow in the left plot and decreases in the right plot. **\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n", "original_text": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n"}, "hash": "2fdb187846df75afdf7d9e165e7518b9075b467ac3301ed29030610248413e9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "240eda53-080c-4151-8a4d-5adf318dff1f", "node_type": "1", "metadata": {"window": "In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n", "original_text": "12) where we highlight the impact of the scalar product choice. "}, "hash": "fd4abecb37dada98c1c1e697a74f93ea1c4e4a2047570d778b7ab50e1e9d8aea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nTo conclude this, we provide a last example (see Fig. ", "mimetype": "text/plain", "start_char_idx": 59024, "end_char_idx": 59083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "240eda53-080c-4151-8a4d-5adf318dff1f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n", "original_text": "12) where we highlight the impact of the scalar product choice. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b677b8b9-543a-4262-8714-c59761706891", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure displays two plots of functional data.  In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. ", "original_text": "***\n\nTo conclude this, we provide a last example (see Fig. "}, "hash": "2eff1da95c723e39d0138b402c78055355d5a4f5440233ddd4649a251ebba764", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "192fcfe4-448b-46e4-ba5c-d2c5c7d3c1c9", "node_type": "1", "metadata": {"window": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1. ", "original_text": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. "}, "hash": "77b7770591955e08a900c0f3152efaa3927f747e56fdbc8cb899d7a2314e4226", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12) where we highlight the impact of the scalar product choice. ", "mimetype": "text/plain", "start_char_idx": 59083, "end_char_idx": 59147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "192fcfe4-448b-46e4-ba5c-d2c5c7d3c1c9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1. ", "original_text": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "240eda53-080c-4151-8a4d-5adf318dff1f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In both plots, the curves are colored based on their anomaly score.  The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n", "original_text": "12) where we highlight the impact of the scalar product choice. "}, "hash": "7319326220d4c84084d84227f57f9ba1ea70f89b2a2f5b257c872af1c9b835d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4029a5e-a70e-4b91-ad9c-94a8e418ce9b", "node_type": "1", "metadata": {"window": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. ", "original_text": "(2007), see Fig. "}, "hash": "2a33825e72c28272758f21c68d08abc37f9362b9b975d7482924b1afbc32c1a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. ", "mimetype": "text/plain", "start_char_idx": 59147, "end_char_idx": 59344, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4029a5e-a70e-4b91-ad9c-94a8e418ce9b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. ", "original_text": "(2007), see Fig. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "192fcfe4-448b-46e4-ba5c-d2c5c7d3c1c9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The left plot uses a scalar product with \u03b1 = 1 (L\u2082 norm), where the few noisy, spiky curves are colored magenta (low anomaly score) and are embedded within the yellow (high anomaly score) normal curves.  The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1. ", "original_text": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al. "}, "hash": "e00aee3639738bc14565bd987251bd561376241e0e6d0ae781efb3a8aa7b0920", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82132ab7-8f77-4b75-a882-1a15715f915d", "node_type": "1", "metadata": {"window": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al. ", "original_text": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n"}, "hash": "594ecf3ce2d3e691d325e54be8881524f84b4a2d56aa11ec96b5008b55dc2c77", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2007), see Fig. ", "mimetype": "text/plain", "start_char_idx": 59344, "end_char_idx": 59361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82132ab7-8f77-4b75-a882-1a15715f915d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al. ", "original_text": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4029a5e-a70e-4b91-ad9c-94a8e418ce9b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The right plot uses a scalar product with \u03b1 = 0 (L\u2082 norm of derivatives), where the noisy curves are now colored yellow (high anomaly score) and stand out clearly from the magenta (low score) normal curves.  This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. ", "original_text": "(2007), see Fig. "}, "hash": "1c545e22e0d6825d0785c87c9f40f34ee715c598067547c755db523a9832c83d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "52f7191e-b05d-4ca3-9c50-0c8ff9b77dae", "node_type": "1", "metadata": {"window": "***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. ", "original_text": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. "}, "hash": "aa1af95f6be5203ec8a7c4ff23ced2242138a5a5b66237b3191d8b21e9adb1f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n", "mimetype": "text/plain", "start_char_idx": 59361, "end_char_idx": 59554, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "52f7191e-b05d-4ca3-9c50-0c8ff9b77dae", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. ", "original_text": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82132ab7-8f77-4b75-a882-1a15715f915d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This illustrates the effect of the choice of scalar product on detecting different types of anomalies.\n ***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al. ", "original_text": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n"}, "hash": "b89eae3d0d7a444a75fd011530c5686cb1b83270321ae23732b0e99e29aef5c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "526b3e32-ef41-4922-94bd-3ae1a241ab10", "node_type": "1", "metadata": {"window": "12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al. ", "original_text": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n"}, "hash": "82bc762baabd4f9748bffa79809b805106accc7bbb79cfc9b13956a0338ace3c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. ", "mimetype": "text/plain", "start_char_idx": 59554, "end_char_idx": 59723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "526b3e32-ef41-4922-94bd-3ae1a241ab10", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al. ", "original_text": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "52f7191e-b05d-4ca3-9c50-0c8ff9b77dae", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nTo conclude this, we provide a last example (see Fig.  12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. ", "original_text": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account. "}, "hash": "059c18b3aa9d2067920069b4d7cb61b6ca80ec6587aec05fe9a005e850d11232", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7323d94e-b6f2-4af3-a574-816cff9c0493", "node_type": "1", "metadata": {"window": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al. ", "original_text": "### C.1. "}, "hash": "c5e52d03a525c81d7bfac604e4a800b9fc13013c65eecc2e9f82e485cfe7d4de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n", "mimetype": "text/plain", "start_char_idx": 59723, "end_char_idx": 59832, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7323d94e-b6f2-4af3-a574-816cff9c0493", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al. ", "original_text": "### C.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "526b3e32-ef41-4922-94bd-3ae1a241ab10", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "12) where we highlight the impact of the scalar product choice.  To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al. ", "original_text": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n"}, "hash": "63321750366e18e9bcd80dde8e1141b817e7a2901134802a5bee41548dd3c432", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c23cfb9-666a-4269-811c-6dbff3f6d72f", "node_type": "1", "metadata": {"window": "(2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)). ", "original_text": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. "}, "hash": "e52be8a2ae802ed2265d58901ec29c1c29e4749f6a7c541226f4251df534c5d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "### C.1. ", "mimetype": "text/plain", "start_char_idx": 59832, "end_char_idx": 59841, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c23cfb9-666a-4269-811c-6dbff3f6d72f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)). ", "original_text": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7323d94e-b6f2-4af3-a574-816cff9c0493", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To illustrate the score change caused by different values of \u03b1, we calculate the FIF anomaly scores with \u03b1 = 1 and \u03b1 = 0 for a sample consisting of 100 curves as follows (inspired by Cuevas et al.  (2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al. ", "original_text": "### C.1. "}, "hash": "f69ef80b7d2a1db213360b0d1f6d7c7ce2915d76d5f835d05f661195f1a5b528", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00eaff12-78fa-4e2b-b738-8adbfc142b4b", "node_type": "1", "metadata": {"window": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. ", "original_text": "Breiman (2001), Geurts et al. "}, "hash": "3c9390f4f781ea8f9d4dade9a1db7b411c6c5c6f6df56cc2851de7727d893619", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. ", "mimetype": "text/plain", "start_char_idx": 59841, "end_char_idx": 59974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "00eaff12-78fa-4e2b-b738-8adbfc142b4b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. ", "original_text": "Breiman (2001), Geurts et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c23cfb9-666a-4269-811c-6dbff3f6d72f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2007), see Fig.  12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)). ", "original_text": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g. "}, "hash": "492443d8bac4fd02623f582b9b4f33f31318b58763cb1dda745c1098aab68f12", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22922797-a703-489b-b486-0c0a18ac8a15", "node_type": "1", "metadata": {"window": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. ", "original_text": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. "}, "hash": "a245ddf94ca7612c3f95a6af72e5de3152d52022013b3745db548d6ac1259a7f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Breiman (2001), Geurts et al. ", "mimetype": "text/plain", "start_char_idx": 59974, "end_char_idx": 60004, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22922797-a703-489b-b486-0c0a18ac8a15", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. ", "original_text": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00eaff12-78fa-4e2b-b738-8adbfc142b4b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "12):\n\n- 90 curves defined by x(t) = 30(1 \u2212 t)^q t^q with q equispaced in [1, 1.4],\n- 10 abnormal curves defined by x(t) = 30(1 \u2212 t)\u00b9\u00b7\u00b2t\u00b9\u00b7\u00b2 noised by \u03b5 ~ N(0, 0.3\u00b2) on the interval [0.2, 0.8].\n\n One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. ", "original_text": "Breiman (2001), Geurts et al. "}, "hash": "5cc3842ea1aeb4884a5304615bed280c770409b5cf806976862c120fc7fe6aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e9cc18d-99c9-4d07-8616-3d100de56187", "node_type": "1", "metadata": {"window": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. ", "original_text": "(2008), Liu et al. "}, "hash": "a6a0f606da0e1daee3e4ce9569e7ddfcf5664bd713c9f8ead105aa757096e9dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. ", "mimetype": "text/plain", "start_char_idx": 60004, "end_char_idx": 60093, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e9cc18d-99c9-4d07-8616-3d100de56187", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. ", "original_text": "(2008), Liu et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22922797-a703-489b-b486-0c0a18ac8a15", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "One can see that even though the 10 noisy curves are abnormal for the majority of the data, they are considered as normal ones when only location is taken into account.  On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. ", "original_text": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al. "}, "hash": "18c10cc12f475aec1fb69d57b6160a6140f6babfc6de66887b8f20925e5dfa9a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7036451c-5d88-4ada-8ac9-ef7ff3184c92", "node_type": "1", "metadata": {"window": "### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). ", "original_text": "(2012) and Hariri et al. "}, "hash": "5f96efe3b2db860a1656285623d356641e6de66e213cae247ca85146372c1e7c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2008), Liu et al. ", "mimetype": "text/plain", "start_char_idx": 60093, "end_char_idx": 60112, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7036451c-5d88-4ada-8ac9-ef7ff3184c92", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). ", "original_text": "(2012) and Hariri et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e9cc18d-99c9-4d07-8616-3d100de56187", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "On the other hand, they are easily distinguished with the high anomaly score when derivatives are examined.\n\n ### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. ", "original_text": "(2008), Liu et al. "}, "hash": "965174a929a8ee537b44a6b4b59aa500205a1dcbd3f961e995a416578a1da16d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb1cc234-09a9-402f-b718-980b9c1ff94b", "node_type": "1", "metadata": {"window": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. ", "original_text": "(2018)). "}, "hash": "923c5b7e4e3384930b7f2bbb134daa4de68698238f1636eb86ed106364d4b7f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2012) and Hariri et al. ", "mimetype": "text/plain", "start_char_idx": 60112, "end_char_idx": 60137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb1cc234-09a9-402f-b718-980b9c1ff94b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. ", "original_text": "(2018)). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7036451c-5d88-4ada-8ac9-ef7ff3184c92", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "### C.1.  Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). ", "original_text": "(2012) and Hariri et al. "}, "hash": "4ece0b65c25bebe3a0ada7e12e8aab50c962b6facb9ce2d53f278ef02c31dffc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d38babb0-45fd-4aba-b6e1-30936b0d803d", "node_type": "1", "metadata": {"window": "Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample. ", "original_text": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. "}, "hash": "a7d7a012206ca38b2eb5352aeb42001345d65dca4272c0c2ab97480818c8c864", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2018)). ", "mimetype": "text/plain", "start_char_idx": 60137, "end_char_idx": 60146, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d38babb0-45fd-4aba-b6e1-30936b0d803d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample. ", "original_text": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb1cc234-09a9-402f-b718-980b9c1ff94b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Direction importance of finite size Dictionaries\n\nAlthough feature importance have been tackled in supervised random trees (see e.g.  Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. ", "original_text": "(2018)). "}, "hash": "edffe239317cb01dbb4a2c67314d2d41654d0b302efc7545e9f66240afc4d92a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c57340c-b4de-4009-b9cc-54e899a35d10", "node_type": "1", "metadata": {"window": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). ", "original_text": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. "}, "hash": "e272aaf4b7f8d3499c8be52284603066397658dca74285f09ee085ecd4d5739b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. ", "mimetype": "text/plain", "start_char_idx": 60146, "end_char_idx": 60266, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c57340c-b4de-4009-b9cc-54e899a35d10", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). ", "original_text": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d38babb0-45fd-4aba-b6e1-30936b0d803d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Breiman (2001), Geurts et al.  (2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample. ", "original_text": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting. "}, "hash": "14717b1383cf8fa270f4dd68a88b56397724a7be0da1850bf517694d335d980b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b2ace5b-b79a-4fa9-99a3-351451219f3f", "node_type": "1", "metadata": {"window": "(2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. ", "original_text": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. "}, "hash": "14fad3d6b0314700ebb9fc883c477144677c00acfd6c6d0ddb83e3e3b95db828", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. ", "mimetype": "text/plain", "start_char_idx": 60266, "end_char_idx": 60474, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2b2ace5b-b79a-4fa9-99a3-351451219f3f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. ", "original_text": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c57340c-b4de-4009-b9cc-54e899a35d10", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2006)), this has not been adressed in the *Isolation Forest* literature (see Liu et al.  (2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). ", "original_text": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves. "}, "hash": "b4b409da5e0c94003c8925764f896e1d0527802fcbf9dc688fd2c099970ac81a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4c29cb1-a280-4342-90b1-99471e5bc458", "node_type": "1", "metadata": {"window": "(2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. ", "original_text": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). "}, "hash": "9aa0d4477034bf3c017f1ea385166c389ed1dfd12ad78c0fadd4415f08dea971", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. ", "mimetype": "text/plain", "start_char_idx": 60474, "end_char_idx": 60675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4c29cb1-a280-4342-90b1-99471e5bc458", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. ", "original_text": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b2ace5b-b79a-4fa9-99a3-351451219f3f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2008), Liu et al.  (2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. ", "original_text": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model. "}, "hash": "82b23cadc02868add5fba3c66de0f2ae4510af22f24809f4bd56bee4450084cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "954081a4-6d83-44b4-9983-c6381a0844ee", "node_type": "1", "metadata": {"window": "(2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. ", "original_text": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. "}, "hash": "6932a3ab88236b92473d9d7aa14c96f6efa2aae638b5179fb58f80117e37fac9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). ", "mimetype": "text/plain", "start_char_idx": 60675, "end_char_idx": 60852, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "954081a4-6d83-44b4-9983-c6381a0844ee", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. ", "original_text": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4c29cb1-a280-4342-90b1-99471e5bc458", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2012) and Hariri et al.  (2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. ", "original_text": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality). "}, "hash": "1a95211cc03714dcb540951732f79a811dfc7c326bdd03e965a643f347d5296e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d93e701c-8168-4658-95b9-0fdba46b92c0", "node_type": "1", "metadata": {"window": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13. ", "original_text": "The general idea is to give importance to elements of D which allows to discriminate between the sample. "}, "hash": "7f15cddc9b36942af54ef52ff2c0dc14a3fa8212e9f80b56f88cb876f332a8a3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. ", "mimetype": "text/plain", "start_char_idx": 60852, "end_char_idx": 61022, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d93e701c-8168-4658-95b9-0fdba46b92c0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13. ", "original_text": "The general idea is to give importance to elements of D which allows to discriminate between the sample. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "954081a4-6d83-44b4-9983-c6381a0844ee", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2018)).  As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. ", "original_text": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves. "}, "hash": "f131fcd35b9d6e77addffca6793f7a3ea951faa50c5b10d5ec5434aff56a493b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c8a1418-a086-4c71-aeca-73355ba0fbc6", "node_type": "1", "metadata": {"window": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). ", "original_text": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). "}, "hash": "fb7a58f1d06bcb3fb41882e2a364f49dfdd7136f0116cfbb17eef66f71a3410e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The general idea is to give importance to elements of D which allows to discriminate between the sample. ", "mimetype": "text/plain", "start_char_idx": 61022, "end_char_idx": 61127, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9c8a1418-a086-4c71-aeca-73355ba0fbc6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). ", "original_text": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d93e701c-8168-4658-95b9-0fdba46b92c0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As a very randomized procedure, there is no incrementally way to define feature importance from the supervised setting.  Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13. ", "original_text": "The general idea is to give importance to elements of D which allows to discriminate between the sample. "}, "hash": "c41a72b3807c3dc07a07387af0137a713e0c0c272ecdc0b5f665a3fa85041d08", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca6fb637-b1e4-4343-8c21-7816b8275c07", "node_type": "1", "metadata": {"window": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. ", "original_text": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. "}, "hash": "9d9e769f0f1bd1c77972ca6e08903823c9fd0456045476aea4dff1415f7bc37c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). ", "mimetype": "text/plain", "start_char_idx": 61127, "end_char_idx": 61361, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ca6fb637-b1e4-4343-8c21-7816b8275c07", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. ", "original_text": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c8a1418-a086-4c71-aeca-73355ba0fbc6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Nevertheless, it is a matter of interest in many anomaly detection applications to get interpretability of models, especially when dealing with functional data where many information are contained in curves.  Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). ", "original_text": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction). "}, "hash": "b5bd29939f3a516961a92c93be93bbf9f214bfe163a013e08ae9613bdc94b666", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2a1a894-b04e-46a4-8335-e24f08518cac", "node_type": "1", "metadata": {"window": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. ", "original_text": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. "}, "hash": "724e7b652a50104266a735ddf7d4d8d3e2742e98a6d3c7c3c7232b5e4fd06b52", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. ", "mimetype": "text/plain", "start_char_idx": 61361, "end_char_idx": 61496, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2a1a894-b04e-46a4-8335-e24f08518cac", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. ", "original_text": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ca6fb637-b1e4-4343-8c21-7816b8275c07", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Thus, it is rewarding to get an *a posteriori* sparse representation of the dictionary D which corresponds to the discriminating directions that have great importance in the construction of the model.  Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. ", "original_text": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding. "}, "hash": "78d8f12a76e05523f6bc469b46973d56b22a61580dea950ccc5ea72ad398fc49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13ce4652-2a0f-446e-bd26-c33ebf8452ca", "node_type": "1", "metadata": {"window": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n", "original_text": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. "}, "hash": "c54c37aca6df0f7d043754b336a70e06c95661ad6a0bc24a5ae7948221cbc211", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. ", "mimetype": "text/plain", "start_char_idx": 61496, "end_char_idx": 61603, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "13ce4652-2a0f-446e-bd26-c33ebf8452ca", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n", "original_text": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2a1a894-b04e-46a4-8335-e24f08518cac", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Furthermore, it could bring some information on the distribution of normal data by studying the dispersion of the projection coefficients on a direction d (e.g multi-modality).  To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. ", "original_text": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated. "}, "hash": "746bb5eb5c9d6bf679505e354daae82f4ce43cfdcf5ff45478d3867ffa2599c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4eef68ee-8180-47c4-9452-61a7ec5b8dcf", "node_type": "1", "metadata": {"window": "The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. ", "original_text": "An example of the latter is given in Figure 13. "}, "hash": "c09692ccf131b4ac7cf5e43a00d73989291bc6953d8e073a58f967ccd8264b39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. ", "mimetype": "text/plain", "start_char_idx": 61603, "end_char_idx": 61723, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4eef68ee-8180-47c4-9452-61a7ec5b8dcf", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. ", "original_text": "An example of the latter is given in Figure 13. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13ce4652-2a0f-446e-bd26-c33ebf8452ca", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To extend this notion to the *Functional Isolation Forest* algorithm, we propose two ways to evaluate the importance of the elements of D to discriminate anomaly curves.  The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n", "original_text": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree. "}, "hash": "7a3c5c89531f64c237047196ba1eb7290300e6d4ccff535cdfc0428b56287344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f6e6bc9-9edf-4792-b225-45e35133e498", "node_type": "1", "metadata": {"window": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c).", "original_text": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). "}, "hash": "c019fc219647efb74749ce2ba275469ae878a430922a994419561492ccfed5f6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An example of the latter is given in Figure 13. ", "mimetype": "text/plain", "start_char_idx": 61723, "end_char_idx": 61771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f6e6bc9-9edf-4792-b225-45e35133e498", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c).", "original_text": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4eef68ee-8180-47c4-9452-61a7ec5b8dcf", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The general idea is to give importance to elements of D which allows to discriminate between the sample.  The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. ", "original_text": "An example of the latter is given in Figure 13. "}, "hash": "35351dda8dcd37c70a46d4127c5d8977ecdb8c1c65e1868ed3de56533264fdac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b0f1d16-a929-4c1e-a12b-abed36e6e126", "node_type": "1", "metadata": {"window": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots. ", "original_text": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. "}, "hash": "de325725d33b3f654ea399ba8289e52913be1d3848950d4d429289c47f78f2b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). ", "mimetype": "text/plain", "start_char_idx": 61771, "end_char_idx": 61868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b0f1d16-a929-4c1e-a12b-abed36e6e126", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots. ", "original_text": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f6e6bc9-9edf-4792-b225-45e35133e498", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The naive idea is to add \"+1\" to the elements of D where an instance of the node sample is isolated (except for the cells with only two instances) such that good directions are those with a high score (after the forest construction).  A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c).", "original_text": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2). "}, "hash": "14b7e70959cc6b4a6c40649df520507f423fc1552cd2163868b46ada82a1fd9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0458dd5-55ac-4b62-a700-5c807732aff0", "node_type": "1", "metadata": {"window": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. ", "original_text": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. "}, "hash": "6789c6284fb7b5578191e285542e40c612ab2c30802ea407cbc29ff5dcc096ca", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. ", "mimetype": "text/plain", "start_char_idx": 61868, "end_char_idx": 61945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e0458dd5-55ac-4b62-a700-5c807732aff0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. ", "original_text": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b0f1d16-a929-4c1e-a12b-abed36e6e126", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A clever one, more adaptive, would be to get weighted gain since curves isolated at nodes closer to the root should be more rewarding.  To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots. ", "original_text": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product. "}, "hash": "1152bf0d938ab72f71daa51e61f7b7eed8155a8fd796cef6385c4d698bd2f6a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0085401a-319d-4288-bbcc-e463359807bb", "node_type": "1", "metadata": {"window": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. ", "original_text": "These leads to some interpretability of a \"black box\" procedure.\n\n"}, "hash": "ec0ae557ec44e5a8af01fdd85ce19982c5922b9b170f844161cd5b0d4d761c17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. ", "mimetype": "text/plain", "start_char_idx": 61945, "end_char_idx": 62132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0085401a-319d-4288-bbcc-e463359807bb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. ", "original_text": "These leads to some interpretability of a \"black box\" procedure.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0458dd5-55ac-4b62-a700-5c807732aff0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "To do this, we choose to give a reward depending on the size of the sample node where a curve is isolated.  Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. ", "original_text": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones. "}, "hash": "918a1bbe49b068d47724fc595aeada6800bcca6de85d9ed4dfe01d717e9dfda0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "619f7bef-83a9-4600-ac70-45f0d5ac5159", "node_type": "1", "metadata": {"window": "An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). ", "original_text": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. "}, "hash": "16212170d65c397caa5d5aa22b6b4c1e0f7435731fb75b4e96e551adc661575d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These leads to some interpretability of a \"black box\" procedure.\n\n", "mimetype": "text/plain", "start_char_idx": 62132, "end_char_idx": 62198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "619f7bef-83a9-4600-ac70-45f0d5ac5159", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). ", "original_text": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0085401a-319d-4288-bbcc-e463359807bb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Precisely, the given reward is equal to the size of the node sample divided by the (sub)-sample used to build the tree.  An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. ", "original_text": "These leads to some interpretability of a \"black box\" procedure.\n\n"}, "hash": "38db32d58f16548d26bb03cf6c3e54b40179b8c420b4e0d7ab0a3088f1b168f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bad19856-961e-4300-a3d1-f984b8eb4483", "node_type": "1", "metadata": {"window": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n", "original_text": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c)."}, "hash": "9be48587f06d6ef6f10438fa5fa1962105a6dec0573a8d07d41fc25abc2b6989", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. ", "mimetype": "text/plain", "start_char_idx": 62198, "end_char_idx": 62318, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bad19856-961e-4300-a3d1-f984b8eb4483", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n", "original_text": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "619f7bef-83a9-4600-ac70-45f0d5ac5159", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "An example of the latter is given in Figure 13.  The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). ", "original_text": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data. "}, "hash": "fd8ba70ce43f32751f655511cd93f2d4beeb39e5a5b7ac61fd0567d55507950e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd07c4b3-397f-4d02-bac0-817b9f8118e1", "node_type": "1", "metadata": {"window": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. ", "original_text": "**\n\n**Description:** The figure contains three plots. "}, "hash": "a752732af26bbe992e118d71b8f26be083fda9f87265e7be2e7d465579a53f9c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c).", "mimetype": "text/plain", "start_char_idx": 62318, "end_char_idx": 62513, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd07c4b3-397f-4d02-bac0-817b9f8118e1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. ", "original_text": "**\n\n**Description:** The figure contains three plots. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bad19856-961e-4300-a3d1-f984b8eb4483", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The experiment is conducted on the real-world CinECGTorso dataset (more details in Section 4.2).  We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n", "original_text": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c)."}, "hash": "695792d561e01221071a5e1ca404298367630806222f3e7577f0fef76d931051", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ca1d0c7-ea13-4c96-841d-648fca83b8de", "node_type": "1", "metadata": {"window": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. ", "original_text": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. "}, "hash": "4a3621e65efc23676f54794e5a5370c75850e6e2e2a7458502442e925ca1fcd2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure contains three plots. ", "mimetype": "text/plain", "start_char_idx": 62513, "end_char_idx": 62567, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8ca1d0c7-ea13-4c96-841d-648fca83b8de", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. ", "original_text": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd07c4b3-397f-4d02-bac0-817b9f8118e1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We use FIF with the *Dyadic indicator dictionary* and the L\u2082 scalar product.  As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. ", "original_text": "**\n\n**Description:** The figure contains three plots. "}, "hash": "836d27b59acf188b79e45f416214abd1bb3684c4dba37da3a36002076fb67ce8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3c8c3d6-c6ba-46fc-bdd4-a13c672f12d0", "node_type": "1", "metadata": {"window": "These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n", "original_text": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. "}, "hash": "d69fb18304e0998e13cb9a29a5029e6dafe5204a226efd6c37af47f0073d5cad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. ", "mimetype": "text/plain", "start_char_idx": 62567, "end_char_idx": 62663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b3c8c3d6-c6ba-46fc-bdd4-a13c672f12d0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n", "original_text": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ca1d0c7-ea13-4c96-841d-648fca83b8de", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "As we can see, the two most important elements of the dictionary are indicator functions which localize the peak around t = 0.4 where anomalies are really different from the normal ones.  These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. ", "original_text": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red. "}, "hash": "7cd0e263b8921ddca37e27884aec75b12ea7fdbe6e365dae6f496d66f9bd9d60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fcec553-6477-4564-b3e9-af7120f495d5", "node_type": "1", "metadata": {"window": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n", "original_text": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). "}, "hash": "b1d8d9c367d42cd4dfaa001d981a721efdc0c3d8dd2f9c381872f3865b98e22e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. ", "mimetype": "text/plain", "start_char_idx": 62663, "end_char_idx": 62815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fcec553-6477-4564-b3e9-af7120f495d5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n", "original_text": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3c8c3d6-c6ba-46fc-bdd4-a13c672f12d0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These leads to some interpretability of a \"black box\" procedure.\n\n ***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n", "original_text": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest. "}, "hash": "7b05e9910cb6d9cb10cad9cc05e8fd98541535afc464d84c8ab169edcb4887a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64aaa30d-148d-4d29-b49d-fe1ac7df43c9", "node_type": "1", "metadata": {"window": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n", "original_text": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n"}, "hash": "febcb28a2c4bd4e926222af374186e844b3acfdc5dccd0b26668418a6930c359", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). ", "mimetype": "text/plain", "start_char_idx": 62815, "end_char_idx": 62950, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "64aaa30d-148d-4d29-b49d-fe1ac7df43c9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n", "original_text": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fcec553-6477-4564-b3e9-af7120f495d5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 13: CinECGTorso training dataset (a), red curves correspond to anomalies while blue curves to normal data.  The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n", "original_text": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b). "}, "hash": "a8adc30034887e471a33b3d719d3e0d31f16df3a14a1549e018140ffbcf8bb41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c9859ab-b7e9-4376-8d8f-a6ea5b80aa99", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n", "original_text": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. "}, "hash": "b6badd6a517bd4f0dd02e5b08c9826be732fd443b93e77d3182ebae201d8bc88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n", "mimetype": "text/plain", "start_char_idx": 62950, "end_char_idx": 63070, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c9859ab-b7e9-4376-8d8f-a6ea5b80aa99", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n", "original_text": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "64aaa30d-148d-4d29-b49d-fe1ac7df43c9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The direction importances given by the \"adaptative\" way are represented by (b) and the two most important functions (from the dyadic dictionary) used by FIF to build the model are plotted in (c). **\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n", "original_text": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n"}, "hash": "c0dc406a9650e100559fdcf079de95e6fc2ca6e9b0c9a5a1e5f5b055b1ee1ae5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94860d33-11ff-485d-9c11-fa088992cbbb", "node_type": "1", "metadata": {"window": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n", "original_text": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. "}, "hash": "012532fa0bc9f426ea6d970da1409f6a53090cb34e1452ebd7b1acdbebb02cba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. ", "mimetype": "text/plain", "start_char_idx": 63070, "end_char_idx": 63210, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94860d33-11ff-485d-9c11-fa088992cbbb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n", "original_text": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c9859ab-b7e9-4376-8d8f-a6ea5b80aa99", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure contains three plots.  Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n", "original_text": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm. "}, "hash": "71adc28858517986bac4da2976de29b5a7b0c395ec41edc52f8d1a0f582f08e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "798c0454-2288-4b0e-a464-d579db46fafc", "node_type": "1", "metadata": {"window": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n", "original_text": "We fixed the size of the dictionary to 1000.\n\n"}, "hash": "c6d80df987280db8c4946fced961a774a0c05794fd239e64e845574380d84234", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. ", "mimetype": "text/plain", "start_char_idx": 63210, "end_char_idx": 63647, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "798c0454-2288-4b0e-a464-d579db46fafc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n", "original_text": "We fixed the size of the dictionary to 1000.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94860d33-11ff-485d-9c11-fa088992cbbb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Plot (a) shows the CinECGTorso training dataset, with normal data in blue and anomalies in red.  Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n", "original_text": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4]. "}, "hash": "78f1ed5426a28e718ddfdb0494bf59a7e90c39ea9f611080759cab7d37d0a8be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3731f253-a687-4823-ba7f-4bb128e00474", "node_type": "1", "metadata": {"window": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs. ", "original_text": "*Scalar product:* L\u00b2 dot product.\n\n"}, "hash": "8469000a0d8c9ba7922993cb47ead786d471ec6f5e7ca446c9dca95f10209bbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We fixed the size of the dictionary to 1000.\n\n", "mimetype": "text/plain", "start_char_idx": 63647, "end_char_idx": 63693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3731f253-a687-4823-ba7f-4bb128e00474", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs. ", "original_text": "*Scalar product:* L\u00b2 dot product.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "798c0454-2288-4b0e-a464-d579db46fafc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Plot (b) is a bar chart showing the importance of different directions (dictionary elements), with a few bars being significantly taller than the rest.  Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n", "original_text": "We fixed the size of the dictionary to 1000.\n\n"}, "hash": "cd6661d8240a3ccf6e25c084bc03021dd4170ee61adea66c33432d2d29ca892e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd757add-3fc9-4c64-acbf-3c897481694f", "node_type": "1", "metadata": {"window": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n", "original_text": "*Size of the dataset:* n = 500.\n\n"}, "hash": "558ca5370b934c54fb7c952b37b84e6993c2ba0358957e26198941499ed9dc8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Scalar product:* L\u00b2 dot product.\n\n", "mimetype": "text/plain", "start_char_idx": 63693, "end_char_idx": 63728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fd757add-3fc9-4c64-acbf-3c897481694f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n", "original_text": "*Size of the dataset:* n = 500.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3731f253-a687-4823-ba7f-4bb128e00474", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Plot (c) shows two indicator functions (step functions), which correspond to the two most important directions identified in plot (b).  These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs. ", "original_text": "*Scalar product:* L\u00b2 dot product.\n\n"}, "hash": "c76154552fdfb40068579baa8694c0907a393ff85929fcba457e59047256a5cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff97e87d-7af1-409b-b7c3-ef4b58300561", "node_type": "1", "metadata": {"window": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "*Subsampling size:* \u03c8 = 64.\n\n"}, "hash": "1d5a6f4ccccc0bc8ca0982d7380fb46f3c41ae34c46b64a668a2680a5c81cbbf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Size of the dataset:* n = 500.\n\n", "mimetype": "text/plain", "start_char_idx": 63728, "end_char_idx": 63761, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ff97e87d-7af1-409b-b7c3-ef4b58300561", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "*Subsampling size:* \u03c8 = 64.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd757add-3fc9-4c64-acbf-3c897481694f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These functions are non-zero around t = 0.4, highlighting the region where normal and anomalous curves differ the most.\n ***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n", "original_text": "*Size of the dataset:* n = 500.\n\n"}, "hash": "54a38d2f85d5bd974f499627d889f9f10e0cc5e7dd4bb7a8910de156e7fc0fe3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb3c6284-9388-4874-9c2e-efa4cd65debd", "node_type": "1", "metadata": {"window": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "*The number of trees:* N = 100.\n\n"}, "hash": "18cb8032949cc08d0c018dc8c339ea35626dba6b4b7191f31a4938a54aaf7554", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*Subsampling size:* \u03c8 = 64.\n\n", "mimetype": "text/plain", "start_char_idx": 63761, "end_char_idx": 63790, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fb3c6284-9388-4874-9c2e-efa4cd65debd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "*The number of trees:* N = 100.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff97e87d-7af1-409b-b7c3-ef4b58300561", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n### D. Study of the parameters of FIF\n\nIn this section, we present results of a simulation study of the variance of the FIF algorithm.  The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "*Subsampling size:* \u03c8 = 64.\n\n"}, "hash": "c907231a854c2893be2256743face500ba4ef826991318fe266a6428d2fd86c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfc5a5b1-35d7-4fa9-bc60-9ed2e94f27d8", "node_type": "1", "metadata": {"window": "We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "original_text": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n"}, "hash": "f4ff3ee08659507ea41fd7591e31c79a1682de02c9ddd824123f542d26be37a0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*The number of trees:* N = 100.\n\n", "mimetype": "text/plain", "start_char_idx": 63790, "end_char_idx": 63823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bfc5a5b1-35d7-4fa9-bc60-9ed2e94f27d8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "original_text": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fb3c6284-9388-4874-9c2e-efa4cd65debd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The experiments were conducted on the datasets (a) and (b) from Section 4 (see also Figure 3 of Section 4), for each of the four specified observations x\u2080, x\u2081, x\u2082, x\u2083 using the following settings (except varying parameter):\n\n*Dictionary:* Gaussian wavelets (negative second derivative of the standard Gaussian density) with random variance selected in an uniform way in [0.2, 1] and a translation parameter selected randomly in [\u22124, 4].  We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "*The number of trees:* N = 100.\n\n"}, "hash": "ff71bdfb20a7a0ac460d04cf503b2852d4bed64ecf8ffb36d8d4c1d0bca57c36", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94e21186-a8dd-4375-8200-372bae73ebb9", "node_type": "1", "metadata": {"window": "*Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score. ", "original_text": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs. "}, "hash": "880b996fbddc1a49415ba43be502efa569fee484c0d315f2836e36e29b083721", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n", "mimetype": "text/plain", "start_char_idx": 63823, "end_char_idx": 63868, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94e21186-a8dd-4375-8200-372bae73ebb9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score. ", "original_text": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfc5a5b1-35d7-4fa9-bc60-9ed2e94f27d8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We fixed the size of the dictionary to 1000.\n\n *Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "original_text": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n"}, "hash": "fd45bd64b759cfc34fc86ebc2760bac583392aee6a2f7090810f5c67d957b7d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c24d448d-2fd7-4ac0-820d-9dba7c3c6635", "node_type": "1", "metadata": {"window": "*Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). ", "original_text": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n"}, "hash": "232fe7d5ff54bcb42cd75ba349c849f1766769d7f8b1c5c13d3de4e867683234", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs. ", "mimetype": "text/plain", "start_char_idx": 63868, "end_char_idx": 63945, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c24d448d-2fd7-4ac0-820d-9dba7c3c6635", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). ", "original_text": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94e21186-a8dd-4375-8200-372bae73ebb9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Scalar product:* L\u00b2 dot product.\n\n *Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score. ", "original_text": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs. "}, "hash": "2f8fea968a8d0d16777021e65e43cec08006246fa1ef698d2053827dc39f9665", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01829b54-fc21-41c8-b26d-500b3b47ac73", "node_type": "1", "metadata": {"window": "*Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\". ", "original_text": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "d641f2dba6c2338d50fd1c1db2216364eb6370f9d6eabeb7f2e013944ca29f99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n", "mimetype": "text/plain", "start_char_idx": 63945, "end_char_idx": 64069, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01829b54-fc21-41c8-b26d-500b3b47ac73", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\". ", "original_text": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c24d448d-2fd7-4ac0-820d-9dba7c3c6635", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Size of the dataset:* n = 500.\n\n *Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). ", "original_text": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n"}, "hash": "92536c2765aaf074f5f9e9a44a151ed09ea93e60dc3288c24385a5ba4901e834", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c62a3120-e346-47ba-a9ff-6824c33d4a4d", "node_type": "1", "metadata": {"window": "*The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "647890165de5cea83927a10b1690768780523a3fffabf859ac52b425d42ce6d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "mimetype": "text/plain", "start_char_idx": 64069, "end_char_idx": 64198, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c62a3120-e346-47ba-a9ff-6824c33d4a4d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01829b54-fc21-41c8-b26d-500b3b47ac73", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*Subsampling size:* \u03c8 = 64.\n\n *The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\". ", "original_text": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "75f3e167b6e66c479cc4a25e052de3e4fe95987d8a4d3ba0a0b76126306759a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01b699b5-52e7-4bb1-92c8-67d2f06d796c", "node_type": "1", "metadata": {"window": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n", "original_text": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). "}, "hash": "4aff009ab6e86f83e3188f309ecf78af1cd6f8aaac7c2753f18d8692e460f89b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 64198, "end_char_idx": 64296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01b699b5-52e7-4bb1-92c8-67d2f06d796c", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n", "original_text": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c62a3120-e346-47ba-a9ff-6824c33d4a4d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*The number of trees:* N = 100.\n\n *The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "41e6053b521b5ea90c73b40762e963ad007e91983b14a5354abeb18532078fba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7276afdf-b99d-43b7-9cd4-736d04ebc468", "node_type": "1", "metadata": {"window": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n", "original_text": "Each panel contains horizontal boxplots showing the distribution of the FIF score. "}, "hash": "096d40aa1ba34ce950909ad163c22a1379043965ecf2ba9eccc4ac3e8e5017bc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). ", "mimetype": "text/plain", "start_char_idx": 64296, "end_char_idx": 64388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7276afdf-b99d-43b7-9cd4-736d04ebc468", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n", "original_text": "Each panel contains horizontal boxplots showing the distribution of the FIF score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01b699b5-52e7-4bb1-92c8-67d2f06d796c", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*The height limit:* fixed to l = [log\u2082(\u03c8)].\n\n The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n", "original_text": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083). "}, "hash": "bd80e5d2e4a742338905c9ea3c75ab2372fe5f32054c380eea331673e5fbe84c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da44e05f-d715-4588-b4dd-9c5aed23da0d", "node_type": "1", "metadata": {"window": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). "}, "hash": "28111e79ea0a56aaaf2482d688e6815da6c5ae4d3bfc8ecaafde76110bae8e79", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each panel contains horizontal boxplots showing the distribution of the FIF score. ", "mimetype": "text/plain", "start_char_idx": 64388, "end_char_idx": 64471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "da44e05f-d715-4588-b4dd-9c5aed23da0d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7276afdf-b99d-43b7-9cd4-736d04ebc468", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The figures below indicate boxplots of the FIF anomaly score, over 100 runs.  Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n", "original_text": "Each panel contains horizontal boxplots showing the distribution of the FIF score. "}, "hash": "92de771ed6c8842f54e5d67edc938c5fb917beb0a35b149dc9822191f35f2ccf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a74b5cf9-5fe8-428e-ba20-f78ba09e3314", "node_type": "1", "metadata": {"window": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The x-axis is the \"Score\". "}, "hash": "f76639ff11b0e626406ab7015cb65d5cddf5b189f91c5443678445ddd46fff57", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). ", "mimetype": "text/plain", "start_char_idx": 64471, "end_char_idx": 64570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a74b5cf9-5fe8-428e-ba20-f78ba09e3314", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The x-axis is the \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da44e05f-d715-4588-b4dd-9c5aed23da0d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Empirical study of the FIF anomaly score and its variance when increasing the number of F-itrees is depicted in Figure 14.\n\n ***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500). "}, "hash": "c255002be6536f9ebb1c03924c1d5dd486257c28c5672936d5cfef6b466895d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "063ad723-941a-4136-b52b-a1cc0dc8ba11", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). "}, "hash": "a1e8e8368828481e457b68954f28a729ee46fc5a065c16074cd9531fc9bb1eb8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is the \"Score\". ", "mimetype": "text/plain", "start_char_idx": 64570, "end_char_idx": 64597, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "063ad723-941a-4136-b52b-a1cc0dc8ba11", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a74b5cf9-5fe8-428e-ba20-f78ba09e3314", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 14: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The x-axis is the \"Score\". "}, "hash": "dff00e4c61cddc8d37a431fdaa7dbd904c256f486ba47497dbfc9318586b1848", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4c3abe5-262d-406d-bdd1-91d9cf258277", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). ", "original_text": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n"}, "hash": "62924596b41236dcdd3fddc1d2014621e4d7b1028646f7c5df3b462d5b701442", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). ", "mimetype": "text/plain", "start_char_idx": 64597, "end_char_idx": 64693, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4c3abe5-262d-406d-bdd1-91d9cf258277", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). ", "original_text": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "063ad723-941a-4136-b52b-a1cc0dc8ba11", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b). "}, "hash": "f6eebbb0568695dad74cc7e9f6029aeb081aa38b21e2d1cf253a86f7ccfcf414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d77a7a69-d98f-4cd3-aad4-84fc609e4ca0", "node_type": "1", "metadata": {"window": "Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n"}, "hash": "ba617a147d9bdae2565ea3d9c80bdbc5faf2c721fa12d186f703e7b170875592", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n", "mimetype": "text/plain", "start_char_idx": 64693, "end_char_idx": 64818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d77a7a69-d98f-4cd3-aad4-84fc609e4ca0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4c3abe5-262d-406d-bdd1-91d9cf258277", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure has four panels, one for each observation (X\u2080, X\u2081, X\u2082, X\u2083).  Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). ", "original_text": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n"}, "hash": "6a4c3f5985f7044c63b5589cfa7cf14475357dfa7bbd6b9c93b3ea7d151a026f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd0ceef6-3493-43d9-ac45-257e9ff150dd", "node_type": "1", "metadata": {"window": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n", "original_text": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "73272ed0742a98fb4fc9fa15e9e96169e173995eff906e406711b17b7d8ced97", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n", "mimetype": "text/plain", "start_char_idx": 64818, "end_char_idx": 64943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd0ceef6-3493-43d9-ac45-257e9ff150dd", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n", "original_text": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d77a7a69-d98f-4cd3-aad4-84fc609e4ca0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Each panel contains horizontal boxplots showing the distribution of the FIF score.  The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n"}, "hash": "810b1f4dbc14ac73371dbf0dc735c3bde497aad9afe76933ab474ec8dda89d1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2e0bb549-fc6e-43a1-aad9-f2bebb16fe77", "node_type": "1", "metadata": {"window": "The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "b7c1749152df8ec0a7705fa23dca7d115de8d2f67992e199267e933620e2c88f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "mimetype": "text/plain", "start_char_idx": 64943, "end_char_idx": 65072, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2e0bb549-fc6e-43a1-aad9-f2bebb16fe77", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd0ceef6-3493-43d9-ac45-257e9ff150dd", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis (labeled \"N\") represents the number of trees in the forest (10, 20, 50, 100, 200, 500).  The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n", "original_text": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "f0d4da1485137612f0a46db50e279f93b3452fbb9af508b4c019f6142b0827e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d4a739b-0f9e-4b47-9932-cf9de2ab7e36", "node_type": "1", "metadata": {"window": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "7afd399cbd33169f6de2e0e1f51474577c2fbdbf26539555f22d22d1b0b73e82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 65072, "end_char_idx": 65170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d4a739b-0f9e-4b47-9932-cf9de2ab7e36", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2e0bb549-fc6e-43a1-aad9-f2bebb16fe77", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is the \"Score\".  For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "854dcc91bf6fb4132ee873e809c968a1a9edf829917ede2ee686e2a2a269823f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b9796ee1-485c-4d8c-b536-7901b5eb7d39", "node_type": "1", "metadata": {"window": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). "}, "hash": "b3e3bed34c6fef497750e561a1c147dfbc014944e69a0e41674a3fc3d6317975", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "mimetype": "text/plain", "start_char_idx": 65170, "end_char_idx": 65276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b9796ee1-485c-4d8c-b536-7901b5eb7d39", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d4a739b-0f9e-4b47-9932-cf9de2ab7e36", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For each value of N, there are two boxplots: orange for dataset (a) and purple for dataset (b).  The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "f90f55e3627e722d4294aa1551c213fd22f2608133eb100f267e68326948438f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17b4c8da-b066-4d52-96c3-2e2cc80d02f7", "node_type": "1", "metadata": {"window": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "hash": "08b6890e8c4230cdb89cf5825a3dba5fa815e01de9c367ee493591ecd39e20b8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). ", "mimetype": "text/plain", "start_char_idx": 65276, "end_char_idx": 65347, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "17b4c8da-b066-4d52-96c3-2e2cc80d02f7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b9796ee1-485c-4d8c-b536-7901b5eb7d39", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The plots show that as the number of trees increases, the variance of the score decreases, leading to more stable estimates.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500). "}, "hash": "d05f288689b291cd3d133d85c9d9f184451a9bfa6272e584e8297cb21c450823", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4939577-0304-4360-92e6-5b18da062cf9", "node_type": "1", "metadata": {"window": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8). ", "original_text": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n"}, "hash": "728c3209863801152849e24fcd27069c38d3300078d7faf615fbdc528f64fecc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Score\". ", "mimetype": "text/plain", "start_char_idx": 65347, "end_char_idx": 65370, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f4939577-0304-4360-92e6-5b18da062cf9", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8). ", "original_text": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17b4c8da-b066-4d52-96c3-2e2cc80d02f7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the subsample size is depicted in Figure 15.\n\n ***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "hash": "e048bb6cfcad7da9a8e14551e4e8ea9d4ba743116ebfd0db1f6224106e77579d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0103dd6-c754-49c6-95ab-3feaca07163f", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n"}, "hash": "168745506fdf0dd0137e7c09415d1e61767c094d1ca438a86052040000ce4f46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n", "mimetype": "text/plain", "start_char_idx": 65370, "end_char_idx": 65532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0103dd6-c754-49c6-95ab-3feaca07163f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4939577-0304-4360-92e6-5b18da062cf9", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 15: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8). ", "original_text": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n"}, "hash": "9d60cfb0c1c59d591db56e871467cff06e856eb6fd4b338e5194f94743abaaf5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5cbfd8b-dbe7-4c64-898c-c402b089de3d", "node_type": "1", "metadata": {"window": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n", "original_text": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "86fca0de415fea1ab2bd33ad9f3e7fefcf9b7d88292bdacf8ed840686836ed18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n", "mimetype": "text/plain", "start_char_idx": 65532, "end_char_idx": 65670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5cbfd8b-dbe7-4c64-898c-c402b089de3d", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n", "original_text": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0103dd6-c754-49c6-95ab-3feaca07163f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n"}, "hash": "3e349014ad08956306a01ea088ea4d623a35e764804d2afda2eecc64f9fec786", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "376a0f48-7219-4243-8623-786bed0ce1b4", "node_type": "1", "metadata": {"window": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "2e2ee74533a0e91b162a6f3d76530481aeba55525907a0d8b3b48cb2c8f585c3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "mimetype": "text/plain", "start_char_idx": 65670, "end_char_idx": 65799, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "376a0f48-7219-4243-8623-786bed0ce1b4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5cbfd8b-dbe7-4c64-898c-c402b089de3d", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure is similar to the previous one, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n", "original_text": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "4dce306621148f939ddfa40de948a762ec087f2faf558157ec867778b59b9f00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8505af7-80ac-4ad9-b713-133e47b345c8", "node_type": "1", "metadata": {"window": "The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "467fe813b44ff20ceb58fef87fcae7fd576c4544a696fc1ae0275547e1e53558", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 65799, "end_char_idx": 65897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8505af7-80ac-4ad9-b713-133e47b345c8", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "376a0f48-7219-4243-8623-786bed0ce1b4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Here, the y-axis is \"Subsample size\" (16, 32, 64, 128, 256, 350, 500).  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "5eed582d73accc80a360110f41f63eb9f7f77d9c9d4b4c5a10bb451d31cf09bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4bf1b23b-f7dc-45bb-9e02-d5806b574d76", "node_type": "1", "metadata": {"window": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The y-axis is \"Height limit\" (from 3 to 8). "}, "hash": "e3b2e9544ecc3f8cd5e73cce577deb518169d1bbda3e3b46333d5ca3eec5b4b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "mimetype": "text/plain", "start_char_idx": 65897, "end_char_idx": 65969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4bf1b23b-f7dc-45bb-9e02-d5806b574d76", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The y-axis is \"Height limit\" (from 3 to 8). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8505af7-80ac-4ad9-b713-133e47b345c8", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "original_text": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "033895e17511664819fe070512d9383689a656949604a797fc2a3fb58f04bf7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50144602-2ac6-4005-8824-b4a1c8bb5458", "node_type": "1", "metadata": {"window": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "hash": "ae2ea5941e6dd0bd48a738caa5ae4a2ed6710428c1c55ffda1231a3a658719c8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis is \"Height limit\" (from 3 to 8). ", "mimetype": "text/plain", "start_char_idx": 65969, "end_char_idx": 66013, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50144602-2ac6-4005-8824-b4a1c8bb5458", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4bf1b23b-f7dc-45bb-9e02-d5806b574d76", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The boxplots (orange for dataset (a), purple for dataset (b)) illustrate how the score distribution changes as the subsample size used to build the trees varies.\n ***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The y-axis is \"Height limit\" (from 3 to 8). "}, "hash": "5f0c8baece9fa22f9d3f9477cb3d8d3c48628198536bb0134d36f4110648e38d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2d7459f-8214-4e82-acb5-04ab59703332", "node_type": "1", "metadata": {"window": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). ", "original_text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n"}, "hash": "b6d3c8384f7fcb419945faeeeea358bcf5a1973eac26c48c5d6db2c3467e30bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Score\". ", "mimetype": "text/plain", "start_char_idx": 66013, "end_char_idx": 66036, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2d7459f-8214-4e82-acb5-04ab59703332", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). ", "original_text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50144602-2ac6-4005-8824-b4a1c8bb5458", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nEmpirical study of the FIF anomaly score and its variance when increasing the height limit of the F-itree is depicted in Figure 16.\n\n ***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "hash": "1beac6efbd44088f2f691c38d46ca6beb52b0c6149d71bfdea037e1292bcefe6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cc864474-feea-4835-8480-d4b4cb84ca4a", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\". ", "original_text": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n"}, "hash": "4c12e7d926b9e6e5588f2579c4352ca3fb304c0b4770af4ff61d31264f98d502", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n", "mimetype": "text/plain", "start_char_idx": 66036, "end_char_idx": 66224, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cc864474-feea-4835-8480-d4b4cb84ca4a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\". ", "original_text": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2d7459f-8214-4e82-acb5-04ab59703332", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 16: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). ", "original_text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n"}, "hash": "1b5676737e4f9c80d78cc7e635508b109e60e62174bdf804c4f59420364728a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad76b567-46be-45d8-9828-153fb5eafa85", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n", "original_text": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "26fb34644a51dfdf09997363330349df137bd7f4225b324d4f496f7d187a0890", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n", "mimetype": "text/plain", "start_char_idx": 66224, "end_char_idx": 66436, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ad76b567-46be-45d8-9828-153fb5eafa85", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n", "original_text": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cc864474-feea-4835-8480-d4b4cb84ca4a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\". ", "original_text": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n"}, "hash": "704f45675cb4607c9f0389b05eb896da59f89481c5d86999e19fc1d08e693b05", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b97f9df-d23d-457d-b296-3aeb4a7599ad", "node_type": "1", "metadata": {"window": "The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "a3c365cb903d800def81cf8a08ba5c441b2cbffe84dbd07e2e0965bf959c37af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. ", "mimetype": "text/plain", "start_char_idx": 66436, "end_char_idx": 66565, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b97f9df-d23d-457d-b296-3aeb4a7599ad", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad76b567-46be-45d8-9828-153fb5eafa85", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n", "original_text": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes. "}, "hash": "dcfd940aeeeb67841e0cb82aef0912ffe32269806be4493998d4927f18a71e2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "186f3f48-5555-46f0-b59a-d0f316729f12", "node_type": "1", "metadata": {"window": "The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. ", "original_text": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "1047682fa1d6e6e58e47f1e18e849951a0634ea4e5e5a98eaf0ce4eecb6ec3b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 66565, "end_char_idx": 66663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "186f3f48-5555-46f0-b59a-d0f316729f12", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. ", "original_text": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b97f9df-d23d-457d-b296-3aeb4a7599ad", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis is \"Height limit\" (from 3 to 8).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "58647ea501a027ba0f3629ae612ee4b15f04218fab3a6c8abcb300dd277c096c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e696fba9-d8f4-4482-a2b2-435fd1e3b8b3", "node_type": "1", "metadata": {"window": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). "}, "hash": "750e9df47a09e9a561e25c4c6cd761c63446cd7661afb7c6886487702b95a845", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "mimetype": "text/plain", "start_char_idx": 66663, "end_char_idx": 66736, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e696fba9-d8f4-4482-a2b2-435fd1e3b8b3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "186f3f48-5555-46f0-b59a-d0f316729f12", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. ", "original_text": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "2954073f1381eb7f40c9a070d31a4d46d161a44ced60bad3a8ec9a916b66ce19", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d0d43df3-f903-4278-92ae-97c179a80319", "node_type": "1", "metadata": {"window": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "hash": "1daa79c5aa41a2cf5a2551d4173b29fb479b3a3c67fac67b6164fa3eeca3ee9a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). ", "mimetype": "text/plain", "start_char_idx": 66736, "end_char_idx": 66825, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d0d43df3-f903-4278-92ae-97c179a80319", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e696fba9-d8f4-4482-a2b2-435fd1e3b8b3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show the effect of changing the maximum allowed depth of the F-itrees on the resulting anomaly score distribution.\n ***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "original_text": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000). "}, "hash": "4d638bf5dcd30080957190c6f359b91889e22fd10945936bd9a3611e3479445b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b33420d-8253-4dfe-85b3-94fc29a099e4", "node_type": "1", "metadata": {"window": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. ", "original_text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n"}, "hash": "e509568694ee96f142282bd98c1dccc6b435f1f536d7935b13ff283b0d269220", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Score\". ", "mimetype": "text/plain", "start_char_idx": 66825, "end_char_idx": 66848, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b33420d-8253-4dfe-85b3-94fc29a099e4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. ", "original_text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d0d43df3-f903-4278-92ae-97c179a80319", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nTaking finite size versions of the infinite *gaussian wavelets* dictionary, an empirical study of the FIF anomaly score and its variance when increasing the size of the dictionary is depicted in Figure 17.\n\n ***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "original_text": "The x-axis is \"Score\". "}, "hash": "31c5d6bad79052a14152741d17f00dd0792029b74b68db33ede5497d2449b9f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb222075-4566-406f-84b5-267f77372086", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n"}, "hash": "82d4fb05dc40efbb9f89471c60d3cf026d01c290ed6f42cccba3f3aea106399a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n", "mimetype": "text/plain", "start_char_idx": 66848, "end_char_idx": 67011, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb222075-4566-406f-84b5-267f77372086", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b33420d-8253-4dfe-85b3-94fc29a099e4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 17: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different sample sizes.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. ", "original_text": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n"}, "hash": "75a532ee64328f6635e3d1341251c524b59d8eb7a6646dfb74abc864987b7f9e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bf61f442-ea37-4f1d-81d7-a5e4dd7e9f02", "node_type": "1", "metadata": {"window": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n", "original_text": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. "}, "hash": "e73ef3732f83b6c9922e284ab7f400ccf09ccd5f3c444d2b3c5ec7c2cc8c0397", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n", "mimetype": "text/plain", "start_char_idx": 67011, "end_char_idx": 67160, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bf61f442-ea37-4f1d-81d7-a5e4dd7e9f02", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n", "original_text": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb222075-4566-406f-84b5-267f77372086", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\". ", "original_text": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n"}, "hash": "69f6ade2fcb817a4110b54be8d9e22112633727c291a5d3d6d9af573365bb20b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d15dbe9e-cafa-4b70-8e19-fe706ef74b50", "node_type": "1", "metadata": {"window": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "35dca158ea7fa185ddbbec80bf027df97fea0420ce3ffbda16714d72eb780cad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. ", "mimetype": "text/plain", "start_char_idx": 67160, "end_char_idx": 67336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d15dbe9e-cafa-4b70-8e19-fe706ef74b50", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bf61f442-ea37-4f1d-81d7-a5e4dd7e9f02", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure has four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n", "original_text": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives. "}, "hash": "13a5fc11f176b5d68d86e025070fcfb736ec8f4799c985b019088cc8bf951d76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4571bb74-32bb-482e-b87c-6bad3571c4b6", "node_type": "1", "metadata": {"window": "The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. ", "original_text": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "c8fe69a9666239cff497086ba0e5655e2a66395c5824100812fff797a2e201cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b).", "mimetype": "text/plain", "start_char_idx": 67336, "end_char_idx": 67434, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4571bb74-32bb-482e-b87c-6bad3571c4b6", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. ", "original_text": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d15dbe9e-cafa-4b70-8e19-fe706ef74b50", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The y-axis is \"Dictionary size\" (50, 100, 200, 500, 750, 1000, 1500, 2000, 5000, 10000).  The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). ", "original_text": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b)."}, "hash": "e0791ad049369711dd9dcb17cefea4d1a39c2142ca4f60d681c17650cbba64d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bfc89a43-0fe1-402b-8f2a-5add362f5445", "node_type": "1", "metadata": {"window": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16. ", "original_text": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. "}, "hash": "06b3e76e6dba34b43a7685cfb8dd76c80525dd0bde26f0b8c82e1ec575c1099d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. ", "mimetype": "text/plain", "start_char_idx": 67434, "end_char_idx": 67532, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bfc89a43-0fe1-402b-8f2a-5add362f5445", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16. ", "original_text": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4571bb74-32bb-482e-b87c-6bad3571c4b6", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. ", "original_text": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083. "}, "hash": "34f03b733d27df1f8c15e5399369f815ec8077627a40e46d904fc5756f70930b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffa01ef3-3e7e-4f5c-8ee9-99576a476b41", "node_type": "1", "metadata": {"window": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. ", "original_text": "The x-axis is \"Score\". "}, "hash": "af74c48143bf5f92e1d5f1c62bfb3dd2f9923b6aba7cc9c01eb27c141a9f5d93", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. ", "mimetype": "text/plain", "start_char_idx": 67532, "end_char_idx": 67711, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ffa01ef3-3e7e-4f5c-8ee9-99576a476b41", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. ", "original_text": "The x-axis is \"Score\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bfc89a43-0fe1-402b-8f2a-5add362f5445", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The horizontal boxplots (orange for dataset (a), purple for dataset (b)) show that as the dictionary size increases, the variance of the score tends to stabilize.\n ***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16. ", "original_text": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives. "}, "hash": "3ccdb87e962db2507e488f51c9a89bd276dde128daf9621c66d25ae74519afef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "44863407-dfb2-4ae6-861b-c20b4646c9b0", "node_type": "1", "metadata": {"window": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n", "original_text": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n"}, "hash": "ddcec900aa5c736674b75dcceee29309b2da401f1a763d1c3e54ff1727d514af", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The x-axis is \"Score\". ", "mimetype": "text/plain", "start_char_idx": 67711, "end_char_idx": 67734, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "44863407-dfb2-4ae6-861b-c20b4646c9b0", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n", "original_text": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffa01ef3-3e7e-4f5c-8ee9-99576a476b41", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nEmpirical study of the FIF anomaly score for a variety of dictionaries with the L\u2082 scalar product of the derivatives is depicted in Figure 18.\n\n ***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. ", "original_text": "The x-axis is \"Score\". "}, "hash": "ba0c78c3b75fe74f85107338d981b296100680901062b89a333f5794a06e0eb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74836b5a-02a0-4112-bba1-c9f6ed7478c5", "node_type": "1", "metadata": {"window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1. ", "original_text": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). "}, "hash": "934834b4139d18215a44947a27e8d6f08d76a07023317c947eb4d4a3a37fbc81", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n", "mimetype": "text/plain", "start_char_idx": 67734, "end_char_idx": 67874, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74836b5a-02a0-4112-bba1-c9f6ed7478c5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1. ", "original_text": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "44863407-dfb2-4ae6-861b-c20b4646c9b0", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Figure 18: Boxplot (over 100 repetitions) of the FIF score for the observations x\u2080, x\u2081, x\u2082, x\u2083 for different dictionaries using the L\u2082 scalar product of the derivatives.  The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n", "original_text": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n"}, "hash": "56e2246c819a8607a87e0f73e5cfa22f7e93435552b71dd60e0be5a80c0cbca8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86adc609-0187-47c4-87cb-a8e4a50e9eb1", "node_type": "1", "metadata": {"window": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. ", "original_text": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. "}, "hash": "73e994d46abbb10401db9f10a87af8f309718635da365f1c37ad29aeed1645fd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). ", "mimetype": "text/plain", "start_char_idx": 67874, "end_char_idx": 68118, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86adc609-0187-47c4-87cb-a8e4a50e9eb1", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. ", "original_text": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74836b5a-02a0-4112-bba1-c9f6ed7478c5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The orange boxplots represent the dataset (a) while the purple boxplots represent the dataset (b). **\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1. ", "original_text": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14). "}, "hash": "e36c3cdc273bdc7a7c22e4b3c9fc1c31ce93705f76edb5f77e8854a1e8e9d414", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f415c06-3513-46dd-97ce-3235e07d8edc", "node_type": "1", "metadata": {"window": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n", "original_text": "A similar behavior is observed with increasing height limit l in Figure 16. "}, "hash": "322353552b152b794708a7d67196b667cfd9ccbd17c06eaa6dda57f278d3af96", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. ", "mimetype": "text/plain", "start_char_idx": 68118, "end_char_idx": 68457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f415c06-3513-46dd-97ce-3235e07d8edc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n", "original_text": "A similar behavior is observed with increasing height limit l in Figure 16. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86adc609-0187-47c4-87cb-a8e4a50e9eb1", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** This figure is similar to Figure 5, with four panels for X\u2080, X\u2081, X\u2082, and X\u2083.  It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. ", "original_text": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations. "}, "hash": "5427bd2d879992b55e3a54e9018c29681cacdd2036de261dc6ef1734a68a2425", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "071b9138-8a4f-4e84-a9a2-d569b7cb55df", "node_type": "1", "metadata": {"window": "The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. ", "original_text": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. "}, "hash": "5c754bf964a57f12b5f9bf50c841c585dfd7aa136c909e0d7d675e33d8687f8e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A similar behavior is observed with increasing height limit l in Figure 16. ", "mimetype": "text/plain", "start_char_idx": 68457, "end_char_idx": 68533, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "071b9138-8a4f-4e84-a9a2-d569b7cb55df", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. ", "original_text": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f415c06-3513-46dd-97ce-3235e07d8edc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "It compares the anomaly scores obtained with different dictionaries (Self, BB, UId, DId, Cos, B, MHW) on the y-axis, but this time using the L\u2082 scalar product on the derivatives.  The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n", "original_text": "A similar behavior is observed with increasing height limit l in Figure 16. "}, "hash": "f97e54c41e6947a7052cc65e228088f9a39e009c54ac6904d93503c4b34edc62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "819f3254-da83-478b-b90a-1abc06b4c643", "node_type": "1", "metadata": {"window": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. ", "original_text": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n"}, "hash": "2bff8eb4fd30dbedb75f65a03c0dce1769e324608eeb1cbdb73282f86c8eacb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. ", "mimetype": "text/plain", "start_char_idx": 68533, "end_char_idx": 68653, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "819f3254-da83-478b-b90a-1abc06b4c643", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. ", "original_text": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "071b9138-8a4f-4e84-a9a2-d569b7cb55df", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The x-axis is \"Score\".  The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. ", "original_text": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities. "}, "hash": "7ed133a2ed52f5d43c512b26b5dd8f5c7c5f6dd2a5783fefdc4b2692038eb016", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "516592cf-5a13-401b-a449-db618a16cf26", "node_type": "1", "metadata": {"window": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue. ", "original_text": "## E. Complementary results on the performance comparison\n\n### E.1. "}, "hash": "e5b567b073989d918ecc334ff626ff1045e8ccc199a1280874506bae9d9ad123", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n", "mimetype": "text/plain", "start_char_idx": 68653, "end_char_idx": 68915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "516592cf-5a13-401b-a449-db618a16cf26", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue. ", "original_text": "## E. Complementary results on the performance comparison\n\n### E.1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "819f3254-da83-478b-b90a-1abc06b4c643", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The boxplots (orange for dataset (a), purple for dataset (b)) show the performance of different dictionaries for detecting shape anomalies.\n ***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. ", "original_text": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n"}, "hash": "6eb1d6574cd1bbe553865518448977d1ab0b760fa09e6e991b405b448a0ce92d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "40d3106c-58f9-44e9-a891-73a2bb44af20", "node_type": "1", "metadata": {"window": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n", "original_text": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. "}, "hash": "647d6dde7b06b950c8011a2ab051e187ebb6339ad1100c93632787a02452bd11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## E. Complementary results on the performance comparison\n\n### E.1. ", "mimetype": "text/plain", "start_char_idx": 68915, "end_char_idx": 68983, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "40d3106c-58f9-44e9-a891-73a2bb44af20", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n", "original_text": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "516592cf-5a13-401b-a449-db618a16cf26", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n**Analysis of the results of Section C** In a first experiment, we show the boxplots of the score estimated by FIF when increasing the number of F-itrees and observe that, as expected, the variance diminishes when N grows (see Figure 14).  We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue. ", "original_text": "## E. Complementary results on the performance comparison\n\n### E.1. "}, "hash": "74fd66480d6cd40b699be8b5468721d3f36b01b7f97bc2ff8280085bbe4b994e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46aec724-8a97-48ec-985c-4799ab724a69", "node_type": "1", "metadata": {"window": "A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2. ", "original_text": "Anomalies are represented by blue color while normal data are drawn in red.\n\n"}, "hash": "d634985bb3d477f5e765e84ef46553e2834deef72d261b47391ec1f1f0dd8f99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. ", "mimetype": "text/plain", "start_char_idx": 68983, "end_char_idx": 69079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46aec724-8a97-48ec-985c-4799ab724a69", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2. ", "original_text": "Anomalies are represented by blue color while normal data are drawn in red.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40d3106c-58f9-44e9-a891-73a2bb44af20", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "We also see in Figure 15 that with an increasing subsample size \u03c8 the FIF anomaly score increases for anomalies since these are more often present in the subsample and thus isolated faster (with shorter path length) when calculating the score than when they were absent in the subsample; this effect is reciprocal for normal observations.  A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n", "original_text": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment. "}, "hash": "4af6cc4e8148704da9a4a867bcd374650520a3d14158d15a842b625b6a46f3ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "50172132-b6be-45ec-b3dc-16b00f239efb", "node_type": "1", "metadata": {"window": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. ", "original_text": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. "}, "hash": "3f60ef8ad95e46ce4b31d2c9a721cb3e82c1ea36560447bc027a1d1063e2b556", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Anomalies are represented by blue color while normal data are drawn in red.\n\n", "mimetype": "text/plain", "start_char_idx": 69079, "end_char_idx": 69156, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "50172132-b6be-45ec-b3dc-16b00f239efb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. ", "original_text": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46aec724-8a97-48ec-985c-4799ab724a69", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A similar behavior is observed with increasing height limit l in Figure 16.  The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2. ", "original_text": "Anomalies are represented by blue color while normal data are drawn in red.\n\n"}, "hash": "518f0989a3bc58fee50ecd379414eef826f783008bdc6046879a606ccaec5259", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5e4304a-5306-43a9-b643-9e28a495ff4a", "node_type": "1", "metadata": {"window": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. ", "original_text": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. "}, "hash": "91852942b19a763fd513d1caade79725c17ebb74d37cc03f6148766fb8a0b2dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. ", "mimetype": "text/plain", "start_char_idx": 69156, "end_char_idx": 69281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5e4304a-5306-43a9-b643-9e28a495ff4a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. ", "original_text": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "50172132-b6be-45ec-b3dc-16b00f239efb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The variance of the score tends to slightly increase with \u03c8 and l because of more observations/branching possibilities.  If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. ", "original_text": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments. "}, "hash": "cce23f8fe302b585133b10101a55bee94817747ea014dc441fdc22a0caf84a03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "753f577b-7dd7-469e-8959-ba2d6562ee13", "node_type": "1", "metadata": {"window": "## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al. ", "original_text": "In each plot, normal time series are shown in red and anomalous time series are shown in blue. "}, "hash": "30c155910cb993b8b9bdb0d25c3c2d51bda12f6495c48be3f8cf4e08ea7355f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. ", "mimetype": "text/plain", "start_char_idx": 69281, "end_char_idx": 69457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "753f577b-7dd7-469e-8959-ba2d6562ee13", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al. ", "original_text": "In each plot, normal time series are shown in red and anomalous time series are shown in blue. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5e4304a-5306-43a9-b643-9e28a495ff4a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "If the dictionary is sufficiently rich, its size does not influence the FIF anomaly score and its variance stabilizes relatively fast while growing the size of the dictionary (see Figure 17) which encourages the use of massive (and infinite size) dictionaries.\n\n ## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. ", "original_text": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000. "}, "hash": "05d0a61a4acd478120d3eff1dba688e78152fec6e7fdfa2fe5ed17c868ca1e0c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "46ba067d-a60a-4922-b523-446fd573e19b", "node_type": "1", "metadata": {"window": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al. ", "original_text": "This provides a visual reference for the data used in the benchmarking study.\n"}, "hash": "b09f2f55707e7b3980ab26f22b9cd1cfe2c645ebb37014402a045d4bab29b60e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In each plot, normal time series are shown in red and anomalous time series are shown in blue. ", "mimetype": "text/plain", "start_char_idx": 69457, "end_char_idx": 69552, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "46ba067d-a60a-4922-b523-446fd573e19b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al. ", "original_text": "This provides a visual reference for the data used in the benchmarking study.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "753f577b-7dd7-469e-8959-ba2d6562ee13", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "## E. Complementary results on the performance comparison\n\n### E.1.  benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al. ", "original_text": "In each plot, normal time series are shown in red and anomalous time series are shown in blue. "}, "hash": "67e20836ef95ee8e4e07d0f9f4db3b1b868fc37bcc70a6a904272847d3e6a88b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7be2eaac-5d47-49c1-880f-e6b13c1499e3", "node_type": "1", "metadata": {"window": "Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n", "original_text": "***\n\n### E.2. "}, "hash": "53af74b7e8552e4f830124aa678505261d6e7806232c41acd54a26ecb4219d46", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This provides a visual reference for the data used in the benchmarking study.\n", "mimetype": "text/plain", "start_char_idx": 69552, "end_char_idx": 69630, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7be2eaac-5d47-49c1-880f-e6b13c1499e3", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n", "original_text": "***\n\n### E.2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "46ba067d-a60a-4922-b523-446fd573e19b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "benchmark datasets\n\nHere, we plot the thirteen benchmark train datasets used in the experiment.  Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al. ", "original_text": "This provides a visual reference for the data used in the benchmarking study.\n"}, "hash": "3e0f16a009fa29e0b8ab56e9f245165d3245572d53bb359481bd7c9621419114", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb662628-85fb-4253-bf2d-6c5d8961d7ff", "node_type": "1", "metadata": {"window": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods.", "original_text": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. "}, "hash": "a6dd5e88d6455c7451cea7984ec3a330ade4c1757c3a1934df252787370f0ce7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\n### E.2. ", "mimetype": "text/plain", "start_char_idx": 69630, "end_char_idx": 69644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eb662628-85fb-4253-bf2d-6c5d8961d7ff", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods.", "original_text": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7be2eaac-5d47-49c1-880f-e6b13c1499e3", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Anomalies are represented by blue color while normal data are drawn in red.\n\n ***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n", "original_text": "***\n\n### E.2. "}, "hash": "5646a98638218f6d80cb8319d8cd682d23e0fd2e457e45e86ef9b9a77caa58f2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4884c004-4493-4091-8768-155756f0c676", "node_type": "1", "metadata": {"window": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. ", "original_text": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. "}, "hash": "78d49590f7196fc926b16ebda21ed4ebe53b0276248393920ac95369dd7f96cf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. ", "mimetype": "text/plain", "start_char_idx": 69644, "end_char_idx": 69872, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4884c004-4493-4091-8768-155756f0c676", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. ", "original_text": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eb662628-85fb-4253-bf2d-6c5d8961d7ff", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n**Description:** The figure contains 13 small plots, each showing a different benchmark dataset used in the experiments.  The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods.", "original_text": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance. "}, "hash": "902e739d273c5fdb3c9b4c203beb726999ba4f9619e3fa51af8babaf5dafb6b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c34c8b70-18e5-428b-ad1a-c622f4ff5b84", "node_type": "1", "metadata": {"window": "In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. ", "original_text": "The reader is referred to Cuevas et al. "}, "hash": "2bfb5a9ae0b7bd4ec1f60088387cab5c4f741128d7e1521bbefe924d166c8822", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. ", "mimetype": "text/plain", "start_char_idx": 69872, "end_char_idx": 70228, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c34c8b70-18e5-428b-ad1a-c622f4ff5b84", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. ", "original_text": "The reader is referred to Cuevas et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4884c004-4493-4091-8768-155756f0c676", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The datasets are: Chinatown, Coffee, ECGFiveDays, ECG200, Handoutlines, SonyRobotAI1, SonyRobotAI2, StarLightCurves, TwoLeadECG, Yoga, EOGHorizontal, CinECGTorso, and ECG5000.  In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. ", "original_text": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*. "}, "hash": "3d87f77dcb93409ae3d79f4b81645926847eab8f66e244adb554a2eaa91653d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7eeda4bd-65bf-4048-85e1-fa6c59cba896", "node_type": "1", "metadata": {"window": "This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "original_text": "(2007); Fraiman and Muniz (2001); Hubert et al. "}, "hash": "a4cac8f99611606f3e890634071ba1f543b97d7eec7733442928f5ee6a8513cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The reader is referred to Cuevas et al. ", "mimetype": "text/plain", "start_char_idx": 70228, "end_char_idx": 70268, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7eeda4bd-65bf-4048-85e1-fa6c59cba896", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "original_text": "(2007); Fraiman and Muniz (2001); Hubert et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c34c8b70-18e5-428b-ad1a-c622f4ff5b84", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "In each plot, normal time series are shown in red and anomalous time series are shown in blue.  This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. ", "original_text": "The reader is referred to Cuevas et al. "}, "hash": "313a0cfe36d89f72d102f0666ea542befbdf8c21f62e0afd53e951eb3c6a50ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0f1a124-3226-4d83-851b-eab33c2e1bc5", "node_type": "1", "metadata": {"window": "***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary. ", "original_text": "(2015) for the bibliography on employed functional data depth notions.\n\n"}, "hash": "b763179b2ac1f737d052a30a2115799d7bf331d2472cb137bc78f8fbaba294d4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2007); Fraiman and Muniz (2001); Hubert et al. ", "mimetype": "text/plain", "start_char_idx": 70268, "end_char_idx": 70316, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0f1a124-3226-4d83-851b-eab33c2e1bc5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary. ", "original_text": "(2015) for the bibliography on employed functional data depth notions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7eeda4bd-65bf-4048-85e1-fa6c59cba896", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This provides a visual reference for the data used in the benchmarking study.\n ***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "original_text": "(2007); Fraiman and Muniz (2001); Hubert et al. "}, "hash": "9ff0975dc1dc706eab3e86e2a644d1acdcde578cf022cb861b8f4cdc567564ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "85314ae9-08db-42e5-807f-8a1a19da1bcb", "node_type": "1", "metadata": {"window": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) ", "original_text": "**Table of AUC results for various functional depth methods."}, "hash": "a5db0e1d1e3a3b165bc8a67aaa01091abfe3ebeb0f5490fe55259a2b2905bec6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2015) for the bibliography on employed functional data depth notions.\n\n", "mimetype": "text/plain", "start_char_idx": 70316, "end_char_idx": 70388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "85314ae9-08db-42e5-807f-8a1a19da1bcb", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) ", "original_text": "**Table of AUC results for various functional depth methods."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0f1a124-3226-4d83-851b-eab33c2e1bc5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\n### E.2.  Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary. ", "original_text": "(2015) for the bibliography on employed functional data depth notions.\n\n"}, "hash": "48d1d9bce8e89ee28bc3c6ad5364e3cacb66207ddf9b311c091642c21aab33f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a95512b7-a3df-45b1-9ae0-b249966fa957", "node_type": "1", "metadata": {"window": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis.", "original_text": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. "}, "hash": "a52b7e1f6e8d9c834edee3644b5dcfde933ecfe7f9ecfeab7db413e593a64cf0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**Table of AUC results for various functional depth methods.", "mimetype": "text/plain", "start_char_idx": 70388, "end_char_idx": 70448, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a95512b7-a3df-45b1-9ae0-b249966fa957", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis.", "original_text": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "85314ae9-08db-42e5-807f-8a1a19da1bcb", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Functional depth\n\nIn this part, we present further 8 functional depths which are outperformed (on an average) by the two depth functions presented in Section 4 on the 13 real-world datasets and we display their AUC performance.  SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) ", "original_text": "**Table of AUC results for various functional depth methods."}, "hash": "b0f8cb0d52a831fcbc3429ea01b1051f030897126e12651455a6f8ae4649ad9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1993f75-1cea-46aa-ac0b-962a5af86cce", "node_type": "1", "metadata": {"window": "The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4. ", "original_text": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. "}, "hash": "a2859a876c8c91a4e6e20e08e474c2d6da9a2da68864a25687ef92f1f1340a71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. ", "mimetype": "text/plain", "start_char_idx": 70448, "end_char_idx": 71491, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f1993f75-1cea-46aa-ac0b-962a5af86cce", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4. ", "original_text": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a95512b7-a3df-45b1-9ae0-b249966fa957", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "SFD corresponds to *simplicial integrated depth*, HFD to *Halfspace integrated depth*, RP-SD to the *random projection method with simplicial depth*, RP-RHD to the *random projection method with random halfspace depth*, fAO to the *functional adjusted outlyingness*, fDO to the *functional directional outlyingness* to and fbd to *functional bagdistance*.  The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis.", "original_text": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3. "}, "hash": "91651b43f1b79259f032e93c982141e08ebc4a863923764c1660742f0f06c48b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bc3c8a8f-3c68-419e-b4cd-6c24b26f9ac4", "node_type": "1", "metadata": {"window": "(2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. ", "original_text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n"}, "hash": "23087205b92b9c589729ac612d0de2f0def7f93152030140fa8cc2c0de4a66e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. ", "mimetype": "text/plain", "start_char_idx": 71491, "end_char_idx": 71712, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bc3c8a8f-3c68-419e-b4cd-6c24b26f9ac4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. ", "original_text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1993f75-1cea-46aa-ac0b-962a5af86cce", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The reader is referred to Cuevas et al.  (2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4. ", "original_text": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis. "}, "hash": "44b9edc7874f40c92741b64d046d2ea99cb008a8f4d224266c813b13b889cbe0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3708fbbc-28ce-4b59-a135-1752c70b413e", "node_type": "1", "metadata": {"window": "(2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "original_text": "*(Due to the extreme size and density of the following tables, they are omitted in this summary. "}, "hash": "042e41527c61cbac2b2b91317a088d24201a206833574ba399d7e2d4fc0ae3dd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "mimetype": "text/plain", "start_char_idx": 71712, "end_char_idx": 71838, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3708fbbc-28ce-4b59-a135-1752c70b413e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "original_text": "*(Due to the extreme size and density of the following tables, they are omitted in this summary. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bc3c8a8f-3c68-419e-b4cd-6c24b26f9ac4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2007); Fraiman and Muniz (2001); Hubert et al.  (2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. ", "original_text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n"}, "hash": "c212b9fe7c6e22d17eea38b23e220a76f08a8696a9d00f733d88d423d0d384c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "853997a0-4e38-40a6-8b3a-66f4c4fd9aaa", "node_type": "1", "metadata": {"window": "**Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. ", "original_text": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) "}, "hash": "e337da18da49c408fded79ef5259f547190f67751c38c259143e49cd7a9999e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(Due to the extreme size and density of the following tables, they are omitted in this summary. ", "mimetype": "text/plain", "start_char_idx": 71838, "end_char_idx": 71935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "853997a0-4e38-40a6-8b3a-66f4c4fd9aaa", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. ", "original_text": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3708fbbc-28ce-4b59-a135-1752c70b413e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2015) for the bibliography on employed functional data depth notions.\n\n **Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "original_text": "*(Due to the extreme size and density of the following tables, they are omitted in this summary. "}, "hash": "ff9a65470e6642780a911e7bb3ba2c3a19db51d8aabac4cea31f3cc865fd5afd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ff509fd-4f2a-4788-ae9b-a8543e5a14fa", "node_type": "1", "metadata": {"window": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step.", "original_text": "and Fourier basis, reporting the AUC for each combination of dataset and basis."}, "hash": "b5b920e47d9da73b968c028e6fac8a021c071adf8ad47e524e4d9245b362b1f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) ", "mimetype": "text/plain", "start_char_idx": 71935, "end_char_idx": 72096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ff509fd-4f2a-4788-ae9b-a8543e5a14fa", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step.", "original_text": "and Fourier basis, reporting the AUC for each combination of dataset and basis."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "853997a0-4e38-40a6-8b3a-66f4c4fd9aaa", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**Table of AUC results for various functional depth methods. **\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. ", "original_text": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.) "}, "hash": "ae3d09754754aa094145df36f11e30414006e814dbae628eb82bb45422bb0ef5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b33c6c28-944e-4469-9f45-fdd8d2671850", "node_type": "1", "metadata": {"window": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). ", "original_text": ")*\n\n### E.4. "}, "hash": "61ff675ea8ed391a182221f57006453dd28db0810151fd323e0750017f4a5735", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "and Fourier basis, reporting the AUC for each combination of dataset and basis.", "mimetype": "text/plain", "start_char_idx": 72096, "end_char_idx": 72175, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b33c6c28-944e-4469-9f45-fdd8d2671850", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). ", "original_text": ")*\n\n### E.4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ff509fd-4f2a-4788-ae9b-a8543e5a14fa", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n| Datasets | S-FD | H-FD | Modal | RP-SD | RP-RHD | fAO | fDO | fbd |\n|---|---|---|---|---|---|---|---|---|\n| Chinatown | 0.74 | 0.77 | 0.75 | 0.77 | 0.73 | 0.70 | 0.83 | 0.83 |\n| Coffee | 0.60 | 0.59 | 0.69 | 0.70 | 0.51 | 0.53 | 0.59 | 0.60 |\n| ECGFiveDays | 0.65 | 0.64 | 0.60 | 0.64 | 0.56 | 0.72 | 0.76 | 0.80 |\n| ECG200 | 0.82 | 0.82 | 0.84 | 0.85 | 0.74 | 0.78 | 0.82 | 0.82 |\n| Handoutlines | 0.70 | 0.70 | 0.75 | 0.72 | 0.63 | 0.60 | 0.71 | 0.73 |\n| SonyRobotAI1 | 0.89 | 0.89 | 0.94 | 0.83 | 0.62 | 0.90 | 0.93 | 0.93 |\n| SonyRobotAI2 | 0.82 | 0.82 | 0.92 | 0.86 | 0.71 | 0.80 | 0.82 | 0.80 |\n| StarLightCurves | 0.80 | 0.80 | 0.85 | 0.78 | 0.68 | 0.80 | 0.82 | 0.83 |\n| TwoLeadECG | 0.68 | 0.67 | 0.68 | 0.66 | 0.60 | 0.67 | 0.69 | 0.69 |\n| Yoga | 0.55 | 0.53 | 0.57 | 0.57 | 0.54 | 0.54 | 0.55 | 0.56 |\n| EOGHorizontal | 0.59 | 0.52 | 0.84 | 0.74 | 0.64 | 0.53 | 0.59 | 0.66 |\n| CinECGTorso | 0.69 | 0.69 | 0.73 | 0.62 | 0.66 | 0.85 | 0.83 | 0.79 |\n| ECG5000 | 0.90 | 0.90 | 0.92 | 0.92 | 0.84 | 0.87 | 0.92 | 0.92 |\n\n### E.3.  Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step.", "original_text": "and Fourier basis, reporting the AUC for each combination of dataset and basis."}, "hash": "46f0e9176fc52c769f84c8c484e46d369dddcb4a85b8c1aa6484f3cf78c5892c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "813f183d-a25c-47d3-bc64-1c287d31430e", "node_type": "1", "metadata": {"window": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves. ", "original_text": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. "}, "hash": "0fa7acef1ac508fb74acf08abb41ab2a565ead25523c32d5a2513c24c8dbf2c4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n\n### E.4. ", "mimetype": "text/plain", "start_char_idx": 72175, "end_char_idx": 72188, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "813f183d-a25c-47d3-bc64-1c287d31430e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves. ", "original_text": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b33c6c28-944e-4469-9f45-fdd8d2671850", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Isolation forest after dimension reduction by filtering methods on the benchmark datasets\n\nHere, we show the results of the filtering approach using 106 bases from the **PyWavelets** python library and the Fourier basis.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). ", "original_text": ")*\n\n### E.4. "}, "hash": "1668ed25139ca7a053d20f900b791feb9b4741d7e6ad07ecd9c2dd476e3b345b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c51b7977-9d2e-4d72-982c-825fb7f99cfc", "node_type": "1", "metadata": {"window": "*(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. ", "original_text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n"}, "hash": "f5843fd20639866e8f0b89bf04e130207bdc3af3b470fe3951672aa6187b06ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. ", "mimetype": "text/plain", "start_char_idx": 72188, "end_char_idx": 72417, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c51b7977-9d2e-4d72-982c-825fb7f99cfc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. ", "original_text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "813f183d-a25c-47d3-bc64-1c287d31430e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves. ", "original_text": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage. "}, "hash": "20f25c2a7e29e72a10b7279d2b27c0bf0ccb0161f37d36dbfb0819ca3e0210c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5b8eef6-8c61-46ee-858d-d4f002387c44", "node_type": "1", "metadata": {"window": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies.", "original_text": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. "}, "hash": "7da18eaf2e1c987501a5a8fe9113d8d70146e4f78ec87a7464c311482761780c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n", "mimetype": "text/plain", "start_char_idx": 72417, "end_char_idx": 72543, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e5b8eef6-8c61-46ee-858d-d4f002387c44", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies.", "original_text": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c51b7977-9d2e-4d72-982c-825fb7f99cfc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*(Due to the extreme size and density of the following tables, they are omitted in this summary.  They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. ", "original_text": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n"}, "hash": "b65b00b4e323245a692d2c3cdee96a92fc9ce37daf49cb5f11bf68cf666dd5e7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5695d632-89d1-45cf-9583-9c226e73aaa7", "node_type": "1", "metadata": {"window": "and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts. ", "original_text": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step."}, "hash": "431544f4ec861066dacdce72b5e5543020380780b6c12cbe4102ebf7a175ea36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. ", "mimetype": "text/plain", "start_char_idx": 72543, "end_char_idx": 72646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5695d632-89d1-45cf-9583-9c226e73aaa7", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts. ", "original_text": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e5b8eef6-8c61-46ee-858d-d4f002387c44", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "They present a comprehensive benchmark of using standard Isolation Forest after projecting the functional data onto various wavelet bases (bior, coif, db, etc.)  and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies.", "original_text": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size. "}, "hash": "08ed58c1f408a865f1ca51add678d9bca775bdb045e3fe29690a04b4d5559901", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b95b8f7-369e-4a81-92b3-28ddf4dda7b4", "node_type": "1", "metadata": {"window": ")*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. ", "original_text": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). "}, "hash": "0c368369c0fa7ffbe0d529aa71c3bdd3369b078f90f9709e74653e072fba21ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step.", "mimetype": "text/plain", "start_char_idx": 72646, "end_char_idx": 72819, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9b95b8f7-369e-4a81-92b3-28ddf4dda7b4", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ")*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. ", "original_text": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5695d632-89d1-45cf-9583-9c226e73aaa7", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "and Fourier basis, reporting the AUC for each combination of dataset and basis. )*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts. ", "original_text": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step."}, "hash": "b39d4f749a9d975e5fd72a8dda9b8e7444d11f54c99bf2db9fe5277a88e506bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6723bf1-3d76-4e1f-830e-5aa02488ec77", "node_type": "1", "metadata": {"window": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold. ", "original_text": "Left plot corresponds to the sorted score of these curves. "}, "hash": "71cdc1efa6b4b60edde0fb7acf72bd09f133e4a19be481038732ced53d36c14d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). ", "mimetype": "text/plain", "start_char_idx": 72819, "end_char_idx": 72976, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b6723bf1-3d76-4e1f-830e-5aa02488ec77", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold. ", "original_text": "Left plot corresponds to the sorted score of these curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b95b8f7-369e-4a81-92b3-28ddf4dda7b4", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ")*\n\n### E.4.  IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. ", "original_text": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two). "}, "hash": "357d0388c7a99ce822d2ad1dc22f042975acc01d9dccad23ceb3c978ceb7ad94", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60e05c2a-0f76-47de-ae99-1f46b6fde07b", "node_type": "1", "metadata": {"window": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space. ", "original_text": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. "}, "hash": "e849b42c92873c9b57a9fb9575d75e815a8ad9aeb8a7e73200c2f642e414909f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Left plot corresponds to the sorted score of these curves. ", "mimetype": "text/plain", "start_char_idx": 72976, "end_char_idx": 73035, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60e05c2a-0f76-47de-ae99-1f46b6fde07b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space. ", "original_text": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6723bf1-3d76-4e1f-830e-5aa02488ec77", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "IF/FPCA with different filtering step on the benchmark datasets\n\nHere, we show the results of the FPCA approach using the Fourier basis and 106 further bases from the **PyWavelets** python library as preliminary filtering stage.  Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold. ", "original_text": "Left plot corresponds to the sorted score of these curves. "}, "hash": "4199219e6696139bd9ad41478c18004acd3f74bdc5f7e3c7f19b70afbce753c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c1b64da-a549-4759-b16e-ca61e5576a13", "node_type": "1", "metadata": {"window": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. ", "original_text": "Bottom plot shows the fifteen detected anomalies."}, "hash": "e8d5de25c7d06a6b8430fd091498bf12163d6faa9e6b780665120bbd3b5717d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. ", "mimetype": "text/plain", "start_char_idx": 73035, "end_char_idx": 73178, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c1b64da-a549-4759-b16e-ca61e5576a13", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. ", "original_text": "Bottom plot shows the fifteen detected anomalies."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60e05c2a-0f76-47de-ae99-1f46b6fde07b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Afterwards, we apply (multivariate) Isolation Forest on the coefficients of the projections and display the AUC performance.\n\n *(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space. ", "original_text": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red. "}, "hash": "81df50cdf6306a38fcaf4baf9b6a9739b197dd3368ce2704e8ea2d03c3fa120c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c118091-4bb5-45fb-871f-56c1204026ad", "node_type": "1", "metadata": {"window": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores. ", "original_text": "**\n\n**Description:** The figure has three parts. "}, "hash": "0b7b6dc426e451968740e64d214b74a1b2adf6ac8b746c9939231f9d2f65061f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bottom plot shows the fifteen detected anomalies.", "mimetype": "text/plain", "start_char_idx": 73178, "end_char_idx": 73227, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7c118091-4bb5-45fb-871f-56c1204026ad", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores. ", "original_text": "**\n\n**Description:** The figure has three parts. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c1b64da-a549-4759-b16e-ca61e5576a13", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "*(Similar to the previous section, the tables for IF/FPCA results are omitted due to their large size.  They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. ", "original_text": "Bottom plot shows the fifteen detected anomalies."}, "hash": "e12addf259247abc009c8724446cf1e68edc412272fa989be84189c6dd8d1d16", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22247df0-4a7b-4465-bab9-0694e99ce4dc", "node_type": "1", "metadata": {"window": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n", "original_text": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. "}, "hash": "c7d0cba486ae3aa210b3850ce5a260f53423ad6b194986e24abd6ca22ab6bf1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n**Description:** The figure has three parts. ", "mimetype": "text/plain", "start_char_idx": 73227, "end_char_idx": 73276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22247df0-4a7b-4465-bab9-0694e99ce4dc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n", "original_text": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c118091-4bb5-45fb-871f-56c1204026ad", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "They detail the AUC performance of Isolation Forest combined with Functional Principal Component Analysis, using a wide variety of basis functions for the initial FPCA step. )*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores. ", "original_text": "**\n\n**Description:** The figure has three parts. "}, "hash": "886e7d9b9bd7f263c32f685f61ed1bd0f7bc89434413e11a28b5171ff61a602f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d1ab6cd1-324b-439d-9b2d-a1741b0d2412", "node_type": "1", "metadata": {"window": "Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e. ", "original_text": "A red line indicates a threshold. "}, "hash": "87b1f5cc5158f646b69b256ef737b56c50c961123a933b7a855ddd7e5e6b6d17", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. ", "mimetype": "text/plain", "start_char_idx": 73276, "end_char_idx": 73425, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d1ab6cd1-324b-439d-9b2d-a1741b0d2412", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e. ", "original_text": "A red line indicates a threshold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22247df0-4a7b-4465-bab9-0694e99ce4dc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": ")*\n\n## F. Multivariate Functional Isolation Forest and depth mapping\n\n***\n**Figure 19: FIF anomaly scores for a sample of 110 digits (100 seven and 10 two).  Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n", "original_text": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data. "}, "hash": "2019e7df5ffb42a56788f00832f9790059d6a2d1e35d8e55e42363dc0ae458d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "604a7827-b9bc-44a6-bbf4-bc2c77817266", "node_type": "1", "metadata": {"window": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n", "original_text": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space. "}, "hash": "dc67e62b7d3fa5637c6928df189a03c0abad9a03fc65708108e709ab7f96bd69", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A red line indicates a threshold. ", "mimetype": "text/plain", "start_char_idx": 73425, "end_char_idx": 73459, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "604a7827-b9bc-44a6-bbf4-bc2c77817266", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n", "original_text": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1ab6cd1-324b-439d-9b2d-a1741b0d2412", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Left plot corresponds to the sorted score of these curves.  Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e. ", "original_text": "A red line indicates a threshold. "}, "hash": "41b662eec5a68389d068cd7d466727234421c9e2302033706d67bbe0777daa44", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2f20bfb-cd68-4ef1-b043-d541a12988c5", "node_type": "1", "metadata": {"window": "Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d. ", "original_text": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. "}, "hash": "14f9afa69a0433e3d74d2efdc2c293c090276b9a898e351d95aabe1b4c9682ee", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space. ", "mimetype": "text/plain", "start_char_idx": 73459, "end_char_idx": 73550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2f20bfb-cd68-4ef1-b043-d541a12988c5", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d. ", "original_text": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "604a7827-b9bc-44a6-bbf4-bc2c77817266", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Right plot represents the digits in three dimensions, green ones correspond to normal data, anomalies score increases from orange to dark red.  Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n", "original_text": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space. "}, "hash": "2746a240228c7f847e23c5fa19729ad32e8e7901e7badc0d05d10418429f3a20", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2addceee-5276-45a4-b926-2e7d26cf0a79", "node_type": "1", "metadata": {"window": "**\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. ", "original_text": "The bottom part shows images of the 15 curves with the highest anomaly scores. "}, "hash": "52dc7839c21cbe37810b58a86c7d00c19ada57657ef6ac7ecc9e795a73a37f3b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. ", "mimetype": "text/plain", "start_char_idx": 73550, "end_char_idx": 73685, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2addceee-5276-45a4-b926-2e7d26cf0a79", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. ", "original_text": "The bottom part shows images of the 15 curves with the highest anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2f20bfb-cd68-4ef1-b043-d541a12988c5", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Bottom plot shows the fifteen detected anomalies. **\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d. ", "original_text": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score. "}, "hash": "3e647c7cff0ce114fc145340118dab7abd6f56adc5e6df092c33985d8a8adada", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c806a84-ecc4-4e07-a7bb-f99620fc42e2", "node_type": "1", "metadata": {"window": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. ", "original_text": "These include all ten '2's and five unusually shaped '7's.\n"}, "hash": "842eafe70338e53d89bb5f4f5510406a4cc3f49940932a25172c0599d1ea5352", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom part shows images of the 15 curves with the highest anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 73685, "end_char_idx": 73764, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c806a84-ecc4-4e07-a7bb-f99620fc42e2", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. ", "original_text": "These include all ten '2's and five unusually shaped '7's.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2addceee-5276-45a4-b926-2e7d26cf0a79", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "**\n\n**Description:** The figure has three parts.  The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. ", "original_text": "The bottom part shows images of the 15 curves with the highest anomaly scores. "}, "hash": "f04d627e780da307f0341f0e4509976032b06e5a31fca6fcf46fb89d814b65a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05a2b67e-3f83-4d46-963a-d7c6f21602bc", "node_type": "1", "metadata": {"window": "A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset. ", "original_text": "***\n\nFIF can be easily extended to the multivariate functional data, i.e. "}, "hash": "1fa6a1be6ad841ce4ab03cb629143fd7af51d34da2715db4700b470f48808665", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These include all ten '2's and five unusually shaped '7's.\n", "mimetype": "text/plain", "start_char_idx": 73764, "end_char_idx": 73823, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05a2b67e-3f83-4d46-963a-d7c6f21602bc", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset. ", "original_text": "***\n\nFIF can be easily extended to the multivariate functional data, i.e. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c806a84-ecc4-4e07-a7bb-f99620fc42e2", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The top-left is a scatter plot of sorted anomaly scores, showing a clear \"elbow\" separating the high-score anomalies from the low-score normal data.  A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. ", "original_text": "These include all ten '2's and five unusually shaped '7's.\n"}, "hash": "56f0f8f8db6206af057773df9c4a5deae0aaf1c9788106b51a71e2c49394ddc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1dac8c21-74d0-41b3-856c-569da44fcfae", "node_type": "1", "metadata": {"window": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. ", "original_text": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n"}, "hash": "f0d16d42bc6e54442e2cc41ca0b72dbcf2105c1ed706586adda277028e1fec82", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n\nFIF can be easily extended to the multivariate functional data, i.e. ", "mimetype": "text/plain", "start_char_idx": 73823, "end_char_idx": 73897, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1dac8c21-74d0-41b3-856c-569da44fcfae", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. ", "original_text": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05a2b67e-3f83-4d46-963a-d7c6f21602bc", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "A red line indicates a threshold.  The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset. ", "original_text": "***\n\nFIF can be easily extended to the multivariate functional data, i.e. "}, "hash": "7a076c296c108ef646bc80f6be39935a1630016331b31b92bcf3879e8744e299", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daf3adec-621f-4a90-9f67-f8b83ab3b34e", "node_type": "1", "metadata": {"window": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014). ", "original_text": "Further, a dictionary should be defined in (H([0,1]))^\u2297d. "}, "hash": "91a0aacd9164839787ddd33444a23e996a7ed52fd6bf01cc0c78183c0a6267a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n", "mimetype": "text/plain", "start_char_idx": 73897, "end_char_idx": 74218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "daf3adec-621f-4a90-9f67-f8b83ab3b34e", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014). ", "original_text": "Further, a dictionary should be defined in (H([0,1]))^\u2297d. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1dac8c21-74d0-41b3-856c-569da44fcfae", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The top-right is a 3D plot showing the functional data (digit contours) embedded in space.  Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. ", "original_text": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n"}, "hash": "e090bf8ca0173c92945a6182384f614116500f96497a57c34f9418685133f1c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99b597da-5597-40de-a0e3-c300f783665f", "node_type": "1", "metadata": {"window": "The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. ", "original_text": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. "}, "hash": "df83c1bf83d0096f33ccb748351e05693c424d3e41a4ca3e2dbc4145ff448e24", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Further, a dictionary should be defined in (H([0,1]))^\u2297d. ", "mimetype": "text/plain", "start_char_idx": 74218, "end_char_idx": 74276, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99b597da-5597-40de-a0e3-c300f783665f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. ", "original_text": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "daf3adec-621f-4a90-9f67-f8b83ab3b34e", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Normal data ('7's) are green, while anomalies ('2's and abnormal '7's) are colored on a scale from orange to red based on their score.  The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014). ", "original_text": "Further, a dictionary should be defined in (H([0,1]))^\u2297d. "}, "hash": "ed1922f866ab4b66e2466c5552469a5d52fc8be8088a0729fd00f96b031f9716", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8760094d-c25b-45f4-ad49-90aefc907c6a", "node_type": "1", "metadata": {"window": "These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. ", "original_text": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. "}, "hash": "4fcb859ef118cbc2e51f833d250efde392f1c158b76d557a962b75f1311aa1e8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. ", "mimetype": "text/plain", "start_char_idx": 74276, "end_char_idx": 74447, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8760094d-c25b-45f4-ad49-90aefc907c6a", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. ", "original_text": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99b597da-5597-40de-a0e3-c300f783665f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "The bottom part shows images of the 15 curves with the highest anomaly scores.  These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. ", "original_text": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones. "}, "hash": "658afaaf3f6ff8deb9cc3777a4e7b89b33171e4cfd695f8f5a7bafd458a6cb6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a5c9f23-33b5-4a84-a691-f4bfcda44d2f", "node_type": "1", "metadata": {"window": "***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. ", "original_text": "(1998) dataset. "}, "hash": "2a3593161cb89b4c0e2d8a98f290286dec02e4e34034f46934d10d0f6b6a0803", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. ", "mimetype": "text/plain", "start_char_idx": 74447, "end_char_idx": 74547, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7a5c9f23-33b5-4a84-a691-f4bfcda44d2f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. ", "original_text": "(1998) dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8760094d-c25b-45f4-ad49-90aefc907c6a", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "These include all ten '2's and five unusually shaped '7's.\n ***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. ", "original_text": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al. "}, "hash": "acbf031aa6e90102bb02ceadeec9238568941456453b03ecce22bc4979228dc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c656842b-0856-4b55-86a1-7467404a246f", "node_type": "1", "metadata": {"window": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. ", "original_text": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. "}, "hash": "d0e44519aac871c319aad1ecd8832ead287ed605f5b1e8dbf84085e25297de8b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(1998) dataset. ", "mimetype": "text/plain", "start_char_idx": 74547, "end_char_idx": 74563, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c656842b-0856-4b55-86a1-7467404a246f", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. ", "original_text": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a5c9f23-33b5-4a84-a691-f4bfcda44d2f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "***\n\nFIF can be easily extended to the multivariate functional data, i.e.  when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. ", "original_text": "(1998) dataset. "}, "hash": "ba0621be0fec02aab0a7d2633736088261c197b5a77802cf0c818fc19bc237c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a27203e2-98ce-44ea-a177-b981a5614b8b", "node_type": "1", "metadata": {"window": "Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. ", "original_text": "(2014). "}, "hash": "08e89142f5af34ed3dde41408436a0ca1d13c279ed88a2a74a52f3b34c2820e3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. ", "mimetype": "text/plain", "start_char_idx": 74563, "end_char_idx": 74663, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a27203e2-98ce-44ea-a177-b981a5614b8b", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. ", "original_text": "(2014). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c656842b-0856-4b55-86a1-7467404a246f", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "when the quantity of interest lies in R^d for each moment of time:\n\nX : \u03a9 \u2192 (H([0, 1]))^\u2297d\n\u03c9 \u21a6 ((X\u2081(\u03c9))_{t\u2208[0,1]}, ..., (X_d(\u03c9))_{t\u2208[0,1]})\n\nFor this, the coordinate-wise sum of the d corresponding scalar products is used to project the data onto a chosen dictionary element:\n\n(f, g)_{H^\u2297d} := \u2211_{i=1}^d (f\u207d\u2071\u207e, g\u207d\u2071\u207e)_H.\n\n Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. ", "original_text": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al. "}, "hash": "c585d19f4b0465e7bf274cdb37cd31de27f43c4f7d49d0d54499835a9f7cb6db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "45d27ee2-6426-4576-a072-6a3dd9db2213", "node_type": "1", "metadata": {"window": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. "}, "hash": "a682f9cbef145e83f888f1f7905cd7a2fed3a4c766e6768212c03cdf89546653", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "(2014). ", "mimetype": "text/plain", "start_char_idx": 74663, "end_char_idx": 74671, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "45d27ee2-6426-4576-a072-6a3dd9db2213", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a27203e2-98ce-44ea-a177-b981a5614b8b", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Further, a dictionary should be defined in (H([0,1]))^\u2297d.  This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. ", "original_text": "(2014). "}, "hash": "ee0264ec7db496a88f2b059373c94f510786f135829160783467a15033c36195", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a08539a7-09f0-4773-9a55-02cfdc3b0946", "node_type": "1", "metadata": {"window": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. "}, "hash": "52a691e87e4a504a3471cd66f99fde59625fd53935639a29cbf7118ab1bd0dc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. ", "mimetype": "text/plain", "start_char_idx": 74671, "end_char_idx": 74835, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a08539a7-09f0-4773-9a55-02cfdc3b0946", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "45d27ee2-6426-4576-a072-6a3dd9db2213", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "This can be done, e.g., by either componentwise application of one or several univariate dictionaries from Sections 3 and B, or by constructing of special d-variate ones.  For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1]. "}, "hash": "4b763927bf48ae5f0d8b47d85f4664a2f4bf91cf6c4cf4c22829ef5b4905d155", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b1937a6-a523-4b02-8f84-2b7813e8f323", "node_type": "1", "metadata": {"window": "(1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. "}, "hash": "2f5678e7e6a29a399c8228e0558bd2eec797326a838cd6ea670c52b11230d4f7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. ", "mimetype": "text/plain", "start_char_idx": 74835, "end_char_idx": 74935, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3b1937a6-a523-4b02-8f84-2b7813e8f323", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a08539a7-09f0-4773-9a55-02cfdc3b0946", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "For illustration purposes, regard the following example constructed based on the MNIST Lecun et al.  (1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2. "}, "hash": "04bb3e373b0206f81d042fb371e5cce3c93f1fe5e364e0bb770e3ea2f3e7cb77", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d769d61d-6b5d-42c8-8082-72858e2b5733", "node_type": "1", "metadata": {"window": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "*sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. "}, "hash": "a58b4d16df4a6eca9ba20dca1910a06543c5b5a72ff44ad52b6eb5a9a7bdbd36", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. ", "mimetype": "text/plain", "start_char_idx": 74935, "end_char_idx": 75047, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d769d61d-6b5d-42c8-8082-72858e2b5733", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "*sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b1937a6-a523-4b02-8f84-2b7813e8f323", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(1998) dataset.  First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}. "}, "hash": "e28a21aba968c494c2aeb68dce3632b737edcd8609c1f5280d600a46ecf15e27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54c074d4-a8ed-4454-8d79-6f3b4e93dd16", "node_type": "1", "metadata": {"window": "(2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. "}, "hash": "d5d31489cd4bbb924638ce6ad106a079f29ad5a08f901ae2cd9926e95075c536", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. ", "mimetype": "text/plain", "start_char_idx": 75047, "end_char_idx": 75208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54c074d4-a8ed-4454-8d79-6f3b4e93dd16", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d769d61d-6b5d-42c8-8082-72858e2b5733", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "First, we extract the digits\u2019 contours (skeletons) using skimage python library van der Walt et al.  (2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "*sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates. "}, "hash": "f661416e1bd4bfa370eb620e4e7bb89154ed0088a0e12cc8735659ddff17a598", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "462c9532-1b25-4139-a648-c3c020778d14", "node_type": "1", "metadata": {"window": "Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies."}, "hash": "07ab65eb8393f8749711ef19b4dda8928eb8d26a264c84ca87fce7fdc4ccfa45", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. ", "mimetype": "text/plain", "start_char_idx": 75208, "end_char_idx": 75295, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "462c9532-1b25-4139-a648-c3c020778d14", "embedding": null, "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "68fbbd65-99bb-454c-b9da-d25f500df7d1", "node_type": "4", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf"}, "hash": "c327a4bbcdb4765496a1dc50e5d2094a45f6664efecb2fcafa73e0831da647f3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54c074d4-a8ed-4454-8d79-6f3b4e93dd16", "node_type": "1", "metadata": {"title": "Functional Isolation Forest", "authors": "Staerman", "year": 2019, "file_path": "ad-papers-pdf/functional_isolation_forest.pdf", "window": "(2014).  Then each observation is transformed into a curve in (L\u2082([0, 1]) \u00d7 L\u2082([0, 1])) (one vertical and one horizontal coordinates) using length parametrization on [0,1].  We construct the problem by taking 100 curves from class 7 and adding 10 observations from class 2.  We apply FIF with two-dimensional *sinuscosine* dictionary and the following scalar product : (f, g)_{(L\u2082)^\u2297d}.  *sinuscosine* is constructed as a direct extension of *cosine* dictionary introduced for FIF by selecting randomly cosine or sinus function on each coordinates.  Figure 19 shows anomaly detection using the visual elbow rule to define the threshold.  Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "original_text": "Figure 19 shows anomaly detection using the visual elbow rule to define the threshold. "}, "hash": "4913de4d9f0ec12204de63a14dd228ce245b9413cf5576466dbfbd599382a8d9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Among those detected, five digits are indeed 7s, but do not resemble them and thus are identified as anomalies.", "mimetype": "text/plain", "start_char_idx": 75295, "end_char_idx": 75406, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]