[{"id_": "c91e5df7-2a53-4301-bb0d-65e5eb34e955", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n", "original_text": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "afe1364a-8357-4bc9-96df-0ae1b1c37b4c", "node_type": "1", "metadata": {"window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. ", "original_text": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds. "}, "hash": "9c080b93d261da163b0b3c108c155b55c6677e4064b9696a06c4c644b93cc5de", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition. ", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 575, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "afe1364a-8357-4bc9-96df-0ae1b1c37b4c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. ", "original_text": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c91e5df7-2a53-4301-bb0d-65e5eb34e955", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n", "original_text": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition. "}, "hash": "7d01303d3318b8ea5fe3c9bc496227800829bbdcd22fe8171ae74a5137da8b0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "886519f2-845f-4af1-8620-ec7233499946", "node_type": "1", "metadata": {"window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n", "original_text": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n"}, "hash": "e5efa2c05375daecf7dd02802f1f9d8422fed35a426595031773d425f6917e08", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds. ", "mimetype": "text/plain", "start_char_idx": 575, "end_char_idx": 940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "886519f2-845f-4af1-8620-ec7233499946", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n", "original_text": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "afe1364a-8357-4bc9-96df-0ae1b1c37b4c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. ", "original_text": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds. "}, "hash": "5bdcd6292d01f8b880a294b9aea6b55d0d86eed4149761e57d1adb3e1ccd5ed7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0536511f-3099-461b-bf2b-16b4c36196a9", "node_type": "1", "metadata": {"window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n", "original_text": "## 1 Introduction\n\nThe isolation forest (a.k.a. "}, "hash": "c76948627aa91403dfbddd1be842a91d6988fc0323787c94b8cdd0ba68929c7b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n", "mimetype": "text/plain", "start_char_idx": 940, "end_char_idx": 1096, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0536511f-3099-461b-bf2b-16b4c36196a9", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n", "original_text": "## 1 Introduction\n\nThe isolation forest (a.k.a. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "886519f2-845f-4af1-8620-ec7233499946", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n", "original_text": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n"}, "hash": "404e8cd1556e634f85cd9d3f8349add2538009b25e83e4d3dd6ec7383d61f24f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d4ebcfe-22bd-4fa7-b478-daad09b270a5", "node_type": "1", "metadata": {"window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. ", "original_text": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points. "}, "hash": "ba7ecc0242a2bd8e5036b81a984be5b6d2ded60b59e1564fb6b52e840891b4dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 1 Introduction\n\nThe isolation forest (a.k.a. ", "mimetype": "text/plain", "start_char_idx": 1096, "end_char_idx": 1144, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d4ebcfe-22bd-4fa7-b478-daad09b270a5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. ", "original_text": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0536511f-3099-461b-bf2b-16b4c36196a9", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n", "original_text": "## 1 Introduction\n\nThe isolation forest (a.k.a. "}, "hash": "25359da03489744d502c35e8060d9422b233c6d6eef5c87a37365297d3dced74", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ceec5999-bbf5-4d0a-85fb-eb15d6c2a71f", "node_type": "1", "metadata": {"window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. ", "original_text": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n"}, "hash": "351d03d6292f1af1315fa375545d3a1f52ee9790095eb39c4467da605299e587", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points. ", "mimetype": "text/plain", "start_char_idx": 1144, "end_char_idx": 1296, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ceec5999-bbf5-4d0a-85fb-eb15d6c2a71f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. ", "original_text": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d4ebcfe-22bd-4fa7-b478-daad09b270a5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. ", "original_text": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points. "}, "hash": "820c3e9a2e820696c81c6a200bb9fc62c11069c13a9eac9d4a306da2abd058b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecec8365-7b22-415c-bc3f-a66cd7a99361", "node_type": "1", "metadata": {"window": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n", "original_text": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. "}, "hash": "b8c2d3b34679762742742ca482803c22bfe567141f274a70c99d51501abd4e5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n", "mimetype": "text/plain", "start_char_idx": 1296, "end_char_idx": 1818, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ecec8365-7b22-415c-bc3f-a66cd7a99361", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n", "original_text": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ceec5999-bbf5-4d0a-85fb-eb15d6c2a71f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv:2110.13402v3 [stat.ML] 6 Dec 2021\n\n# Revisiting randomized choices in isolation forests\n\nDavid Cortes\n\nDecember 7, 2021\n\n## Abstract\n\nIsolation forest or \"iForest\" is an intuitive and widely used algorithm for anomaly detection that follows a simple yet effective idea: in a given data distribution, if a threshold (split point) is selected uniformly at random within the range of some variable and data points are divided according to whether they are greater or smaller than this threshold, outlier points are more likely to end up alone or in the smaller partition.  The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. ", "original_text": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n"}, "hash": "f7d1dc28faad26fa5ca0f8df80ef6741e355b99af56275d2491e4d5fb44e6fbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82d88c48-37bf-4339-9653-87641af5db74", "node_type": "1", "metadata": {"window": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. ", "original_text": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n"}, "hash": "9db1b98261daa1b5df8e6b3366064588712d758b047ecc9354f4b307e480cea8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. ", "mimetype": "text/plain", "start_char_idx": 1818, "end_char_idx": 2304, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "82d88c48-37bf-4339-9653-87641af5db74", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. ", "original_text": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ecec8365-7b22-415c-bc3f-a66cd7a99361", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The original procedure suggested the choice of variable to split and split point within a variable to be done uniformly at random at each step, but this paper shows that \"clustered\" diverse outliers - oftentimes a more interesting class of outliers than others can be more easily identified by applying a non-uniformly-random choice of variables and/or thresholds.  Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n", "original_text": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met. "}, "hash": "3809c9610855582b4628b777db5a5f7ef7b6f5e1f37c305bb92d5386c38d9522", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "172c2fe7-bfee-47f5-adae-011023d67afb", "node_type": "1", "metadata": {"window": "## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. ", "original_text": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n"}, "hash": "d68fcbdf8bc4bf7ea84c55e673afc9c0846812f66ee5c4c5fc1ff0537200da6c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n", "mimetype": "text/plain", "start_char_idx": 2304, "end_char_idx": 2537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "172c2fe7-bfee-47f5-adae-011023d67afb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. ", "original_text": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82d88c48-37bf-4339-9653-87641af5db74", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Different split guiding criteria are compared and some are found to result in significantly better outlier discrimination for certain classes of outliers.\n\n ## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. ", "original_text": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n"}, "hash": "8721e49bf401c4faeb3cf0c95391033523b9e9a8cdab4cf10c818e05cd340c0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae8b420f-fd27-49d9-a39f-bd89e9343a56", "node_type": "1", "metadata": {"window": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]). ", "original_text": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. "}, "hash": "02feb627b28ddd6430876c4442cad4b7338fbbf3d382e1137871627ff4dbca73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n", "mimetype": "text/plain", "start_char_idx": 2537, "end_char_idx": 3075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae8b420f-fd27-49d9-a39f-bd89e9343a56", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]). ", "original_text": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "172c2fe7-bfee-47f5-adae-011023d67afb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 1 Introduction\n\nThe isolation forest (a.k.a.  IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. ", "original_text": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n"}, "hash": "5faf5bc543cb15f3a7376aade5eca8283f93c5e9932d9ca6e18fbb9610103274", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5f619480-7a1e-42e2-a363-193d3ee4b5e8", "node_type": "1", "metadata": {"window": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n", "original_text": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. "}, "hash": "14c7e1e919ebd738558605c5bbcaf8b32e13cf2e831b7ae5b22555b0190a303c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. ", "mimetype": "text/plain", "start_char_idx": 3075, "end_char_idx": 3469, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5f619480-7a1e-42e2-a363-193d3ee4b5e8", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n", "original_text": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae8b420f-fd27-49d9-a39f-bd89e9343a56", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "IFOREST, [14]) algorithm for anomaly detection tries to exploit the idea that outliers, by definition, are \"few and different\u201d from normal data points.  The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]). ", "original_text": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data. "}, "hash": "616cd969354af83b9b52aa9e504e1cab504822bc62c83c05bf3b064bfedd0dcb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03dc7a4c-d400-4d3e-a931-7a15c2091706", "node_type": "1", "metadata": {"window": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. ", "original_text": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n"}, "hash": "fe80e3600937ea8205920d31ff516cdd39d623eccafdd22b862764f62228cbd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. ", "mimetype": "text/plain", "start_char_idx": 3469, "end_char_idx": 3661, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "03dc7a4c-d400-4d3e-a931-7a15c2091706", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. ", "original_text": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5f619480-7a1e-42e2-a363-193d3ee4b5e8", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The premise behind the algorithm is very intuitive: in a given distribution of data points X \u2208 R\u1d50\u02e3\u207f, if picking a variable (column) at random within the data, if one selects a value uniformly at random within the range of the data distribution for that variable and then groups observations (rows) according to whether they are greater or smaller than this threshold across the chosen variable, then outliers (anomalies) have a larger chance of ending up alone or in a smaller group when compared to normal observations.\n\n The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n", "original_text": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g. "}, "hash": "870c1d22d624a026e8ecc524f16e273769dff047b4d5ad03a681dc40e84d462d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24749134-b8d9-4c6f-a341-a00ee3a27dad", "node_type": "1", "metadata": {"window": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n", "original_text": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. "}, "hash": "dedb5f989ddb9a52e52175757ba9a48045b2feef76d96c93568d89884f4da4bd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n", "mimetype": "text/plain", "start_char_idx": 3661, "end_char_idx": 3731, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24749134-b8d9-4c6f-a341-a00ee3a27dad", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n", "original_text": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03dc7a4c-d400-4d3e-a931-7a15c2091706", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The algorithm exploits this idea by performing recursive partitions of the data - that is, a given set or subset of points is first split according to this random choice, then at each branch or group that this division produces, the procedure is repeated again but this time considering only the points that reached this subset, and it continues doing so until each point becomes isolated (is the only point present in a given division) or until some other termination criteria is met.  Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. ", "original_text": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n"}, "hash": "22b59d89bbbfa41fb799fc41f51d3ecda893944e5b1b66adcafcf5e10ef2230e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8d9f35b-617b-4f98-985a-395f78df009e", "node_type": "1", "metadata": {"window": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. ", "original_text": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. "}, "hash": "847c89aff63bc0e7072a1b5a3c5947ee8060287a9825357c02579c828ffe9a11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. ", "mimetype": "text/plain", "start_char_idx": 3731, "end_char_idx": 3916, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8d9f35b-617b-4f98-985a-395f78df009e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. ", "original_text": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24749134-b8d9-4c6f-a341-a00ee3a27dad", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Under this scheme, outlier points will in expectation take fewer steps/divisions/branchings to become isolated, and thus the expected isolation depth can be used as an anomaly score for each point (the shorter, the more anomalous).\n\n For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n", "original_text": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g. "}, "hash": "7a0f924dd5a36445bcce30fd38044c5a1502d844dcfe248d2cbbfe8898615321", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54591de3-33aa-44db-908b-e71d267736ec", "node_type": "1", "metadata": {"window": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n", "original_text": "in [2]). "}, "hash": "22febf8b605f23680940cca44fd184e09947971acef123f3170861f475c6b201", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. ", "mimetype": "text/plain", "start_char_idx": 3916, "end_char_idx": 4052, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54591de3-33aa-44db-908b-e71d267736ec", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n", "original_text": "in [2]). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8d9f35b-617b-4f98-985a-395f78df009e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For better results, this procedure is repeated many times (suggestion in [14] was 100 times), with each different run (from here on, a \"decision tree\" or simply \"tree\") taking only a small sub-sample of the data (suggestion in [14] was to sub-sample 256 points without replacement for each tree), with the observed isolation depth for each observation averaged across trees - the set of which form a \"forest\" - and compared against the expected isolation depth if all the data were distributed uniformly at random across all dimensions.\n\n For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. ", "original_text": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g. "}, "hash": "d09089bf188bb1fae5985e6b6a5faf89d0df5f9c15877961ec8c394deaca3a66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e369db1e-dd0e-4412-9468-5bc325e1b3af", "node_type": "1", "metadata": {"window": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n", "original_text": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n"}, "hash": "18d93aab0f6c440080557391919009b228ff4ec0660adf416e0f45b15ad818b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "in [2]). ", "mimetype": "text/plain", "start_char_idx": 4052, "end_char_idx": 4061, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e369db1e-dd0e-4412-9468-5bc325e1b3af", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n", "original_text": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54591de3-33aa-44db-908b-e71d267736ec", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For a more computationally efficient implementation, the procedure may be stopped before reaching isolation of every point at a given tree, since outliers can only be considered to be so if their isolation depth is below average, and thus a remainder can be extrapolated for points that reach a non-isolated node with depth greater or equal than the expected average for uniformly-random data.  The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n", "original_text": "in [2]). "}, "hash": "d4cc4828ba1110969ca3d53cf197d2fbac64730c8c3607937f96a1139f5d69d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5567f4f8-31ad-4ddc-bbca-c2a41c8b24c0", "node_type": "1", "metadata": {"window": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n", "original_text": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. "}, "hash": "947f780e4b41ff94cc2a656cd8b13f3121e27246a107c4b99170f4aed380093a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n", "mimetype": "text/plain", "start_char_idx": 4061, "end_char_idx": 4208, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5567f4f8-31ad-4ddc-bbca-c2a41c8b24c0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n", "original_text": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e369db1e-dd0e-4412-9468-5bc325e1b3af", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The expected isolation depth (and thus the extrapolated remainder) can be approximated efficiently through a closed-form formula (see [1]), resulting in a very fast procedure compared to e.g.  distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n", "original_text": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n"}, "hash": "689bf798f59adfeecdcfb23f863a451374e4f1411847b665da0b5eb92541b1f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bdf5ec9f-0e72-4dd9-965a-4392baa7e694", "node_type": "1", "metadata": {"window": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. ", "original_text": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n"}, "hash": "e350a7e82061dd416a1270de49293fc72ae2fcdcb595007b6a6074b69426aa5c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. ", "mimetype": "text/plain", "start_char_idx": 4208, "end_char_idx": 4570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bdf5ec9f-0e72-4dd9-965a-4392baa7e694", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. ", "original_text": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5567f4f8-31ad-4ddc-bbca-c2a41c8b24c0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "distance-based algorithms such as LOF (\"local outlier factor\", [4]).\n\n The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n", "original_text": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them. "}, "hash": "a4f2dfcbd2b13c51838ed2b287ec0ee339690ac079884daf0f771c35447b520b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18a3d760-4a27-4acd-aa68-149d54fbecc2", "node_type": "1", "metadata": {"window": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n", "original_text": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. "}, "hash": "e5c1721b23397a00cf841515a6b73474c23e5594c9bb1eab5f3fe9ca85419c49", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n", "mimetype": "text/plain", "start_char_idx": 4570, "end_char_idx": 4924, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18a3d760-4a27-4acd-aa68-149d54fbecc2", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n", "original_text": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bdf5ec9f-0e72-4dd9-965a-4392baa7e694", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The isolation forest algorithm as-is has enjoyed wide success and widespread usage, achieving good results in a variety of benchmarks encompassing different types of outliers (see e.g.  [5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. ", "original_text": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n"}, "hash": "9945e10202570c540c8ff02d05d5e90760ceaebc5fb9390ee5cdb9a9588d94b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f21d93a-6b55-4f15-9254-4ef4c2a780b6", "node_type": "1", "metadata": {"window": "in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. ", "original_text": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n"}, "hash": "ff62d3bf4d97d072634c8bb72c0746c4f3f1ecd5a338fe2b1e42c37d857ca5b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. ", "mimetype": "text/plain", "start_char_idx": 4924, "end_char_idx": 5083, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f21d93a-6b55-4f15-9254-4ef4c2a780b6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. ", "original_text": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18a3d760-4a27-4acd-aa68-149d54fbecc2", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[5]) and usually outperforming other model classes, although some issues have been identified with the logic behind the algorithm (e.g.  in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n", "original_text": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it. "}, "hash": "2410698b56a97f041c7d482c23b49d1e05680dda731db1354e7be22fda41d329", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd1b32fe-d344-4305-8b85-b4540c0caf04", "node_type": "1", "metadata": {"window": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n", "original_text": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n"}, "hash": "58f9036e9e4af4270a94860febf63ae20955d9c690d76048a7b4263df15dcb8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n", "mimetype": "text/plain", "start_char_idx": 5083, "end_char_idx": 5560, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd1b32fe-d344-4305-8b85-b4540c0caf04", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n", "original_text": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f21d93a-6b55-4f15-9254-4ef4c2a780b6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "in [2]).  Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. ", "original_text": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n"}, "hash": "805e34264c646ad8259e5e2fa7da71ca865f8809147c58cbc61423bdc448fcb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cb82299-5a71-4f5a-bf93-ecf1089783b6", "node_type": "1", "metadata": {"window": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. ", "original_text": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n"}, "hash": "e3d64332b174a94fc5fc943bac80fba887f397fb7871a7d0fe918812159865ec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n", "mimetype": "text/plain", "start_char_idx": 5560, "end_char_idx": 6032, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cb82299-5a71-4f5a-bf93-ecf1089783b6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. ", "original_text": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd1b32fe-d344-4305-8b85-b4540c0caf04", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Many subsequent works have built upon the idea to create better variations of the procedure, some of which will be analyzed in the next sections.\n\n While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n", "original_text": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n"}, "hash": "eba7d7315aa7cbf3480e4cb239dc1952c469fea78d34fccdeb8f981ea4459be0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea2733f6-6c23-4710-b74a-e6aed6900b29", "node_type": "1", "metadata": {"window": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit. ", "original_text": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. "}, "hash": "4523aa4ea3d682493095bf05c81e7529d05a693b566c54a273aed07a074ffe0b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n", "mimetype": "text/plain", "start_char_idx": 6032, "end_char_idx": 6372, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ea2733f6-6c23-4710-b74a-e6aed6900b29", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit. ", "original_text": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cb82299-5a71-4f5a-bf93-ecf1089783b6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While overall useful as a general outlier detector and perhaps one of the best all-around methods as measured by aggregated metrics across benchmarks, a deeper look at performance comparisons in the literature would reveal that IFOREST and modifications thereof typically outperform other classes of methods in certain types of datasets, but not in all of them.  This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. ", "original_text": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n"}, "hash": "afecffb2a3c77a8483353f8ce40298206e76a5fca7a4a165936849de7a293d5a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dc311064-30a4-45ae-808f-0548887fcaab", "node_type": "1", "metadata": {"window": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. ", "original_text": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n"}, "hash": "fe3a1ceb592c9ce6e4ef6b4788f56d2124c73b279bb0e470ab1906cfeff78a94", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. ", "mimetype": "text/plain", "start_char_idx": 6372, "end_char_idx": 6951, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dc311064-30a4-45ae-808f-0548887fcaab", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. ", "original_text": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea2733f6-6c23-4710-b74a-e6aed6900b29", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This work tries to characterize these cases and proposes a different guiding heuristic for split selection in IFOREST, which is compared against similar ideas and found to outperform them in datasets in which isolation-based methods dominate others, at the expense of hindered performance in datasets in which non-tree-based methods outperform IFOREST.\n\n ## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit. ", "original_text": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features. "}, "hash": "3d71c519b269a70d1e7e7992d6548c3ac88ee7b8f18664216a4a20108010c670", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "279c5953-9e23-4f5c-a704-2d164247bf55", "node_type": "1", "metadata": {"window": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n", "original_text": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. "}, "hash": "165e0feb6e84409894a0ad026f2e2a328fe3aef1094dd409361c725ad9dbf438", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n", "mimetype": "text/plain", "start_char_idx": 6951, "end_char_idx": 7129, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "279c5953-9e23-4f5c-a704-2d164247bf55", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n", "original_text": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dc311064-30a4-45ae-808f-0548887fcaab", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 2 Related work\n\nThere are many potential ways in which the isolation forest procedure can be improved upon while following the same core concept behind it.  For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. ", "original_text": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n"}, "hash": "725d8760c58c9a30e0c219d891f2dd76d3fa7798365c1ddbd8a63117360e1563", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "973d5613-4d37-4c0e-bbc8-522a859fbefb", "node_type": "1", "metadata": {"window": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. ", "original_text": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n"}, "hash": "6d987139e87a8daf21fe7c36111d5e24cd5980e25516adecff86df1e1d1b056e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. ", "mimetype": "text/plain", "start_char_idx": 7129, "end_char_idx": 7556, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "973d5613-4d37-4c0e-bbc8-522a859fbefb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. ", "original_text": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "279c5953-9e23-4f5c-a704-2d164247bf55", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, [12] highlights biases that are introduced by the way that data splits are created and shows some simple ways in which the procedure can fail to produce adequate results, proposing a simple fix (the \"extended isolation forest\" algorithm or EIF) which, instead of producing splits by one variable at a time (or \"axis-parallel splits\u201d), produces split thresholds with respect to randomly-generated hyperplanes, which encompass only a couple of variables at a time.\n\n The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n", "original_text": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest. "}, "hash": "db836557dbc77532d0a0cd9186da24a147551969cfbb534517e1ef9f4d9f38b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8530184-1e2e-42fc-92ce-5d0c67acd9c5", "node_type": "1", "metadata": {"window": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n", "original_text": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. "}, "hash": "434fd7e609645858f92aa1194e6ea5baaba7041093448e5a397c3435702e00cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n", "mimetype": "text/plain", "start_char_idx": 7556, "end_char_idx": 7767, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f8530184-1e2e-42fc-92ce-5d0c67acd9c5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n", "original_text": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "973d5613-4d37-4c0e-bbc8-522a859fbefb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The same extension is taken a step further in the \"split criterion isolation forest\" or SCIFOREST from [15], proposing not only doing the splits by random hyperplanes, but also determining the split threshold by a deterministic criterion that aims at making each tree branch (groups that are obtained after dividing points according to the threshold) more homogeneous, and reporting increased performance metrics in typical anomaly detection datasets from these changes.\n\n The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. ", "original_text": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n"}, "hash": "1380e536c48a308f6e28409ca85310b42284a6a5829e648b672db555e48292d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a523cb93-b951-4280-82cd-47c126e873a7", "node_type": "1", "metadata": {"window": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers. ", "original_text": "by incorrect data entry that types an additional digit. "}, "hash": "14dc4f765784b9020472e459999a19181734d0fac0a2e2ea20a49734f00e7074", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. ", "mimetype": "text/plain", "start_char_idx": 7767, "end_char_idx": 8021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a523cb93-b951-4280-82cd-47c126e873a7", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers. ", "original_text": "by incorrect data entry that types an additional digit. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8530184-1e2e-42fc-92ce-5d0c67acd9c5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The \"robust random cut forest\" or RRCF from [11] proposed choosing the variable to split with a probability proportional to the range that it spans instead of choosing it uniformly at random, following the idea that variables with a wider ranger are more important in a multivariate distribution and more likely to differentiate outliers.\n\n Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n", "original_text": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g. "}, "hash": "d0f161666bc3c6d7d9f6bc63918a47ed2da18bdb542aa2ace4b25e1b922124a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0010d8df-bc16-4164-84de-be4e16718707", "node_type": "1", "metadata": {"window": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. ", "original_text": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. "}, "hash": "24151630788a7da72488e82f4a5e00e27249da4158092f265246fca72e6c7541", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "by incorrect data entry that types an additional digit. ", "mimetype": "text/plain", "start_char_idx": 8021, "end_char_idx": 8077, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0010d8df-bc16-4164-84de-be4e16718707", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. ", "original_text": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a523cb93-b951-4280-82cd-47c126e873a7", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Other models such as the \"density estimation trees\" or DET from [17] also propose more elaborate criteria for choosing the variable and the split point, in the case of [17] considering the distribution of all variables rather than just a subset of them as in [15], and choosing both the variable to split and the split point within the variable according to a deterministic criterion that aims at grouping together more points where the multivariate density of the data is higher, with density viewed as the amount of points per volume unit in the space spanned by the features.  The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers. ", "original_text": "by incorrect data entry that types an additional digit. "}, "hash": "fac436e16c2bc69e965c07139ad18a9d1a780d57ef6a74b0504c8be1c1dc4fd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e352fd9f-c5f5-4fb8-9af7-36b3ec5c1f94", "node_type": "1", "metadata": {"window": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n", "original_text": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n"}, "hash": "6b9fb63495d5a3dbb74319de1feb4688bfecebd9923374de1ac8d639af6f06b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. ", "mimetype": "text/plain", "start_char_idx": 8077, "end_char_idx": 8335, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e352fd9f-c5f5-4fb8-9af7-36b3ec5c1f94", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n", "original_text": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0010d8df-bc16-4164-84de-be4e16718707", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The \"generalized isolation forest\" (GIF) from [5] and the \"one-class random forest\" (OCRF) from [9] also proposed similar ideas, but with slightly different splitting criteria.\n\n ## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. ", "original_text": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest. "}, "hash": "648703605343a99be87ae4e17e8d48ec26fd811cdf545df82d46dd4f7c742a55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7f4cc76-f8bb-41dc-861a-51b67aff8048", "node_type": "1", "metadata": {"window": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. ", "original_text": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. "}, "hash": "766b71eac325cc65b1d0b0e3d1d30cf2218271bb2a4270bc41806f9ed534f28d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n", "mimetype": "text/plain", "start_char_idx": 8335, "end_char_idx": 8458, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7f4cc76-f8bb-41dc-861a-51b67aff8048", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. ", "original_text": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e352fd9f-c5f5-4fb8-9af7-36b3ec5c1f94", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 3 Different types of outliers\n\nSo far, outliers or anomalies have only been characterized here as being \"few and different\" - for example, in typical datasets for outlier detection, outliers are sampled from a minority group which differs in some fundamental aspect from the majority, such as cross-sectional measurements of soil samples in which outliers are measurements taken from a different type of soil than the rest.  This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n", "original_text": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n"}, "hash": "2360db1fa772eb4cb77940366173fc363a8d76b842d2f427985836e585ad5ce7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71bda848-c890-490a-ba43-c800951ae00a", "node_type": "1", "metadata": {"window": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n", "original_text": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n"}, "hash": "43aaaa263f6d58d39a2eed99e3c42cc9a8765602854796d032ebdf2665db91dc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. ", "mimetype": "text/plain", "start_char_idx": 8458, "end_char_idx": 8796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "71bda848-c890-490a-ba43-c800951ae00a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n", "original_text": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7f4cc76-f8bb-41dc-861a-51b67aff8048", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This nevertheless does not provide the full picture, as \"outliers\u201d or \"anomalies\u201d in a dataset can be generated by different events or processes, such as incorrect data entry or attempted fraudulent purchases.\n\n A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. ", "original_text": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events. "}, "hash": "2b60931a29df00c055a0c4f085b2e09f2c9e0733ed7f7cf24671d802eb29fe0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c94e86a-9324-4cd0-82c8-58d09033a948", "node_type": "1", "metadata": {"window": "by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n", "original_text": "Most of the literature nevertheless makes no distinctions between classes of outliers. "}, "hash": "cf6a354fe977b4f6829a6fd708525e43b3297e236b5f81dc34f06a61e4e4661e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n", "mimetype": "text/plain", "start_char_idx": 8796, "end_char_idx": 8940, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c94e86a-9324-4cd0-82c8-58d09033a948", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n", "original_text": "Most of the literature nevertheless makes no distinctions between classes of outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71bda848-c890-490a-ba43-c800951ae00a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A lot of literature has been written about \"extreme value\" identification in univariate distributions, which is one type of anomaly in which the value for some particular variable in an observation is very different from the rest, and can be caused e.g.  by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n", "original_text": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n"}, "hash": "2190907a9044fbbac9f74fde4bd4e072de4da3294e3313e6b0116cb5cc846808", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b057efc-3bec-4fcf-b847-03e0fd973f63", "node_type": "1", "metadata": {"window": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. ", "original_text": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. "}, "hash": "7d39b42b46f336508527481029fbceaee41a12386ce9e76719739ad9bd1fa68d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Most of the literature nevertheless makes no distinctions between classes of outliers. ", "mimetype": "text/plain", "start_char_idx": 8940, "end_char_idx": 9027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b057efc-3bec-4fcf-b847-03e0fd973f63", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. ", "original_text": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c94e86a-9324-4cd0-82c8-58d09033a948", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "by incorrect data entry that types an additional digit.  More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n", "original_text": "Most of the literature nevertheless makes no distinctions between classes of outliers. "}, "hash": "974b1a5d914934c687f76516bf7bf0fb5d82ee421f8be5487783d2981e506467", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6657c1b1-a8a2-4177-8db8-99a502afed80", "node_type": "1", "metadata": {"window": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g. ", "original_text": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n"}, "hash": "50f6be4356d0110b3224610dca2ad2e62ddf025320177505a22f789af3f41dae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. ", "mimetype": "text/plain", "start_char_idx": 9027, "end_char_idx": 9277, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6657c1b1-a8a2-4177-8db8-99a502afed80", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g. ", "original_text": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b057efc-3bec-4fcf-b847-03e0fd973f63", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "More interesting classes of anomalies however typically involve relationships between many variables, with outliers not necessarily having extreme values in any particular variable, but presenting relationships between them which do not match with the rest.  Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. ", "original_text": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector. "}, "hash": "ed974d4d20eae770603dadfc6a77c6212d5c76b666dcd68da42f97046307ef23", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7586cea0-2b33-4800-b86b-23a73b1a0f25", "node_type": "1", "metadata": {"window": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g. ", "original_text": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. "}, "hash": "f040b6d9544d3bf50172fe4de3234d4d55e664f7bdd11ad3007a385122e58e06", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n", "mimetype": "text/plain", "start_char_idx": 9277, "end_char_idx": 9573, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7586cea0-2b33-4800-b86b-23a73b1a0f25", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g. ", "original_text": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6657c1b1-a8a2-4177-8db8-99a502afed80", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Most of the \"anomaly detection\" or \"one-class classification\" literature as well as public datasets focus on these cases.\n\n A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g. ", "original_text": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n"}, "hash": "8734b9e62f822fff75a2983f6bd0dcb5a8ef8664c2df73df91c16ea80e21eb60", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebca8c0f-d6f6-4d41-85ee-dca252615adf", "node_type": "1", "metadata": {"window": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. ", "original_text": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n"}, "hash": "2bbb32140c9dfe6fab36742a41ec774bbd0efe9de9baa997969ab7060c4fc18f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. ", "mimetype": "text/plain", "start_char_idx": 9573, "end_char_idx": 10091, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebca8c0f-d6f6-4d41-85ee-dca252615adf", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. ", "original_text": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7586cea0-2b33-4800-b86b-23a73b1a0f25", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A distinction is made in [15] between \"scattered\" and \"clustered\" outliers, with the \"clustered\" outliers being considered as more \"interesting\", as they are classes of outliers which typically originate through some repeated process (such as fraudulent activity) as opposed to \"scattered\" outliers which originate from unrelated events.  These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g. ", "original_text": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes. "}, "hash": "6587b2d26afb80575449fc958899db9e604a28d8ce7bf88dabc9c8827b72d958", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4167bb60-1ed1-4cfd-a370-4233bf8fd855", "node_type": "1", "metadata": {"window": "Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. ", "original_text": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n"}, "hash": "f31580d2f4b4cca267a00795a887f0ecb364bdefe8859fccd676d4d6f7af8616", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n", "mimetype": "text/plain", "start_char_idx": 10091, "end_char_idx": 10333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4167bb60-1ed1-4cfd-a370-4233bf8fd855", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. ", "original_text": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebca8c0f-d6f6-4d41-85ee-dca252615adf", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These \"clustered\u201d outliers are deemed harder to identify due to aspects such as \"masking\", which [15] tries to address in its proposed method.\n\n Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. ", "original_text": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n"}, "hash": "bdbb35721c5241c65dea76b91f6dacd0cf7fd708292b82d50a3fecb446ef493d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c404c677-12f2-4272-b583-2dee6d82098f", "node_type": "1", "metadata": {"window": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate. ", "original_text": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. "}, "hash": "ddb243080ab4274843fe88e63c6bd83d19d01e831df706c62901442f2e12a311", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n", "mimetype": "text/plain", "start_char_idx": 10333, "end_char_idx": 10506, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c404c677-12f2-4272-b583-2dee6d82098f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate. ", "original_text": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4167bb60-1ed1-4cfd-a370-4233bf8fd855", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Most of the literature nevertheless makes no distinctions between classes of outliers.  Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. ", "original_text": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n"}, "hash": "62f5f068270081f4737c56ed6198b725a480be5964af8be7166b513a1fb57626", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee48de79-f6a3-4007-8174-a3ebbb092eb6", "node_type": "1", "metadata": {"window": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n", "original_text": "In the public datasets that are typically used to compare algorithms (from e.g. "}, "hash": "90de99891b2f1728bd972ff9d2861b177f584f7515e82d0be2707c6c50e7078e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. ", "mimetype": "text/plain", "start_char_idx": 10506, "end_char_idx": 10691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ee48de79-f6a3-4007-8174-a3ebbb092eb6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n", "original_text": "In the public datasets that are typically used to compare algorithms (from e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c404c677-12f2-4272-b583-2dee6d82098f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Oftentimes, a new method is introduced, and its utility demonstrated by identifying a pattern of anomality in randomly-generated data and aggregating performance metrics across different datasets, evaluating the method as a general anomaly detector.  Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate. ", "original_text": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them. "}, "hash": "3154d259ed26a559f79cedf4d23cfbabd562d8f585efafa0429589fdb709910f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5f8f8a6-205d-441a-b89a-532598546829", "node_type": "1", "metadata": {"window": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2. ", "original_text": "[18] or [6], which have been used in e.g. "}, "hash": "8ac0299b6b409293149aaf2cd5086a4b3f92d381ba4a77049e08d19e11be292c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the public datasets that are typically used to compare algorithms (from e.g. ", "mimetype": "text/plain", "start_char_idx": 10691, "end_char_idx": 10771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c5f8f8a6-205d-441a-b89a-532598546829", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2. ", "original_text": "[18] or [6], which have been used in e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee48de79-f6a3-4007-8174-a3ebbb092eb6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Such a presentation however provides an incomplete depiction, as different methods make a trade-off in being better at identifying certain classes of outliers at the expense of being worse at identifying others as will be shown in the following sections, but such trade-off is rarely discussed.\n\n For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n", "original_text": "In the public datasets that are typically used to compare algorithms (from e.g. "}, "hash": "6a301074ee8fd8806f83a072d3181024075397c47c82648e7ffddb32577a29fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ded4503-9e79-47b7-a54f-97914a58bc10", "node_type": "1", "metadata": {"window": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. ", "original_text": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. "}, "hash": "cf45b1ad03394db7acba3fa4348efd7d5ede1f506d1ea6438fdb8c0c7777b5fe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] or [6], which have been used in e.g. ", "mimetype": "text/plain", "start_char_idx": 10771, "end_char_idx": 10813, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ded4503-9e79-47b7-a54f-97914a58bc10", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. ", "original_text": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5f8f8a6-205d-441a-b89a-532598546829", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, methods based on density or likelihood such as DET will determine outlierness according to how uncommon a combination of values would be, which in a multi-modal distribution would give a more anomalous value to observations that cluster near one of the non-majority modes compared to those that cluster near the largest mode, while methods based on local outlierness or neighborhoods such as iNNE from [2] would not deem observations near minority modes any less anomalous than those near majority modes.  For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2. ", "original_text": "[18] or [6], which have been used in e.g. "}, "hash": "d9c4390aad3f2da664c196b4ab42fea46dfd76ee965fd53a19d2f8359ab8e288", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61acc5dd-8dec-4985-977b-3032cdea329e", "node_type": "1", "metadata": {"window": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n", "original_text": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. "}, "hash": "6ea83b57395b8910f89936ce70b2efee5d2f0e7d46bf5997f2f5798233e96ef1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. ", "mimetype": "text/plain", "start_char_idx": 10813, "end_char_idx": 10948, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61acc5dd-8dec-4985-977b-3032cdea329e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n", "original_text": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ded4503-9e79-47b7-a54f-97914a58bc10", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For outliers such as those in the \"Satellite\" dataset\u00b9 for example, the latter might\n\n\u00b9http://odds.cs.stonybrook.edu/satellite-dataset/\n\nbe a disadvantage, while for outliers in the \"ForestCover\" dataset\u00b2 such a property would be desirable.\n\n This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. ", "original_text": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1. "}, "hash": "0508337f964af3d489122ff6bc1dae446b954e6d193839f0851fb00991ed61ca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c93b47b7-f8e3-434c-b5d4-8f48c3246645", "node_type": "1", "metadata": {"window": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3. ", "original_text": "These are in general the easiest datasets to simulate. "}, "hash": "dd00827cd2398019c696d9b49ae62c5ae1d2fdbbb32fcdfb19c9d58a8d4cdb30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. ", "mimetype": "text/plain", "start_char_idx": 10948, "end_char_idx": 11082, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c93b47b7-f8e3-434c-b5d4-8f48c3246645", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3. ", "original_text": "These are in general the easiest datasets to simulate. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61acc5dd-8dec-4985-977b-3032cdea329e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This work shows that there is no \"silver bullet\" and suggests that different methods should be employed for different outlier types or desired anomalous patterns to catch.\n\n While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n", "original_text": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong. "}, "hash": "625124647092c31afec3c47e635b6733cfcb97d26b5c8ff1056bec940307db49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d77063e7-cc06-4bb6-8630-9ce9b8043ebf", "node_type": "1", "metadata": {"window": "In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. ", "original_text": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n"}, "hash": "189d1892470f30b2e280094d0063b57533f33fd11b4fb6d6901ac5d6e336f900", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These are in general the easiest datasets to simulate. ", "mimetype": "text/plain", "start_char_idx": 11082, "end_char_idx": 11137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d77063e7-cc06-4bb6-8630-9ce9b8043ebf", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. ", "original_text": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c93b47b7-f8e3-434c-b5d4-8f48c3246645", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While many previous works have tried to make a distinction between \"global\" and \"local\" outliers, there are other perhaps more important aspects which could be used to categorize them.  In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3. ", "original_text": "These are in general the easiest datasets to simulate. "}, "hash": "ad4d735997a863821acf0407b676538d6588574996678e9ec91ef1083a7cbd96", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91963239-a45f-4f1a-8294-faba51b14dd5", "node_type": "1", "metadata": {"window": "[18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n", "original_text": "2. "}, "hash": "ba327c308809ed287d82f1609d71ce57c606a9d445bccd8a5a2375ac9cb2f4c6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n", "mimetype": "text/plain", "start_char_idx": 11137, "end_char_idx": 11214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91963239-a45f-4f1a-8294-faba51b14dd5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n", "original_text": "2. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d77063e7-cc06-4bb6-8630-9ce9b8043ebf", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In the public datasets that are typically used to compare algorithms (from e.g.  [18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. ", "original_text": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n"}, "hash": "08cc78655855a5a1efecee5a18981d3334a3b654d2cb4bc0c516b6207a63168f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "86615fab-10cb-423c-a7fa-79db12eb4cdd", "node_type": "1", "metadata": {"window": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. ", "original_text": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. "}, "hash": "c787eb24f657735789bb2cb527b8ff161772203638f051582511aff3a696ce22", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "2. ", "mimetype": "text/plain", "start_char_idx": 11214, "end_char_idx": 11217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "86615fab-10cb-423c-a7fa-79db12eb4cdd", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. ", "original_text": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91963239-a45f-4f1a-8294-faba51b14dd5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[18] or [6], which have been used in e.g.  [14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n", "original_text": "2. "}, "hash": "09d64b33c24455859b3b36fdb2d69944fdf93e884b7e7d7a3d6a85339b7d351e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dab8a834-b633-4484-a26e-9aae31ecf662", "node_type": "1", "metadata": {"window": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. ", "original_text": "Datasets such as \"ALOI\" belong to this group.\n"}, "hash": "3fde0d380be134069ff48b70ec2c7092e795b389ecafdffc1746dc48e2bcb003", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. ", "mimetype": "text/plain", "start_char_idx": 11217, "end_char_idx": 11398, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dab8a834-b633-4484-a26e-9aae31ecf662", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. ", "original_text": "Datasets such as \"ALOI\" belong to this group.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86615fab-10cb-423c-a7fa-79db12eb4cdd", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[14], [15], [5], [10], and many others), one could also make a distinction according to the data generation process - for example:\n\n1.  Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. ", "original_text": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes. "}, "hash": "245268b89e634389c0a2cf82c925fa778137adf47a596e361e24e81cf186a673", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "290ffc06-2599-4d3b-9c74-34f4fb582c51", "node_type": "1", "metadata": {"window": "These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. ", "original_text": "3. "}, "hash": "82cd5f0932d8abdcf1b446c2004bd874ee136efe28b8961e29e3f6ac2c923f2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Datasets such as \"ALOI\" belong to this group.\n", "mimetype": "text/plain", "start_char_idx": 11398, "end_char_idx": 11444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "290ffc06-2599-4d3b-9c74-34f4fb582c51", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. ", "original_text": "3. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dab8a834-b633-4484-a26e-9aae31ecf662", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Imbalanced binary classification, in which anomalies are all from a different class to which less than 1% of the observations belong.  These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. ", "original_text": "Datasets such as \"ALOI\" belong to this group.\n"}, "hash": "d3805341833864f35e9c3cee44e289d01c97b33eb7808d3dd61d91123a78db4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9677d444-b892-406c-a1c9-735db4dad18d", "node_type": "1", "metadata": {"window": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n", "original_text": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. "}, "hash": "86a9f0c7574f8cae578b8329f934bc14dd714215647ebc377b6f5034a0d19d71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "3. ", "mimetype": "text/plain", "start_char_idx": 11444, "end_char_idx": 11447, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9677d444-b892-406c-a1c9-735db4dad18d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n", "original_text": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "290ffc06-2599-4d3b-9c74-34f4fb582c51", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These are in general the easiest datasets to simulate.  Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. ", "original_text": "3. "}, "hash": "8251fa984654f29ee0e2bc7fdcec6a52d3438710490b37070ce6a6d42f979949", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94998f53-e694-4e4e-9feb-c655b811f654", "node_type": "1", "metadata": {"window": "2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g. ", "original_text": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n"}, "hash": "2ecc5c30c05f0dde7c5b7bd6bb5853ac574fe54f76facdc56824738becfd3fd2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. ", "mimetype": "text/plain", "start_char_idx": 11447, "end_char_idx": 11728, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94998f53-e694-4e4e-9feb-c655b811f654", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g. ", "original_text": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9677d444-b892-406c-a1c9-735db4dad18d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Datasets such as as \"Pima\", \"Forest Cover\" and \"Mnist\" belong to this group.\n 2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n", "original_text": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes. "}, "hash": "0b1ce0c6968b9aa157d80e573d26b22b2574b10069335894da4ed83243e59405", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89f43d4b-c9c7-487e-a9b3-97b9c257b1c0", "node_type": "1", "metadata": {"window": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. ", "original_text": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. "}, "hash": "6fd75ba791620dc2605e968cc470d8235985bd3bd5579d6de416fd38f578e171", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n", "mimetype": "text/plain", "start_char_idx": 11728, "end_char_idx": 11812, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "89f43d4b-c9c7-487e-a9b3-97b9c257b1c0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. ", "original_text": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94998f53-e694-4e4e-9feb-c655b811f654", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "2.  Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g. ", "original_text": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n"}, "hash": "5820e8249ec049215cb5f9cae88110382b152f282f92ca377a97b92ed7ab7cb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24fc12be-8872-4197-b216-3147e53cd261", "node_type": "1", "metadata": {"window": "Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n", "original_text": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. "}, "hash": "b0876839dae27ae329de3159c218c17d4dd212149995c55734d82eb3854a31e6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. ", "mimetype": "text/plain", "start_char_idx": 11812, "end_char_idx": 11921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "24fc12be-8872-4197-b216-3147e53cd261", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n", "original_text": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89f43d4b-c9c7-487e-a9b3-97b9c257b1c0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Rare and \"scattered\" outliers, typically from multi-class classification, in which very few points belonging to different minority classes are added to a majority class or classes.  Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. ", "original_text": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g. "}, "hash": "a8a86bb4dca3f5b7b5c9c61ceb99bc82b153f0880f764b878057f2ae896b7ad3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0472a2c9-915c-4f69-8183-493a90061f56", "node_type": "1", "metadata": {"window": "3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n", "original_text": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. "}, "hash": "4b42bd587a09313785cb79c158f52c7996b43b7844ad4ea03e7f05ae798b5261", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. ", "mimetype": "text/plain", "start_char_idx": 11921, "end_char_idx": 12192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0472a2c9-915c-4f69-8183-493a90061f56", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n", "original_text": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24fc12be-8872-4197-b216-3147e53cd261", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Datasets such as \"ALOI\" belong to this group.\n 3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n", "original_text": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes. "}, "hash": "90e30cda3378048220b42100515c67c61564575d0e7bdc33a171cd98732721ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8bc0835-4fda-4d02-8c28-18f644d8e864", "node_type": "1", "metadata": {"window": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). ", "original_text": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n"}, "hash": "ff57801e0732e2f1e4015e2da87916aeef921e69aa21c604be511811560a1c80", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. ", "mimetype": "text/plain", "start_char_idx": 12192, "end_char_idx": 12300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e8bc0835-4fda-4d02-8c28-18f644d8e864", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). ", "original_text": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0472a2c9-915c-4f69-8183-493a90061f56", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "3.  Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n", "original_text": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers. "}, "hash": "5a69c1da750c688fd3c87d9ed71bfc4e73de842f53a94bf3cf6def513f88a546", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5f147e3-6926-4218-96ba-d4585a2b67ae", "node_type": "1", "metadata": {"window": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n", "original_text": "When looked from this perspective, it becomes logical to suspect that e.g. "}, "hash": "0239788bf2ff75676079dfb70dd299776a714234a082cb77ef43e212d1a51e62", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n", "mimetype": "text/plain", "start_char_idx": 12300, "end_char_idx": 12537, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b5f147e3-6926-4218-96ba-d4585a2b67ae", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n", "original_text": "When looked from this perspective, it becomes logical to suspect that e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8bc0835-4fda-4d02-8c28-18f644d8e864", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Multi-label classification in which minority classes are grouped together as outliers, typically representing a much larger percentage of the total than in the imbalanced classification scenario, and the non-anomalous cases sometimes being a mixture of different majority classes.  Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). ", "original_text": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n"}, "hash": "ae5f34ebd5d6142b864039eb715621206911b6a5fe4fe5dbfbf436a041390de8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "daddaef1-f359-4286-b211-2345743076e7", "node_type": "1", "metadata": {"window": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n", "original_text": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. "}, "hash": "08e2ba45ccfe05c494413a2fd09baf548f287d3fb2602abd2eb9d78d6ff39723", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When looked from this perspective, it becomes logical to suspect that e.g. ", "mimetype": "text/plain", "start_char_idx": 12537, "end_char_idx": 12612, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "daddaef1-f359-4286-b211-2345743076e7", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n", "original_text": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5f147e3-6926-4218-96ba-d4585a2b67ae", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Datasets such as \"Satellite\", \"Arrhythmia\" and \"Annthy-roid\" belong to this group.\n\n This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n", "original_text": "When looked from this perspective, it becomes logical to suspect that e.g. "}, "hash": "1a960637e2067a74187a82fd80262a5f8643888a94a3b31f0a2e5d43b60b7cc8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c497a205-2c12-4de3-b4d2-865b13806fac", "node_type": "1", "metadata": {"window": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. ", "original_text": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n"}, "hash": "7e817f1b95063651292ced63b8e9c8132370fc8f28eb129a06da9c16d7cec06f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. ", "mimetype": "text/plain", "start_char_idx": 12612, "end_char_idx": 12817, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c497a205-2c12-4de3-b4d2-865b13806fac", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. ", "original_text": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "daddaef1-f359-4286-b211-2345743076e7", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This is not by any means a comprehensive categorization (one could also make a distinction according to e.g.  variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n", "original_text": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group. "}, "hash": "5d08f528d09c27f84445760b1a04c22cfa5c709975e85e5ca2aecc317f56733c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "94af2588-a02f-4560-9603-68cd455f5773", "node_type": "1", "metadata": {"window": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n", "original_text": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n"}, "hash": "e8858c0298574a2b2fcbe42db3a44e0f08948b87bb260abd648f0467bf5bfe2f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n", "mimetype": "text/plain", "start_char_idx": 12817, "end_char_idx": 13057, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "94af2588-a02f-4560-9603-68cd455f5773", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n", "original_text": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c497a205-2c12-4de3-b4d2-865b13806fac", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "variable distributions), and not all datasets or outlier types belong to any of these groups - for instance, the \"Pendigits\" dataset is a multi-label classification scenario in which outliers are a single down-sampled class while inliers are the non-downsampled classes.  Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. ", "original_text": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n"}, "hash": "18512ac553eac7f1dea5be040dc38132284c419b108258fc26af413b5dfd3740", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c51f957c-f3cc-41d2-b36e-01197f10a15d", "node_type": "1", "metadata": {"window": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. ", "original_text": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). "}, "hash": "0ae9718bda4ef4f1d966fa9d4a3b20b3307b2682efd4111e23b2b9d2860b5aad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n", "mimetype": "text/plain", "start_char_idx": 13057, "end_char_idx": 14336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c51f957c-f3cc-41d2-b36e-01197f10a15d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. ", "original_text": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94af2588-a02f-4560-9603-68cd455f5773", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Other datasets such as \"SpamBase\u201d could be considered a mixture of minority-classes and scattered outliers.  Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n", "original_text": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n"}, "hash": "a99298be72de0e6be7337c60f328924faefb391c3a9a613f6ebb705281933920", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79c39c7a-cfb4-4fb4-9f27-20485fb5b32b", "node_type": "1", "metadata": {"window": "When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. ", "original_text": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n"}, "hash": "e54f360cac98ef6308d45427d96daaeb6212357708d5ef3414a72cdd511d83a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). ", "mimetype": "text/plain", "start_char_idx": 14336, "end_char_idx": 14581, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "79c39c7a-cfb4-4fb4-9f27-20485fb5b32b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. ", "original_text": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c51f957c-f3cc-41d2-b36e-01197f10a15d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Practical applications with non-artificial outliers are likely to encompass a mixture of different such scenarios, with areas like fraud detection being likely more similar to the \"SpamBase\" case than to the \"ForestCover\u201d, for example.\n\n When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. ", "original_text": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset). "}, "hash": "f47cbd7210b227bfce375b9a0ee6a839aee27f21163a40ed20b5c3a5f346e3d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6f962a1-1163-4793-9551-858fd5e9a21e", "node_type": "1", "metadata": {"window": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. ", "original_text": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n"}, "hash": "b8368aa8490f23431dac7a4c9064753204693c9b006704732f725fd730b616f3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n", "mimetype": "text/plain", "start_char_idx": 14581, "end_char_idx": 14766, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f6f962a1-1163-4793-9551-858fd5e9a21e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. ", "original_text": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79c39c7a-cfb4-4fb4-9f27-20485fb5b32b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When looked from this perspective, it becomes logical to suspect that e.g.  highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. ", "original_text": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n"}, "hash": "7e39454986915e63447e7356e510cb3692d059decd8797d32762946bc895e7ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f164efd0-bda5-4f61-b599-ac3bce2ecd2f", "node_type": "1", "metadata": {"window": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n", "original_text": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. "}, "hash": "1c34c36a8cfc245225aa086262fd8ecc115329066665e6a1ebe961e46757353c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n", "mimetype": "text/plain", "start_char_idx": 14766, "end_char_idx": 15079, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f164efd0-bda5-4f61-b599-ac3bce2ecd2f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n", "original_text": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6f962a1-1163-4793-9551-858fd5e9a21e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "highly-local methods might be very suitable for the second group, but not for the third group, while not-so-granular methods such as the linear models from [19] could be very suitable for the first group.  The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. ", "original_text": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n"}, "hash": "d2821cbee4e3f4ac4e8c8952ed422fca1ba17563285b104f9417ec0e2b99fc8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f81b472a-b8b8-412f-8eb5-cf9d342e1a2f", "node_type": "1", "metadata": {"window": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. ", "original_text": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n"}, "hash": "a1a4c4d8d4baecd35fbebda456fb67a7f3fd3684ab7a98b956a873fd8992eacb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. ", "mimetype": "text/plain", "start_char_idx": 15079, "end_char_idx": 15426, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f81b472a-b8b8-412f-8eb5-cf9d342e1a2f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. ", "original_text": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f164efd0-bda5-4f61-b599-ac3bce2ecd2f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The first and third group could both be considered to be \"clustered\" outliers, and both are likely to be the cases of utmost interest in different types of applications, but methods that do well for one might not do so well for the other.\n\n If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n", "original_text": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split. "}, "hash": "0dc084a78d72271985fd89b58d03e54ac61b62f438ffa7d371872f3c8bc59f9d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "081eab5c-ccb1-4a51-bf50-a61e8c76947e", "node_type": "1", "metadata": {"window": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes. ", "original_text": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. "}, "hash": "19ad0fa252be4dbd06180af563bf13883245e508b6be1afecff759251ae6ea25", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n", "mimetype": "text/plain", "start_char_idx": 15426, "end_char_idx": 16095, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "081eab5c-ccb1-4a51-bf50-a61e8c76947e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes. ", "original_text": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f81b472a-b8b8-412f-8eb5-cf9d342e1a2f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "If taking IFOREST as a base, a quick experiment would show that different ways of choosing the split threshold or the variable to split would present visible trade-offs across datasets among others, one could think of using kurtosis as a way to choose variables in order to catch extreme-valued outliers (in this experiment, by picking columns with a probability proportional to their kurtosis), or use an information gain criterion to determine the split threshold in the chosen variable similarly to [7] (more on this method to follow in the next sections):\n\n\u00b2http://odds.cs.stonybrook.edu/forestcovercovertype-dataset/\n\n| Table 1: iForest with different feature and split selection strategies | | | |\n| :--- | :---: | :---: | :---: |\n| | **Area under the ROC curve** | | |\n| **Guiding heuristic** | **Satellite** | **Annthyroid** | **Pendigits** |\n| Uniformly random | 0.718 | 0.827 | 0.957 |\n| Kurtosis | 0.711 | 0.979 | 0.945 |\n| Pooled gain | 0.857 | 0.829 | 0.948 |\n\nBoth of which would lead to improved outlier detection in one dataset but not in the other two, thus making it impossible to say that one configuration is better than the others as a general outlier detector, but suggesting that a single universal outlier detector might not be the best approach either.\n\n The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. ", "original_text": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n"}, "hash": "db33ff26a7b45fddf6f8571db8777775a2b8de361b21c070936a512fd78ec5c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a27b4f9-ad61-48d4-86b3-400a295e6739", "node_type": "1", "metadata": {"window": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes. ", "original_text": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. "}, "hash": "29f406da36348accd675318f9b4b482c9e183da7271c0710bd9ec441af0a3725", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. ", "mimetype": "text/plain", "start_char_idx": 16095, "end_char_idx": 16374, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3a27b4f9-ad61-48d4-86b3-400a295e6739", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes. ", "original_text": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "081eab5c-ccb1-4a51-bf50-a61e8c76947e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The overall hardest classes of outliers are perhaps those in which variable distributions are multi-modal and outliers are \"clustered\" around many different minority modes of different combinations of variables (such as the \"Satellite\u201ddataset).  These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes. ", "original_text": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values. "}, "hash": "243b4815d30525fb44d860761977a288eb87d967a6405841702fbd015dc47293", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77bf21bc-c2f8-4e40-a7cc-9309906d5929", "node_type": "1", "metadata": {"window": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node. ", "original_text": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. "}, "hash": "7457be46c0c31853e966b972960dba32146c7c87af9799062b507bfdf9dc472e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. ", "mimetype": "text/plain", "start_char_idx": 16374, "end_char_idx": 16690, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77bf21bc-c2f8-4e40-a7cc-9309906d5929", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node. ", "original_text": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a27b4f9-ad61-48d4-86b3-400a295e6739", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These are coincidentally the cases in which IFOREST tends to outperform other families of methods, and will be the focus of interest here, at the expense of other classes of outliers.\n\n ## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes. ", "original_text": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same. "}, "hash": "222e6a830148231c9ed04b944e240e74621315a979c0ed4583fe891df2e0a63f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cee8c2e2-92dc-44c8-bfd3-677499bc464f", "node_type": "1", "metadata": {"window": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes.", "original_text": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n"}, "hash": "9be83b70f181ed7818f8ac6fc1704720b3801dde307845894b0920a363792935", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. ", "mimetype": "text/plain", "start_char_idx": 16690, "end_char_idx": 16981, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cee8c2e2-92dc-44c8-bfd3-677499bc464f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes.", "original_text": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77bf21bc-c2f8-4e40-a7cc-9309906d5929", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 4 Non-uniformly-random trees\n\nDET and SCIFOREST both suggest that carefully-chosen split points could result in improved outlier detectors compared to uniformly-random splits, but the metrics they aim at maximizing are quite different and lead to very dissimilar types of data partitions and tree structures.\n\n The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node. ", "original_text": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points. "}, "hash": "e17ef6dbdf439b2ca253e6b4d161a5cbab8d11b6c9f64ecb997df8c49a561384", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d4051870-cb17-4ce3-a536-77733a66497c", "node_type": "1", "metadata": {"window": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n", "original_text": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. "}, "hash": "1914fc986890503fe4927f89fd723179ad5cd1452ff28e59085849c6098aa2ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n", "mimetype": "text/plain", "start_char_idx": 16981, "end_char_idx": 17141, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d4051870-cb17-4ce3-a536-77733a66497c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n", "original_text": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cee8c2e2-92dc-44c8-bfd3-677499bc464f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The SCIFOREST model, looked from a slightly different perspective, will at each step first generate a random linear combination of a few variables (suggestion was two variables at a time), and then look for the split threshold that minimizes the average of the standard deviation of this linear combination of variables in each side of the split.  For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes.", "original_text": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n"}, "hash": "435e4a57562915b9456a366b0f7b5406f3bbd2bfb1749b652fbda72a9d58709b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b60a895-a158-4021-a8aa-dd089df532ed", "node_type": "1", "metadata": {"window": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. ", "original_text": "A root node labeled '1' splits into two child nodes. "}, "hash": "95a2867608876a5a8ef5d0b4c88ac7122175553f74bdbb901710b011554e54b2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. ", "mimetype": "text/plain", "start_char_idx": 17141, "end_char_idx": 17278, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b60a895-a158-4021-a8aa-dd089df532ed", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. ", "original_text": "A root node labeled '1' splits into two child nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d4051870-cb17-4ce3-a536-77733a66497c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For a better-quality partition, [15] suggests trying many different linear combinations (suggestion is 10) at each step, and standardizing these numbers to make them comparable, proposing a \"gain\" criterion in a similar spirit as in supervised decision tree algorithms:\n\ngain_avg = (\u03c3_all - (\u03c3_left + \u03c3_right)/2) / \u03c3_all\n\nWhere \u03c3_left and \u03c3_right are the standard deviations observed in the distribution of the randomly-generated linear combination among the points that go to each side of the comparison threshold (\"less than\" and \"greater than\") set upon this same linear combination, and \u03c3_all is the standard deviation among these points before partitioning them.\n\n This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n", "original_text": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree. "}, "hash": "357e7a7cc384445043f839c7194e3e3fead537afb857057b8b19a7222450ba7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "14ee8ee6-7c0a-4753-9574-8b5be7364208", "node_type": "1", "metadata": {"window": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\". ", "original_text": "The left child, labeled '2', splits into two leaf nodes. "}, "hash": "39cbb0d3215a5fb7b005f20db4329a1c49bc6a1ec4ac348d28805828f742372f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A root node labeled '1' splits into two child nodes. ", "mimetype": "text/plain", "start_char_idx": 17278, "end_char_idx": 17331, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "14ee8ee6-7c0a-4753-9574-8b5be7364208", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\". ", "original_text": "The left child, labeled '2', splits into two leaf nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b60a895-a158-4021-a8aa-dd089df532ed", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This split criterion will tend to favor partitions in which only one or a few points are put into a single branch while the majority go to the other, leading to quick isolation of extreme-valued points but perhaps generating insufficient discrimination among non-extreme values.  As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. ", "original_text": "A root node labeled '1' splits into two child nodes. "}, "hash": "7af6527db347582b937cbb320aeb2f5d7fc0407151245b216a693b3f4fef74ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56005f29-654a-465e-b73a-56811f6b85ba", "node_type": "1", "metadata": {"window": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n", "original_text": "The right child is a leaf node. "}, "hash": "5689998efb85680e47c3586b32f29d23244f13c6c0e791ce177a2c0b23feedb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left child, labeled '2', splits into two leaf nodes. ", "mimetype": "text/plain", "start_char_idx": 17331, "end_char_idx": 17388, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "56005f29-654a-465e-b73a-56811f6b85ba", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n", "original_text": "The right child is a leaf node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "14ee8ee6-7c0a-4753-9574-8b5be7364208", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "As many points quickly start becoming isolated in earlier splits, it takes relatively fewer splits to reach the same termination criterion used by \"iForest\", resulting in smaller trees (containing fewer nodes) and thus in principle a computationally-efficient procedure if the termination criteria remains the same.  It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\". ", "original_text": "The left child, labeled '2', splits into two leaf nodes. "}, "hash": "ed8507de3c45e22990d6b7bfa7d23518b686a7285251378823017ebdfc797b41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "046a842d-477d-4634-b96c-827060489636", "node_type": "1", "metadata": {"window": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes. ", "original_text": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes."}, "hash": "8d31f4dae1118a9586f25b4d4e63e6d550f5e25e08e2f0f42ea625574c716ee6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right child is a leaf node. ", "mimetype": "text/plain", "start_char_idx": 17388, "end_char_idx": 17420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "046a842d-477d-4634-b96c-827060489636", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes. ", "original_text": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56005f29-654a-465e-b73a-56811f6b85ba", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It should be noted however that the expected isolation depth for uniformly-random data in this case would not be the same as for the simpler IFOREST, but would rather be a usually larger number, given by:\n\nE[d(m)] = (\u03a3_{i=1}^{m} i) - 1 / m\n\nWhere m is the number of uniformly-random points.  This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n", "original_text": "The right child is a leaf node. "}, "hash": "d0d21b43862abcf0dea785b48052bef4d7c86dc47a128d323bb9ff5506e4d7e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb8c1b07-52c0-4fdc-8d5a-8a421c58ddbf", "node_type": "1", "metadata": {"window": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772.", "original_text": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n"}, "hash": "69ba88b9473c7d7b1fe00eae37997d0e3ab3d51bdf947275c138d2780cc6e9f4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes.", "mimetype": "text/plain", "start_char_idx": 17420, "end_char_idx": 17530, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bb8c1b07-52c0-4fdc-8d5a-8a421c58ddbf", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772.", "original_text": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "046a842d-477d-4634-b96c-827060489636", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This comes from the fact that the optimal split for uniformly-random data corresponds to leaving a single observation in one branch and the rest in the other.\n\n ***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes. ", "original_text": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes."}, "hash": "fa22b50e69c9cea84cfc05114d169f6ef7d1493b6a9909925d1db8be92779344", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f37b585f-52d4-4b00-9292-dd706f0ecd80", "node_type": "1", "metadata": {"window": "A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n", "original_text": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. "}, "hash": "836e623291b24ac16e90cbc294fdf96dd62ffa48482d72e72595a8f8cdac5758", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n", "mimetype": "text/plain", "start_char_idx": 17530, "end_char_idx": 17821, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f37b585f-52d4-4b00-9292-dd706f0ecd80", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n", "original_text": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb8c1b07-52c0-4fdc-8d5a-8a421c58ddbf", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "***\n**Figure 1: Expected isolation depth for 4 uniform points under averaged gain**\n\n*Description: A diagram shows a simple binary tree.  A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772.", "original_text": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n"}, "hash": "411ab57460a4dbf20b2daed997c6e54436d3176ee962d2bdbdfdefa1890e6162", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2142766-3cad-4ac4-9845-62e9672d6a44", "node_type": "1", "metadata": {"window": "The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. ", "original_text": "\"Extrapolated isolation depth\". "}, "hash": "6e87d49adc46ed21ba5223d219812b37c387ff73515c1470ec06eefd0b82133a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. ", "mimetype": "text/plain", "start_char_idx": 17821, "end_char_idx": 17887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b2142766-3cad-4ac4-9845-62e9672d6a44", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. ", "original_text": "\"Extrapolated isolation depth\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f37b585f-52d4-4b00-9292-dd706f0ecd80", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A root node labeled '1' splits into two child nodes.  The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n", "original_text": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs. "}, "hash": "55667450f2bb360a02a64156b42d1ee1c3de7a0263f1e9b5fbfb753a68420515", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30ca9e2c-2bf9-4268-ae91-f9f4206d634a", "node_type": "1", "metadata": {"window": "The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score.", "original_text": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n"}, "hash": "7f7e21075b5a88829057f269c62690d806be191b0f2d55bf916dc6c309c97766", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Extrapolated isolation depth\". ", "mimetype": "text/plain", "start_char_idx": 17887, "end_char_idx": 17919, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "30ca9e2c-2bf9-4268-ae91-f9f4206d634a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score.", "original_text": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2142766-3cad-4ac4-9845-62e9672d6a44", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left child, labeled '2', splits into two leaf nodes.  The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. ", "original_text": "\"Extrapolated isolation depth\". "}, "hash": "a87f1dba4e6d61387d9e53bea7c2ed44e760f814584bab589f89be0f2c87078b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2d0fff3f-c50d-4a78-bed6-66b20296e5f0", "node_type": "1", "metadata": {"window": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n", "original_text": "The right plot, titled \"SCiForest\", shows the same axes. "}, "hash": "06edfe6ea70d8b664daf2f1e05319e10559c6723405502d05387ef9316472c32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n", "mimetype": "text/plain", "start_char_idx": 17919, "end_char_idx": 17995, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2d0fff3f-c50d-4a78-bed6-66b20296e5f0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n", "original_text": "The right plot, titled \"SCiForest\", shows the same axes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30ca9e2c-2bf9-4268-ae91-f9f4206d634a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right child is a leaf node.  The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score.", "original_text": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n"}, "hash": "b2a5e0f91a5801b5f68ad3fd15c179add63d1cc890fff84f38a77cff8acbfc0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5e98135e-12b7-4053-853e-e3730ac73169", "node_type": "1", "metadata": {"window": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". ", "original_text": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772."}, "hash": "ce915d406adac39d2637cc6a866617e82f62f0a2997f3833e68fb3a6eb0f7242", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot, titled \"SCiForest\", shows the same axes. ", "mimetype": "text/plain", "start_char_idx": 17995, "end_char_idx": 18052, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5e98135e-12b7-4053-853e-e3730ac73169", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". ", "original_text": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2d0fff3f-c50d-4a78-bed6-66b20296e5f0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The calculation `E[d(4)] = (1+2+3+3)/4 = 2.25` is displayed, indicating the average depth of the 4 leaf nodes. *\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n", "original_text": "The right plot, titled \"SCiForest\", shows the same axes. "}, "hash": "7560a8440c8af8ff4a173803d2d7a460a18d30ce6ff3c16773217944d7fdf233", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73a0b216-25ba-47a3-b53d-a2f310b2071f", "node_type": "1", "metadata": {"window": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. ", "original_text": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n"}, "hash": "c3f6ade73665a35b8420a8f155b513b1e6e4ddedc4b02e961bbbe3d7fb30e5f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772.", "mimetype": "text/plain", "start_char_idx": 18052, "end_char_idx": 18158, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "73a0b216-25ba-47a3-b53d-a2f310b2071f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. ", "original_text": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e98135e-12b7-4053-853e-e3730ac73169", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nA small experiment with the \"Satellite\" dataset reveals that the expected isolation depth from IFOREST might indeed not be as appropriate for SCIFOREST:\n\n***\n**Figure: Real vs extrapolated isolation depth (Satellite dataset)**\n\n*Description: Two scatter plots are shown side-by-side.\n The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". ", "original_text": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772."}, "hash": "b7fe65c03ec57b99985165ded145d41ebb30d8399baa96e562ac60b79b37356f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab4fbcf9-6419-4951-974f-e8159efa88f5", "node_type": "1", "metadata": {"window": "\"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n", "original_text": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. "}, "hash": "baebfcd6db76209ce7b5dd594af33f8e2ffd37ff96f974af476f7568d9de81b0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n", "mimetype": "text/plain", "start_char_idx": 18158, "end_char_idx": 18281, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ab4fbcf9-6419-4951-974f-e8159efa88f5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n", "original_text": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73a0b216-25ba-47a3-b53d-a2f310b2071f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left plot, titled \"iForest\", shows \"Full isolation depth\" vs.  \"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. ", "original_text": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n"}, "hash": "cd70af095a93b13dd8316a04e5c4aec1248ee12e0d03b0c5efc35535ab4fcdde", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29d865c9-e2f1-4b40-849e-c93c2b4b45c3", "node_type": "1", "metadata": {"window": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\". ", "original_text": "The redder, the higher the anomaly score."}, "hash": "05441ddaea14c3245e8ed5ac8549e2dfce04892c4365fadf43268aa53115b5c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. ", "mimetype": "text/plain", "start_char_idx": 18281, "end_char_idx": 18733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "29d865c9-e2f1-4b40-849e-c93c2b4b45c3", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\". ", "original_text": "The redder, the higher the anomaly score."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab4fbcf9-6419-4951-974f-e8159efa88f5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Extrapolated isolation depth\".  The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n", "original_text": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible. "}, "hash": "30b1bf8f1b3e91560e2883c01900b84bb89a88a1e2aba8efd0939d31971c54cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4e40c2b-29d3-494b-801e-0f95eec64630", "node_type": "1", "metadata": {"window": "The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them.", "original_text": "**\n\n*Description: Two heatmaps are shown side-by-side.\n"}, "hash": "ea31cd83a2a1d57f519f5a482078169e925d7c578c55ede8815a128f9e8c66ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The redder, the higher the anomaly score.", "mimetype": "text/plain", "start_char_idx": 18733, "end_char_idx": 18774, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4e40c2b-29d3-494b-801e-0f95eec64630", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them.", "original_text": "**\n\n*Description: Two heatmaps are shown side-by-side.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29d865c9-e2f1-4b40-849e-c93c2b4b45c3", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The points form a tight, linear cluster, with a correlation (rho) of 0.958.\n The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\". ", "original_text": "The redder, the higher the anomaly score."}, "hash": "40e3da004427c589c777540f75e041afc3595807474b81dfbde208b7e113f970", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51bcc9fc-5ff9-4fb9-a4de-8aff39638c64", "node_type": "1", "metadata": {"window": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n", "original_text": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". "}, "hash": "1d6623384e4a929d1c2435a75e89d74fef08ba64f85f789cbf9b93d9ad7c9cc4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Description: Two heatmaps are shown side-by-side.\n", "mimetype": "text/plain", "start_char_idx": 18774, "end_char_idx": 18829, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51bcc9fc-5ff9-4fb9-a4de-8aff39638c64", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n", "original_text": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4e40c2b-29d3-494b-801e-0f95eec64630", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right plot, titled \"SCiForest\", shows the same axes.  The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them.", "original_text": "**\n\n*Description: Two heatmaps are shown side-by-side.\n"}, "hash": "41bba3521c44cd71e179e117ca5462018f43d6f67dccaaba6b0882642c515f5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2a816b8-66e1-4490-8d35-cf8c656ff32f", "node_type": "1", "metadata": {"window": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\". ", "original_text": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. "}, "hash": "7323f40b927d614cef9d966c200e211bb59e68c9b3fb3097de7f9feb59ee73e5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". ", "mimetype": "text/plain", "start_char_idx": 18829, "end_char_idx": 18905, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e2a816b8-66e1-4490-8d35-cf8c656ff32f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\". ", "original_text": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51bcc9fc-5ff9-4fb9-a4de-8aff39638c64", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The points are much more scattered, forming a wider, less linear cloud, with a correlation (rho) of 0.772. *\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n", "original_text": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\". "}, "hash": "ded0555fa017d86664cdbfa98a0ab5485cfaafc9f3ae459035067e944e6c4405", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97226d9d-1adc-44a4-b5ac-aaee7d58a168", "node_type": "1", "metadata": {"window": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "There is also a red region between the two modes, the \"ghost region\".\n"}, "hash": "d0f6ab227d4dc8908bd7ef970252d7be7f8094f900e7b1c5ba0bc385a5e2367c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. ", "mimetype": "text/plain", "start_char_idx": 18905, "end_char_idx": 19017, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97226d9d-1adc-44a4-b5ac-aaee7d58a168", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "There is also a red region between the two modes, the \"ghost region\".\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2a816b8-66e1-4490-8d35-cf8c656ff32f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nNevertheless, [15] kept the same threshold and remainder extrapolation as in the original IFOREST in their design.\n\n When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\". ", "original_text": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data. "}, "hash": "4f069d98f567f2e547f3e02c8c8286ead02627d9c1faef2b3f153c319b5b38ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a119524-9798-48e4-a9e1-d30733247a31", "node_type": "1", "metadata": {"window": "The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data. ", "original_text": "The right heatmap is titled \"Full depths\". "}, "hash": "d12e65f2e941b934b69929a2890cbdd4f38fc5e03b2a75b4d3c0e552beaac7b7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "There is also a red region between the two modes, the \"ghost region\".\n", "mimetype": "text/plain", "start_char_idx": 19017, "end_char_idx": 19087, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8a119524-9798-48e4-a9e1-d30733247a31", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data. ", "original_text": "The right heatmap is titled \"Full depths\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97226d9d-1adc-44a4-b5ac-aaee7d58a168", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When data distributions are multi-modal and these modes very separable, this averaged gain criterion can tend to result in quick separation of groups, but at the expense of so-called \"ghost regions\u201d (see [12]), the moreso if a remaining depth is extrapolated instead of running the procedure until full isolation:\n\n***\n**Figure 2: Anomaly scores from SCiForest in randomly-generated bimodal data, with no sub-sampling to make the effects more visible.  The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "There is also a red region between the two modes, the \"ghost region\".\n"}, "hash": "b1d73356a4f3713374a1ffc6162ec4844d23a818ee999824324c15d97c0e0292", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51e3f099-c968-4f50-ad12-b3c507ab6975", "node_type": "1", "metadata": {"window": "**\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n", "original_text": "It is very similar to the left one, showing two red circular regions and a red ghost region between them."}, "hash": "af2839591d1234efb3e1537c152296a19cbf7137583806e2cd2f4c7622e41173", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right heatmap is titled \"Full depths\". ", "mimetype": "text/plain", "start_char_idx": 19087, "end_char_idx": 19130, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51e3f099-c968-4f50-ad12-b3c507ab6975", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n", "original_text": "It is very similar to the left one, showing two red circular regions and a red ghost region between them."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a119524-9798-48e4-a9e1-d30733247a31", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The redder, the higher the anomaly score. **\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data. ", "original_text": "The right heatmap is titled \"Full depths\". "}, "hash": "e520246212e46c58b339765a3ef38c25be045eb17c50b0f2537ceb29ccd891a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8acb77ae-6edf-46d7-bac9-132f0100e185", "node_type": "1", "metadata": {"window": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\". ", "original_text": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n"}, "hash": "4c9af3a0b053cc02bfaadf3c81ef763f21b390960660932a6dab2992a02f943b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It is very similar to the left one, showing two red circular regions and a red ghost region between them.", "mimetype": "text/plain", "start_char_idx": 19130, "end_char_idx": 19235, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8acb77ae-6edf-46d7-bac9-132f0100e185", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\". ", "original_text": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51e3f099-c968-4f50-ad12-b3c507ab6975", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: Two heatmaps are shown side-by-side.\n The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n", "original_text": "It is very similar to the left one, showing two red circular regions and a red ghost region between them."}, "hash": "ecd31cfe671dfa0c938b3454e9b6f67be0344f018b06d34cf8b02bf219b1bc1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5ee7e3d-1507-42a1-aedc-cb50232b4e35", "node_type": "1", "metadata": {"window": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions. ", "original_text": "The top row is titled \"Uniformly-random splits\". "}, "hash": "ebd14d54d6072efe1c08e182f167b6a398452626d4ee8ea119a7c85849eecd1c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n", "mimetype": "text/plain", "start_char_idx": 19235, "end_char_idx": 19585, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c5ee7e3d-1507-42a1-aedc-cb50232b4e35", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions. ", "original_text": "The top row is titled \"Uniformly-random splits\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8acb77ae-6edf-46d7-bac9-132f0100e185", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left heatmap is titled \"SCiForest anomaly scores, Extrapolated depths\".  It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\". ", "original_text": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n"}, "hash": "8525deff908a4159f497eecb434861c10327f55c547cec49e87868af276b45f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "084dd5f6-977d-4823-a9aa-884bff150331", "node_type": "1", "metadata": {"window": "There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent. ", "original_text": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "hash": "6fc92df354aa519593139778dec74e4cd4884b4e8f1cff8afa03d91f69bb7e88", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top row is titled \"Uniformly-random splits\". ", "mimetype": "text/plain", "start_char_idx": 19585, "end_char_idx": 19634, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "084dd5f6-977d-4823-a9aa-884bff150331", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent. ", "original_text": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5ee7e3d-1507-42a1-aedc-cb50232b4e35", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It shows two distinct red (high anomaly score) circular regions corresponding to the modes of the bimodal data.  There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions. ", "original_text": "The top row is titled \"Uniformly-random splits\". "}, "hash": "d4820acb8ef641f2840c6db4def307d130f64af688dcec09a4d6919a51a9c570", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f9195fd-be91-4b03-8f05-7e9f593473cc", "node_type": "1", "metadata": {"window": "The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row.", "original_text": "The heatmaps clearly show the two modes of the bimodal data. "}, "hash": "9ef10bb05de906b7f3014e0b71c504dc8cb572e7bf19c4ba183d62d38497ba64", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "mimetype": "text/plain", "start_char_idx": 19634, "end_char_idx": 19745, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f9195fd-be91-4b03-8f05-7e9f593473cc", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row.", "original_text": "The heatmaps clearly show the two modes of the bimodal data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "084dd5f6-977d-4823-a9aa-884bff150331", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "There is also a red region between the two modes, the \"ghost region\".\n The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent. ", "original_text": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "hash": "3246114fd7c36e3e2260c7e8c59ed14e576d374fb2f70069fa778979cb97cd4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8825f27c-5255-4d62-b7ef-af14e62dc370", "node_type": "1", "metadata": {"window": "It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. ", "original_text": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n"}, "hash": "4b9b72ba975a83d019d126e62baeb54fadb3906d44ddbf9b1735be94f88f6e8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The heatmaps clearly show the two modes of the bimodal data. ", "mimetype": "text/plain", "start_char_idx": 19745, "end_char_idx": 19806, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8825f27c-5255-4d62-b7ef-af14e62dc370", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. ", "original_text": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f9195fd-be91-4b03-8f05-7e9f593473cc", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right heatmap is titled \"Full depths\".  It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row.", "original_text": "The heatmaps clearly show the two modes of the bimodal data. "}, "hash": "3350913af6363229c96e2be1246e3157f2e2a8038e97f837aedd6e36204426bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b127a9aa-b4e9-4c8a-88b3-a244fffe7cd8", "node_type": "1", "metadata": {"window": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. ", "original_text": "The bottom row is titled \"Averaged-gain splits\". "}, "hash": "b9eeabb10be1cb11f6a2960d88a849d8f874ede426531c7789d4dcb4d5799aaf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n", "mimetype": "text/plain", "start_char_idx": 19806, "end_char_idx": 19921, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b127a9aa-b4e9-4c8a-88b3-a244fffe7cd8", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. ", "original_text": "The bottom row is titled \"Averaged-gain splits\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8825f27c-5255-4d62-b7ef-af14e62dc370", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It is very similar to the left one, showing two red circular regions and a red ghost region between them. *\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. ", "original_text": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n"}, "hash": "4fbe00e387dee79bc1e5b46e9c2ac790a5569056d547f8e59891439c5693a78b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e383abae-fbdf-4c2b-8dbb-814685565cd9", "node_type": "1", "metadata": {"window": "The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n", "original_text": "The three heatmaps correspond to the same three conditions. "}, "hash": "d4e9561be1a8d51170ea20f4558ae3ec7546c297a03cc256b07a81d97bbc51f8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom row is titled \"Averaged-gain splits\". ", "mimetype": "text/plain", "start_char_idx": 19921, "end_char_idx": 19970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e383abae-fbdf-4c2b-8dbb-814685565cd9", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n", "original_text": "The three heatmaps correspond to the same three conditions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b127a9aa-b4e9-4c8a-88b3-a244fffe7cd8", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nPerhaps counterintuitively, the splits generated following this criterion seem to be less sensitive to the presence of local outliers in the training data compared to uniformly-random choices:\n\n***\n**Figure 3: Influence of an outlier in the splits generated through different guiding criteria**\n\n*Description: A 2x3 grid of heatmaps is shown.\n The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. ", "original_text": "The bottom row is titled \"Averaged-gain splits\". "}, "hash": "eea12544826d9fceaa139b99f6cb89045c6340dd031bbd0c6b2e68afc5fcc9b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "00bd78d8-0099-4d94-b892-138d0082153c", "node_type": "1", "metadata": {"window": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n", "original_text": "The ghost region between the two main modes is prominent. "}, "hash": "4dccb6163c51b100f93d14745a038004fab322248e1a88e01a4d9f8877c092ae", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The three heatmaps correspond to the same three conditions. ", "mimetype": "text/plain", "start_char_idx": 19970, "end_char_idx": 20030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "00bd78d8-0099-4d94-b892-138d0082153c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n", "original_text": "The ghost region between the two main modes is prominent. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e383abae-fbdf-4c2b-8dbb-814685565cd9", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top row is titled \"Uniformly-random splits\".  The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n", "original_text": "The three heatmaps correspond to the same three conditions. "}, "hash": "47e889f3a0871fac82f232071d7fa00fb40cbc1d270b633babecca0a9fbe9d27", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7fbb34b-6add-4e83-9230-90ffbb9264e6", "node_type": "1", "metadata": {"window": "The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. ", "original_text": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row."}, "hash": "9cd6caa248dcdcd8340e169bf73d9c040152e012de02e285c49c3c2174471ebb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The ghost region between the two main modes is prominent. ", "mimetype": "text/plain", "start_char_idx": 20030, "end_char_idx": 20088, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7fbb34b-6add-4e83-9230-90ffbb9264e6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. ", "original_text": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "00bd78d8-0099-4d94-b892-138d0082153c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The three heatmaps show anomaly scores for \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n", "original_text": "The ghost region between the two main modes is prominent. "}, "hash": "9f85c7bb120e0d1c541d30c2a91d75fcc9c0383e89ef584900b580d8a7c22435", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d864b493-8aa8-4090-af05-0d40a3a82ad8", "node_type": "1", "metadata": {"window": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n", "original_text": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. "}, "hash": "22e3ed8d77f24409a416cf77c0c94098d96157c948cf0c9faab1a60f551ab95e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row.", "mimetype": "text/plain", "start_char_idx": 20088, "end_char_idx": 20207, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d864b493-8aa8-4090-af05-0d40a3a82ad8", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n", "original_text": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7fbb34b-6add-4e83-9230-90ffbb9264e6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps clearly show the two modes of the bimodal data.  When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. ", "original_text": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row."}, "hash": "b0ed47b1d144659e043e2fa6c9a5a72f0aaadd42f5b3b0ac1b831940227841ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "624aaa9c-1503-4d79-91d8-a0489df2258f", "node_type": "1", "metadata": {"window": "The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data.", "original_text": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. "}, "hash": "f0d66a1eb52e91d00f25abe33ad1c8665df8b77f803201dbad413a195d734759", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. ", "mimetype": "text/plain", "start_char_idx": 20207, "end_char_idx": 20484, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "624aaa9c-1503-4d79-91d8-a0489df2258f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data.", "original_text": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d864b493-8aa8-4090-af05-0d40a3a82ad8", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When an outlier is added, its location becomes a small, high-anomaly (red) spot, influencing the scores around it.\n The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n", "original_text": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative. "}, "hash": "df4df0ba8d8d919dc686c4dc8a1f67a8e05214159d4503a583357cdc59187ef1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f42b36c-87c8-4aaf-b856-8e33d3ee1481", "node_type": "1", "metadata": {"window": "The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". ", "original_text": "\"Novelty detection\" is however not the focus of this work.\n\n"}, "hash": "e7a6a41fba7a8fea2862441b2489d62503a79b73195181094c39517afa7aa399", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. ", "mimetype": "text/plain", "start_char_idx": 20484, "end_char_idx": 20740, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f42b36c-87c8-4aaf-b856-8e33d3ee1481", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". ", "original_text": "\"Novelty detection\" is however not the focus of this work.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "624aaa9c-1503-4d79-91d8-a0489df2258f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom row is titled \"Averaged-gain splits\".  The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data.", "original_text": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model. "}, "hash": "8149ccf269c5f13dfbbf28bcc264ef435f8a15ac6f039bead62e1cd32adb48cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d2b383ca-a370-4d43-bb69-f56140824c37", "node_type": "1", "metadata": {"window": "The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n"}, "hash": "ff7b06ee85c00764a342384c2de0c4990f74bc9c561417caf0a6e8bfd747712c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Novelty detection\" is however not the focus of this work.\n\n", "mimetype": "text/plain", "start_char_idx": 20740, "end_char_idx": 20800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d2b383ca-a370-4d43-bb69-f56140824c37", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f42b36c-87c8-4aaf-b856-8e33d3ee1481", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The three heatmaps correspond to the same three conditions.  The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". ", "original_text": "\"Novelty detection\" is however not the focus of this work.\n\n"}, "hash": "cd17f9085b8559083e341af1bbbbc52199a7fc017d7732953dc5762df2851173", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b187d68c-200e-4863-a057-b8d6a61592a6", "node_type": "1", "metadata": {"window": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. ", "original_text": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. "}, "hash": "88b85bae7cc7f80ce0f9d91741a45d0fe607f915b1e2798e4ca00d68c3e72522", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n", "mimetype": "text/plain", "start_char_idx": 20800, "end_char_idx": 21249, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b187d68c-200e-4863-a057-b8d6a61592a6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. ", "original_text": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d2b383ca-a370-4d43-bb69-f56140824c37", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The ghost region between the two main modes is prominent.  The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n"}, "hash": "586da459f14928b25e99cdfc9eaf537dd198f58079d55ad1a1b8a375d1b045d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a14b9e37-901e-4d0e-a82f-73646fb6722a", "node_type": "1", "metadata": {"window": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score. ", "original_text": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n"}, "hash": "3cd251bc21e93df1317a9d7140c7b7ae5d5aa4f5367edce60fc6e78d558bd9f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. ", "mimetype": "text/plain", "start_char_idx": 21249, "end_char_idx": 21721, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a14b9e37-901e-4d0e-a82f-73646fb6722a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score. ", "original_text": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b187d68c-200e-4863-a057-b8d6a61592a6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The presence of an outlier has a less noticeable effect on the overall anomaly score landscape compared to the top row. *\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. ", "original_text": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions. "}, "hash": "941beb21ca9ea720a619d3ea48145b861d0b8792f594d78d3e0337d607c133b2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99972e70-9856-47fb-a6c9-bc3c3f90ed5d", "node_type": "1", "metadata": {"window": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps.", "original_text": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data."}, "hash": "45200416e754bbf895246969b7508e460a0df1ecaae338855c78e63c4747944e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n", "mimetype": "text/plain", "start_char_idx": 21721, "end_char_idx": 22027, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99972e70-9856-47fb-a6c9-bc3c3f90ed5d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps.", "original_text": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a14b9e37-901e-4d0e-a82f-73646fb6722a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nIn this particular example, these ghost regions and the leverage of local outliers are not a problem for either method, but in real larger-dimensional datasets it could result in undesirable patterns if the sub-sample of data that a tree uses is not too representative.  It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score. ", "original_text": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n"}, "hash": "7e5c642026f25850fa5bb9ed86097db2edd0afdfc874566d8d29ef1ecbc6bea9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b65e4d20-0ac0-4e86-a277-f17d625c7967", "node_type": "1", "metadata": {"window": "\"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. ", "original_text": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". "}, "hash": "7764cf05bc58f552635a49b59c7a3e3c07a9debe4f141fb2e3ccff982f4bff4f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data.", "mimetype": "text/plain", "start_char_idx": 22027, "end_char_idx": 22336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b65e4d20-0ac0-4e86-a277-f17d625c7967", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. ", "original_text": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99972e70-9856-47fb-a6c9-bc3c3f90ed5d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It might also pose a problem if a model is to be used for \"novelty detection\" - that is, to identify anomalies in new data to which the model will not be fit, as opposed to identifying outliers within the same that that is available for producing a model.  \"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps.", "original_text": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data."}, "hash": "bf1fba9e95834d8d3b443e68c3f3d0354956c0204cdcb3c1ebeb9257dc26d8fd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "31e5a6ba-f4e6-4537-8265-ef52df525c3f", "node_type": "1", "metadata": {"window": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data.", "original_text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "hash": "9d023060ec83f9e824013801fde4e24f2676e67004ee94a9d9e39cfbd31a9ab9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". ", "mimetype": "text/plain", "start_char_idx": 22336, "end_char_idx": 22420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "31e5a6ba-f4e6-4537-8265-ef52df525c3f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data.", "original_text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b65e4d20-0ac0-4e86-a277-f17d625c7967", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Novelty detection\" is however not the focus of this work.\n\n In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. ", "original_text": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\". "}, "hash": "764c531d6aefe80164024d1eadc74f67ae7cc8dbdc981af9de16d88f920fb009", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b3747471-5d03-4846-9a19-fd204ab8d63b", "node_type": "1", "metadata": {"window": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". ", "original_text": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. "}, "hash": "8e49a3e82153bc5edab110a31bd5f59cc83bc6b1efdb1590d0dc2ce31af86439", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "mimetype": "text/plain", "start_char_idx": 22420, "end_char_idx": 22515, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b3747471-5d03-4846-9a19-fd204ab8d63b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". ", "original_text": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "31e5a6ba-f4e6-4537-8265-ef52df525c3f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In contrast to the gain criterion from SCIFOREST which is set on a random linear combination of a few variables, DET and related models such as OCRF and GIF will look at a larger part or even the entirety of the multivariate distribution of the data when evaluating a partition, and their criteria tend to favor partitions which divide the data more evenly in the earlier branches as opposed to leaving only a few points at one side of a division.\n\n DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data.", "original_text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "hash": "6e92529160ee72ef6b0a76b3eecdaa5e967e1b204fba49e23a0812347fe82d76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8375dd4-9fed-47be-baf9-544cd485561b", "node_type": "1", "metadata": {"window": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "The space between and around them has a uniform high anomaly score. "}, "hash": "0cbe5f7a5dbc7a8f2aaf3daa551e1a90c50465006e8d37590694f926fb3fd126", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. ", "mimetype": "text/plain", "start_char_idx": 22515, "end_char_idx": 22619, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d8375dd4-9fed-47be-baf9-544cd485561b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "The space between and around them has a uniform high anomaly score. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b3747471-5d03-4846-9a19-fd204ab8d63b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "DET\u2019s splitting criterion and scoring metric do not directly relate to the concept of isolation, and its intended use-case in [17] was not exactly anomaly detection either, but as can be seen from their experiments and from the criterion that is maximized, the splits it generates will implicity isolate outlier observations faster (taking fewer splits) as they are assigned to higher-volume regions, which are in turn defined by fewer splits than smaller-volume regions.  In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". ", "original_text": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions. "}, "hash": "2cb4cc6ac83f2520a24ab87137318dd5ef19f05fe26f9d6238a3ff98906dd4ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8d01563-6e3b-4060-b1d2-5d9803bd1b1a", "node_type": "1", "metadata": {"window": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. ", "original_text": "The presence of an outlier does not visibly alter the heatmaps."}, "hash": "59168608b28cb303b91792fe10faf05248f1ac762e7052c8cf23b5cdcc877f4a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The space between and around them has a uniform high anomaly score. ", "mimetype": "text/plain", "start_char_idx": 22619, "end_char_idx": 22687, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b8d01563-6e3b-4060-b1d2-5d9803bd1b1a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. ", "original_text": "The presence of an outlier does not visibly alter the heatmaps."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8375dd4-9fed-47be-baf9-544cd485561b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In [10] they found isolation depth to be a useful scoring metric after generating partitions by a density-based criterion, although the same density score from DET can also be used as an anomaly score (the lower, the more anomalous), thereby avoiding the issue of having to extrapolate a depth remainder.\n\n Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "original_text": "The space between and around them has a uniform high anomaly score. "}, "hash": "0c0b0ea6c323ee089db5f197a8da192924d73c9b3507117ece52f79be7f76a02", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "21231e79-e555-4e5d-9689-d5ecbedef53c", "node_type": "1", "metadata": {"window": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location.", "original_text": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. "}, "hash": "82fe60470b4dbddb86968c80f368cda1011ea9966d0388129b6a0a242503b7d1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The presence of an outlier does not visibly alter the heatmaps.", "mimetype": "text/plain", "start_char_idx": 22687, "end_char_idx": 22750, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "21231e79-e555-4e5d-9689-d5ecbedef53c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location.", "original_text": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8d01563-6e3b-4060-b1d2-5d9803bd1b1a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Just like SCIFOREST, DET also tends to separate easily-divisible multimodal distributions quickly and to be relatively insensitive to the presence of local outliers in the data, but without suffering from the issue of ghost regions:\n\n***\n**Figure 4: Anomaly scores from DET in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. ", "original_text": "The presence of an outlier does not visibly alter the heatmaps."}, "hash": "a49c645ca3891646bafac2eaa2b5c473033f2d14e97b9e7181767471dddf418f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ed2d5a88-a8d9-43ee-97b8-c07d4b37b7d3", "node_type": "1", "metadata": {"window": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n", "original_text": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data."}, "hash": "32265b983622691f30c22a7490c13fb6d3bcb109b4b09445ec3a2a3816f4b786", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. ", "mimetype": "text/plain", "start_char_idx": 22750, "end_char_idx": 22892, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ed2d5a88-a8d9-43ee-97b8-c07d4b37b7d3", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n", "original_text": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "21231e79-e555-4e5d-9689-d5ecbedef53c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: A row of three heatmaps titled \"DET inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location.", "original_text": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST. "}, "hash": "a44909a5c34587fd203042d51a57a3f80877e3a743bed64af97d0d5536eec4c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2cefe806-9d43-493a-86bb-3d858ccbae1d", "node_type": "1", "metadata": {"window": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. ", "original_text": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". "}, "hash": "24712ff854613f98d0ad6932fa7ab02470f305081e10d9cdaf241ac05f22ef87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data.", "mimetype": "text/plain", "start_char_idx": 22892, "end_char_idx": 23231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2cefe806-9d43-493a-86bb-3d858ccbae1d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. ", "original_text": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed2d5a88-a8d9-43ee-97b8-c07d4b37b7d3", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n", "original_text": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data."}, "hash": "2bcfafa98eaad1b9a6fdfad1599e010e12defcf8fc3f905a121f253611d54f40", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0fba958-e2c6-4d80-b13c-e8475f562ca6", "node_type": "1", "metadata": {"window": "The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. ", "original_text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "hash": "b88856e73b7087998a0c1bf222b9da46efa590dfb4e0f5eaab6de1751e51f779", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". ", "mimetype": "text/plain", "start_char_idx": 23231, "end_char_idx": 23323, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c0fba958-e2c6-4d80-b13c-e8475f562ca6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. ", "original_text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2cefe806-9d43-493a-86bb-3d858ccbae1d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The two modes of the bimodal data are shown as distinct high-density (low anomaly) rectangular regions.  The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. ", "original_text": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\". "}, "hash": "f41da01f461d85db467ccf6239c357d6a845a315337ff84c0197f65d4a4cace6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9a55c03f-6fab-4c93-a73e-9d4c1f13562e", "node_type": "1", "metadata": {"window": "The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n", "original_text": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. "}, "hash": "d3251641bab73b7375534f30460a1023358c01699633dbf25369064eb40d9eab", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". ", "mimetype": "text/plain", "start_char_idx": 23323, "end_char_idx": 23418, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9a55c03f-6fab-4c93-a73e-9d4c1f13562e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n", "original_text": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0fba958-e2c6-4d80-b13c-e8475f562ca6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The space between and around them has a uniform high anomaly score.  The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. ", "original_text": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\". "}, "hash": "8b7c9f7733c5e7c1c6d34dda3580426ae30cc1b5b0dc11b9ed77f9a619e13305", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc67e8a2-65a6-44c7-bd74-c6416b314421", "node_type": "1", "metadata": {"window": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. ", "original_text": "The presence of an outlier still does not create a distinct high-anomaly spot at its location."}, "hash": "454f8b21bc0ced6416beb405334250ab805dff68e01f888acf1f9b3926812b73", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. ", "mimetype": "text/plain", "start_char_idx": 23418, "end_char_idx": 23550, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fc67e8a2-65a6-44c7-bd74-c6416b314421", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. ", "original_text": "The presence of an outlier still does not create a distinct high-anomaly spot at its location."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9a55c03f-6fab-4c93-a73e-9d4c1f13562e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The presence of an outlier does not visibly alter the heatmaps. *\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n", "original_text": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging. "}, "hash": "8e68ec7c0055918bf8a2213455c7cd8d5cc6c9b576179a7571f766a91d146b71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22676c21-00ec-4bbb-8d20-7463d9089ba1", "node_type": "1", "metadata": {"window": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) ", "original_text": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n"}, "hash": "bd232e11e500d920787a09d2fc1ede63bdfe8cbecc6d8cfa49bc8281fa15bfa6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The presence of an outlier still does not create a distinct high-anomaly spot at its location.", "mimetype": "text/plain", "start_char_idx": 23550, "end_char_idx": 23644, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "22676c21-00ec-4bbb-8d20-7463d9089ba1", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) ", "original_text": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc67e8a2-65a6-44c7-bd74-c6416b314421", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nUnfortunately, it also tends to produce little discrimination in scores among non-extreme values, perhaps even less so than SCIFOREST.  A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. ", "original_text": "The presence of an outlier still does not create a distinct high-anomaly spot at its location."}, "hash": "ca27c2d9c756a4168e6301b5a549f9ab6110d9fa3ebe40d4258c9f277a67a2db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2ee02db-19b0-44cd-870b-de01ba2a82c7", "node_type": "1", "metadata": {"window": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. ", "original_text": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. "}, "hash": "f9bed77bf74dbfede9b71aa2d394ee9ac77a9f0ee014712603aa2a6f1222779d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n", "mimetype": "text/plain", "start_char_idx": 23644, "end_char_idx": 24016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c2ee02db-19b0-44cd-870b-de01ba2a82c7", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. ", "original_text": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22676c21-00ec-4bbb-8d20-7463d9089ba1", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A logical thought to remedy this issue would be to employ a \"forest\" of DETs with sub-sampled observations just like IFOREST and without pruning the trees as originally proposed, but this does not eliminate the issue either:\n\n***\n**Figure 5: Anomaly scores from an ensemble of DETs with subsampled data, in randomly-generated bimodal data. **\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) ", "original_text": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n"}, "hash": "ea55fe8c61256b420c062b1073a91f5f0a7a06e846bf78ce8552a0b28a0980a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c34baad-c795-44ca-b574-31e2e39d78c2", "node_type": "1", "metadata": {"window": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n", "original_text": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. "}, "hash": "74d37139623939371093b9ca5544eb2d841b6b85b647815fc06fd3cfbbea6722", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. ", "mimetype": "text/plain", "start_char_idx": 24016, "end_char_idx": 24180, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c34baad-c795-44ca-b574-31e2e39d78c2", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n", "original_text": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c2ee02db-19b0-44cd-870b-de01ba2a82c7", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: A row of three heatmaps titled \"DET forest, inverse predicted densities\".  The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. ", "original_text": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods. "}, "hash": "08b035c4a25b7aea19d2198f5c0c584793e3ed2f37c6f6c924ac11f36dae9a0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0af6b2bb-ce13-45c6-bb03-70c3d8323998", "node_type": "1", "metadata": {"window": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. ", "original_text": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n"}, "hash": "07e10dc2762d23edcd52e9eb1f314ad11bb79380da43b48b2400b5b35648af99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. ", "mimetype": "text/plain", "start_char_idx": 24180, "end_char_idx": 24520, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0af6b2bb-ce13-45c6-bb03-70c3d8323998", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. ", "original_text": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c34baad-c795-44ca-b574-31e2e39d78c2", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps correspond to \"No outlier\", \"Outlier at top-left\", and \"Outlier at bottom-right\".  Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n", "original_text": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically. "}, "hash": "ab0569b8c40ee70b4d803fb4ec4754fb3014f961b6c7fe8b793fb24a4a5de8e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "309d58a8-d378-49f7-866c-3760d9e06f3d", "node_type": "1", "metadata": {"window": "The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. ", "original_text": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. "}, "hash": "553b6288ee382f0976c094b210e270f867598fbd4bf2423cb8ce01b42f045ad9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n", "mimetype": "text/plain", "start_char_idx": 24520, "end_char_idx": 24649, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "309d58a8-d378-49f7-866c-3760d9e06f3d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. ", "original_text": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0af6b2bb-ce13-45c6-bb03-70c3d8323998", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Similar to Figure 4, they show two low-anomaly regions for the modes, but the boundaries are fuzzier due to the ensemble averaging.  The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. ", "original_text": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n"}, "hash": "97475729abd1b6aaf09b08c093e34ef762148fe7d2fee4f30d20a10c4f39d6cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0e1faec-0f9c-452f-b958-bfc00a4a1267", "node_type": "1", "metadata": {"window": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. ", "original_text": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) "}, "hash": "99fbc0a39482ac89563650a917a50e8ad23ab917941086ec9b0951b3f04d5f9b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. ", "mimetype": "text/plain", "start_char_idx": 24649, "end_char_idx": 24733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f0e1faec-0f9c-452f-b958-bfc00a4a1267", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. ", "original_text": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "309d58a8-d378-49f7-866c-3760d9e06f3d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The presence of an outlier still does not create a distinct high-anomaly spot at its location. *\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. ", "original_text": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g. "}, "hash": "f5e9b28a95a932c45c843ce6a93af25bab98993d0fae48bc6f86a0fd43399bb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffc3cf7c-db6a-457f-bedd-db12b9001138", "node_type": "1", "metadata": {"window": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp. ", "original_text": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. "}, "hash": "4fcfe33070ba101d37ea42bd07f2c9c553802f98cb5ea140345efc54410a597d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) ", "mimetype": "text/plain", "start_char_idx": 24733, "end_char_idx": 24906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ffc3cf7c-db6a-457f-bedd-db12b9001138", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp. ", "original_text": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0e1faec-0f9c-452f-b958-bfc00a4a1267", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nAs another IFOREST variation, the RRCF model from [11] does not look at guiding the split threshold selection, but at guiding the selection of variable to split, doing so with a probability proportional to the range of each variable at a given node and relying on averaging out many randomized choices instead of always aiming for the best one as DET tries to do.\n\n Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. ", "original_text": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.) "}, "hash": "40edfae004c50b68142667ab021cebba9080070107c47eaa2c141c9ae8d10b75", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5986c4e7-551c-4792-91f4-aee5c194fc82", "node_type": "1", "metadata": {"window": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n", "original_text": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n"}, "hash": "dc931da4bbeb0baaa8061c3bd594fb0fc6bca7cb2885cc3162686a023c982d09", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. ", "mimetype": "text/plain", "start_char_idx": 24906, "end_char_idx": 25232, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5986c4e7-551c-4792-91f4-aee5c194fc82", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n", "original_text": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffc3cf7c-db6a-457f-bedd-db12b9001138", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Such a criterion tries to address the issue of producing splits which are not helpful for discriminating outliers, but does so very differently from other methods.  Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp. ", "original_text": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would. "}, "hash": "f148548ce4382c8e7b6c83f6f7e8a8b5320df3406223be95ed6fef4000139d5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ebcaad61-2ca4-4bc9-9315-c1894940b6f3", "node_type": "1", "metadata": {"window": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g. ", "original_text": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. "}, "hash": "e0646134f7b01c8fc1af452313544e33c059011dea6e417955723c3c4486184d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n", "mimetype": "text/plain", "start_char_idx": 25232, "end_char_idx": 25363, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ebcaad61-2ca4-4bc9-9315-c1894940b6f3", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g. ", "original_text": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5986c4e7-551c-4792-91f4-aee5c194fc82", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Variable selection is not a new idea for IFOREST - for example, in [14] they briefly mention using kurtosis as a variable screener, selecting only the top variables ranked by kurtosis to be included in a model, while SCIFOREST tries many variables at random in order to identify good candidates and DET tries all of them deterministically.  RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n", "original_text": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n"}, "hash": "d40e806bfbffab1774ee5aa6cdc16f7cbb48b7b190589b51c907b4a52570050b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93bd1120-a3be-4536-8a93-cc17101b7f7e", "node_type": "1", "metadata": {"window": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. ", "original_text": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. "}, "hash": "b03e98b22b5e2c089c4f985cebbf97b4adc6d965ffee4453ebb6dcd18fce1dc5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. ", "mimetype": "text/plain", "start_char_idx": 25363, "end_char_idx": 25859, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93bd1120-a3be-4536-8a93-cc17101b7f7e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. ", "original_text": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ebcaad61-2ca4-4bc9-9315-c1894940b6f3", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "RRCF is different in the sense that it relies on averaging out errors, introducing less-granular guidance to the split choices.\n\n In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g. ", "original_text": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables. "}, "hash": "c2d6cc256bf4b045dcc9f694f9331eb68e05ecb4d1b6f238fe602878c582762e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4426d147-fe0f-442d-92f8-8fc0ee7fe5e6", "node_type": "1", "metadata": {"window": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n", "original_text": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. "}, "hash": "d1528065fdb67fca7853b2254870bd82d1018f3d9fe41291149cf4b8b313d09b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. ", "mimetype": "text/plain", "start_char_idx": 25859, "end_char_idx": 26066, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4426d147-fe0f-442d-92f8-8fc0ee7fe5e6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n", "original_text": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93bd1120-a3be-4536-8a93-cc17101b7f7e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In theory, RRCF suffers from many of the same issues as IFOREST as outlined in e.g.  [12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. ", "original_text": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria. "}, "hash": "7741f8ebed290e91acee434f4ef67e0da4b2e8730c62d51cd1f2cd25c68e0573", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2575d2c5-fd62-44e2-ac37-c22d33307796", "node_type": "1", "metadata": {"window": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. ", "original_text": "From all these criteria, the most natural (esp. "}, "hash": "d08bb62310b2d37a22e4f691615aeeea76362c95cc4409b6f0052a175bdd73a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. ", "mimetype": "text/plain", "start_char_idx": 26066, "end_char_idx": 26300, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2575d2c5-fd62-44e2-ac37-c22d33307796", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. ", "original_text": "From all these criteria, the most natural (esp. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4426d147-fe0f-442d-92f8-8fc0ee7fe5e6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[12] and [2], and visualizing its calculated anomaly scores on randomly-generated points that exemplify an extreme pattern (such as inner and outer rings, sine waves, etc.)  would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n", "original_text": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution. "}, "hash": "798dbfd32fe1e9da238849c7069b55caff63cecd6c6846093b771cbd65d4d9ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "93be4279-4ffb-489f-8307-f018e9a35dbe", "node_type": "1", "metadata": {"window": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. ", "original_text": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n"}, "hash": "ef3103f260cfd8fc00e68dab52913d113fcc9ebca2bd6cf95eecf524f816c1ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "From all these criteria, the most natural (esp. ", "mimetype": "text/plain", "start_char_idx": 26300, "end_char_idx": 26348, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "93be4279-4ffb-489f-8307-f018e9a35dbe", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. ", "original_text": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2575d2c5-fd62-44e2-ac37-c22d33307796", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "would not look too differently from the scores produced by IFOREST - for example, in the bimodal random numbers from earlier plots, it would not identify the top-left or bottom-right outliers as the most anomalous observations, although it would rank them as relatively more anomalous compared to the rest than IFOREST would.  In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. ", "original_text": "From all these criteria, the most natural (esp. "}, "hash": "77f7495889eeff511072dd11d171c1517cc1471d47b7129a6237259a73ba4248", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f13c8494-395f-4c55-8244-a627dafb00a3", "node_type": "1", "metadata": {"window": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. ", "original_text": "Typically, supervised decision trees (e.g. "}, "hash": "fdf5ab85866b03a284f4db5e4a99b9aab399601ab2a9078ee784fb911a766a47", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n", "mimetype": "text/plain", "start_char_idx": 26348, "end_char_idx": 26526, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f13c8494-395f-4c55-8244-a627dafb00a3", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. ", "original_text": "Typically, supervised decision trees (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "93be4279-4ffb-489f-8307-f018e9a35dbe", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In practice however, RRCF has demonstrated improved performance metrics when applied on real datasets of varying characteristics.\n\n ## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. ", "original_text": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n"}, "hash": "e999d6ba6865f67329fd91d41fae394b0c0f3c14c529f89bd0259e6ee1222287", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac607675-6736-4e52-a360-abaf07e26315", "node_type": "1", "metadata": {"window": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain.", "original_text": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. "}, "hash": "e465ff59e2818ccc880738d23a3e719462b50f504c816e496612c009eb874910", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Typically, supervised decision trees (e.g. ", "mimetype": "text/plain", "start_char_idx": 26526, "end_char_idx": 26569, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac607675-6736-4e52-a360-abaf07e26315", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain.", "original_text": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f13c8494-395f-4c55-8244-a627dafb00a3", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 5 Better data separations\n\nIdeally, one would want the splits made by an isolation tree to assign more points to regions where the multivariate density or likelihood of the data distribution is higher and to make these regions as narrow as needed, or under an alternative view to group together observations which are more similar, making the points in a node closer and closer further down each tree, which in broad terms requires making splits on good boundaries across the right variables.  This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. ", "original_text": "Typically, supervised decision trees (e.g. "}, "hash": "0dcb76d263d225786d712d34193d57b9054a4f5c9edf9a14adb898295567a09a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77b749aa-8741-4bc5-8cdf-7788d75864cb", "node_type": "1", "metadata": {"window": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n", "original_text": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n"}, "hash": "b78ed0cdcff22d4583e4f8579b4772fb13f8e23df5dbc1db28b6748eff247b30", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. ", "mimetype": "text/plain", "start_char_idx": 26569, "end_char_idx": 26771, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "77b749aa-8741-4bc5-8cdf-7788d75864cb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n", "original_text": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac607675-6736-4e52-a360-abaf07e26315", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This kind of problem has been approached from different angles in different problem domains such as anomaly detection, clustering, or approximate nearest neighbor search, using different measuring criteria.  DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain.", "original_text": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split. "}, "hash": "1cd23b456b553a9544a09bae956744d1884f427e0d82a44a1e201ca89bc354fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b47d575-9f4d-47c6-a7ed-f8be2accda67", "node_type": "1", "metadata": {"window": "From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. ", "original_text": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. "}, "hash": "e2c98d219ead4d7ab1f05a2e282df835062b9668cdf09ce6455853d089272170", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n", "mimetype": "text/plain", "start_char_idx": 26771, "end_char_idx": 26880, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b47d575-9f4d-47c6-a7ed-f8be2accda67", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. ", "original_text": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77b749aa-8741-4bc5-8cdf-7788d75864cb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "DET looked at volume, RRCF at ranges of variables, SCIFOREST at variance, [14] briefly looked at kurtosis, and given the logic of IFOREST one might also think of looking at other measurements of deviation from a uniform distribution.  From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n", "original_text": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n"}, "hash": "fc533ec9be6facb7bdad62f789f1190dc28aacea01c6f1c35df3f4d4c68ebeb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11f5b35a-8ac6-4018-be9b-1d2b93d9b3be", "node_type": "1", "metadata": {"window": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". ", "original_text": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. "}, "hash": "ee09559eb049847cae6666c948f1135cdfd150d72baeca633d4763751ac4604c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. ", "mimetype": "text/plain", "start_char_idx": 26880, "end_char_idx": 27488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "11f5b35a-8ac6-4018-be9b-1d2b93d9b3be", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". ", "original_text": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b47d575-9f4d-47c6-a7ed-f8be2accda67", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "From all these criteria, the most natural (esp.  for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. ", "original_text": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores. "}, "hash": "26862624975d6bf14714ebaf7646bcbebd6890bd1d2d01a133a764d09d15021c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "99698856-c9fa-4e93-beea-71fbd60751c4", "node_type": "1", "metadata": {"window": "Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n", "original_text": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. "}, "hash": "166d8bc2257480d551fa642c5aafad6948d751f0e2de3b4c152000e5ecdc0623", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. ", "mimetype": "text/plain", "start_char_idx": 27488, "end_char_idx": 27884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "99698856-c9fa-4e93-beea-71fbd60751c4", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n", "original_text": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11f5b35a-8ac6-4018-be9b-1d2b93d9b3be", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "for multimodal distributions) and most widely used in different algorithms is perhaps variance, but the way in which other algorithms use it is rather different from SCIFOREST.\n\n Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". ", "original_text": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g. "}, "hash": "12de0686049a510dc39a91924320dbb84418292f7c3ef9a8a5ea4fb4df7f4d0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "61af8cfd-b4f9-43d0-badf-3c823bea8e1d", "node_type": "1", "metadata": {"window": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. ", "original_text": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain."}, "hash": "b432637cc10ef4bb33971b1fc3493ffddf0f3bbc5f24eb3bfb64b54cbd61ab6d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. ", "mimetype": "text/plain", "start_char_idx": 27884, "end_char_idx": 28016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "61af8cfd-b4f9-43d0-badf-3c823bea8e1d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. ", "original_text": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "99698856-c9fa-4e93-beea-71fbd60751c4", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Typically, supervised decision trees (e.g.  [16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n", "original_text": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers. "}, "hash": "3e722e0261660cc7920e4d493338c6fe0fcd99c8ceb65233b3d2f0237b47b578", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4a66522-1cae-4508-af08-a44b5fdfd46b", "node_type": "1", "metadata": {"window": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n", "original_text": "**\n\n*Description: Three histograms are shown.\n"}, "hash": "1a4270f79cfe16522ebf7e389b606452d3658ba08e4f24ef544b8c239c53c5ff", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain.", "mimetype": "text/plain", "start_char_idx": 28016, "end_char_idx": 28170, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b4a66522-1cae-4508-af08-a44b5fdfd46b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n", "original_text": "**\n\n*Description: Three histograms are shown.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "61af8cfd-b4f9-43d0-badf-3c823bea8e1d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[16] and [3]) make a trade-off in their guiding heuristic between branch purity (homonegenity of the points in a given partition) and number of observations that are assigned to each branch of a split.  DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. ", "original_text": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain."}, "hash": "8fd16e0d92c0c232de30c8732890362da834bf697b989fbe754175c2aaecead8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58cdbf0e-5b1a-44a9-b80d-1385c439164b", "node_type": "1", "metadata": {"window": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution. ", "original_text": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. "}, "hash": "401f3cd9eb8df7fd94011b34e6f873c12eb6c353b27796e93b98780b439d6fb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "**\n\n*Description: Three histograms are shown.\n", "mimetype": "text/plain", "start_char_idx": 28170, "end_char_idx": 28216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "58cdbf0e-5b1a-44a9-b80d-1385c439164b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution. ", "original_text": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4a66522-1cae-4508-af08-a44b5fdfd46b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "DET and similar also make such a trade-off implicitly, while SCIFOREST and RRCF do not look at this aspect.\n\n More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n", "original_text": "**\n\n*Description: Three histograms are shown.\n"}, "hash": "8835298dbf692410215e1dffb0bf9b25c6582a5d3825dd00e803bea0ccc8bf1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "914c9ea9-4f25-40cc-90b1-84d7d7f2039a", "node_type": "1", "metadata": {"window": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset.", "original_text": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". "}, "hash": "2720e6b27193938a7b1a947cdeb5fc2cbf01993d4d7107fb648ba5eeaf50f347", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. ", "mimetype": "text/plain", "start_char_idx": 28216, "end_char_idx": 28287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "914c9ea9-4f25-40cc-90b1-84d7d7f2039a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset.", "original_text": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "58cdbf0e-5b1a-44a9-b80d-1385c439164b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "More concretly, in the case of regression trees, it is typical to use as splitting criterion a pooled standard deviation gain, which is slightly different from the averaged standard deviation gain used by SCIFOREST\n\ngain_pooled = (\u03c3_all - (n_left\u03c3_left + n_right\u03c3_right)/(n_left+n_right)) / \u03c3_all\n\nThe \"Fair-Cut Forest\" or FCF from [7] applied this pooled gain criterion on random linear combinations of variables in order to determine splits by these same linear combinations, similarly to SCIFOREST but with the goal of finding neighbors for missing value imputations rather than producing anomaly scores.  While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution. ", "original_text": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution. "}, "hash": "0af899abb2febc22a7a24cdd77a0bd76e29170908ab43d9eb169f08cf9b26b67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cecb8d00-276a-4bcd-be7c-e3b22b005559", "node_type": "1", "metadata": {"window": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n", "original_text": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n"}, "hash": "e11a651d6404c6603f0a5922f2ed1fcc467083f3a40fe5d375d9ce0df7e631f2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". ", "mimetype": "text/plain", "start_char_idx": 28287, "end_char_idx": 28391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cecb8d00-276a-4bcd-be7c-e3b22b005559", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n", "original_text": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "914c9ea9-4f25-40cc-90b1-84d7d7f2039a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While not the original goal of FCF, this split guiding criterion turns out to produce splits that are also useful for outlier detection as will be seen in the next sections, as these splits tend to represent more natural separations, which can come especially useful in clustered or multimodal distributions (as opposed to outliers represented by extreme values which could be identified by e.g.  kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset.", "original_text": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\". "}, "hash": "feee9cb4380d7341b8efe72d25e6331cafb59621d23fc53e03bffd6cf3dd1588", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "62c24e23-291e-4251-8d82-dd51952b2e6a", "node_type": "1", "metadata": {"window": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. ", "original_text": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. "}, "hash": "b27cfcb7c646bc4efb369ee99118549e42098f0c8710d37ea07b6ad590d205cc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n", "mimetype": "text/plain", "start_char_idx": 28391, "end_char_idx": 28493, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "62c24e23-291e-4251-8d82-dd51952b2e6a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. ", "original_text": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cecb8d00-276a-4bcd-be7c-e3b22b005559", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "kurtosis or deviation w.r.t a uniform distribution):\n\n***\n**Figure 6: Splits chosen by each criteria in randomly-generated numbers.  In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n", "original_text": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n"}, "hash": "90889a7f60f46c0ead47526b80094b7bdaf76d88207b59627cda4aad35d223cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb900ae5-2a6f-4bdd-859b-6bbaa1aeab28", "node_type": "1", "metadata": {"window": "**\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. ", "original_text": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n"}, "hash": "8c4ad89a5e205d9741de692710e3673551631c023ae58580f403330470dea280", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. ", "mimetype": "text/plain", "start_char_idx": 28493, "end_char_idx": 28570, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb900ae5-2a6f-4bdd-859b-6bbaa1aeab28", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. ", "original_text": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "62c24e23-291e-4251-8d82-dd51952b2e6a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In the left picture, the density-chosen point matches with that of pooled gain, while in the center picture it closely matches with that of averaged gain. **\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. ", "original_text": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution. "}, "hash": "ad7f325e12cd06b6a1de78a8172a440081935a58cd90316de82004cf15b260a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60da6139-1809-4813-9d08-57ea0ee67255", "node_type": "1", "metadata": {"window": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n", "original_text": "The third, \"Gaussian mixture\", shows a bimodal distribution. "}, "hash": "c06654da507b3584b286571b121b91432b31b92ba1b63efc53207cda6930fe90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n", "mimetype": "text/plain", "start_char_idx": 28570, "end_char_idx": 28682, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "60da6139-1809-4813-9d08-57ea0ee67255", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n", "original_text": "The third, \"Gaussian mixture\", shows a bimodal distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb900ae5-2a6f-4bdd-859b-6bbaa1aeab28", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "**\n\n*Description: Three histograms are shown.\n The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. ", "original_text": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n"}, "hash": "eb5c4ebe772f49805fb50781edb53e9c666938705e9ef195c4bee556413c54c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01cfc805-0977-4273-af83-26bd7fe07dbe", "node_type": "1", "metadata": {"window": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n", "original_text": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset."}, "hash": "b93a9390e43df2e71a8281751653be9cd7f0d8ed23e28228e0be00f487a7bb55", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The third, \"Gaussian mixture\", shows a bimodal distribution. ", "mimetype": "text/plain", "start_char_idx": 28682, "end_char_idx": 28743, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "01cfc805-0977-4273-af83-26bd7fe07dbe", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n", "original_text": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60da6139-1809-4813-9d08-57ea0ee67255", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The first, \"Random Numbers ~Normal(0,1)\", shows a normal distribution.  Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n", "original_text": "The third, \"Gaussian mixture\", shows a bimodal distribution. "}, "hash": "0f20d156d63c3b8b25bfd3f0102264cce9a7cd19ef2b5286ce7034046bd88547", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c428902a-9294-45ea-8edf-d4fafd5fab03", "node_type": "1", "metadata": {"window": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". ", "original_text": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n"}, "hash": "a8fa72086ed9f394504390ce5c0d44aa76bc73231ae1d57d169b117ca0aa1cc2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset.", "mimetype": "text/plain", "start_char_idx": 28743, "end_char_idx": 28906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c428902a-9294-45ea-8edf-d4fafd5fab03", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". ", "original_text": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01cfc805-0977-4273-af83-26bd7fe07dbe", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Three vertical lines indicate the split points chosen by \"Averaged gain\", \"Pooled gain\", and \"Density\".  \"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n", "original_text": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset."}, "hash": "f9d1ee45a261155257befe1d125c574c6dc5ef34cb0bdf969c46a4909bee4c76", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f0ed539-cc1e-4668-b9ae-426fffebb575", "node_type": "1", "metadata": {"window": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. ", "original_text": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. "}, "hash": "723dd636574b92d7fb38357d3552ef8d69ff6273e3261731f550680dc96f6183", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n", "mimetype": "text/plain", "start_char_idx": 28906, "end_char_idx": 29192, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f0ed539-cc1e-4668-b9ae-426fffebb575", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. ", "original_text": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c428902a-9294-45ea-8edf-d4fafd5fab03", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Pooled gain\" and \"Density\" are very close to the peak, while \"Averaged gain\" is slightly off-center.\n The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". ", "original_text": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n"}, "hash": "dab3412fcaecbdbe75bcc9540ebdca5a89b02df89fea1c2ce2ebb630f04d8abe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aac9a112-8482-46f0-bb8f-6a5975e194bb", "node_type": "1", "metadata": {"window": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n", "original_text": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. "}, "hash": "7a8ec8595d05e9b1758827e082cd440b8c604117b2692de8e61b337dbb4d642b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. ", "mimetype": "text/plain", "start_char_idx": 29192, "end_char_idx": 29702, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "aac9a112-8482-46f0-bb8f-6a5975e194bb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n", "original_text": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f0ed539-cc1e-4668-b9ae-426fffebb575", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The second, \"Random Numbers ~Gamma(1,1)\", shows a skewed gamma distribution.  Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. ", "original_text": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step. "}, "hash": "b1531e0a3a9272494d85fa1bd791c8698c3e2fac06369920d66d80529eb803b8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b494864c-cd13-487f-a936-c0d74ea58848", "node_type": "1", "metadata": {"window": "The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". ", "original_text": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n"}, "hash": "6379f5be9d8beeb4859bfd92b3eb0c23484247c4364e49a923f7fb945c5eebbd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. ", "mimetype": "text/plain", "start_char_idx": 29702, "end_char_idx": 30006, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b494864c-cd13-487f-a936-c0d74ea58848", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". ", "original_text": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aac9a112-8482-46f0-bb8f-6a5975e194bb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Here, \"Averaged gain\" and \"Density\" are very close, near the mode, while \"Pooled gain\" is further to the right.\n The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n", "original_text": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity. "}, "hash": "d56502c0c683f9a26ff3c543d62c20562a11db4168df829101913b4fdf2ec016", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eac44f98-a4f2-423b-9cc3-c6f0d0a9083a", "node_type": "1", "metadata": {"window": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling.", "original_text": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n"}, "hash": "fa4bcdb4cb3da874c2bd924a45ee098f425264b0ae64b983968fca9ed3040f48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n", "mimetype": "text/plain", "start_char_idx": 30006, "end_char_idx": 30153, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eac44f98-a4f2-423b-9cc3-c6f0d0a9083a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling.", "original_text": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b494864c-cd13-487f-a936-c0d74ea58848", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The third, \"Gaussian mixture\", shows a bimodal distribution.  All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". ", "original_text": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n"}, "hash": "960f1ddcfb1a3b2e061942a8f93da1df04e28669d162a09d9603a0c2c2c00907", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "651e5ee3-22e6-4020-8ee0-c76ca257ad9c", "node_type": "1", "metadata": {"window": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. ", "original_text": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". "}, "hash": "479f89b0560b4ac3212176e4e72a08d66eb8b23ad2ec2ecac5e3ecfca50c22d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n", "mimetype": "text/plain", "start_char_idx": 30153, "end_char_idx": 30595, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "651e5ee3-22e6-4020-8ee0-c76ca257ad9c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. ", "original_text": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eac44f98-a4f2-423b-9cc3-c6f0d0a9083a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "All three criteria choose a split point in the valley between the two modes, with \"Pooled gain\" and \"Density\" being very close and \"Averaged gain\" slightly offset. *\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling.", "original_text": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n"}, "hash": "5464a3bf5a93e3bbd4ff3f7305b9bd22e4589d4cffe254a91c75df97c341e8d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36d73a7e-93f9-49de-92a2-5fdf809bec61", "node_type": "1", "metadata": {"window": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children. ", "original_text": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. "}, "hash": "6b4c5ba846a6f2240ceb1bd746f13a59cb36320f97c4505192c6b8de6dc1a321", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". ", "mimetype": "text/plain", "start_char_idx": 30595, "end_char_idx": 30664, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "36d73a7e-93f9-49de-92a2-5fdf809bec61", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children. ", "original_text": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "651e5ee3-22e6-4020-8ee0-c76ca257ad9c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nIf the best possible pooled gain from a split point is evaluated across different variables, those in which clusters are more easily formed or in which separations are more clear will result in a higher gain, perhaps making it also a potential heuristic for variable selection.\n\n In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. ", "original_text": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\". "}, "hash": "180577c4dbe5aeb90753c300f00b9da03518676a0119aa5be833b237980044a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ff22ee8-13d2-41d3-9280-641a995bbeae", "node_type": "1", "metadata": {"window": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes. ", "original_text": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n"}, "hash": "e7e0896b1ba487e65c63a985949ff3c01729723990c228d695caa942d35456f9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. ", "mimetype": "text/plain", "start_char_idx": 30664, "end_char_idx": 30791, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2ff22ee8-13d2-41d3-9280-641a995bbeae", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes. ", "original_text": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36d73a7e-93f9-49de-92a2-5fdf809bec61", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In a way, pooled gain applied on a single variable or linear combination is trying to \"predict\" the same values that it is input (using group means as \"prediction\"), and the groups that it produces will in expectation become more and more homogeneous after each split, resulting in something similar to an autoencoder; but it is a myopic criterion: if measuring a global squared error objective across all variables, \"predictions\" (per-column means in each node) are not guaranteed to improve after each step.  One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children. ", "original_text": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials. "}, "hash": "5f2df8f42796a41eb14f69fa39accf7b9715558da001117a10496e1932098c2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9982c279-0d2b-4483-9d5c-1c7c2ac86ef8", "node_type": "1", "metadata": {"window": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes. ", "original_text": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". "}, "hash": "6f6ec5467cdfcec0648b3fff2f4444b49130421bd7080c3aee958b8751bfb772", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n", "mimetype": "text/plain", "start_char_idx": 30791, "end_char_idx": 30906, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9982c279-0d2b-4483-9d5c-1c7c2ac86ef8", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes. ", "original_text": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ff22ee8-13d2-41d3-9280-641a995bbeae", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "One could also think of minimizing instead a pooled gain criterion calculated across all variables in the data, but such a criterion, while perhaps reasonable in theory, would in practice result in much slower evaluation and would perhaps lead to sub-optimal splits in the presence of multicollinearity.  A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes. ", "original_text": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n"}, "hash": "1d771b01ca03e3afaa70dcc826e0a924e0e5e1b029d8ddbfca61db3ddde5e2ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9ad191ac-113a-4feb-bf7e-50728a23e4d0", "node_type": "1", "metadata": {"window": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed.", "original_text": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling."}, "hash": "0924b62bf870592c12ad366a0668a142e19b69478f2a065d09101a515a1f7f5b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". ", "mimetype": "text/plain", "start_char_idx": 30906, "end_char_idx": 30980, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9ad191ac-113a-4feb-bf7e-50728a23e4d0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed.", "original_text": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9982c279-0d2b-4483-9d5c-1c7c2ac86ef8", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A shorter and myopic gain calculated on one or a few variables can nevertheless rely on averaging out errors across many runs for better results.\n\n Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes. ", "original_text": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\". "}, "hash": "aface9e6238a47bc0971924223d21608f10d5cf2145de386ea9d95ada53efc7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d05e0e5-4196-41ab-999d-2d9104c27043", "node_type": "1", "metadata": {"window": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. ", "original_text": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. "}, "hash": "3589660e59da9edcffefd8bbcc2733eca209bed21476fb8a0bfacce6ef700beb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling.", "mimetype": "text/plain", "start_char_idx": 30980, "end_char_idx": 31132, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d05e0e5-4196-41ab-999d-2d9104c27043", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. ", "original_text": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9ad191ac-113a-4feb-bf7e-50728a23e4d0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Compared to SCIFOREST, FCF can also be reasonably fast at isolating scattered outliers within each tree, but the partitions that it chooses to isolate them are rather different and can result in much larger variability in the final results according to the data sub-samples that each tree uses:\n\n***\n**Figure 7: Anomaly scores from FCF in randomly-generated bimodal data**\n\n*Description: A 2x3 grid of heatmaps shows anomaly scores from FCF.\n The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed.", "original_text": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling."}, "hash": "4ee7b73655bf2db72246b68e5ab6c22e29347be799b31f54e0495e547017239f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49143986-f6bb-4028-841b-0fd505919a07", "node_type": "1", "metadata": {"window": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. ", "original_text": "A root node splits into two children. "}, "hash": "d1c6861115ff58d5b71722fadef8c76f7a81ceed67c582d16f1490886c6dd868", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. ", "mimetype": "text/plain", "start_char_idx": 31132, "end_char_idx": 31894, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "49143986-f6bb-4028-841b-0fd505919a07", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. ", "original_text": "A root node splits into two children. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d05e0e5-4196-41ab-999d-2d9104c27043", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top row is titled \"Fair-Cut Forest (3 trials), no sub-sampling\".  The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. ", "original_text": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points. "}, "hash": "add0c2a428b7f5047e9549359194ac5a24535ee15660feb4cbe99ddc75788bcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fc64731-6ddc-4ec4-8215-c7a9ba5d6c40", "node_type": "1", "metadata": {"window": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. ", "original_text": "The left child has 2 leaf nodes. "}, "hash": "f46379ffbe354d1b443d283a5597628b53eace136ac16e6cea6056708d9395fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A root node splits into two children. ", "mimetype": "text/plain", "start_char_idx": 31894, "end_char_idx": 31932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0fc64731-6ddc-4ec4-8215-c7a9ba5d6c40", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. ", "original_text": "The left child has 2 leaf nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49143986-f6bb-4028-841b-0fd505919a07", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The three heatmaps (\"No outlier\", \"Outlier at top-left\", \"Outlier at bottom-right\") show significant variation between trials.  The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. ", "original_text": "A root node splits into two children. "}, "hash": "92d0cace5024ec116fdb25512088053f5a9fd725f7e1d339bcfabc7e61833385", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4ef4127-ae44-44f3-8ecf-29d377b99be0", "node_type": "1", "metadata": {"window": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n", "original_text": "The right child splits again, leading to 3 leaf nodes. "}, "hash": "26f007435706b7a8f74c3113cec31743cd09cf1cc3919697c04c2fe9ee237ef3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left child has 2 leaf nodes. ", "mimetype": "text/plain", "start_char_idx": 31932, "end_char_idx": 31965, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4ef4127-ae44-44f3-8ecf-29d377b99be0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n", "original_text": "The right child splits again, leading to 3 leaf nodes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fc64731-6ddc-4ec4-8215-c7a9ba5d6c40", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The two modes are generally identified as low-anomaly regions, but the shape and intensity of anomaly scores vary.\n The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. ", "original_text": "The left child has 2 leaf nodes. "}, "hash": "6bdcaa3b4c5ec4b20ea5d2a40d8f5decf4f37fcbf1f93bf0a64d7db4007d445b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d5b2ce0-1993-4aef-95cd-28a3358a4597", "node_type": "1", "metadata": {"window": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n", "original_text": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed."}, "hash": "025844a47902425b39ee0756048a1fcd3c9d6f242965e1b6c1873cb7ea03e8b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right child splits again, leading to 3 leaf nodes. ", "mimetype": "text/plain", "start_char_idx": 31965, "end_char_idx": 32020, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1d5b2ce0-1993-4aef-95cd-28a3358a4597", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n", "original_text": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4ef4127-ae44-44f3-8ecf-29d377b99be0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom row is titled \"Fair-Cut Forest (3 trials), with sub-sampling\".  The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n", "original_text": "The right child splits again, leading to 3 leaf nodes. "}, "hash": "d6367758d51920d091a61680fdff3ceccddd66631db44f8bb994cfd28661059f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04b78f57-3abf-468f-93d3-b28040f18527", "node_type": "1", "metadata": {"window": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n", "original_text": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. "}, "hash": "3c6b7e3a52f15290a12e503d7dcd65043e438d92602d1a5af8ced57f9e583753", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed.", "mimetype": "text/plain", "start_char_idx": 32020, "end_char_idx": 32081, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04b78f57-3abf-468f-93d3-b28040f18527", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n", "original_text": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d5b2ce0-1993-4aef-95cd-28a3358a4597", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The heatmaps here also show high variability between trials, and the patterns are less clear than in the top row, reflecting the effect of sub-sampling. *\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n", "original_text": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed."}, "hash": "5edda9d283dc385f40d4deb17e2597a63d79c8dce81652cd777c540f3506ab6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fae0758-8dcb-4727-b432-0e6d22858c1e", "node_type": "1", "metadata": {"window": "A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. ", "original_text": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. "}, "hash": "3ec37909cefc5d6766f5a3b6cc8727422cb7f113fe11fd3f8d03e169bc597886", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. ", "mimetype": "text/plain", "start_char_idx": 32081, "end_char_idx": 32287, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fae0758-8dcb-4727-b432-0e6d22858c1e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. ", "original_text": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04b78f57-3abf-468f-93d3-b28040f18527", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nThe tree structure produced by splits that follow this pooled gain criterion is also different from that of IFOREST - in the case of uniformly-random data, the optimal split would always correspond to assigning half of the points to one branch, leading to an expected isolation depth of E[d(m)] = log\u2082m when m is a power of 2, which is strictly smaller than the expected depth for uniformly random splits (given by E[d(m)] = 2(H\u2098 - 1), where H\u2098 is the m-th harmonic number) for m > 2, but with a structure that produces larger trees (containing more nodes) at the same height as the equivalent IFOREST:\n\n***\n**Figure 8: Expected isolation depth for 5 uniform points under pooled gain**\n\n*Description: A diagram shows a binary tree structure for 5 points.  A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n", "original_text": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top. "}, "hash": "df367a05f8b8ff3f3080115f16bd6f933e396cd0c0e9d3f9c71385f8151edfbe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d094bff0-7922-4fcd-b369-739e6510f958", "node_type": "1", "metadata": {"window": "The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n", "original_text": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. "}, "hash": "fa688771da95956b637fa412b35545dec2e8694c77915d7895170585b2469e00", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. ", "mimetype": "text/plain", "start_char_idx": 32287, "end_char_idx": 32439, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d094bff0-7922-4fcd-b369-739e6510f958", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n", "original_text": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fae0758-8dcb-4727-b432-0e6d22858c1e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A root node splits into two children.  The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. ", "original_text": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used. "}, "hash": "0b6a86649795b738a3980e44725529e98734a507b4b4d0dba8f4eab30957c623", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42e1df2-43e2-4dba-997a-504f70d053a5", "node_type": "1", "metadata": {"window": "The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". ", "original_text": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n"}, "hash": "5aca667eaa17268296d406347325012f3b9e83e8c25190420e0f25df4b69726b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. ", "mimetype": "text/plain", "start_char_idx": 32439, "end_char_idx": 32854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e42e1df2-43e2-4dba-997a-504f70d053a5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". ", "original_text": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d094bff0-7922-4fcd-b369-739e6510f958", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left child has 2 leaf nodes.  The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n", "original_text": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion. "}, "hash": "986d040061c36bbe7df4a4184855e0b45612b70e912a35028e537f424626c244", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80322415-c306-4565-80bc-1fe997ea244f", "node_type": "1", "metadata": {"window": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n", "original_text": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n"}, "hash": "69489f39ca03fb9357822e280ba8d6040062eeaccc154bde1259549fe415c169", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n", "mimetype": "text/plain", "start_char_idx": 32854, "end_char_idx": 33063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "80322415-c306-4565-80bc-1fe997ea244f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n", "original_text": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42e1df2-43e2-4dba-997a-504f70d053a5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right child splits again, leading to 3 leaf nodes.  The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". ", "original_text": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n"}, "hash": "d48302589fe2ea7e552175c6644565a813a01d66a2c0af882b10af1f6b16e7a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e9f51c4-b607-4cbb-8227-5698f536f12f", "node_type": "1", "metadata": {"window": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n", "original_text": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n"}, "hash": "a13b96bea110b6b826e33b7ab4e358e8201e59c50c8d2c95a91beeb8470594ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n", "mimetype": "text/plain", "start_char_idx": 33063, "end_char_idx": 33360, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3e9f51c4-b607-4cbb-8227-5698f536f12f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n", "original_text": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "80322415-c306-4565-80bc-1fe997ea244f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The calculation `E[d(5)] = (2+2+2+3+3)/5 = 2.4` is displayed. *\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n", "original_text": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n"}, "hash": "9bba4135cbcbcf058db53d004487e34db90383c5e91afc3911b221aa05239943", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd0cc34e-e4cb-4a43-ab1a-625bd5ec7e34", "node_type": "1", "metadata": {"window": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n", "original_text": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. "}, "hash": "400c6607151e30c54e950c166ae173741790be10f3dc48a1669f3b7fb95e0214", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n", "mimetype": "text/plain", "start_char_idx": 33360, "end_char_idx": 33598, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "bd0cc34e-e4cb-4a43-ab1a-625bd5ec7e34", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n", "original_text": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e9f51c4-b607-4cbb-8227-5698f536f12f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\n## 6 Hyperparameters under pooled gain\n\nIn [15] they kept most of the same IFOREST hyperparameters for the proposed SCIFOREST algorithm, adding a randomized trial of many linear combinations on top.  These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n", "original_text": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n"}, "hash": "3fa9f7fb5cffbd66761b0c7ce65c03832e3f188a8a860f03c493e31f0c5895b3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "980c4913-9e0e-4885-9b73-b5bcea8d546e", "node_type": "1", "metadata": {"window": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n", "original_text": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n"}, "hash": "1ccfda1e9208d0b1d56ba57b79795149ce5adc7029350dc8e0581ee1f4daee99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. ", "mimetype": "text/plain", "start_char_idx": 33598, "end_char_idx": 33974, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "980c4913-9e0e-4885-9b73-b5bcea8d546e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n", "original_text": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd0cc34e-e4cb-4a43-ab1a-625bd5ec7e34", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These same designs might not be the most optimal for FCF when used for anomaly detection however, and are rather different from what DET and OCRF used.  In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n", "original_text": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g. "}, "hash": "208dddb0fd3e843413c65c37096097a57601465566948ca7530324099ae76124", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0cce0e01-d577-4b16-b45e-93c81b08766f", "node_type": "1", "metadata": {"window": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate.", "original_text": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". "}, "hash": "3978cc8460dbdfd78ae2a13f89cd21f4b41a4083c2bff8e437b7b144816de300", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n", "mimetype": "text/plain", "start_char_idx": 33974, "end_char_idx": 34056, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0cce0e01-d577-4b16-b45e-93c81b08766f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate.", "original_text": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "980c4913-9e0e-4885-9b73-b5bcea8d546e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In the case of FCF, it should be taken into consideration that:\n\n*   If making splits by an averaged gain criterion, typically most of the cases will assign a single observation to one of the branches, and it could take many trials or evaluations across columns or linear combinations to choose a split that puts more than one observation in the smaller branch, which is not the case for the pooled gain criterion.  What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n", "original_text": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n"}, "hash": "294807126185337c2183b5a06f111784731261c942b5eae3c45b963cb73c3276", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "778e00e5-5e34-4140-a156-f368ef1b0b22", "node_type": "1", "metadata": {"window": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. ", "original_text": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n"}, "hash": "dd7c1eb28103f319de4821ed4919c04af7459bc75cdbee9376c5904c3969e7fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". ", "mimetype": "text/plain", "start_char_idx": 34056, "end_char_idx": 34990, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "778e00e5-5e34-4140-a156-f368ef1b0b22", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. ", "original_text": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0cce0e01-d577-4b16-b45e-93c81b08766f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "What's more, if the split is performed on a linear combination of variables, it is typically enough for at least one of them to not be irrelevant in order for the linear combination to produce helpful splits.\n *   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate.", "original_text": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\". "}, "hash": "eae7618e71c8cad21b79e5a1b31670c64f14f69bb65669169a1ff0a2a2dd7ded", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dbc24e2-a57c-40cc-9dc5-6280c4982484", "node_type": "1", "metadata": {"window": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. ", "original_text": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n"}, "hash": "07698275a9a98a130ba495b72a70d19dd526b472f04d1cbe74650e4c6d69ec66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n", "mimetype": "text/plain", "start_char_idx": 34990, "end_char_idx": 35101, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3dbc24e2-a57c-40cc-9dc5-6280c4982484", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. ", "original_text": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "778e00e5-5e34-4140-a156-f368ef1b0b22", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   In IFOREST it was concluded that 100 trees was enough to get convergent results, but the splits generated by a pooled gain criterion on a sub-sample of the data have more expected variability, thus perhaps more trees would be needed to reach convergent results, especially in larger datasets.\n *   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. ", "original_text": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n"}, "hash": "ae028e8f87e7fdf12768311d45cc7994a0a2cee684f7d1b159f9d8c5aa97d947", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e27bf13a-eabf-4b9a-89a6-60b292f62158", "node_type": "1", "metadata": {"window": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n", "original_text": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n"}, "hash": "3ba721219ed391b86fcc68c168109a24e1cb8c27a5b641458cbdfd6b63c5d4b6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n", "mimetype": "text/plain", "start_char_idx": 35101, "end_char_idx": 35214, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e27bf13a-eabf-4b9a-89a6-60b292f62158", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n", "original_text": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3dbc24e2-a57c-40cc-9dc5-6280c4982484", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Both IFOREST and SCIFOREST used sub-sampling with 256 observations per tree, which in the case of IFOREST is helpful for dealing with some of the inherent problems in the method, but FCF might not suffer from the exact same problems.\n *   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. ", "original_text": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n"}, "hash": "02989a4a9014017262896976aa8fcd995cfcdb0ce118b2fb24eebd688ddbd615", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0438624b-b12f-41ba-b4ee-cdd7dbb58034", "node_type": "1", "metadata": {"window": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. ", "original_text": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n"}, "hash": "e5ca3be232b03bbe95c39104938505659670cf12d8c4281d10929cdb41f2b9bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n", "mimetype": "text/plain", "start_char_idx": 35214, "end_char_idx": 35307, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0438624b-b12f-41ba-b4ee-cdd7dbb58034", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. ", "original_text": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e27bf13a-eabf-4b9a-89a6-60b292f62158", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Both IFOREST and SCIFOREST chose a maximum depth equal to that of balanced-tree height as terminating criterion, but in the case of standardized pooled gain, it is also possible - and perhaps a more reasonable choice - to set this termination criterion on the gain itself, with nodes in which the best possible gain is less than some pre-determined acceptance level (e.g.  25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n", "original_text": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n"}, "hash": "f51d58c764cca8f082f60090b054c99a493854711ae02b090d19010d62698271", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a89a80be-27bd-4c24-9562-73b71a9c0f97", "node_type": "1", "metadata": {"window": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. ", "original_text": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate."}, "hash": "33eb5e719fe2006f1f94482fe8ac98c270c930c6c6af06302efc79ace6eab26f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n", "mimetype": "text/plain", "start_char_idx": 35307, "end_char_idx": 35445, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a89a80be-27bd-4c24-9562-73b71a9c0f97", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. ", "original_text": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0438624b-b12f-41ba-b4ee-cdd7dbb58034", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "25%) potentially being set as terminal nodes and a remainder depth extrapolated.\n\n A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. ", "original_text": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n"}, "hash": "e232632298cfa0ca9383b2b59253ae3429015aa6e87972bc8f5c23f86a941f1a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "67ba4992-b9b9-49d2-88fe-f564da11aea9", "node_type": "1", "metadata": {"window": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "original_text": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. "}, "hash": "38b1d93d2a62f74e812c2aab8172833825945d6cf24c28120fdde7e999563434", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate.", "mimetype": "text/plain", "start_char_idx": 35445, "end_char_idx": 35642, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "67ba4992-b9b9-49d2-88fe-f564da11aea9", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "original_text": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a89a80be-27bd-4c24-9562-73b71a9c0f97", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "A small experiment with the \"SpamBase\" dataset would suggest that deeper trees are beneficial, regardless of the split criterion that is used:\n\n| Table 2: Results on \"SpamBase\" dataset with different termination criteria | | | |\n| :--- | :--- | :--- | :--- |\n| **Model** | **Termination** | **AUROC** | **Nodes** |\n| IFOREST | depth \u2265 8 | 0.6405 | 22,346 |\n| IFOREST | depth \u2265 16 | 0.6545 | 60,514 |\n| IFOREST | isolation | 0.6926 | 184,032 |\n| FCF | depth \u2265 8 | 0.5897 | 44,100 |\n| FCF | depth \u2265 16 | 0.6112 | 114,606 |\n| FCF | gain < 0.5 | 0.6197 | 132,694 |\n| FCF | isolation | 0.6220 | 174,960 |\n\nIncreasing the depth however also increases the standard error for the expected isolation depth that each observation would have:\n\n***\n**Figure 9: Average standard error for expected isolation depth of each observation**\n\n*Description: A 2x2 grid of line plots titled \"Standard error for average isolation depth (SpamBase Dataset)\".  Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. ", "original_text": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate."}, "hash": "c36643a8f762dd4933405cc8dcb5d5c0815876a2c26ab7c8241d114e9bedd1b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c81dea1-dca0-4a93-be7d-01128e2d88a4", "node_type": "1", "metadata": {"window": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "original_text": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. "}, "hash": "cc262ab9409c98ab0a2814ae627177d8abc8bd619fda2148a0092593a55beafa", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. ", "mimetype": "text/plain", "start_char_idx": 35642, "end_char_idx": 35808, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3c81dea1-dca0-4a93-be7d-01128e2d88a4", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "original_text": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67ba4992-b9b9-49d2-88fe-f564da11aea9", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Each plot shows \"Average standard error\" on the y-axis versus \"Number of trees\" on the x-axis (from 0 to 300).\n The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "original_text": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times. "}, "hash": "c84b71df0086bdf390ce4a8a506ffdacb5b0fda6592fe29a2cc7b2b2a2247c5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a4c3c9c0-0093-49ad-9e14-417051772d42", "node_type": "1", "metadata": {"window": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. ", "original_text": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n"}, "hash": "44a907a1137d31ffd029d30bfda87d6fe7fa86e69ed864d1fdd70af4f0fef8fb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. ", "mimetype": "text/plain", "start_char_idx": 35808, "end_char_idx": 36052, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a4c3c9c0-0093-49ad-9e14-417051772d42", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. ", "original_text": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c81dea1-dca0-4a93-be7d-01128e2d88a4", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top-left plot, \"iForest (max depth)\", shows the error quickly decreasing and stabilizing around a low value.\n The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "original_text": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset. "}, "hash": "4b7119c203d054afea93b4a5792ba60481d645aa12d5786ca4005870f2cfb2c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a96288ce-190e-48e2-8db7-c8e610e1d3b0", "node_type": "1", "metadata": {"window": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048). ", "original_text": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. "}, "hash": "c48ed5b8a1533add25da440a2ddfb986ee95941d4ba424dc80fec20fe4b1940d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n", "mimetype": "text/plain", "start_char_idx": 36052, "end_char_idx": 36405, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a96288ce-190e-48e2-8db7-c8e610e1d3b0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048). ", "original_text": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a4c3c9c0-0093-49ad-9e14-417051772d42", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The top-right plot, \"iForest (unlimited)\", shows a similar but slightly higher stable error.\n The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. ", "original_text": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n"}, "hash": "99bff209f3d64fd2a9c58aac397a50dbb63a4a6160aaff853d5d589ba388246d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51c015a1-99f6-4a71-94b8-340a5e5a6579", "node_type": "1", "metadata": {"window": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n", "original_text": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. "}, "hash": "f309164ee598ebca30ab7e2afd65389e4de1b11fa4b4cff146660495f0dfa211", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. ", "mimetype": "text/plain", "start_char_idx": 36405, "end_char_idx": 36549, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "51c015a1-99f6-4a71-94b8-340a5e5a6579", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n", "original_text": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a96288ce-190e-48e2-8db7-c8e610e1d3b0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom-left plot, \"FCF (max depth)\", shows a higher initial error that decreases with more trees but remains higher than for iForest.\n The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048). ", "original_text": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node. "}, "hash": "c8f673ae26b021fcc7ed778bac50c5f93b074a14def75fd3ca0770a87aac6fe4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c7f3b52-a4cb-43c4-9d36-e51c79e2d155", "node_type": "1", "metadata": {"window": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n", "original_text": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n"}, "hash": "32c3d7099f341b37457262f61acc6151cdbb7a50117a6db3bffc35690c82000c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. ", "mimetype": "text/plain", "start_char_idx": 36549, "end_char_idx": 36775, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1c7f3b52-a4cb-43c4-9d36-e51c79e2d155", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n", "original_text": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51c015a1-99f6-4a71-94b8-340a5e5a6579", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The bottom-right plot, \"FCF (unlimited)\", shows the highest error, which also decreases but remains significantly higher than the others, indicating more trees are needed to stabilize the estimate. *\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n", "original_text": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs. "}, "hash": "41ef924486ff1da6eee9999bc48d859b37b4f34457a99984db96ffe94a05ac46", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f3a3a12-1f0c-4a1f-a2a5-62df4a49afde", "node_type": "1", "metadata": {"window": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines.", "original_text": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n"}, "hash": "00ebf45a232076477722656e2f08d44c424afc64ea88b1f2fe47550771388a03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "mimetype": "text/plain", "start_char_idx": 36775, "end_char_idx": 37423, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f3a3a12-1f0c-4a1f-a2a5-62df4a49afde", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines.", "original_text": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c7f3b52-a4cb-43c4-9d36-e51c79e2d155", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nThis increase in the standard error can nevertheless be easily offset by an increase in the number of isolation trees, at the expense of longer running times.  For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n", "original_text": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n"}, "hash": "df712c8896baf865805550f8cc6c6eeb6c9b172f1c173c1eea490f8258acdab6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "605975fd-b8e8-4918-bb3c-49dfc71451aa", "node_type": "1", "metadata": {"window": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. ", "original_text": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. "}, "hash": "e01d8a8e1b04de101ec2b61a7d532f1cd19ad4ba9107202e35b7f2168d5612bb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n", "mimetype": "text/plain", "start_char_idx": 37423, "end_char_idx": 38049, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "605975fd-b8e8-4918-bb3c-49dfc71451aa", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. ", "original_text": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f3a3a12-1f0c-4a1f-a2a5-62df4a49afde", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, reaching the same average standard error per observation for the expected isolation depth as in IFOREST with 100 trees and limited depth under FCF with unlimited depth would require a bit over 200 trees in this particular dataset.  Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines.", "original_text": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n"}, "hash": "e115840e321cdc8c77f47465024939bc34086a0e30f55ab3b1e41ea95d26488a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a225787e-8beb-4621-a41d-5e33c9479926", "node_type": "1", "metadata": {"window": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n", "original_text": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048). "}, "hash": "9fdf41fbf8cb05933a6acc1ff2b1539ee64cc90d0b61777da2974cd16b6d1a1a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. ", "mimetype": "text/plain", "start_char_idx": 38049, "end_char_idx": 38472, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a225787e-8beb-4621-a41d-5e33c9479926", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n", "original_text": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "605975fd-b8e8-4918-bb3c-49dfc71451aa", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Fitting such a model (trees growing until full isolation, twice as many trees) is roughly an order of magnitude slower than if using the hyperparameters proposed in [14], but with a careful software implementation, this is still much faster than many competing methods for anomaly detection across several datasets as will be seen in the next section.\n\n In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. ", "original_text": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side. "}, "hash": "36be5ff4c67e702b096f4e6f6ea263f325d5d6e0a9bb38859ac2a3d7457b9682", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3e0282e-26c8-4fcc-abb4-ad1530373e4f", "node_type": "1", "metadata": {"window": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n", "original_text": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n"}, "hash": "458547730c13d5718738902fc4a50753ce650ea548d7e584d40613fd133a9039", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048). ", "mimetype": "text/plain", "start_char_idx": 38472, "end_char_idx": 38540, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3e0282e-26c8-4fcc-abb4-ad1530373e4f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n", "original_text": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a225787e-8beb-4621-a41d-5e33c9479926", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In [15] the suggestion was to make splits on random linear combinations of 2 variables, trying 10 such random linear combinations at each node.  Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n", "original_text": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048). "}, "hash": "f407dde6846c2c09cd11245498f385d70ff09221601754beb26f1e8993c65839", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6872c82e-a907-4000-ab74-f2996c753746", "node_type": "1", "metadata": {"window": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size.", "original_text": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n"}, "hash": "54f10d77a22a8f09e27880587c18fd935e4c1d2cfbfae08a297fb82d26f523d3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n", "mimetype": "text/plain", "start_char_idx": 38540, "end_char_idx": 38628, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6872c82e-a907-4000-ab74-f2996c753746", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size.", "original_text": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3e0282e-26c8-4fcc-abb4-ad1530373e4f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Some short experiments would suggest that 10 trials is perhaps too much for a pooled gain criterion, with better performance being achieved when this number is decreased, and only a small difference from using hyperplanes vs.  axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n", "original_text": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n"}, "hash": "7ab9e961b902883fcba50d0a3738ace9cbc43455e827b42b9d541c01cd20ddb5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f0bf169-c3bc-4abb-8a8c-43911b8d1d1f", "node_type": "1", "metadata": {"window": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n", "original_text": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines."}, "hash": "55cd839bca31649862073cabdc61549d3bea8d30ab965fedb0f4038bfdcde75d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n", "mimetype": "text/plain", "start_char_idx": 38628, "end_char_idx": 38833, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1f0bf169-c3bc-4abb-8a8c-43911b8d1d1f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n", "original_text": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6872c82e-a907-4000-ab74-f2996c753746", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "axis-parallel splits:\n\n| Table 3: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"SpamBase\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.7312 | 0.7095 |\n| 1 | all | 0.4405 | 0.4697\\* |\n| 2 | 1 | 0.7245 | 0.6950 |\n| 2 | 3 | 0.6220 | 0.5868 |\n| 2 | 10 | 0.4986 | 0.4861 |\n| 5 | 1 | 0.6893 | 0.6683 |\n| 5 | 3 | 0.6696 | 0.6199 |\n| 5 | 10 | 0.6127 | 0.5619 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n | Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size.", "original_text": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n"}, "hash": "93ca97271ba1ae70a2b6878a811d31b3dbfb70f1a5ec5a8adbcbb541500632f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c989acd6-3c84-4975-94ad-81f11998971f", "node_type": "1", "metadata": {"window": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n", "original_text": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. "}, "hash": "e439c86a0e2e931a8f551a9dfe244810d28d53ceb72e6e8cb7e14bd761765456", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines.", "mimetype": "text/plain", "start_char_idx": 38833, "end_char_idx": 39009, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c989acd6-3c84-4975-94ad-81f11998971f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n", "original_text": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f0bf169-c3bc-4abb-8a8c-43911b8d1d1f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 4: Varying the number of trials for different variables or random linear combinations of several variables in FCF, results on \"Satellite\" dataset | | | |\n| :--- | :--- | :--- | :--- |\n| **Variables per split** | **Trials** | **AUROC (256 samples)** | **AUROC (no subsampling)** |\n| 1 | 1 | 0.8368 | 0.7261 |\n| 1 | all | 0.8057 | 0.7152\\* |\n| 2 | 1 | 0.8186 | 0.7465 |\n| 2 | 3 | 0.8414 | 0.7451 |\n| 2 | 10 | 0.8298 | 0.7312 |\n| 5 | 1 | 0.8246 | 0.7544 |\n| 5 | 3 | 0.8235 | 0.7618 |\n| 5 | 10 | 0.8080 | 0.7509 |\n\n* This case was sampled with replacement in order to avoid generating the exact same splits in each tree.\n\n The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n", "original_text": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines."}, "hash": "2f38a0a6adb61e49f0bd6a4dc7a1e3ac658dd5e2b2f53d3afb6535b1e6df8604", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e09dce4-0fd3-4942-81ff-75d6da7a7b90", "node_type": "1", "metadata": {"window": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n", "original_text": "The sample size on the x-axis goes up to 8192.\n"}, "hash": "b9eace7e3d7fd09172d69770f52f8149ff1e8e03afb91aecfd840521bde70a2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. ", "mimetype": "text/plain", "start_char_idx": 39009, "end_char_idx": 39333, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9e09dce4-0fd3-4942-81ff-75d6da7a7b90", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n", "original_text": "The sample size on the x-axis goes up to 8192.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c989acd6-3c84-4975-94ad-81f11998971f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The number of samples to take for each tree has a similar influence as in IFOREST, but a deeper look at some datasets would reveal that the optimal sample size for some datasets can be much smaller than the 256 proposed in [14] which was also used in others such as [12] or [15]:\n\n***\n**Figure 10: AUROC by sample size and model characteristics in \"SpamBase\" dataset**\n\n*Description: Two line plots are shown side-by-side.  Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n", "original_text": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset. "}, "hash": "ac822ec34e33fcd0dab5d47377faf9f473287bf88eae21d1d470e257c6c0a3c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70138687-2393-43ad-a11e-1908322e394c", "node_type": "1", "metadata": {"window": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n", "original_text": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n"}, "hash": "956dd05190a32a47c80c19d7dd0bdcafc4d124690dfafc7fd2569d03d3ce37a9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The sample size on the x-axis goes up to 8192.\n", "mimetype": "text/plain", "start_char_idx": 39333, "end_char_idx": 39380, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70138687-2393-43ad-a11e-1908322e394c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n", "original_text": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e09dce4-0fd3-4942-81ff-75d6da7a7b90", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Both plot AUROC versus Sample size (on a log scale from 8 to 2048).  Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n", "original_text": "The sample size on the x-axis goes up to 8192.\n"}, "hash": "c3d38e0b7c29164f1994855ba5b4892560a42e0b67a8e3658db819b80813baea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cb652567-3c57-49bf-8635-d09e5c43a0eb", "node_type": "1", "metadata": {"window": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n", "original_text": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size."}, "hash": "050a778b12de5d2d01865fe12dfd15ce854d1207649f95e00576b0a1fc4b94a6", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n", "mimetype": "text/plain", "start_char_idx": 39380, "end_char_idx": 39609, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cb652567-3c57-49bf-8635-d09e5c43a0eb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n", "original_text": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70138687-2393-43ad-a11e-1908322e394c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Different colored lines represent different numbers of \"Variables per split\" (1, 2, 5).\n The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n", "original_text": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n"}, "hash": "b353549d996b038f189472a15a062830ba11b383608e73040da5a65585581e11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a64d96a4-167f-44c5-96b1-618032d4e1db", "node_type": "1", "metadata": {"window": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below. ", "original_text": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n"}, "hash": "e0c4f47a874bc0da76d57efbccefb55282a26c48a2afea0eab74c447cbd78817", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size.", "mimetype": "text/plain", "start_char_idx": 39609, "end_char_idx": 39772, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a64d96a4-167f-44c5-96b1-618032d4e1db", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below. ", "original_text": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cb652567-3c57-49bf-8635-d09e5c43a0eb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left plot, \"AUROC by sample size, SpamBase datase (Pooled gain splits)\", shows that for all variable settings, the AUROC peaks at a sample size of 32 or 64 and then decreases as the sample size grows.\n The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n", "original_text": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size."}, "hash": "fdee7b3024be3fc00390111e49e220b967503a267f141519191c50c3cefc9a1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a60be3e-3235-4959-9e06-5974bbf5f228", "node_type": "1", "metadata": {"window": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n", "original_text": "*   200 trees.\n"}, "hash": "d27540369c7e0d0a9a2a040b2c54cfbdcc253e3b1cdbcf70661fe4aa6c135f92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n", "mimetype": "text/plain", "start_char_idx": 39772, "end_char_idx": 40071, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6a60be3e-3235-4959-9e06-5974bbf5f228", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n", "original_text": "*   200 trees.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a64d96a4-167f-44c5-96b1-618032d4e1db", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right plot, \"AUROC by sample size, SpamBase datase (Uniformly-random splits)\", shows a similar trend where AUROC peaks at a small sample size (around 64) and then declines. *\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below. ", "original_text": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n"}, "hash": "6afd11c69d6214d2ec80a84f465c55a8611435ae93104da2eafb8cf8863f8373", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f23a8ba8-e18b-4415-9ddb-e2c2fd0ca62f", "node_type": "1", "metadata": {"window": "The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n", "original_text": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n"}, "hash": "600221bab3e08c2569fd5fc0e1248b78ca79e1001f33d96b52b93d9504afad31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   200 trees.\n", "mimetype": "text/plain", "start_char_idx": 40071, "end_char_idx": 40086, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f23a8ba8-e18b-4415-9ddb-e2c2fd0ca62f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n", "original_text": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6a60be3e-3235-4959-9e06-5974bbf5f228", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nYet in other datasets, sub-sampling is not helpful, with trees fitted to the full number of rows achieving better performance:\n\n***\n**Figure 11: AUROC by sample size and model characteristics in sub-sampled \"Forest-Cover\" dataset**\n\n*Description: Two line plots similar to Figure 10, but for the ForestCover dataset.  The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n", "original_text": "*   200 trees.\n"}, "hash": "bd6795284eb300eb2040aefee757a2458e6d64da56fc94198308a7c187a6cc21", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10f26222-befd-4e39-b1b8-7c621a59b21f", "node_type": "1", "metadata": {"window": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. ", "original_text": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n"}, "hash": "08d5e566c99770fc731ac8aa77052de39b941fc414fc204f9336a1519e3e1689", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n", "mimetype": "text/plain", "start_char_idx": 40086, "end_char_idx": 40341, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "10f26222-befd-4e39-b1b8-7c621a59b21f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. ", "original_text": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f23a8ba8-e18b-4415-9ddb-e2c2fd0ca62f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The sample size on the x-axis goes up to 8192.\n The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n", "original_text": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n"}, "hash": "15466cc9eec2c8f151cd4848760f7c20fd7325f89e539d4697336b744a105f8c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4193e45-f36e-45bd-938d-0c1b7da4e3b4", "node_type": "1", "metadata": {"window": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n", "original_text": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n"}, "hash": "dd92d4e316bfb26e3d1c606a3405d3ca4b5110140605ba67ae0e810fdad0b65b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n", "mimetype": "text/plain", "start_char_idx": 40341, "end_char_idx": 40561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4193e45-f36e-45bd-938d-0c1b7da4e3b4", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n", "original_text": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10f26222-befd-4e39-b1b8-7c621a59b21f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The left plot, \"AUROC by sample size, ForestCover dataset... (Pooled gain splits)\", shows that AUROC generally increases with sample size for all variable settings, with the best performance achieved at the largest sample sizes.\n The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. ", "original_text": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n"}, "hash": "a959949f54057ddf7ff95b96a044e8eb06839e2f242b2aa669524cbeccc5f03a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42babc6-d16f-4eb3-8499-26dcfbe64202", "node_type": "1", "metadata": {"window": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n", "original_text": "The full procedure for producing trees and anomaly scores is outlined below. "}, "hash": "8b31817b26c32d9013483470d9c6594bf96be54d0b928a68e4037c0b338e367e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n", "mimetype": "text/plain", "start_char_idx": 40561, "end_char_idx": 40681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e42babc6-d16f-4eb3-8499-26dcfbe64202", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n", "original_text": "The full procedure for producing trees and anomaly scores is outlined below. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4193e45-f36e-45bd-938d-0c1b7da4e3b4", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The right plot, \"...(Uniformly-random splits)\", shows a similar trend, with AUROC consistently improving as the sample size increases toward the full dataset size. *\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n", "original_text": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n"}, "hash": "a03c53d7199497c213dd65aa3a9609674dc28974460a260a6b5284e40c8dd94a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c7044461-7d0d-4372-9e53-739bc6171623", "node_type": "1", "metadata": {"window": "*   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). ", "original_text": "The implementation produced here is made open source and freely available\u00b3.\n\n"}, "hash": "12e40d52f2a40c6e6f5d720ffccf4e3fba866cf17b70721779b57ae0889e612e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The full procedure for producing trees and anomaly scores is outlined below. ", "mimetype": "text/plain", "start_char_idx": 40681, "end_char_idx": 40758, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c7044461-7d0d-4372-9e53-739bc6171623", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). ", "original_text": "The implementation produced here is made open source and freely available\u00b3.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42babc6-d16f-4eb3-8499-26dcfbe64202", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*\n***\n\nAs such, a potentially safe combination of hyperparameters for using FCF in anomaly detection would be as follows:\n\n*   Trees grown until full isolation (only one point is present in a given branch of a split) or until no further split is possible, in which case a remainder is extrapolated.\n *   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n", "original_text": "The full procedure for producing trees and anomaly scores is outlined below. "}, "hash": "818b7c9d74a5c8d2c2d723a63151a27160e7f6b046deff9f687e88e08bf64ed9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54bf9f13-225d-4c34-aae7-4623ea1c026b", "node_type": "1", "metadata": {"window": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. ", "original_text": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n"}, "hash": "42e53ae3bafd3946c6bcaa1de233bc255924fc7ad06677d7983bfae2e19e1764", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The implementation produced here is made open source and freely available\u00b3.\n\n", "mimetype": "text/plain", "start_char_idx": 40758, "end_char_idx": 40835, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "54bf9f13-225d-4c34-aae7-4623ea1c026b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. ", "original_text": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c7044461-7d0d-4372-9e53-739bc6171623", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   200 trees.\n *   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). ", "original_text": "The implementation produced here is made open source and freely available\u00b3.\n\n"}, "hash": "1826f723f75432d0c0e798eccad0916e331dc5ccf825172e887d6b0e1b8e8ec2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7d20a6f3-3eac-4089-a467-38a432d88aae", "node_type": "1", "metadata": {"window": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n", "original_text": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. "}, "hash": "b835f21f3e3be2ef00ed2da255585efaa3e7756e5f5b7c0a18de20ff48e446d0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n", "mimetype": "text/plain", "start_char_idx": 40835, "end_char_idx": 42336, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7d20a6f3-3eac-4089-a467-38a432d88aae", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n", "original_text": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54bf9f13-225d-4c34-aae7-4623ea1c026b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   A sample size of 32 to 256 points chosen without replacement to be used by each tree, but with numbers much smaller or much larger than this resulting in better results in some datasets (perhaps smaller numbers being better for multi-modal datasets).\n *   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. ", "original_text": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n"}, "hash": "2d01d0c5d47c2a91555dc5a3bbd7526fb4ded1b682c326270473f3e31fea37a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "07d7e4d1-035b-44d1-8a6c-507f06096642", "node_type": "1", "metadata": {"window": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). ", "original_text": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n"}, "hash": "8d25774a8cfa1dd2bd8c756111c8c095f6be48cfc927f6e0f39c622fe49f6423", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. ", "mimetype": "text/plain", "start_char_idx": 42336, "end_char_idx": 43517, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "07d7e4d1-035b-44d1-8a6c-507f06096642", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). ", "original_text": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7d20a6f3-3eac-4089-a467-38a432d88aae", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Splitting data according to a threshold on a random linear combination of 2 variables, but with a larger number of variables and with single-variable (axis-parallel) splits producing better results in some datasets.\n *   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n", "original_text": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here. "}, "hash": "eee8b7f5835c6ea420885ea492afa7e591d44bf4558b2d8f281fe5ba4907cd53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9700ccc9-6d3a-400d-b2e9-f463af0ae857", "node_type": "1", "metadata": {"window": "The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n", "original_text": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n"}, "hash": "691e4a5ec85c092c7d3728871f9bf990a88660dc2938aa5310b337fb566cd163", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n", "mimetype": "text/plain", "start_char_idx": 43517, "end_char_idx": 43707, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9700ccc9-6d3a-400d-b2e9-f463af0ae857", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n", "original_text": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07d7e4d1-035b-44d1-8a6c-507f06096642", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   Only one trial of a uniformly-randomly-chosen candidate variable or random linear combination to try at each node.\n\n The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). ", "original_text": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n"}, "hash": "5d144dd47d68905292ed13d9b7d8e0c0fd561fe68877249067e6d5561dc2c5bd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a25158ad-f7ec-4e16-880d-da45c4d68fcd", "node_type": "1", "metadata": {"window": "The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n", "original_text": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). "}, "hash": "7276b721dd5cbefeadb7fda09faba3d0afcb5a8da64ed5bd7f202631a51390ef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n", "mimetype": "text/plain", "start_char_idx": 43707, "end_char_idx": 43824, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a25158ad-f7ec-4e16-880d-da45c4d68fcd", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n", "original_text": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9700ccc9-6d3a-400d-b2e9-f463af0ae857", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The full procedure for producing trees and anomaly scores is outlined below.  The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n", "original_text": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n"}, "hash": "74ee26ea1ca175b57b33247c5d14c483f8846b2e021d1e28196ff071e0bb7b03", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a6d0c43-e3ef-472e-8691-78c9e3fe4314", "node_type": "1", "metadata": {"window": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n", "original_text": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. "}, "hash": "29c47f0d9887f3b54bf0110a10b9d45ec8ea97f2ff9f3d8f86e6a0214faedb92", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). ", "mimetype": "text/plain", "start_char_idx": 43824, "end_char_idx": 43970, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5a6d0c43-e3ef-472e-8691-78c9e3fe4314", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n", "original_text": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a25158ad-f7ec-4e16-880d-da45c4d68fcd", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The implementation produced here is made open source and freely available\u00b3.\n\n \u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n", "original_text": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T). "}, "hash": "7b91f2f2f08cad1c767ba57bb6da6aafae841792732a87bbc758dfdfdd084e22", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4c7ff1b-89c6-44e1-92e3-3506d38963ec", "node_type": "1", "metadata": {"window": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n", "original_text": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n"}, "hash": "14196009ab04b0db9b3a6c1e82c465b4a43321acfc95b481296188020cb0125f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. ", "mimetype": "text/plain", "start_char_idx": 43970, "end_char_idx": 44184, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b4c7ff1b-89c6-44e1-92e3-3506d38963ec", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n", "original_text": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a6d0c43-e3ef-472e-8691-78c9e3fe4314", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\u00b3https://www.github.com/david-cortes/isotree\n\n**Algorithm 1 FairCutTree**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, current depth d\n1: **if** m = 1 **then**\n2:   Set as terminal node with depth d\n3: **if** each of the n columns in **X** has the same value across all of its rows **then**\n4:   Set as terminal node with depth d + E[depth(m)]\n5: Initialize vector **z** := 0\u1d50\n6: **for** 1..p **do**\n7:   Choose a variable v uniformly at random from [1,n] such that it contains more than 1 unique value within **X**, and define **y** = **X**[:,v] (vector with the values of **X** in that variable)\n8:   Draw a random coefficient c ~ Normal(0, 1)\n9:   Update **z** := **z** + c * **y** / \u03c3\u1d67\n10: Find the point s that minimizes\n    (m_left\u03c3_zl + m_right\u03c3_zr) / (m_left + m_right)\n    Where **z**_l = {z \u2208 **z** | z \u2264 s}, **z**_r = {z \u2208 **z** | z > s}, and \u03c3_z is the standard deviation of **z**\n11: Define **X**_l = {x\u1d62 \u2208 **X** | z\u1d62 \u2264 s} and **X**_r = {x\u1d62 \u2208 **X** | z\u1d62 > s}\n12: Set Node_L = FairCutTree(**X**_l, p, d + 1) and Node_R = FairCutTree(**X**_r, p, d + 1)\n\n---\n\n**Algorithm 2 Fair-Cut Forest**\n\n**Inputs** Input data points X\u2098\u2093\u2099, number of splitting variables p, number of trees t, sample size s\n1: **for** 1..t **do**\n2:   Choose a set S of s points uniformly at random from [1, m] without replacement, and set **X**_S = {x\u1d62 \u2208 **X** | i \u2208 S}\n3:   Set Tree\u1d62 = FairCutTree(**X**_S, p, 0)\n4: Calculate q = E[depth(m)]\n5: **return** The generated set T of t trees and q.\n\n ---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n", "original_text": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree. "}, "hash": "b2c130c385e312124e5b719c0d0f355e48836ee0edbb61518a6dd665b8557111", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1eab446a-2ec6-4b3e-a136-d5d875a6acb1", "node_type": "1", "metadata": {"window": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n", "original_text": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). "}, "hash": "e6dd0df86e85805253e36653fa692d95f0a09bd52ec8b16e0273cea5b39030ed", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n", "mimetype": "text/plain", "start_char_idx": 44184, "end_char_idx": 44337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1eab446a-2ec6-4b3e-a136-d5d875a6acb1", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n", "original_text": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4c7ff1b-89c6-44e1-92e3-3506d38963ec", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "---\n\n**Algorithm 3 TreeScore**\n\n**Inputs** Input point vector **x**_n, tree node N\n1: **if** N is a terminal node **then**\n2:   **return** The associated depth d from N\n3: **else**\n4:   initialize z := 0\n5:   **for** v = 1..p from the variables v \u2208 N **do**\n6:     Update z := z + c\u1d65 * y\u1d65 / \u03c3\u1d67\u1d65 with the c\u1d65, y\u1d65 and \u03c3\u1d67\u1d65 that were determined for variable v in N\n7:   **if** z \u2264 s from the optimal s in N **then**\n8:     **return** TreeScore(**x**, Node_L) from the nodes in N\n9:   **else**\n10:    **return** TreeScore(**x**, Node_R) from the nodes in N\n\n---\n\n**Algorithm 4 ScorePoint**\n\n**Inputs** Input point vector **x**_n, forest of t fair-cut trees **T**, expected isolation depth q\n1: Initialize d = 0\n2: **for** i = 1..t **do**\n3:   Update d := d + ScoreTree(**x**, Node\u2080 from T\u1d62)\n4: **return** 2^-( (d/t) / q )\n\n## 7 Evaluating different methods\n\nThe variation of the \"Fair-Cut forest\" algorithm or FCF proposed here was compared against IFOREST, some of its variations, and other competing methods for anomaly detection, including:\n\n*   The original \"isolation forest\u201d or IFOREST from [14], reimplemented according to the paper and sharing the same codebase as the FCF here.  The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n", "original_text": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n"}, "hash": "b4f6a917b6e9c3c73b39096eecd3c1c17482b68d667fbfae0743501bac9d944e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "062d67a8-f6c7-4616-aac0-2c44df73eec1", "node_type": "1", "metadata": {"window": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g. ", "original_text": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n"}, "hash": "509d5fa4fa24de71688eb0a569b7173062a30ecf5a0819fa07f5a57267177f18", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). ", "mimetype": "text/plain", "start_char_idx": 44337, "end_char_idx": 44646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "062d67a8-f6c7-4616-aac0-2c44df73eec1", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g. ", "original_text": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1eab446a-2ec6-4b3e-a136-d5d875a6acb1", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The hyperparameters used were the ones suggested in [14] (100 trees, 256 samples per tree, maximum depth of 8), and most of the other algorithms are based around these same hyperparameters.\n *   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n", "original_text": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D). "}, "hash": "12cf633bc46a891d9ec6c30a6d797ef25f91bc721f186f7de3a89befffe91232", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37763d2a-7bf1-4b81-ad3e-82b447c7471f", "node_type": "1", "metadata": {"window": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF. ", "original_text": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n"}, "hash": "5d6f88d9d8b6a49c2312b61fb6d4531c5c76322d782c0c72db65b6da16d8bd32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n", "mimetype": "text/plain", "start_char_idx": 44646, "end_char_idx": 44877, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "37763d2a-7bf1-4b81-ad3e-82b447c7471f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF. ", "original_text": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "062d67a8-f6c7-4616-aac0-2c44df73eec1", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The original IFOREST but with the same hyperparameters set for FCF (200 trees, unlimited depth, marked as IF-U).\n *   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g. ", "original_text": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n"}, "hash": "46f79764fdabbd73200549fd577b7fe01c5446f2fdc7e089ef0808634745f04d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ebe6eec-3ebf-49f6-836e-85f274b0371f", "node_type": "1", "metadata": {"window": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. ", "original_text": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n"}, "hash": "0649a1afcc744dd3df82cd7f22103080246c2c729bf72bc466212d3106eeba75", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n", "mimetype": "text/plain", "start_char_idx": 44877, "end_char_idx": 44969, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8ebe6eec-3ebf-49f6-836e-85f274b0371f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. ", "original_text": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37763d2a-7bf1-4b81-ad3e-82b447c7471f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"extended isolation forest\" or EIF from [12], using both the authors' implementation\u2074 (EIF-O) and and independent implementation (EIF-T).  Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF. ", "original_text": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n"}, "hash": "63606eeab2eac6912e395b2158e0c9a025a85484d2d4a5dee2d1b2b5eb14e424", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e910eeb0-7cdd-45f9-9557-2c2ba334bfae", "node_type": "1", "metadata": {"window": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n", "original_text": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n"}, "hash": "76782c7bb43af66b65d7a6af920723e79f56c07d348e95cd258ebabb6d0e891b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n", "mimetype": "text/plain", "start_char_idx": 44969, "end_char_idx": 45216, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e910eeb0-7cdd-45f9-9557-2c2ba334bfae", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n", "original_text": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ebe6eec-3ebf-49f6-836e-85f274b0371f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that the implementation here differs from the original EIF in one important aspect: it standardizes variables by calculating the standard deviations at each node rather than just at the beginning of the tree.  The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. ", "original_text": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n"}, "hash": "77007d3c3f28ff00614674ba30dbd59b770e53ac41be987395f73066d3545ef4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b99ea284-53d6-4256-9165-470927b9e5d0", "node_type": "1", "metadata": {"window": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. ", "original_text": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n"}, "hash": "ada240ce7418582b0f6f9d45cd9096e0f0fd5a099a85deef1def1a9127a45fc7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n", "mimetype": "text/plain", "start_char_idx": 45216, "end_char_idx": 45378, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b99ea284-53d6-4256-9165-470927b9e5d0", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. ", "original_text": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e910eeb0-7cdd-45f9-9557-2c2ba334bfae", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The extension level was set to 1 (splitting by 2 variables at a time) as recommended by the authors, plus the same default hyperparameters from IFOREST.\n *   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n", "original_text": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n"}, "hash": "e50af05147294e9dfa85ceef1ff5df0dd330689dec68838f8b460cca11bf58a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0c32ca40-e083-429b-8bcc-2d83d638fe81", "node_type": "1", "metadata": {"window": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n", "original_text": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g. "}, "hash": "e53355f3e2fdcc3fb9eb2416068f4cffddf7e80bff48413352fc76cf64a5c0a8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n", "mimetype": "text/plain", "start_char_idx": 45378, "end_char_idx": 45482, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0c32ca40-e083-429b-8bcc-2d83d638fe81", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n", "original_text": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b99ea284-53d6-4256-9165-470927b9e5d0", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"robust random-cut forest\u201d or RRCF from [11], using a publicy-available implementation\u2075 that uses the \"CoDisplacement\u201d metric as proposed in [11] for anomaly scoring (denoted as RRCF-C), and a different public implementation\u2076 that uses average isolation depth for anomaly scoring (denoted as RRCF-D).  Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. ", "original_text": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n"}, "hash": "e991da9234443bda37eb288336ae11f41af334ffb7cd125ebeff1b81fa3369f6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "119c7eca-d17c-4596-9c51-b99f89f31c49", "node_type": "1", "metadata": {"window": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n", "original_text": "range penalties at prediction time) and sharing the same codebase as FCF. "}, "hash": "aa67fdc351db68566af51694e6297097827225e3552b12fc7f2bd4b68ffb4c29", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g. ", "mimetype": "text/plain", "start_char_idx": 45482, "end_char_idx": 45572, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "119c7eca-d17c-4596-9c51-b99f89f31c49", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n", "original_text": "range penalties at prediction time) and sharing the same codebase as FCF. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0c32ca40-e083-429b-8bcc-2d83d638fe81", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that both of them are pure-Python implementations, which makes them much slower than the rest despite the idea being in theory relatively fast to execute, and that the second implementation did not allow setting random seeds.\n *   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n", "original_text": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g. "}, "hash": "10767a375d62a372cf83416fd3a81ff33095b661c3ad8360ed7eb02fe19e9598", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b18b86c-265a-428f-bc2e-bac1f600c946", "node_type": "1", "metadata": {"window": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n", "original_text": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. "}, "hash": "399e2ac8d64230eaffe72ce8175549167bb7738152c24acc968e2fa6929ab88e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "range penalties at prediction time) and sharing the same codebase as FCF. ", "mimetype": "text/plain", "start_char_idx": 45572, "end_char_idx": 45646, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6b18b86c-265a-428f-bc2e-bac1f600c946", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n", "original_text": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "119c7eca-d17c-4596-9c51-b99f89f31c49", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"density estimation trees\" or DET from [17], using the implementation from MLPack\u2077.\n *   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n", "original_text": "range penalties at prediction time) and sharing the same codebase as FCF. "}, "hash": "7167e8ccf2c1f4bd857cb420e100d15022b2fd3a0870a911c13eafeea2c5bd1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "596225a5-aee4-4bae-9f00-00908a6cc30b", "node_type": "1", "metadata": {"window": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. ", "original_text": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n"}, "hash": "d3fb2e9333ec036d25f0db118dbcac52593488445ae486c561e88e9406f1fa31", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. ", "mimetype": "text/plain", "start_char_idx": 45646, "end_char_idx": 45815, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "596225a5-aee4-4bae-9f00-00908a6cc30b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. ", "original_text": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b18b86c-265a-428f-bc2e-bac1f600c946", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   A \"forest\" (DEF) of 100 DETs using sub-samples of 256 points each, sampled without replacement, and the trees built without pruning, in order to make it directly comparable to IFOREST, with the final prediction averaged across these 100 DETs.\n *   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n", "original_text": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier. "}, "hash": "3185f06c0bd6d576e989cb012d8daa306e19c885ce3195b48fae380ceda58af7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15ef275e-3b71-4724-a6fa-c61886fbf275", "node_type": "1", "metadata": {"window": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains. ", "original_text": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. "}, "hash": "b0719f83ee2cf5f1601b1887fa239bfe7e4660735cefc156b7ce19f4d1f8755d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n", "mimetype": "text/plain", "start_char_idx": 45815, "end_char_idx": 46016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "15ef275e-3b71-4724-a6fa-c61886fbf275", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains. ", "original_text": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "596225a5-aee4-4bae-9f00-00908a6cc30b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"generalized isolation forest\" or GIF from [5], using the implementation provided by the authors\u2078 and the hyperparameters suggested in their web example.\n *   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. ", "original_text": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n"}, "hash": "1ee808384d5b7c45a46c866eef0f602cfb0cce5926d5c1ee2bee366b6b2efe6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "887506fc-c31c-42a0-8501-66d21c45d313", "node_type": "1", "metadata": {"window": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. ", "original_text": "Note that this implementation did not allow setting random seeds and was ran only once.\n"}, "hash": "b460d842b18d8d7bd2c79791f84b55cf93f658c89e684d95e2e99e17d5b8579b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. ", "mimetype": "text/plain", "start_char_idx": 46016, "end_char_idx": 46205, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "887506fc-c31c-42a0-8501-66d21c45d313", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. ", "original_text": "Note that this implementation did not allow setting random seeds and was ran only once.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15ef275e-3b71-4724-a6fa-c61886fbf275", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"one-class random forest\u201d or OCRF from [10], using the implementation provided by the authors\u2079.\n *   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains. ", "original_text": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference. "}, "hash": "302b43e08b2391a0153b6269d96d6412c11c4bf1505569b545fda9002305abf4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "218a9845-162c-4d06-bc8a-e4c47e69fba1", "node_type": "1", "metadata": {"window": "range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. ", "original_text": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n"}, "hash": "f7dba2ab43bf140f0608fccc0f75c894ad4b64968e421b87d0202c544d7f0493", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note that this implementation did not allow setting random seeds and was ran only once.\n", "mimetype": "text/plain", "start_char_idx": 46205, "end_char_idx": 46293, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "218a9845-162c-4d06-bc8a-e4c47e69fba1", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. ", "original_text": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "887506fc-c31c-42a0-8501-66d21c45d313", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The SCIFOREST from [15], reimplemented according to their description (including e.g.  range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. ", "original_text": "Note that this implementation did not allow setting random seeds and was ran only once.\n"}, "hash": "8a1e77c4d8882112acb41f9a873cfcf030d5306b76b0b7cb8716444b6d86bf5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2636a58e-d8e1-4783-8ad3-85ac88a8618d", "node_type": "1", "metadata": {"window": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. ", "original_text": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n"}, "hash": "26b9be46ad65933b48fc76235eedc0d80fb947c8db9fe95ceca7e9f932207c43", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n", "mimetype": "text/plain", "start_char_idx": 46293, "end_char_idx": 46387, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2636a58e-d8e1-4783-8ad3-85ac88a8618d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. ", "original_text": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "218a9845-162c-4d06-bc8a-e4c47e69fba1", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "range penalties at prediction time) and sharing the same codebase as FCF.  Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. ", "original_text": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n"}, "hash": "2237b8edc7e180cbbf176b872f4462865e28eae5969f11bcc9126538680b09e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcdc148b-7f27-49b9-bab6-767f47c5ecd5", "node_type": "1", "metadata": {"window": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. ", "original_text": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. "}, "hash": "aebd790c8b9f243306b5e8a00d5c3be5734359a258f95ea19b77c82b4942bd0f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n", "mimetype": "text/plain", "start_char_idx": 46387, "end_char_idx": 46535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dcdc148b-7f27-49b9-bab6-767f47c5ecd5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. ", "original_text": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2636a58e-d8e1-4783-8ad3-85ac88a8618d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Another variation with 200 trees grown until full isolation of every point was also explored (SCIF-U), driven by the expected isolation depth issue highlighted earlier.  Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. ", "original_text": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n"}, "hash": "f1e26155d58ae2ab369a52c5b5bb5e833d269ae5e295774cacc04e11fa7819aa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "519dff4a-525d-431a-844b-9ef4ea65831c", "node_type": "1", "metadata": {"window": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n", "original_text": "minority-class, extreme-valued, minority-mode) and problem domains. "}, "hash": "1b0fd8771d119283e00a810ae3b1c358af1f78d61f7b97c74a2d3b15e7262943", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. ", "mimetype": "text/plain", "start_char_idx": 46535, "end_char_idx": 46675, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "519dff4a-525d-431a-844b-9ef4ea65831c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n", "original_text": "minority-class, extreme-valued, minority-mode) and problem domains. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcdc148b-7f27-49b9-bab6-767f47c5ecd5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that the original IFOREST code from the authors\u00b9\u2070 (of both IF and SCIFOREST) included a similar decision criterion but it used only single-variable splits, and was thus not compared against here.\n *   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. ", "original_text": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g. "}, "hash": "adfa2a73733d8aa51f494359f2445b9386484094eb5a3cf3081c631ee1e72972", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "572e65bb-bcf5-40fe-a528-671132a0732a", "node_type": "1", "metadata": {"window": "Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. ", "original_text": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. "}, "hash": "ab5273dbb3bff4929f5ec39658bb9f6e4f25206bdf57b744caf2eaf3b9e8bfad", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "minority-class, extreme-valued, minority-mode) and problem domains. ", "mimetype": "text/plain", "start_char_idx": 46675, "end_char_idx": 46743, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "572e65bb-bcf5-40fe-a528-671132a0732a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. ", "original_text": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "519dff4a-525d-431a-844b-9ef4ea65831c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"isolation nearest-neighbor ensembles\u201d or iNNE from [2], using one of the implementations from the authors\u00b9\u00b9, with hyperparameters \u03c8 = 32 and t = 100 as suggested in the reference.  Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n", "original_text": "minority-class, extreme-valued, minority-mode) and problem domains. "}, "hash": "a330be9e9d562e7d84c74f09f334ce534395f9a0807214c4549b7c5c5f5513a2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f929419-6bd7-4331-a4d7-bc242940132a", "node_type": "1", "metadata": {"window": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". ", "original_text": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. "}, "hash": "1f7a21ae42582775aa0cb0bc717f914feec8362ac956fad8bdda42d3a2529626", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. ", "mimetype": "text/plain", "start_char_idx": 46743, "end_char_idx": 47488, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9f929419-6bd7-4331-a4d7-bc242940132a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". ", "original_text": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "572e65bb-bcf5-40fe-a528-671132a0732a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note that this implementation did not allow setting random seeds and was ran only once.\n *   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. ", "original_text": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available. "}, "hash": "20059be153401148c9ffb8f3ecc445aeaf42e0f7708bad4a47573a812850b05e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91974869-7310-4834-8ad9-0e35900b4978", "node_type": "1", "metadata": {"window": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. ", "original_text": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. "}, "hash": "2509f1028880691a2bed4bed6a0a9a1c43f984f53118f0088fb2567af8fc6cfd", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. ", "mimetype": "text/plain", "start_char_idx": 47488, "end_char_idx": 47635, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91974869-7310-4834-8ad9-0e35900b4978", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. ", "original_text": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f929419-6bd7-4331-a4d7-bc242940132a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"local outlier factor\" or LOF from [4], using the implementation from scikit-learn\u00b9\u00b2.\n *   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". ", "original_text": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data. "}, "hash": "19e728967208d4b1c2fc96f40fd4352f93e59b756b428836b4697475fdd580fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d7f6354-40a1-4151-98fe-40aefe4da389", "node_type": "1", "metadata": {"window": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n", "original_text": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. "}, "hash": "a9485cf827320c96306a06cdbbac30218f97745484c71ea552c8930b6be86d8f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. ", "mimetype": "text/plain", "start_char_idx": 47635, "end_char_idx": 47977, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d7f6354-40a1-4151-98fe-40aefe4da389", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n", "original_text": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91974869-7310-4834-8ad9-0e35900b4978", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   The \"one-class support vector machine\u201d or OCSVM from [19] in its linear and RBF kernel versions, using the implementation from scikit-learn\u00b9\u00b3.\n\n Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. ", "original_text": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC. "}, "hash": "91386cb336bf9ddcc600ee5718b00bf6298c7ed2023fb513653d025f87900f31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d9527975-927c-4b9f-8d79-ca5546dd861c", "node_type": "1", "metadata": {"window": "minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. ", "original_text": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n"}, "hash": "52c27406291a17003ecd82bb3efdab245e33b1c13c50da5d744b2ec94540834c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. ", "mimetype": "text/plain", "start_char_idx": 47977, "end_char_idx": 48138, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d9527975-927c-4b9f-8d79-ca5546dd861c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. ", "original_text": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d7f6354-40a1-4151-98fe-40aefe4da389", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Models are evaluated on public datasets downloaded from the ODDS repository\u00b9\u2074 and from [6]\u00b9\u2075, representing a variety of outlier types (e.g.  minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n", "original_text": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs. "}, "hash": "2e8b5f9b869b92b61be282e3022b41dab88d0135aaf25b09d15ded14d3b28b6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3af3346d-d918-41ee-bcfc-b01563e8f119", "node_type": "1", "metadata": {"window": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n", "original_text": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. "}, "hash": "d4d05a090c984f5ec00b8e02712b8ff4364cdfcada3b6a0ecd18be2641e63ce1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n", "mimetype": "text/plain", "start_char_idx": 48138, "end_char_idx": 48297, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3af3346d-d918-41ee-bcfc-b01563e8f119", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n", "original_text": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9527975-927c-4b9f-8d79-ca5546dd861c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "minority-class, extreme-valued, minority-mode) and problem domains.  All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. ", "original_text": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n"}, "hash": "eecf24e66455dff871b0a679012937798061e32bb6c977a63c97b62dde6861eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4abadefe-6e0f-4394-858b-87072cce7252", "node_type": "1", "metadata": {"window": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. ", "original_text": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". "}, "hash": "0618bad7d9545ac8b87d7232db0f180372c5503705eca55a50726d87f1ccb1ea", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. ", "mimetype": "text/plain", "start_char_idx": 48297, "end_char_idx": 48490, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4abadefe-6e0f-4394-858b-87072cce7252", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. ", "original_text": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3af3346d-d918-41ee-bcfc-b01563e8f119", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "All datasets are taken in their raw form (which differs from how some of the references use them), prefering the version from ODDS\n\n\u2074https://github.com/sahandha/eif\n\u2075https://github.com/kLabUM/rrcf\n\u2076https://github.com/navdeep-G/robust-random-cut-forest\n\u2077https://www.mlpack.org/doc/mlpack-3.0.4/doxygen/dettutorial.html\n\u2078https://github.com/philippjh/genif\n\u2079https://github.com/ngoix/OCRF\n\u00b9\u2070https://sourceforge.net/projects/iforest/\n\u00b9\u00b9https://github.com/zhuye88/iNNE\n\u00b9\u00b2https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html\n\u00b9\u00b3https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html\n\u00b9\u2074http://odds.cs.stonybrook.edu\n\u00b9\u2075https://www.dbs.ifi.lmu.de/research/outlier-evaluation/\n\nwhen available.  The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n", "original_text": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model. "}, "hash": "1ef7eab8a8534cc7a62c1be30a1446cbefea9a4fff5c8022a5443812037d83d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c9b75ab-fb12-4860-b581-391abab60672", "node_type": "1", "metadata": {"window": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n", "original_text": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. "}, "hash": "e029e0c0c6335ee09d2896bdf4b2adcb6b44bb62539c0552818f26f595556b26", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". ", "mimetype": "text/plain", "start_char_idx": 48490, "end_char_idx": 48591, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8c9b75ab-fb12-4860-b581-391abab60672", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n", "original_text": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4abadefe-6e0f-4394-858b-87072cce7252", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The evaluation is done by fitting said models to the full data under their default hyperparameters and producing outlier scores on this same data.  These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. ", "original_text": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\". "}, "hash": "2208b906d9a1f39c356fc5e800a7cb07e19c57c2b84a54bf07aecea01144f1fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1722d431-6901-4e24-bb99-0d76ebe39659", "node_type": "1", "metadata": {"window": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. ", "original_text": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n"}, "hash": "84442cace6e862938dd018aa8faf3c7ef7e4e09671dd8a687f7bb63bf51414c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. ", "mimetype": "text/plain", "start_char_idx": 48591, "end_char_idx": 49014, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1722d431-6901-4e24-bb99-0d76ebe39659", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. ", "original_text": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c9b75ab-fb12-4860-b581-391abab60672", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "These scores are compared against the true labels for outlierness (which the models do not observe in their fitting procedures) in terms of area under the receiver-operating characteristic curve (ROC) and area under the precision-recall curve (PR), which as analyzed in [8] might be a more appropriate metric than the more commonly used ROC.  Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n", "original_text": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g. "}, "hash": "6f7cbcec64187b8f02196b39c377863ed4bdb4a52822ebb76cc025a41f7107a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eea78d83-a58a-49de-bad8-7c29981fd9f2", "node_type": "1", "metadata": {"window": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n", "original_text": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. "}, "hash": "3adff9183d9616134fc183922a568776a69f8a7bdad0b8ac07543829f494703c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n", "mimetype": "text/plain", "start_char_idx": 49014, "end_char_idx": 49221, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eea78d83-a58a-49de-bad8-7c29981fd9f2", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n", "original_text": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1722d431-6901-4e24-bb99-0d76ebe39659", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Each model that uses randomization is fitted 10 times using different random seeds, and the result reported here is calculated as the mean across these 10 runs.  For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. ", "original_text": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n"}, "hash": "46e68b4ab15e954328abb5ee6b78dae38c08809c7f6de13f0320914b419d29e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d931476-6fa6-4939-b4e1-cc76e8ff3960", "node_type": "1", "metadata": {"window": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n", "original_text": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n"}, "hash": "92a3b0b0cea1fba65146bd7d258f721cc80ecd3d8c1f59e90e4907808cb58a05", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. ", "mimetype": "text/plain", "start_char_idx": 49221, "end_char_idx": 49697, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0d931476-6fa6-4939-b4e1-cc76e8ff3960", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n", "original_text": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eea78d83-a58a-49de-bad8-7c29981fd9f2", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For larger datasets, some of the methods were not compared against due to the long running times that some of them would require as the volume of data grows.\n\n Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n", "original_text": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are. "}, "hash": "4146291a175991abb2ebe2b1f4fdf970b03867c737ceb3719c9839ae5485cf52", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "352e1c9a-3465-4481-8236-13c94fad28be", "node_type": "1", "metadata": {"window": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n", "original_text": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. "}, "hash": "5941739bd53e24a7ae016cea5a04ceb4404010bce8ee3ecbd1475e33ab1bd7bf", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n", "mimetype": "text/plain", "start_char_idx": 49697, "end_char_idx": 49984, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "352e1c9a-3465-4481-8236-13c94fad28be", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n", "original_text": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d931476-6fa6-4939-b4e1-cc76e8ff3960", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Experiments were run on an AMD Ryzen 7 2700 CPU with 8 cores running at 3.2GHz, and times are reported in seconds taken for fitting a given model plus producing outlier scores from said model.  Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n", "original_text": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n"}, "hash": "718c037965182e8820800526f430fe0bfd53131e44f05fe64c13e1cc0117516b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "96c81404-7f06-4923-8e12-4b26c4da62ce", "node_type": "1", "metadata": {"window": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. ", "original_text": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n"}, "hash": "68d30f651b463448933911e09d9f1be5195074de49f4f2c74cd45022f77be239", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. ", "mimetype": "text/plain", "start_char_idx": 49984, "end_char_idx": 50463, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "96c81404-7f06-4923-8e12-4b26c4da62ce", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. ", "original_text": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "352e1c9a-3465-4481-8236-13c94fad28be", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Where possible, software was compiled under GCC version 10.3 with options \"-O3\" and \"-march=native\".  Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n", "original_text": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable. "}, "hash": "d1012529f8604027f5d18cc1242c7033aa78b61fceaf68c91efb096e9f826e7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25d4bc19-1010-4900-8216-39db6c5f8cac", "node_type": "1", "metadata": {"window": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n", "original_text": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. "}, "hash": "516c606ec80900cfac46270a6e9ac80e47aa85d2833aafebaa2637459faa3da2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n", "mimetype": "text/plain", "start_char_idx": 50463, "end_char_idx": 50582, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "25d4bc19-1010-4900-8216-39db6c5f8cac", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n", "original_text": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96c81404-7f06-4923-8e12-4b26c4da62ce", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Note however that, for larger datasets, the times for tree-based methods are dominated by the calculation of outlier scores, which was done with the same data format as the models (column-major order, double precision) and through the same software library that produced each model, but these times could be reduced significantly if, for example, a row-major format were used and the decision trees were pre-compiled (e.g.  through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. ", "original_text": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n"}, "hash": "2b8e8367b436011148bf858aa5058050c1e3121c45f36161137b51776d6ead7d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "220762f4-600c-4de9-aa85-8aa0c93df19d", "node_type": "1", "metadata": {"window": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column. ", "original_text": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n"}, "hash": "3c06f2c70e20a8d64be27fa826d7549b88604b337b274ca675ad87659d27de7e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. ", "mimetype": "text/plain", "start_char_idx": 50582, "end_char_idx": 50800, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "220762f4-600c-4de9-aa85-8aa0c93df19d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column. ", "original_text": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25d4bc19-1010-4900-8216-39db6c5f8cac", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "through software such as \"treelite\u201d\u00b9\u2076), which would make all tree-based methods look significantly faster (roughly 10 to 30 times faster in the larger datasets if the compilation times are not considered).\n\n While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n", "original_text": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail. "}, "hash": "f08e52e7d4c09ce84f5306b65c195e8b03205bf996220d37362f12ad75c18540", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5312bf08-488f-4120-b34c-23d6b7fc6e24", "node_type": "1", "metadata": {"window": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n", "original_text": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n"}, "hash": "7f3b2a9e71c0e31173bd5ab2e136123772d519141a26d8d30b8328cfdb0d4fe0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n", "mimetype": "text/plain", "start_char_idx": 50800, "end_char_idx": 51016, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5312bf08-488f-4120-b34c-23d6b7fc6e24", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n", "original_text": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "220762f4-600c-4de9-aa85-8aa0c93df19d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "While others such as [5] and [2] perform hyperparameter tuning in their experiments based on the true outlier labels, in the case of anomaly detection, this is a problematic aspect as such labels are not supposed to be available in contexts in which models like these would be employed, and it can tend to favor model types which are only good under fine-tuned parameters specific to each dataset without providing a clear picture of how generalizable such models really are.  For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column. ", "original_text": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n"}, "hash": "a578f5fd4e371252f2fba30181d5d320cd023ff48e1d1b3e1123d62fa57f5778", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23ec646b-328d-42fc-84ab-02e408c2a9c7", "node_type": "1", "metadata": {"window": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. ", "original_text": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n"}, "hash": "e0ddf2babf12c1a011b4837fe0ae903c6ba3c72a042ab185ffff60c1e64b4631", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n", "mimetype": "text/plain", "start_char_idx": 51016, "end_char_idx": 51131, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "23ec646b-328d-42fc-84ab-02e408c2a9c7", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. ", "original_text": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5312bf08-488f-4120-b34c-23d6b7fc6e24", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, models such as GIF or iNNE can show either one of the best or one of the worst performances in a given dataset depending on the hyperparameters used, while models such as IF or EIF are quite generalizable across datasets and outlier types under the same set of parameters.\n\n | Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n", "original_text": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n"}, "hash": "03366a793d80bef8d30119e6838a39a82ab5754ec80d7e20e945d7f8aa8ce061", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fed94ecc-8fb0-4a11-85ea-fc2f0f5fb7fb", "node_type": "1", "metadata": {"window": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n", "original_text": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. "}, "hash": "386f5b966ca0c6f817c079bf79389fb73c9015ea5653c18d55a862486c8c3e27", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n", "mimetype": "text/plain", "start_char_idx": 51131, "end_char_idx": 51253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fed94ecc-8fb0-4a11-85ea-fc2f0f5fb7fb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n", "original_text": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23ec646b-328d-42fc-84ab-02e408c2a9c7", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 5: Datasets used for comparisons | | | |\n| :--- | :--- | :--- | :--- |\n| **Dataset** | **Rows** | **Columns** | **Outliers** |\n| Arrhythmia | 452 | 274 | 15% |\n| Pima | 768 | 8 | 35% |\n| SpamBase | 4,601 | 57 | 39.4% |\n| Satellite | 6,435 | 36 | 32% |\n| Pendigits | 6,870 | 16 | 2.27% |\n| Annthyroid | 7,200 | 6 | 7.42% |\n| Mnist | 7,603 | 100 | 9.2% |\n| ALOI | 50,000 | 27* | 3% |\n| Forest Cover | 286,048 | 10 | 0.9% |\n\n\\* This dataset contains a categorical variable.  As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. ", "original_text": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n"}, "hash": "9d3191cfb5e4f0099bddd0b5bb860253171b51f62204ee6032a4d128fb85fbb0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3f0fcc38-9334-42d2-942b-11bccbb7bcf7", "node_type": "1", "metadata": {"window": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value. ", "original_text": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n"}, "hash": "dc761dbf3d40d487585edb0ee8845cd75d5958c827adff3c2cd5cc1367c09170", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. ", "mimetype": "text/plain", "start_char_idx": 51253, "end_char_idx": 51420, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3f0fcc38-9334-42d2-942b-11bccbb7bcf7", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value. ", "original_text": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fed94ecc-8fb0-4a11-85ea-fc2f0f5fb7fb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "As most of the methods compared here do not work with categorical variables, it was left out for a fairer comparison.\n\n Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n", "original_text": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node. "}, "hash": "e41f1722ffdad4aea7c005cc1788e0e48282693cc46d7069690f398c70cf4ab1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98cbb647-3939-4b54-9264-e557f359765b", "node_type": "1", "metadata": {"window": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables. ", "original_text": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column. "}, "hash": "3d883e028453bebc3e208740dde440d7cf13df03b98b095d020653ada037eebe", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n", "mimetype": "text/plain", "start_char_idx": 51420, "end_char_idx": 51730, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "98cbb647-3939-4b54-9264-e557f359765b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables. ", "original_text": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3f0fcc38-9334-42d2-942b-11bccbb7bcf7", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Some additional comments about these datasets:\n\n\u00b9\u2076https://treelite.readthedocs.io/en/latest/index.html\n\n*   **Arrhythmia:** variables are a good mixture of integral, continuous, binary, near-symmetric, and power tail.  This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value. ", "original_text": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n"}, "hash": "6138335e9eafc82b798a1ada240985cc3a0bf3dd9c7d39eae9c1ead90ea14b10", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fbeaf3fc-6bc4-4fff-a74f-0700a8b5ffaf", "node_type": "1", "metadata": {"window": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n", "original_text": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n"}, "hash": "a89252d3250aa03e28ffd9410b84db9c43048de1dc6ba6f8b1ca6d79882dc62d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column. ", "mimetype": "text/plain", "start_char_idx": 51730, "end_char_idx": 51821, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "fbeaf3fc-6bc4-4fff-a74f-0700a8b5ffaf", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n", "original_text": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98cbb647-3939-4b54-9264-e557f359765b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This was originally a multi-class classification dataset about different types of cardiac arrythmia, in which the non-arrythmic cases were mixed together with the most common arrythmias and the rest set as outliers.\n *   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables. ", "original_text": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column. "}, "hash": "a8a7ee9b37bf7cbfb26b7d09590ace21b973b7424866eeeb472f5b60830fb7cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f85da30f-06e2-411e-a3b0-816c739d1d1d", "node_type": "1", "metadata": {"window": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n", "original_text": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. "}, "hash": "179453cdc25ad2551daf395e0c5b2f05e290c6eb9d42dda740f53eb234b0f472", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n", "mimetype": "text/plain", "start_char_idx": 51821, "end_char_idx": 52104, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "f85da30f-06e2-411e-a3b0-816c739d1d1d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n", "original_text": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fbeaf3fc-6bc4-4fff-a74f-0700a8b5ffaf", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Pima:** this was originally a binary-classification dataset in which the minority class was set as outliers.\n *   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n", "original_text": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n"}, "hash": "493d3a4925c4cd60f8aac91c7d937b2b6e6c4cd7665c271aa61a82679ebc4050", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e23b0d0c-8653-4e42-b1c1-b532b42e8cc4", "node_type": "1", "metadata": {"window": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n", "original_text": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n"}, "hash": "c2cc9e6ca062192f2a204975957cee98ab28f2c2f8ac1622e57746030f243eb2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. ", "mimetype": "text/plain", "start_char_idx": 52104, "end_char_idx": 52231, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e23b0d0c-8653-4e42-b1c1-b532b42e8cc4", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n", "original_text": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f85da30f-06e2-411e-a3b0-816c739d1d1d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **SpamBase:** many variables have a skewed distribution with the majority of the rows having a value of exactly zero.\n *   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n", "original_text": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination. "}, "hash": "0ca3851c171eedbe1bb7ee6dc018ce24f612bc340e81b8225fef239e84288873", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5b12da11-4889-4f2b-9440-d12b5574187b", "node_type": "1", "metadata": {"window": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. ", "original_text": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value. "}, "hash": "ae4e56ffbf38649a5456264b1a641ec8264dac9c2cbccb4856f30bafea5f6eb5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n", "mimetype": "text/plain", "start_char_idx": 52231, "end_char_idx": 52456, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5b12da11-4889-4f2b-9440-d12b5574187b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. ", "original_text": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e23b0d0c-8653-4e42-b1c1-b532b42e8cc4", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Satellite:** all variables have integer-only values, which can lead to earlier termination of tree-based methods due to ending with non-unique values in a node.  This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n", "original_text": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n"}, "hash": "05f10eb73731cbcb37a10cfbb1baed575ed272c77a6dac65f3bb335a81ec4b66", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "334d858c-13fa-43f1-90c5-b9ba925e19fa", "node_type": "1", "metadata": {"window": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. ", "original_text": "Also contains some binary and power-tailed variables. "}, "hash": "3aacd85aedc140e398c1fc8fac1791671142d32c649fc095e395380300009da9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value. ", "mimetype": "text/plain", "start_char_idx": 52456, "end_char_idx": 52559, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "334d858c-13fa-43f1-90c5-b9ba925e19fa", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. ", "original_text": "Also contains some binary and power-tailed variables. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5b12da11-4889-4f2b-9440-d12b5574187b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This dataset was originally meant for multi-class classification, and was adapted for anomaly detection by mixing together frequent classes and uncommon classes, making it in a way a more multi-modal dataset in which outliers are those near less common modes, which is quite different from the other datasets.\n *   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. ", "original_text": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value. "}, "hash": "34f95f1ca68e15ad63a3ea11983d1238f95176a3c3d7c67d3306ba2e79583c38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5add484-f0b4-4780-b363-5fccacfe7bd9", "node_type": "1", "metadata": {"window": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n", "original_text": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n"}, "hash": "57dab262032dfba73115e20d39fda71e0a15e131e0865d1064d52bcfbb30edb4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Also contains some binary and power-tailed variables. ", "mimetype": "text/plain", "start_char_idx": 52559, "end_char_idx": 52613, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5add484-f0b4-4780-b363-5fccacfe7bd9", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n", "original_text": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "334d858c-13fa-43f1-90c5-b9ba925e19fa", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Annthyroid:** most outliers can be differentiated through a single high-skew column.  This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. ", "original_text": "Also contains some binary and power-tailed variables. "}, "hash": "dd27b6c2575ced571c5d22ee94c52f137ba32534c468f55ddd9676fa08d34974", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1b1ee63-5a88-48be-8aba-b7748c1cb83e", "node_type": "1", "metadata": {"window": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. ", "original_text": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n"}, "hash": "65cca71dffe10547f51a9eb801dbfa3020ab5abe8c7e124b80f3bdb33814f29a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n", "mimetype": "text/plain", "start_char_idx": 52613, "end_char_idx": 52748, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e1b1ee63-5a88-48be-8aba-b7748c1cb83e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. ", "original_text": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5add484-f0b4-4780-b363-5fccacfe7bd9", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This dataset originally consisted of 3 classes which can be thought of as representing values \"low\", \"normal\", \"high\"; with \"low\" and \"high\" set as outliers (that is, outliers are a mixture of opposite groups, representing a relatively different scenario as in most other datasets).\n *   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n", "original_text": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n"}, "hash": "4d95106c808af8bf7cbb18646a3bf0e201a7beb6d7be1b92324d7c8c0b4f3f35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "70625d28-98d9-4819-a679-28b43e91ba23", "node_type": "1", "metadata": {"window": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. ", "original_text": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n"}, "hash": "3f303c5f9d897f0066aa50f399de4510739a34e4c2fe521e3fc8d433bafd866b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n", "mimetype": "text/plain", "start_char_idx": 52748, "end_char_idx": 52915, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "70625d28-98d9-4819-a679-28b43e91ba23", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. ", "original_text": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1b1ee63-5a88-48be-8aba-b7748c1cb83e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **Mnist:** variables are non-integer but with relatively few unique values, also leading to potential earlier termination.  This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. ", "original_text": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n"}, "hash": "24a434a4701448d8ee98b4434720903688de86ece0617d8e7c8fe93552d3043f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56c2b700-ad0c-43f3-af44-5b398def04ce", "node_type": "1", "metadata": {"window": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). ", "original_text": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. "}, "hash": "9e0f24386ba46a27f392990109425058eefb1d5894473196ba55f817e2d5344b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n", "mimetype": "text/plain", "start_char_idx": 52915, "end_char_idx": 54876, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "56c2b700-ad0c-43f3-af44-5b398def04ce", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). ", "original_text": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70625d28-98d9-4819-a679-28b43e91ba23", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This dataset was originally about multi-class classification, and was converted to anomaly detection by including a majority of rows from one class and a minority of rows from another class (all outliers are the same class).\n *   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. ", "original_text": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n"}, "hash": "78afd183cce6c7b1b33e96f259592c33377cc25a675380f7c1f8d9da64ca84cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4c15952-6970-4bbe-9d6f-2517e6b26be7", "node_type": "1", "metadata": {"window": "Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g. ", "original_text": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. "}, "hash": "14900ce3650f80361e4a56301279e574cb0707a422c800d07cc69fbabc4aee1e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. ", "mimetype": "text/plain", "start_char_idx": 54876, "end_char_idx": 58643, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c4c15952-6970-4bbe-9d6f-2517e6b26be7", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g. ", "original_text": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56c2b700-ad0c-43f3-af44-5b398def04ce", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **ALOI:** contains many very low-variance columns in which most of the points have the same value.  Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). ", "original_text": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types. "}, "hash": "9fd79b521c2bbdc1b466b6d28b52826f21cd6be2836b108d798a46174d635e6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43f994db-6ae1-4762-88b5-d67ebe4ed31c", "node_type": "1", "metadata": {"window": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n", "original_text": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n"}, "hash": "577996b7fc41bd89656c26f0b59356bddd0a9ac6391ee350b77e21090d05ba11", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. ", "mimetype": "text/plain", "start_char_idx": 58643, "end_char_idx": 58917, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "43f994db-6ae1-4762-88b5-d67ebe4ed31c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n", "original_text": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4c15952-6970-4bbe-9d6f-2517e6b26be7", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Also contains some binary and power-tailed variables.  This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g. ", "original_text": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels. "}, "hash": "29ce7ced8c6933523732196035d874442692c50702a40fb65fd54c896fa09dc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74d3a0ff-6e68-4b66-97b5-c21f294531f6", "node_type": "1", "metadata": {"window": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. ", "original_text": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. "}, "hash": "e23da8c87839d516c0c0abfe756c6cdbf7f65b791cbfb1548fb2626f186a47e9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n", "mimetype": "text/plain", "start_char_idx": 58917, "end_char_idx": 59115, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74d3a0ff-6e68-4b66-97b5-c21f294531f6", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. ", "original_text": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43f994db-6ae1-4762-88b5-d67ebe4ed31c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "This is a dataset about images from different physical objects in which outliers constitute \"rare objects with 1-10 instances\u201d ([13]).\n *   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n", "original_text": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n"}, "hash": "df5051c5d2c5c2a5360b207f5c52f409af7b591b9f8457801cc09aee455fe180", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "43f201c9-13b1-4860-b9fa-816b0c9c362d", "node_type": "1", "metadata": {"window": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). ", "original_text": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. "}, "hash": "d3c3dd689d1cdcbd7a7bca607f0f9cfea119b282ac37fe7bbf963cf43b97fa38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. ", "mimetype": "text/plain", "start_char_idx": 59115, "end_char_idx": 59253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "43f201c9-13b1-4860-b9fa-816b0c9c362d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). ", "original_text": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74d3a0ff-6e68-4b66-97b5-c21f294531f6", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "*   **ForestCover:** just like \"Mnist\u201d, this was originally a multi-class classification dataset that was converted by sampling from a majority and a minority class.\n\n | Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. ", "original_text": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g. "}, "hash": "843a81dd79c14ddf681a453418791e2620940eaa4f19b878d31347a7c66203a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "19818019-1c1d-4f56-9ebc-75dea59502a1", "node_type": "1", "metadata": {"window": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n", "original_text": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). "}, "hash": "8cb1f5b46264be3ae29baeb8e36df854d1efcf8dba07cf6bae31a45724027e61", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. ", "mimetype": "text/plain", "start_char_idx": 59253, "end_char_idx": 59499, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "19818019-1c1d-4f56-9ebc-75dea59502a1", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n", "original_text": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "43f201c9-13b1-4860-b9fa-816b0c9c362d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 6: Results obtained by each method, part 1 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Arrythmia** | | | **Pima** | | | **SpamBase** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7938 | 0.4599 | **0.00408** | 0.6795 | 0.4972 | **0.00537** | 0.6362 | 0.4757 | **0.00774** |\n| IF-U | 0.804 | 0.5012 | 0.0103 | 0.6807 | 0.5029 | 0.0119 | 0.6927 | 0.5242 | 0.0294 |\n| EIF-0 | 0.8052 | 0.4707 | 0.798 | 0.6628 | 0.5003 | 0.0784 | 0.6229 | 0.4791 | 0.61 |\n| EIF-T | 0.7986 | 0.4669 | 0.00735 | 0.6778 | 0.4970 | 0.00633 | 0.6596 | 0.5021 | 0.0149 |\n| SCIF | 0.6854 | 0.3368 | 0.0188 | 0.6130 | 0.4236 | 0.0172 | 0.4517 | 0.3801 | 0.0189 |\n| SCIF-U | 0.7333 | 0.3431 | 0.152 | 0.6565 | 0.4548 | 0.122 | 0.4223 | 0.3454 | 0.878 |\n| FCF | 0.8032 | 0.4842 | 0.0411 | **0.7362** | **0.5508** | 0.0363 | **0.7321** | 0.5694 | 0.281 |\n| RRCF-D | **0.8134** | 0.5177 | 10.5 | 0.6277 | 0.4809 | 12.1 | 0.7255 | **0.6317** | 65 |\n| RRCF-C | 0.7874 | 0.4279 | 3.96 | 0.5900 | 0.4307 | 3.09 | 0.5774 | 0.4924 | 4.08 |\n| DET | 0.6706 | 0.3496 | 0.0979 | 0.6042 | 0.4360 | 0.0321 | 0.3862 | 0.4391 | 0.415 |\n| DEF | 0.6845 | 0.3752 | 0.97 | 0.6081 | 0.4940 | 0.212 | 0.5999 | 0.4399 | 0.742 |\n| OCRF | NA* | NA* | 0.286 | 0.6644 | 0.4743 | 0.295 | 0.4335 | 0.3455 | 0.602 |\n| GIF | 0.8031 | **0.5184** | 1.47 | 0.6067 | 0.4627 | 0.076 | 0.6580 | 0.5451 | 0.217 |\n| iNNE | 0.6881 | 0.2776 | 0.1602 | 0.5947 | 0.4155 | 0.0687 | 0.4769 | 0.3698 | 0.1938 |\n| OCSVM-L | 0.5498 | 0.3144 | 0.02 | 0.2598 | 0.2417 | 0.0133 | 0.2349 | 0.2658 | 1.17 |\n| OCSVM-K | 0.7948 | 0.4843 | 0.0378 | 0.6580 | 0.4907 | 0.0275 | 0.5063 | 0.5062 | 1.75 |\n| LOF | 0.7891 | 0.4189 | 0.0322 | 0.5424 | 0.3727 | 0.00741 | 0.4597 | 0.3576 | 0.525 |\n\n\\* OCRF in some cases ended up producing the same outlier score for every single point, thus metrics could not be computed.\n\n | Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). ", "original_text": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge. "}, "hash": "b14b11e6ccda0466122fc788fda24e1c4fe417a3b6f3bbbac42010b2045063a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ae08add-4650-463f-9005-96f32e6167ed", "node_type": "1", "metadata": {"window": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). ", "original_text": "For other types of outliers such as those of minority-in-binary-classes however (e.g. "}, "hash": "2b083d65d9f93fdaf9a7aa2ecd972880eca43ded76cadbd8ddd3c4e112b500be", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). ", "mimetype": "text/plain", "start_char_idx": 59499, "end_char_idx": 60181, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4ae08add-4650-463f-9005-96f32e6167ed", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). ", "original_text": "For other types of outliers such as those of minority-in-binary-classes however (e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "19818019-1c1d-4f56-9ebc-75dea59502a1", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "| Table 7: Results obtained by each method, part 2 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Satellite** | | | **Pendigits** | | | **Annthyroid** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.7164 | 0.6624 | **0.013** | 0.9549 | 0.2268 | **0.0151** | 0.8300 | 0.2820 | 0.0126 |\n| IF-U | 0.7280 | 0.6431 | 0.0382 | 0.9635 | 0.3029 | 0.04 | 0.8385 | 0.3121 | 0.0518 |\n| EIF-0 | 0.6970 | 0.6849 | 0.808 | 0.9736 | 0.3616 | 0.536 | 0.7407 | 0.2675 | 0.393 |\n| EIF-T | 0.6800 | 0.6364 | 0.017 | 0.9592 | 0.3391 | 0.022 | 0.8207 | 0.2859 | 0.021 |\n| SCIF | 0.6083 | 0.5888 | 0.0285 | **0.9788** | **0.4110** | 0.0337 | 0.7715 | 0.4634 | 0.0267 |\n| SCIF-U | 0.7549 | 0.6916 | 0.356 | 0.9785 | 0.3552 | 0.209 | 0.9668 | 0.6594 | 0.721 |\n| FCF | **0.8253** | **0.7300** | 0.139 | 0.9662 | 0.2730 | 0.179 | 0.8712 | 0.3810 | 0.203 |\n| RRCF-D | 0.7270 | 0.6837 | 69 | 0.9374 | 0.2310 | 65 | 0.7109 | 0.2271 | 81 |\n| RRCF-C | 0.7089 | 0.5747 | 4.43 | 0.9198 | 0.2125 | 3.41 | 0.7477 | 0.2457 | 3.43 |\n| DET | 0.7070 | 0.5495 | 1.27 | 0.7088 | 0.0384 | 0.558 | 0.8910 | 0.2687 | 0.825 |\n| DEF | 0.7300 | 0.4644 | 0.973 | 0.8602 | 0.2583 | 0.517 | **0.9789** | **0.7147** | 0.612 |\n| OCRF | 0.7495 | 0.6568 | 0.711 | 0.8833 | 0.1667 | 0.776 | 0.8368 | 0.2094 | 0.613 |\n| GIF | 0.7561 | 0.5485 | 0.218 | 0.8501 | 0.0907 | 0.134 | 0.5342 | 0.0885 | 0.0679 |\n| iNNE | 0.6042 | 0.3892 | 0.189 | 0.8975 | 0.1064 | 0.1265 | 0.7047 | 0.1891 | 0.1309 |\n| OCSVM-L | 0.6759 | 0.4803 | 1.47 | 0.7652 | 0.1233 | 1.28 | 0.4857 | 0.0840 | **0.00125** |\n| OCSVM-K | 0.6669 | 0.6718 | 2.62 | 0.9599 | 0.3206 | 2.39 | 0.5727 | 0.1171 | 2.35 |\n| LOF | 0.5428 | 0.3754 | 0.784 | 0.4821 | 0.0368 | 0.972 | 0.7373 | 0.2055 | 0.0773 |\n\n| Table 8: Results obtained by each method, part 3 | | | | | | | | |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| | **Mnist** | | | **ALOI** | | | **ForestCover** | | |\n| **Model** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** | **ROC** | **PR** | **Time** |\n| IF | 0.8033 | 0.2608 | **0.0147** | 0.5375 | 0.0327 | **0.029** | 0.8509 | 0.0379 | **0.222** |\n| IF-U | 0.8167 | 0.2851 | 0.0474 | 0.5291 | 0.0319 | 0.211 | 0.8552 | 0.0388 | 0.935 |\n| EIF-0 | 0.8150 | 0.2669 | 1.81 | 0.5428 | 0.0351 | 4.01 | 0.9047 | 0.0634 | 16.7 |\n| EIF-T | 0.8130 | 0.2762 | 0.0281 | 0.5428 | 0.0332 | 0.0755 | 0.8720 | 0.0445 | 0.48 |\n| SCIF | 0.8584 | 0.3864 | 0.0285 | 0.5192 | 0.0306 | 0.0698 | 0.7037 | 0.0154 | 0.405 |\n| SCIF-U | 0.8432 | 0.3762 | 0.316 | 0.5173 | 0.0317 | 7.31 | 0.7040 | 0.0157 | 11.3 |\n| FCF | 0.7871 | 0.2532 | 0.228 | 0.5256 | 0.0311 | 2.06 | 0.5463 | 0.0107 | 3.58 |\n| RRCF-D | 0.8204 | 0.2944 | 76 | 0.5484 | 0.0434 | 614 | 0.7269 | 0.0158 | 2393 |\n| RRCF-C | 0.7955 | 0.3026 | 4.45 | 0.5659 | 0.0441 | 20.1 | 0.8994 | 0.0465 | 77 |\n| DET | 0.8087 | 0.2645 | 1.37 | 0.5001 | 0.0317 | 0.0389 | 0.6679 | 0.0166 | 1256 |\n| DEF | 0.6390 | 0.2005 | 0.986 | 0.4868 | 0.0320 | 2.91 | 0.5875 | 0.0178 | 15.5 |\n| OCRF | NA* | NA* | 0.756 | 0.5026 | 0.0310 | 2.44 | 0.8344 | 0.0419 | 24 |\n| GIF | 0.7934 | 0.2412 | 0.642 | 0.5297 | 0.0406 | 0.244 | 0.7051 | 0.0281 | 0.796 |\n| iNNE | 0.8324 | 0.3641 | 0.4657 | **0.5814** | **0.0477** | 0.8570 | 0.9584 | 0.1871 | 2.2602 |\n| OCSVM-L | **0.9449** | **0.6709** | 3.45 | 0.5255 | 0.0412 | 148 | **0.9824** | **0.3904** | 3670 |\n| OCSVM-K | 0.8216 | 0.3231 | 5.4 | 0.5172 | 0.0460 | 162.63 | 0.6565 | 0.0125 | 4316 |\n| LOF | 0.6482 | 0.1874 | 1.33 | - | - | - | - | - | - |\n\n## 8 Discussion\n\nAs expected, there was no dominant outlier detection model across the datasets experimented with here, which consitute a mixture of different oulier types.  Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n", "original_text": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics). "}, "hash": "085bc63a5803f2770611be530d207ed067591a0524cee04c6a2c0239a466c6f7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38e9b402-ef2c-4231-a81a-dde30e27055f", "node_type": "1", "metadata": {"window": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n", "original_text": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n"}, "hash": "0f221820d7d760c8bd74eaaf861bba26682cc331e50bf143cf926782d5a7ec4d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For other types of outliers such as those of minority-in-binary-classes however (e.g. ", "mimetype": "text/plain", "start_char_idx": 60181, "end_char_idx": 60267, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "38e9b402-ef2c-4231-a81a-dde30e27055f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n", "original_text": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ae08add-4650-463f-9005-96f32e6167ed", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Taking IF as a base, the only variation that sees consistently better or not-much-worse results across all types of datasets and outliers is EIF, at least when examined under the default hyperparameters of each model, which one might prefer to use in the absence of labels.  Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). ", "original_text": "For other types of outliers such as those of minority-in-binary-classes however (e.g. "}, "hash": "333647b19590b0d3603e81586c0391ccfb78ae71f9a43d25a98de59a07e11a7c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6f5bf18-0c91-46ae-8ef6-b52970700fa4", "node_type": "1", "metadata": {"window": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. ", "original_text": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. "}, "hash": "4cb95dc21d73c9a2bc5e47ede91caaae7be9afc15e1062f3570213e878b1d9f5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n", "mimetype": "text/plain", "start_char_idx": 60267, "end_char_idx": 60428, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c6f5bf18-0c91-46ae-8ef6-b52970700fa4", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. ", "original_text": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38e9b402-ef2c-4231-a81a-dde30e27055f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Making the trees deeper and the forest larger also results in a small performance improvement at the cost of much longer running times, but these times are still competitive against other methods.\n\n When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n", "original_text": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n"}, "hash": "9165234cbf7b18e82fcb85172a950368f45675fb3a4c4a2f62717c40cb0c8146", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a08f60b8-77ee-44de-b662-91594c59f44f", "node_type": "1", "metadata": {"window": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). ", "original_text": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). "}, "hash": "b3eb9b47f93547a1f1216fc344b4061f119af6b37c83c43712078900ba80459e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. ", "mimetype": "text/plain", "start_char_idx": 60428, "end_char_idx": 60542, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a08f60b8-77ee-44de-b662-91594c59f44f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). ", "original_text": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6f5bf18-0c91-46ae-8ef6-b52970700fa4", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "When these datasets are viewed in terms of outlier types however, one would notice that clustered outliers from multimodal datasets (e.g.  \"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. ", "original_text": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with. "}, "hash": "52acfc2ad03b424d368c378f6136f6b0aa81be3df6e063052876d3e6405ef7cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af92fd29-4547-4d50-a963-57b216220e71", "node_type": "1", "metadata": {"window": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. ", "original_text": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n"}, "hash": "1b706fe0aff63cc5369b415b0a884792364b7d0175ad7132814fb735aa4b12f1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). ", "mimetype": "text/plain", "start_char_idx": 60542, "end_char_idx": 60854, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "af92fd29-4547-4d50-a963-57b216220e71", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. ", "original_text": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a08f60b8-77ee-44de-b662-91594c59f44f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Arrythmia\", \"Satellite\", \"SpamBase\", \"Annthyroid\") - oftentimes those of utmost interest but also the hardest to flag - are better identified under tree-based models than under other families, with non-uniformly-random splits providing an edge.  In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). ", "original_text": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve). "}, "hash": "a015c329121e0874e557cd08978e9809835e315493a456d4c431fce83561d1dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "020105f1-b08e-4bb7-b898-afcba77c7e4b", "node_type": "1", "metadata": {"window": "For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n", "original_text": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). "}, "hash": "08631530f1726b00e5a116444c52d0897146323de88674d4443a5df0bd885702", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n", "mimetype": "text/plain", "start_char_idx": 60854, "end_char_idx": 61102, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "020105f1-b08e-4bb7-b898-afcba77c7e4b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n", "original_text": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af92fd29-4547-4d50-a963-57b216220e71", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In particular, the FCF model with hyperparameters chosen more appropriately for anomaly detection (as compared to its original application field) looks to be the best performer in these cases, followed closely by RRCF-D (which uses isolation depth for calculating anomaly scores instead of the \"co-displacement\u201d metric initially proposed for this model), but with a rather large advantage over RRCF in terms of computational efficiency (should be noted again that the RRCF implementation used here was not as optimized, but the procedure itself requires checking values across all columns of the data at each node, which makes it less scalable than other split guiding heuristics).  For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. ", "original_text": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n"}, "hash": "ddedd59bfffeefad15dbc3d78cda1701b06b2668831d22d836693299296b1041", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "801f8e6a-dc27-4876-bf5b-18e77c4fd8ba", "node_type": "1", "metadata": {"window": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n", "original_text": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n"}, "hash": "3ad3e433d19515fd1ebea07cf7598111e7dfda0b8d08e1218403710697035c66", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). ", "mimetype": "text/plain", "start_char_idx": 61102, "end_char_idx": 61756, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "801f8e6a-dc27-4876-bf5b-18e77c4fd8ba", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n", "original_text": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "020105f1-b08e-4bb7-b898-afcba77c7e4b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For other types of outliers such as those of minority-in-binary-classes however (e.g.  \"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n", "original_text": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U). "}, "hash": "4a5698471ad30aceb8ba4904a4dc2f7f1a9944400bfe8e9ae31140e35599fd9b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2043c902-cc3a-4561-a7bb-73d1e38c249e", "node_type": "1", "metadata": {"window": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n", "original_text": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. "}, "hash": "16e8bf4174cfdb13308391d210522c3b1ac3fc91cc3c523d4da09534bc3a9a71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n", "mimetype": "text/plain", "start_char_idx": 61756, "end_char_idx": 61932, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2043c902-cc3a-4561-a7bb-73d1e38c249e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n", "original_text": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "801f8e6a-dc27-4876-bf5b-18e77c4fd8ba", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "\"Mnist\", \"ForestCover\"), FCF sees degraded performance compared to IF, and other families of less-local methods present better performance than either of them.\n\n SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n", "original_text": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n"}, "hash": "4c84c419a987a54152e899775538c33db6c889dc2aeb63b6be400534a9091e18", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c501f75-72ce-4235-a0c7-b12ef562f931", "node_type": "1", "metadata": {"window": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n", "original_text": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). "}, "hash": "e60fa46f6b89eaa2a9423590a8018cec4d9c6771e5ae01572614754a797f2660", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. ", "mimetype": "text/plain", "start_char_idx": 61932, "end_char_idx": 62218, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c501f75-72ce-4235-a0c7-b12ef562f931", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n", "original_text": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2043c902-cc3a-4561-a7bb-73d1e38c249e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "SCIFOREST was found in [15] to provide an increase in AUROC under the majority of the datasets experimented with.  The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n", "original_text": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms. "}, "hash": "9f8406da1a356222389888b1708ca3af0a3fbe8647e8cade9af1ca4d96e1b510", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1e89a8f-07a4-461b-8937-f319eda40c2b", "node_type": "1", "metadata": {"window": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. ", "original_text": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. "}, "hash": "dacba15a76e5561b1d3e51de60a7cae515ee2a3f8308af17ecfeb53b4c080902", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). ", "mimetype": "text/plain", "start_char_idx": 62218, "end_char_idx": 62796, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "e1e89a8f-07a4-461b-8937-f319eda40c2b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. ", "original_text": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c501f75-72ce-4235-a0c7-b12ef562f931", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The experiments here however show a very different picture once datasets with more varied characteristics are evaluated - for example, in the \"SpamBase\" dataset, SCIFOREST results in a very large performance drop compared to IF, even driving the AUROC below 0.5 (which is what random predictions would achieve).  It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n", "original_text": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables). "}, "hash": "03d10c4e4809bacece04157088cd0400fea26a45be854185b67795bcc4659d6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05a3c31b-13de-420e-93f2-603d9c59e593", "node_type": "1", "metadata": {"window": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. ", "original_text": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n"}, "hash": "f38c965e8e94d418053ef4bd217de4e6e3b362354f84261ff5021fc3f35b0ba8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. ", "mimetype": "text/plain", "start_char_idx": 62796, "end_char_idx": 63021, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "05a3c31b-13de-420e-93f2-603d9c59e593", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. ", "original_text": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1e89a8f-07a4-461b-8937-f319eda40c2b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "It should also be noted that these experiments failed to reproduce similar AUROC numbers under the same datasets used in [15], but once the depth limit was removed from the algorithm, the AUROC metrics became very similar to those reported there.\n\n In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. ", "original_text": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g. "}, "hash": "faa9f8ead3c79b1438ca1e5ce557b5a8de08c918437a9dec36ba3f65ea8cfc1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "741b206d-99fa-4a68-96bf-b5d7dc34c074", "node_type": "1", "metadata": {"window": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n", "original_text": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n"}, "hash": "3b8dbd6ac76c68c2d444953362e0525b2a0ee47b6254770512d13b73f917aa99", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n", "mimetype": "text/plain", "start_char_idx": 63021, "end_char_idx": 63137, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "741b206d-99fa-4a68-96bf-b5d7dc34c074", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n", "original_text": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05a3c31b-13de-420e-93f2-603d9c59e593", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In [10], they report AUROC and AUPR metrics for many of the same datasets and baselines as in here, following a very similar methodology, but the results obtained in this experiment differ very significantly from those reported in [10], both for the proposed OCRF algorithm as well as for the baselines compared against - for example, [10] reports an AUROC of 0.850 in the SpamBase dataset for OCRF, while the experiments here reached an average AUROC of 0.4335 only, and while [10] found OCRF to outperform IF in 10 out of 12 datasets, that was the case only in 2 out of the 9 datasets experimented with in here (down to 1 dataset if considering IF-U).  What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. ", "original_text": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n"}, "hash": "17a3b8c89586b653c5303909746718b86cc0b41e50d5cb817b47f966cea5e955", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd2e609-4a48-4c06-bbb3-29a464b71b0e", "node_type": "1", "metadata": {"window": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. ", "original_text": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n"}, "hash": "2bdb343965ddd21d63e9aa9a079366e489027d0e26e40f13da85e093da8edb8a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n", "mimetype": "text/plain", "start_char_idx": 63137, "end_char_idx": 63654, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1fd2e609-4a48-4c06-bbb3-29a464b71b0e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. ", "original_text": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "741b206d-99fa-4a68-96bf-b5d7dc34c074", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "What's more, the proposed OCRF algorithm in some cases ended up producing the exact same outlier score for all observations, which made calculation of metrics not meaningful.\n\n Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n", "original_text": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n"}, "hash": "f60aac5acd57edf3ab10ad6e2e40ed0a60d616c494d18fad737ac496b314e514", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef483913-705b-463a-918a-43881d093f92", "node_type": "1", "metadata": {"window": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles. ", "original_text": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n"}, "hash": "0a0e62a275f38a614390881c38b3f362ccaefa88a95cf4de8ff17c39be2ad8b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n", "mimetype": "text/plain", "start_char_idx": 63654, "end_char_idx": 64058, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ef483913-705b-463a-918a-43881d093f92", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles. ", "original_text": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fd2e609-4a48-4c06-bbb3-29a464b71b0e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Computation times in this experiment also showed how different implementations of the same algorithm can result in widly different execution speed, suggesting that the running times reported by previous works might not provide fair comparisons of the relative efficiency of algorithms.  For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. ", "original_text": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n"}, "hash": "c0b764a39c7800afaf33bbfe974eda842798291ac11b244731c4f372f3c41ecb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2881da48-8f84-4dad-80c3-5f7e4a21017a", "node_type": "1", "metadata": {"window": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n", "original_text": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. "}, "hash": "465fdb8242034f5d9239ff2ce95dc81e14788391da79323ee561426f47cc4d38", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n", "mimetype": "text/plain", "start_char_idx": 64058, "end_char_idx": 64376, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2881da48-8f84-4dad-80c3-5f7e4a21017a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n", "original_text": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef483913-705b-463a-918a-43881d093f92", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "For example, [5] produced a very efficient software implementation of the proposed GIF algorithm, reporting GIF to run around an order of magnitude faster than EIF on the \"SpamBase\" dataset, and the experiments here could indeed confirm that it ran significantly faster than the EIF provided by its own authors, but once EIF is reimplemented with more attention to speed concerns, the comparison then flips as the EIF used here was an order of magnitude faster than GIF, despite doing extra computations at each step compared to the original EIF (when standardizing variables).  Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles. ", "original_text": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n"}, "hash": "f2ce6755860161ad5adf916fc0ca51800814aff59c9a3f132e300807dc61ed70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d04271b-0596-4f65-ae57-351d9b94e34e", "node_type": "1", "metadata": {"window": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman. ", "original_text": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. "}, "hash": "71d476eadc746f5cb10aa527a195cb8769933a56cf4015824eca30c1f42d1522", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. ", "mimetype": "text/plain", "start_char_idx": 64376, "end_char_idx": 64467, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d04271b-0596-4f65-ae57-351d9b94e34e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman. ", "original_text": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2881da48-8f84-4dad-80c3-5f7e4a21017a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Although not shown here, it was possible to obtain a further order of magnitude speed-up for prediction times in the larger datasets for models that use axis-parallel splits through pre-compilation of the trees (through e.g.  the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n", "original_text": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom. "}, "hash": "b651d67d84520c810160e050220bd08bf7c32a51a6a6951845efd20fcb366f3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0e0999ca-1003-41d7-b456-d3105912ba45", "node_type": "1", "metadata": {"window": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees. ", "original_text": "[Online; accessed 9-September-2021].\n\n"}, "hash": "5eb7041bb47bb743c90757d06d49a6317ea3deeff4387fc992a3adc070b59751", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. ", "mimetype": "text/plain", "start_char_idx": 64467, "end_char_idx": 64551, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0e0999ca-1003-41d7-b456-d3105912ba45", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees. ", "original_text": "[Online; accessed 9-September-2021].\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d04271b-0596-4f65-ae57-351d9b94e34e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "the \"treelite\" software), but most of the software implementations compared here did not offer such functionality.\n\n ## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman. ", "original_text": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019. "}, "hash": "1864034df9867b03a623bf1cddafc2cdac809a4fc32dc6582f612f472b8254c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f896bb9-2530-4428-b862-215f93fbeffa", "node_type": "1", "metadata": {"window": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n", "original_text": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. "}, "hash": "dfb2cd4b55736669dffbbd661fbfdca0ba9307a52b3f6b3a41126a5951fa32fc", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[Online; accessed 9-September-2021].\n\n", "mimetype": "text/plain", "start_char_idx": 64551, "end_char_idx": 64589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2f896bb9-2530-4428-b862-215f93fbeffa", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n", "original_text": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0e0999ca-1003-41d7-b456-d3105912ba45", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## 9 Conclusions\n\nThis work analyzed different heuristics that have been proposed to guide the choice of splits in isolation forests, highlighting some behaviors and properties of interest of the models obtained by following different criteria; and along the way proposed a splitting rule based on uniformly-random column choice with deterministic split threshold selection which is obtained by maximizing a pooled information gain metric, aimed at increasing outlier detection capabilities in multi-modal datasets.\n\n The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees. ", "original_text": "[Online; accessed 9-September-2021].\n\n"}, "hash": "c28f7edea7beeea6624fb96141db01746c7f2404060c446ac5dbc866f1fe8b31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "74ec4a0d-2c77-4b86-883f-67e67ef64d73", "node_type": "1", "metadata": {"window": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. ", "original_text": "Isolation-based anomaly detection using nearest-neighbor ensembles. "}, "hash": "b217edf3f4a208388951aef8ad793e33b93c1c90c5d918daef991fd26a868575", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. ", "mimetype": "text/plain", "start_char_idx": 64589, "end_char_idx": 64692, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "74ec4a0d-2c77-4b86-883f-67e67ef64d73", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. ", "original_text": "Isolation-based anomaly detection using nearest-neighbor ensembles. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f896bb9-2530-4428-b862-215f93fbeffa", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The proposed guiding heuristic was compared against other methods, including the original IFOREST algorithm as well as several variations of it that introduce non-uniformly-random split choice heuristics, and was found to offer increased performance for the outliers of utmost interest (clustered outliers from multi-modal datasets), at the expense of degraded performance in other classes of outliers.\n\n Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n", "original_text": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells. "}, "hash": "3d711a0367374c024f895a148de93d1596ade36bb9880c0102cf5eae8c233928", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18d07087-623d-4f02-8c05-c9f00788a28e", "node_type": "1", "metadata": {"window": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers. ", "original_text": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n"}, "hash": "d7b9e4c2baa18fe6ed718396f7f540e1fbe99e6f0f52b7145a7e6653de3f6d02", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation-based anomaly detection using nearest-neighbor ensembles. ", "mimetype": "text/plain", "start_char_idx": 64692, "end_char_idx": 64760, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "18d07087-623d-4f02-8c05-c9f00788a28e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers. ", "original_text": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "74ec4a0d-2c77-4b86-883f-67e67ef64d73", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Compared to other methods, the proposed FCF algorithm not only showed increased performance as measured by ranking metrics, but also resulted in a more computationally-efficient procedure than other guiding heuristics, making it a promising algorithm for anomaly detection in datasets with multi-modal distributions.\n\n ## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. ", "original_text": "Isolation-based anomaly detection using nearest-neighbor ensembles. "}, "hash": "a4181996dd3540a3145c00f0dc7cb067c798fef33eb598fd9530d6ce91359eb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e8c3653-fa1c-472d-a3c2-ea83630e0e8d", "node_type": "1", "metadata": {"window": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n", "original_text": "[3] Leo Breiman. "}, "hash": "c266178ed2de830bbf0b7140acfdb8d05183bc7ec4405b0dd2cf6e163641b21d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n", "mimetype": "text/plain", "start_char_idx": 64760, "end_char_idx": 64810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e8c3653-fa1c-472d-a3c2-ea83630e0e8d", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n", "original_text": "[3] Leo Breiman. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18d07087-623d-4f02-8c05-c9f00788a28e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "## References\n\n[1] Expected average depth in random binary tree constructed top-to-bottom.  https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers. ", "original_text": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n"}, "hash": "e381c7ae9403ffec4a1e2ffe433333dac5d1e0307079ae1987c3a31fe04f31c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e787c25-5776-46a2-b393-3ad1b8c8d623", "node_type": "1", "metadata": {"window": "[Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. ", "original_text": "Classification and regression trees. "}, "hash": "b57e4892144e8bdf2351d51c02dc1150c6cf7b821427a656176d8e2abcd3c41d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[3] Leo Breiman. ", "mimetype": "text/plain", "start_char_idx": 64810, "end_char_idx": 64827, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1e787c25-5776-46a2-b393-3ad1b8c8d623", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. ", "original_text": "Classification and regression trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e8c3653-fa1c-472d-a3c2-ea83630e0e8d", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "https://math.stackexchange.com/questions/3333220/expected-average-depth-in-ra 2019.  [Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n", "original_text": "[3] Leo Breiman. "}, "hash": "92bf37c48c69c23cf2cdee6475051f36e470ac96aa193415debf8afc375ba827", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "702cb75f-91b0-4e2b-92e8-602ec3874afa", "node_type": "1", "metadata": {"window": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees. ", "original_text": "Routledge, 2017.\n\n"}, "hash": "bf43704f2abe74a111197f71affb9a560a0a208d76be80b6aa63da679e09b874", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Classification and regression trees. ", "mimetype": "text/plain", "start_char_idx": 64827, "end_char_idx": 64864, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "702cb75f-91b0-4e2b-92e8-602ec3874afa", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees. ", "original_text": "Routledge, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1e787c25-5776-46a2-b393-3ad1b8c8d623", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[Online; accessed 9-September-2021].\n\n [2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. ", "original_text": "Classification and regression trees. "}, "hash": "c98a0e5f91f5f5cd80ada47969f319f4df5b66eb74346a07a7fdb98af09c4c2c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eac1c044-bbf8-490a-a4f2-6d304318f323", "node_type": "1", "metadata": {"window": "Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n", "original_text": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. "}, "hash": "b75dd39f8c48d3bd6bda93d5990391e77738caff4fb695347d639874565a86ba", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Routledge, 2017.\n\n", "mimetype": "text/plain", "start_char_idx": 64864, "end_char_idx": 64882, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "eac1c044-bbf8-490a-a4f2-6d304318f323", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n", "original_text": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "702cb75f-91b0-4e2b-92e8-602ec3874afa", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[2] Tharindu R Bandaragoda, Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R Wells.  Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees. ", "original_text": "Routledge, 2017.\n\n"}, "hash": "842e4a94b744ca5bc2f5db3d4b3dcf315671777f14b38be15c791d9fb117aa38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c43b47f-eb6b-495d-b2e3-6510365cc225", "node_type": "1", "metadata": {"window": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. ", "original_text": "Lof: identifying density-based local outliers. "}, "hash": "a295a415d2128f84226a3bc22a8dc5cec47f8fd6ecc96f13a5cfd1aa87dbbf6f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. ", "mimetype": "text/plain", "start_char_idx": 64882, "end_char_idx": 64955, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4c43b47f-eb6b-495d-b2e3-6510365cc225", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. ", "original_text": "Lof: identifying density-based local outliers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eac1c044-bbf8-490a-a4f2-6d304318f323", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Isolation-based anomaly detection using nearest-neighbor ensembles.  Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n", "original_text": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander. "}, "hash": "7bb3cedf6af3f7a37fa4b99b174676cde90193c7e8b45105cf55c9096fa35558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41ec9f09-e8a8-4656-a8bb-3a1b8f888abe", "node_type": "1", "metadata": {"window": "[3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. ", "original_text": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n"}, "hash": "38a652997583da5741ea96c066bf7b6d03f06db180791054bf4b60c0b4779802", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Lof: identifying density-based local outliers. ", "mimetype": "text/plain", "start_char_idx": 64955, "end_char_idx": 65002, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "41ec9f09-e8a8-4656-a8bb-3a1b8f888abe", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. ", "original_text": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c43b47f-eb6b-495d-b2e3-6510365cc225", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Computational Intelligence, 34(4):968\u2013998, 2018.\n\n [3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. ", "original_text": "Lof: identifying density-based local outliers. "}, "hash": "ca5bd93e1faeb15c0a59387d5b19e3d0c54cc806f9f3d7be41e68a72af13c742", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "056df5de-c968-48f1-929a-f3a782d42b34", "node_type": "1", "metadata": {"window": "Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n", "original_text": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. "}, "hash": "f6faec3c5480509af987f486b0ebf6c5531645fc21b97c87f2ad6a9d5ba7902f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n", "mimetype": "text/plain", "start_char_idx": 65002, "end_char_idx": 65109, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "056df5de-c968-48f1-929a-f3a782d42b34", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n", "original_text": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41ec9f09-e8a8-4656-a8bb-3a1b8f888abe", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[3] Leo Breiman.  Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. ", "original_text": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n"}, "hash": "ce7f9b8334a77a315e946288bd22ef29d22b7e3f790af4cd56c091c7ceb6b4cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd7ecb86-c6af-4f24-af3f-5fa7dade9001", "node_type": "1", "metadata": {"window": "Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes. ", "original_text": "Randomized outlier detection with trees. "}, "hash": "10729e786e4e4c521edefdad93f71d0e2b6b43f70f5e95e0f233062bfda72c87", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. ", "mimetype": "text/plain", "start_char_idx": 65109, "end_char_idx": 65176, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd7ecb86-c6af-4f24-af3f-5fa7dade9001", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes. ", "original_text": "Randomized outlier detection with trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "056df5de-c968-48f1-929a-f3a782d42b34", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Classification and regression trees.  Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n", "original_text": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik. "}, "hash": "9dae726c6909922025998c022137959ac8420d09f3083165c526bb19a7aebc0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c48bd9c9-a780-4c1c-8279-a2415f49dcc5", "node_type": "1", "metadata": {"window": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees. ", "original_text": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n"}, "hash": "ec6f4768144c649d021645c870f26167a913ee622ae4b61e8405efa66c8cca50", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Randomized outlier detection with trees. ", "mimetype": "text/plain", "start_char_idx": 65176, "end_char_idx": 65217, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c48bd9c9-a780-4c1c-8279-a2415f49dcc5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees. ", "original_text": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd7ecb86-c6af-4f24-af3f-5fa7dade9001", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Routledge, 2017.\n\n [4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes. ", "original_text": "Randomized outlier detection with trees. "}, "hash": "0c095d6fd6999bddd78a38efd01667424edda3acb66db6142a04e2909aa15a00", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd835efc-d13d-4f3c-a6a2-e9088bd02c44", "node_type": "1", "metadata": {"window": "Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n", "original_text": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. "}, "hash": "704106483a006d12257c1b5909f7eccb49d29e347e145841e79da65e93040196", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n", "mimetype": "text/plain", "start_char_idx": 65217, "end_char_idx": 65289, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "cd835efc-d13d-4f3c-a6a2-e9088bd02c44", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n", "original_text": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c48bd9c9-a780-4c1c-8279-a2415f49dcc5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[4] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00f6rg Sander.  Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees. ", "original_text": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n"}, "hash": "50316b27ae01cdc30debe1063cc06912a8aae999b0cb2f2c6d989a89ba2af25b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c834945-952c-4f54-9dba-286bbf9f729f", "node_type": "1", "metadata": {"window": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich. ", "original_text": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. "}, "hash": "0f8d422b3ced247347d128d77a37cf35514be96ac6895ed34d69afe4c3d3e00a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. ", "mimetype": "text/plain", "start_char_idx": 65289, "end_char_idx": 65430, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "2c834945-952c-4f54-9dba-286bbf9f729f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich. ", "original_text": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd835efc-d13d-4f3c-a6a2-e9088bd02c44", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Lof: identifying density-based local outliers.  In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n", "original_text": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle. "}, "hash": "ccd901a21461de2dd3fb78bd65ace032b82aba845883b7901952a7ab10fd92ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "706390d9-c1a7-4e15-ab3f-4712e9dd9a78", "node_type": "1", "metadata": {"window": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves. ", "original_text": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n"}, "hash": "735dd162cf0dd83de86521d1c95a3ed0018356f5165d4fd3e1a105c5d7f45ed4", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. ", "mimetype": "text/plain", "start_char_idx": 65430, "end_char_idx": 65527, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "706390d9-c1a7-4e15-ab3f-4712e9dd9a78", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves. ", "original_text": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c834945-952c-4f54-9dba-286bbf9f729f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pages 93\u2013104, 2000.\n\n [5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich. ", "original_text": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. "}, "hash": "3427bdf614e2f454df7e701d871ba4cc6f3779307651cec754af9cd08f48c37a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "941e9a54-ce86-4c9d-b997-7425fcccb405", "node_type": "1", "metadata": {"window": "Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n", "original_text": "[7] David Cortes. "}, "hash": "a07dfbe4fa66ddb3e04a20d602caa42eda7ceec9b0f62a356820cb5eff221d8d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n", "mimetype": "text/plain", "start_char_idx": 65527, "end_char_idx": 65586, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "941e9a54-ce86-4c9d-b997-7425fcccb405", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n", "original_text": "[7] David Cortes. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "706390d9-c1a7-4e15-ab3f-4712e9dd9a78", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[5] Sebastian Buschj\u00e4ger, Philipp-Jan Honysz, and Katharina Morik.  Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves. ", "original_text": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n"}, "hash": "329e767a389185c1be0f0ae81e25b8cf6323b58c6c4d24b1c1f80fa52097bce7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fb820ea-2dda-4ffe-b407-fa4e1918f484", "node_type": "1", "metadata": {"window": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "original_text": "Imputing missing values with unsupervised random trees. "}, "hash": "9ac579e9e5ec99ad1265a6d594ffc3f53178327640bba29590d715a2570d6c3d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[7] David Cortes. ", "mimetype": "text/plain", "start_char_idx": 65586, "end_char_idx": 65604, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7fb820ea-2dda-4ffe-b407-fa4e1918f484", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "original_text": "Imputing missing values with unsupervised random trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "941e9a54-ce86-4c9d-b997-7425fcccb405", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Randomized outlier detection with trees.  International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n", "original_text": "[7] David Cortes. "}, "hash": "a8fc270a5e224403f725ed13d041484c0a1f0651acad0d71f03711d76ebdc77e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ade13966-05eb-4b66-ace3-44e1512e880b", "node_type": "1", "metadata": {"window": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests. ", "original_text": "arXiv preprint arXiv:1911.06646, 2019.\n\n"}, "hash": "8670eec73f1efac1cbfe4c92e1d17a5ca30ded197de019ebc74a06ad4e3551d8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Imputing missing values with unsupervised random trees. ", "mimetype": "text/plain", "start_char_idx": 65604, "end_char_idx": 65660, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ade13966-05eb-4b66-ace3-44e1512e880b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests. ", "original_text": "arXiv preprint arXiv:1911.06646, 2019.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fb820ea-2dda-4ffe-b407-fa4e1918f484", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "International Journal of Data Science and Analytics, pages 1-14, 2020.\n\n [6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "original_text": "Imputing missing values with unsupervised random trees. "}, "hash": "6246ff9f9a016597458ce04a27c2089e558a5bcfeeb49958a9666553a2ebe606", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91bbe978-9cbc-4aed-9c7f-6c062d75ca0e", "node_type": "1", "metadata": {"window": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358. ", "original_text": "[8] Jesse Davis and Mark Goadrich. "}, "hash": "04ed6004e1b33af30dd4a61b092f49c31d266f9ef6428cd47fd4c90bf99b1927", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1911.06646, 2019.\n\n", "mimetype": "text/plain", "start_char_idx": 65660, "end_char_idx": 65700, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91bbe978-9cbc-4aed-9c7f-6c062d75ca0e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358. ", "original_text": "[8] Jesse Davis and Mark Goadrich. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ade13966-05eb-4b66-ace3-44e1512e880b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[6] Guilherme O Campos, Arthur Zimek, J\u00f6rg Sander, Ricardo JGB Campello, Barbora Micenkov\u00e1, Erich Schubert, Ira Assent, and Michael E Houle.  On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests. ", "original_text": "arXiv preprint arXiv:1911.06646, 2019.\n\n"}, "hash": "d26e9eba85077095d0720fd45385a11692c7f3d982788344a77e9e89513ec5cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "42e6d7c7-0d8e-4551-9cd7-9ddcbdc615f2", "node_type": "1", "metadata": {"window": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n", "original_text": "The relationship between precision-recall and roc curves. "}, "hash": "c46c9193da7d28f702ace743287f9da337fcf4dd31b602f30da212305c1bbb67", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[8] Jesse Davis and Mark Goadrich. ", "mimetype": "text/plain", "start_char_idx": 65700, "end_char_idx": 65735, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "42e6d7c7-0d8e-4551-9cd7-9ddcbdc615f2", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n", "original_text": "The relationship between precision-recall and roc curves. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91bbe978-9cbc-4aed-9c7f-6c062d75ca0e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study.  Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358. ", "original_text": "[8] Jesse Davis and Mark Goadrich. "}, "hash": "c471a6d14c00902b260e93f9e6deea319bda1d0237d4fc300aae4c7a9a6b2e3b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "705abe22-d340-46b1-b59e-6cf4f54d0c42", "node_type": "1", "metadata": {"window": "[7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "original_text": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n"}, "hash": "31b2232f0c1a02f7499a80db82c9e407530dd2887c19f5f157d9d45f4d827fb9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The relationship between precision-recall and roc curves. ", "mimetype": "text/plain", "start_char_idx": 65735, "end_char_idx": 65793, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "705abe22-d340-46b1-b59e-6cf4f54d0c42", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "original_text": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42e6d7c7-0d8e-4551-9cd7-9ddcbdc615f2", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Data mining and knowledge discovery, 30(4):891-927, 2016.\n\n [7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n", "original_text": "The relationship between precision-recall and roc curves. "}, "hash": "44d89c1106fe2b2e8365f4bfd905db32692247347b49e286f0b1f6389ceb311a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "517fd605-8dba-4684-bb29-b1e599f8e2a2", "node_type": "1", "metadata": {"window": "Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests. ", "original_text": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. "}, "hash": "36f5c54acabdda3a196737ff2bc02275e90fac1596ae3dd71752aae3d33be1c9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n", "mimetype": "text/plain", "start_char_idx": 65793, "end_char_idx": 65888, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "517fd605-8dba-4684-bb29-b1e599f8e2a2", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests. ", "original_text": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "705abe22-d340-46b1-b59e-6cf4f54d0c42", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[7] David Cortes.  Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "original_text": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n"}, "hash": "463c3ae7bfc489b4a21f844cd3d470f4682afc451ea35b30028f0f0ef6009c59", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04a17411-8250-498e-8a61-848ed519e191", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358. ", "original_text": "One class splitting criteria for random forests. "}, "hash": "d0ffb987de99c80973df7077e0c85bf6ff7cb0bdba335ef86c1fab7f7cace26b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "mimetype": "text/plain", "start_char_idx": 65888, "end_char_idx": 65958, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04a17411-8250-498e-8a61-848ed519e191", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358. ", "original_text": "One class splitting criteria for random forests. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "517fd605-8dba-4684-bb29-b1e599f8e2a2", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Imputing missing values with unsupervised random trees.  arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests. ", "original_text": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. "}, "hash": "e340ef19e573e4c0e16f585847f3ac166ea79bd44c8e919173ede67e1f210092", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd5ff5af-b3fb-40b0-977e-5e867b0bf439", "node_type": "1", "metadata": {"window": "[8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n", "original_text": "In Asian Conference on Machine Learning, pages 343-358. "}, "hash": "c8362b58adc9e2c4127a7aa332836e8ba71d22a1dac9bcf32a2968b75bbae873", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One class splitting criteria for random forests. ", "mimetype": "text/plain", "start_char_idx": 65958, "end_char_idx": 66007, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "dd5ff5af-b3fb-40b0-977e-5e867b0bf439", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n", "original_text": "In Asian Conference on Machine Learning, pages 343-358. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04a17411-8250-498e-8a61-848ed519e191", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv preprint arXiv:1911.06646, 2019.\n\n [8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358. ", "original_text": "One class splitting criteria for random forests. "}, "hash": "97cf00d0218e3b9210e1f150ff9606b8d7e0a7c4c4d7f22c50437828afde0108", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04403dee-041d-4448-bf36-47d45ddfde21", "node_type": "1", "metadata": {"window": "The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. ", "original_text": "PMLR, 2017.\n\n"}, "hash": "8961ab73f153042ad88aa23ca98d1a389b1a1475bf74c61e5706c6f9330dddb3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Asian Conference on Machine Learning, pages 343-358. ", "mimetype": "text/plain", "start_char_idx": 66007, "end_char_idx": 66063, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "04403dee-041d-4448-bf36-47d45ddfde21", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. ", "original_text": "PMLR, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dd5ff5af-b3fb-40b0-977e-5e867b0bf439", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[8] Jesse Davis and Mark Goadrich.  The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n", "original_text": "In Asian Conference on Machine Learning, pages 343-358. "}, "hash": "50dcb8a6ae28a8f162962f90695fdcf9f564bbeed696416543ef414c3d677474", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "140f7e40-b8aa-47b7-bfec-d2d309090080", "node_type": "1", "metadata": {"window": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams. ", "original_text": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. "}, "hash": "da715475b97d02d23ad429c3e9eca3c688c7e232fc111915294f28e297928eef", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PMLR, 2017.\n\n", "mimetype": "text/plain", "start_char_idx": 66063, "end_char_idx": 66076, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "140f7e40-b8aa-47b7-bfec-d2d309090080", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams. ", "original_text": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04403dee-041d-4448-bf36-47d45ddfde21", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "The relationship between precision-recall and roc curves.  In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. ", "original_text": "PMLR, 2017.\n\n"}, "hash": "39c8c4b10e7eecb05e4f87aa1cfe81e4245d7947729e77d9d6efafdef94fb6d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a5ec2f10-6391-4bd5-adf0-c73a04443ee4", "node_type": "1", "metadata": {"window": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721. ", "original_text": "One class splitting criteria for random forests. "}, "hash": "00b8d234767a5a60d25dc28a7f8d0b7c131c45262c67ade2e7cd4ac6195afcd7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. ", "mimetype": "text/plain", "start_char_idx": 66076, "end_char_idx": 66147, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a5ec2f10-6391-4bd5-adf0-c73a04443ee4", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721. ", "original_text": "One class splitting criteria for random forests. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "140f7e40-b8aa-47b7-bfec-d2d309090080", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 23rd international conference on Machine learning, pages 233-240, 2006.\n\n [9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams. ", "original_text": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino. "}, "hash": "6d603bfa119e111945793c1d7dd2f354695ddf389f09e89946c1e6f70c851aea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae19d59d-57a8-460e-9f2f-efecbed021f3", "node_type": "1", "metadata": {"window": "One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n", "original_text": "In Asian Conference on Machine Learning, pages 343-358. "}, "hash": "318a9fa987b2aabd5c3c7b494475ba158985cc7d1adcf3aaef6899d09f44025c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "One class splitting criteria for random forests. ", "mimetype": "text/plain", "start_char_idx": 66147, "end_char_idx": 66196, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ae19d59d-57a8-460e-9f2f-efecbed021f3", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n", "original_text": "In Asian Conference on Machine Learning, pages 343-358. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a5ec2f10-6391-4bd5-adf0-c73a04443ee4", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[9] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721. ", "original_text": "One class splitting criteria for random forests. "}, "hash": "df33bdeaf90d5a1bced107203caefb9f5ca033ef2db7add4894087273fc2d98c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3f5e71e-ccc5-4b6c-9547-ddf9a4f46087", "node_type": "1", "metadata": {"window": "In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "original_text": "PMLR, 2017.\n\n"}, "hash": "a5dc9db9054e39f860b5e8de55371788c89d0617d5213d21f4c2f7643a5853b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Asian Conference on Machine Learning, pages 343-358. ", "mimetype": "text/plain", "start_char_idx": 66196, "end_char_idx": 66252, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a3f5e71e-ccc5-4b6c-9547-ddf9a4f46087", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "original_text": "PMLR, 2017.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae19d59d-57a8-460e-9f2f-efecbed021f3", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n", "original_text": "In Asian Conference on Machine Learning, pages 343-358. "}, "hash": "8829c03e6fb22001e1cf398c76fc5edfb47900787d2dd425f947ea1f35009187", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91b53eab-bed6-4c2a-ac1d-71f31e5dad74", "node_type": "1", "metadata": {"window": "PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest. ", "original_text": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. "}, "hash": "c6ec161496be1460fef9b47db88f6a7d957241966204ddbe70617bf7cfc584b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PMLR, 2017.\n\n", "mimetype": "text/plain", "start_char_idx": 66252, "end_char_idx": 66265, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "91b53eab-bed6-4c2a-ac1d-71f31e5dad74", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest. ", "original_text": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3f5e71e-ccc5-4b6c-9547-ddf9a4f46087", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "original_text": "PMLR, 2017.\n\n"}, "hash": "650a67817fe570a66b32cd56ebd9435bb94e440dc7ab7cb4d4a015c9fbe24281", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84c76317-a8e6-43a3-9e3e-d423eb42c61c", "node_type": "1", "metadata": {"window": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n", "original_text": "Robust random cut forest based anomaly detection on streams. "}, "hash": "3d2d1be41116da296edf9f79659731c9fb00ea0819677504b464475afe5573b3", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. ", "mimetype": "text/plain", "start_char_idx": 66265, "end_char_idx": 66330, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "84c76317-a8e6-43a3-9e3e-d423eb42c61c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n", "original_text": "Robust random cut forest based anomaly detection on streams. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91b53eab-bed6-4c2a-ac1d-71f31e5dad74", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "PMLR, 2017.\n\n [10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest. ", "original_text": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers. "}, "hash": "24b8d2b21e5b6333237069424cdb0d18cacb2265bf4c99341004418c24e45196", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b37cee8f-4e30-4eae-aa4b-eb31241fed2a", "node_type": "1", "metadata": {"window": "One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. ", "original_text": "In International conference on machine learning, pages 2712\u20132721. "}, "hash": "52410e6296514dd896b946b2327a743256be8aaf7f212d7a7caf6d411ad6ebeb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Robust random cut forest based anomaly detection on streams. ", "mimetype": "text/plain", "start_char_idx": 66330, "end_char_idx": 66391, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b37cee8f-4e30-4eae-aa4b-eb31241fed2a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. ", "original_text": "In International conference on machine learning, pages 2712\u20132721. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84c76317-a8e6-43a3-9e3e-d423eb42c61c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[10] Nicolas Goix, Nicolas Drougard, Romain Brault, and Mael Chiapino.  One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n", "original_text": "Robust random cut forest based anomaly detection on streams. "}, "hash": "d68950aa90694c6b884fbd265b215c9e2290e9c1a8411aaee38ba5dd6847dbab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8f68b088-4ab7-465c-a538-6c4419fe1563", "node_type": "1", "metadata": {"window": "In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores. ", "original_text": "PMLR, 2016.\n\n"}, "hash": "8856638d97b89a05c3e3fb2e00c514459637d83012c07935007e2cbed4bfd94b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In International conference on machine learning, pages 2712\u20132721. ", "mimetype": "text/plain", "start_char_idx": 66391, "end_char_idx": 66457, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8f68b088-4ab7-465c-a538-6c4419fe1563", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores. ", "original_text": "PMLR, 2016.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b37cee8f-4e30-4eae-aa4b-eb31241fed2a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "One class splitting criteria for random forests.  In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. ", "original_text": "In International conference on machine learning, pages 2712\u20132721. "}, "hash": "89ac4040e4b92818abd33ec8dfbf4fed5df41f73e5f671ef26d5f01b2414997b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6daf5c6f-6d30-4785-8175-bd541bc19f1c", "node_type": "1", "metadata": {"window": "PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. ", "original_text": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. "}, "hash": "484549fc553b855e45e0b2b2063d11954833e2daa49485dd3a9001c9d8c2b458", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "PMLR, 2016.\n\n", "mimetype": "text/plain", "start_char_idx": 66457, "end_char_idx": 66470, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6daf5c6f-6d30-4785-8175-bd541bc19f1c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. ", "original_text": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8f68b088-4ab7-465c-a538-6c4419fe1563", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Asian Conference on Machine Learning, pages 343-358.  PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores. ", "original_text": "PMLR, 2016.\n\n"}, "hash": "b1b6beafd6f252e8839922bc05834ca55be19c8f45fcdf6562cd2cce2d7d6fa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97aba94f-c6f2-4c4f-ab98-5acb20e1c912", "node_type": "1", "metadata": {"window": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n", "original_text": "Extended isolation forest. "}, "hash": "e228e8f1e8c8c2ec19b6055d14a0b1f2a1d61a84db54e1f7552e085da23b47ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. ", "mimetype": "text/plain", "start_char_idx": 66470, "end_char_idx": 66534, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "97aba94f-c6f2-4c4f-ab98-5acb20e1c912", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n", "original_text": "Extended isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6daf5c6f-6d30-4785-8175-bd541bc19f1c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "PMLR, 2017.\n\n [11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. ", "original_text": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner. "}, "hash": "5afda46e08f5990c16d2029ff7f334742677f02d8d0652baefa054e23a21127a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7b2853f3-8861-44d8-8dec-b98e565ea0cb", "node_type": "1", "metadata": {"window": "Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "arXiv preprint arXiv:1811.02141, 2018.\n\n"}, "hash": "e737f48bf1237a5f0cc76b676c4ff1025f36da3e8991ca51503447c8134f27ac", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Extended isolation forest. ", "mimetype": "text/plain", "start_char_idx": 66534, "end_char_idx": 66561, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7b2853f3-8861-44d8-8dec-b98e565ea0cb", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "arXiv preprint arXiv:1811.02141, 2018.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97aba94f-c6f2-4c4f-ab98-5acb20e1c912", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[11] Sudipto Guha, Nina Mishra, Gourav Roy, and Okke Schrijvers.  Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n", "original_text": "Extended isolation forest. "}, "hash": "9af6aff053363a63004cf1a89f04b55c66edc9d4711877df4c67c24f45857a5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b3080a4-b6b7-4046-83f4-518b0e97153f", "node_type": "1", "metadata": {"window": "In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. "}, "hash": "7f282779c9f2331007db7af7f6db7ba861b7f54b68e7b9e50e241e64919669d2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "arXiv preprint arXiv:1811.02141, 2018.\n\n", "mimetype": "text/plain", "start_char_idx": 66561, "end_char_idx": 66601, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8b3080a4-b6b7-4046-83f4-518b0e97153f", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7b2853f3-8861-44d8-8dec-b98e565ea0cb", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Robust random cut forest based anomaly detection on streams.  In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "arXiv preprint arXiv:1811.02141, 2018.\n\n"}, "hash": "9688e8f6737f7f351540371dfd088b052edbb832b34b458cbbb5f4a296b5ec82", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac57bf26-cc48-4e6b-9238-9b4276417f5b", "node_type": "1", "metadata": {"window": "PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. ", "original_text": "Interpreting and unifying outlier scores. "}, "hash": "34ae2536dba4a9918053877b6a2f85ec4bbc51237cd906ab96169b72653cb322", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. ", "mimetype": "text/plain", "start_char_idx": 66601, "end_char_idx": 66673, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "ac57bf26-cc48-4e6b-9238-9b4276417f5b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. ", "original_text": "Interpreting and unifying outlier scores. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b3080a4-b6b7-4046-83f4-518b0e97153f", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In International conference on machine learning, pages 2712\u20132721.  PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest. ", "original_text": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek. "}, "hash": "27090ef9f6739891197293aa417a46e2e1ba0532260098a6431ab8ce2c56c5d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b51d756-2f0c-4309-9eb7-8e9b8338a176", "node_type": "1", "metadata": {"window": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n", "original_text": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. "}, "hash": "af60ccd0d551c87aa9330e012c7beb0a3ef306cbab211cdbea163635a3ae61b9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Interpreting and unifying outlier scores. ", "mimetype": "text/plain", "start_char_idx": 66673, "end_char_idx": 66715, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "4b51d756-2f0c-4309-9eb7-8e9b8338a176", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n", "original_text": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac57bf26-cc48-4e6b-9238-9b4276417f5b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "PMLR, 2016.\n\n [12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. ", "original_text": "Interpreting and unifying outlier scores. "}, "hash": "aacec7c6d60c653c0765e60bd97c5bef509e0f1578a13080c9ca07b30640dbc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b845d38d-a815-4fb6-9ce6-63f9c5bf2569", "node_type": "1", "metadata": {"window": "Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "SIAM, 2011.\n\n"}, "hash": "5a1ed78a56e97386928c292c7da19bf56f855cf33ff14705e5b68e18e51a5e23", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. ", "mimetype": "text/plain", "start_char_idx": 66715, "end_char_idx": 66801, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b845d38d-a815-4fb6-9ce6-63f9c5bf2569", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "SIAM, 2011.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b51d756-2f0c-4309-9eb7-8e9b8338a176", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[12] Sahand Hariri, Matias Carrasco Kind, and Robert J Brunner.  Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n", "original_text": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324. "}, "hash": "4678066001618ef2cc1631b9204cb59f8ce5b4044e44235b09864b687d4f66c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b84cf41e-ef74-4a66-91e6-f97169809867", "node_type": "1", "metadata": {"window": "arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest. ", "original_text": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "b8cdaf74740143b5ceaaa767bd275fdd3a594b24c1ace74d0b5d44e961f17160", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "SIAM, 2011.\n\n", "mimetype": "text/plain", "start_char_idx": 66801, "end_char_idx": 66814, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b84cf41e-ef74-4a66-91e6-f97169809867", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest. ", "original_text": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b845d38d-a815-4fb6-9ce6-63f9c5bf2569", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Extended isolation forest.  arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "original_text": "SIAM, 2011.\n\n"}, "hash": "418b6d88570a6f267121dfea59f021a6cf999120062f1a21b74e5663846b1ee7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7b507c1-8088-459e-9ed4-42bbbaaa1584", "node_type": "1", "metadata": {"window": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. ", "original_text": "Isolation forest. "}, "hash": "633099a64dcb243db2d11963ec3dd3c66fd1c1ce51f812ae80d15da74a8c4a60", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "mimetype": "text/plain", "start_char_idx": 66814, "end_char_idx": 66866, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b7b507c1-8088-459e-9ed4-42bbbaaa1584", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. ", "original_text": "Isolation forest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b84cf41e-ef74-4a66-91e6-f97169809867", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "arXiv preprint arXiv:1811.02141, 2018.\n\n [13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest. ", "original_text": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "660ce4e952edbf08117efb71fb08cb4728cf1176c01efa4ae60753338d4c33cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8fb34598-7c31-41e1-9dda-6947eb2a6b8b", "node_type": "1", "metadata": {"window": "Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n", "original_text": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. "}, "hash": "a873e9e2f84c11bcab6af978f6da5ca8656f3361fc3c7bff4505701129f5b770", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Isolation forest. ", "mimetype": "text/plain", "start_char_idx": 66866, "end_char_idx": 66884, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "8fb34598-7c31-41e1-9dda-6947eb2a6b8b", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n", "original_text": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7b507c1-8088-459e-9ed4-42bbbaaa1584", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[13] Hans-Peter Kriegel, Peer Kroger, Erich Schubert, and Arthur Zimek.  Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. ", "original_text": "Isolation forest. "}, "hash": "9f003aa0f1d5f88ee1012e04102a159f4a5c0a3560854d4e103819ca7dee7f49", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b534fa11-8d60-4804-9300-bec7a03d1454", "node_type": "1", "metadata": {"window": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan. ", "original_text": "IEEE, 2008.\n\n"}, "hash": "ee57ee94441a11e23e7699066efca8656fb1cb06f3ee4f9bd94a4ea9ccfab064", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. ", "mimetype": "text/plain", "start_char_idx": 66884, "end_char_idx": 66960, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "b534fa11-8d60-4804-9300-bec7a03d1454", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan. ", "original_text": "IEEE, 2008.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8fb34598-7c31-41e1-9dda-6947eb2a6b8b", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Interpreting and unifying outlier scores.  In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n", "original_text": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422. "}, "hash": "0f1d0b7c7102df3038c6b7ad95ecbd19008d55daa35768126cf41ddbf6180780", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c566720d-1da3-4542-8d88-cf018cba7637", "node_type": "1", "metadata": {"window": "SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4. ", "original_text": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "42463d5aa71cfa9a98548d7a93246ffabab328bce87be12cc5664d049dae76c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "IEEE, 2008.\n\n", "mimetype": "text/plain", "start_char_idx": 66960, "end_char_idx": 66973, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c566720d-1da3-4542-8d88-cf018cba7637", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4. ", "original_text": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b534fa11-8d60-4804-9300-bec7a03d1454", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 2011 SIAM International Conference on Data Mining, pages 13\u201324.  SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan. ", "original_text": "IEEE, 2008.\n\n"}, "hash": "39c29b2b48f5fd31b710e60a3c75551efbbdf5202175ab222ed7f1adc1ac4c28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c10b9706-7c09-423c-a810-b24593adf408", "node_type": "1", "metadata": {"window": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning. ", "original_text": "On detecting clustered anomalies using sciforest. "}, "hash": "2d37dee65bb817098f1f1695e564f955b08d65174abdbd55517219aca52d7b4b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. ", "mimetype": "text/plain", "start_char_idx": 66973, "end_char_idx": 67025, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "c10b9706-7c09-423c-a810-b24593adf408", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning. ", "original_text": "On detecting clustered anomalies using sciforest. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c566720d-1da3-4542-8d88-cf018cba7637", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "SIAM, 2011.\n\n [14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4. ", "original_text": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. "}, "hash": "ba5ef92e56bb51c801041761502bccf26f908f45af761a8434c742e7aaceb4e1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3a3cac4-e8cf-4e8b-b075-3896ec094ad8", "node_type": "1", "metadata": {"window": "Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n", "original_text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. "}, "hash": "68998e2e29547097489cdcfd9f749baf74d4d7efe25a4f3e69fb8dd5aec8ddf2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "On detecting clustered anomalies using sciforest. ", "mimetype": "text/plain", "start_char_idx": 67025, "end_char_idx": 67075, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d3a3cac4-e8cf-4e8b-b075-3896ec094ad8", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n", "original_text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c10b9706-7c09-423c-a810-b24593adf408", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[14] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning. ", "original_text": "On detecting clustered anomalies using sciforest. "}, "hash": "55bf1cb7f76c13dc10cbc76f6c735b9799996ebae2cff3659641289169a13af9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7499cc10-ffce-41f5-9252-b11de2074432", "node_type": "1", "metadata": {"window": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray. ", "original_text": "Springer, 2010.\n\n"}, "hash": "776409b7ea0886bddc1369e7affdf96069a59f308f9ebb4d675e33ee4a6b839a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. ", "mimetype": "text/plain", "start_char_idx": 67075, "end_char_idx": 67177, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "7499cc10-ffce-41f5-9252-b11de2074432", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray. ", "original_text": "Springer, 2010.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3a3cac4-e8cf-4e8b-b075-3896ec094ad8", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Isolation forest.  In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n", "original_text": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290. "}, "hash": "6cfc69cd6cf0822792897f4db506dd7e3c8fb477d0ac3660fac6f2827fae8db6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6c20afa0-11de-45cb-8e81-518a9515e254", "node_type": "1", "metadata": {"window": "IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees. ", "original_text": "[16] J Ross Quinlan. "}, "hash": "339bc0d0a0649e329d869632d023b4470a8d6ac7c8cc4ef9c13b89af31f4d4a1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Springer, 2010.\n\n", "mimetype": "text/plain", "start_char_idx": 67177, "end_char_idx": 67194, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "6c20afa0-11de-45cb-8e81-518a9515e254", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees. ", "original_text": "[16] J Ross Quinlan. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7499cc10-ffce-41f5-9252-b11de2074432", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In 2008 Eighth IEEE International Conference on Data Mining, pages 413-422.  IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray. ", "original_text": "Springer, 2010.\n\n"}, "hash": "8586ee9577067b8f96109fece72694ccbba6049d716bba274a487c5bb8958abf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3904cc21-3fd1-4080-9a08-2a38755c821e", "node_type": "1", "metadata": {"window": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n", "original_text": "C4. "}, "hash": "234a484ee2395ee1f4e1d00b66d7c2402c4df4fcb1de8f91238189f81332ae71", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[16] J Ross Quinlan. ", "mimetype": "text/plain", "start_char_idx": 67194, "end_char_idx": 67215, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3904cc21-3fd1-4080-9a08-2a38755c821e", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n", "original_text": "C4. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6c20afa0-11de-45cb-8e81-518a9515e254", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "IEEE, 2008.\n\n [15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees. ", "original_text": "[16] J Ross Quinlan. "}, "hash": "fb28fa5286b258dcb43f7b01533dff9a977bd7ae888516f2000af2b63c1cae8a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1894fc09-34e2-4a49-8b7a-ff17c07e67b9", "node_type": "1", "metadata": {"window": "On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana. ", "original_text": "5: programs for machine learning. "}, "hash": "f04d020d50941f529895e7355b6f25d54b8bee3690406466f0f57b88f7df3e19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "C4. ", "mimetype": "text/plain", "start_char_idx": 67215, "end_char_idx": 67219, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "1894fc09-34e2-4a49-8b7a-ff17c07e67b9", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana. ", "original_text": "5: programs for machine learning. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3904cc21-3fd1-4080-9a08-2a38755c821e", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[15] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou.  On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n", "original_text": "C4. "}, "hash": "cd9e353f396befcd9cb7a4a7509660a14c598be73a8de5da4cc1f89798ae896e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7bb79eb-f866-4c98-b39a-8ad35b62c28c", "node_type": "1", "metadata": {"window": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library. ", "original_text": "Elsevier, 2014.\n\n"}, "hash": "6df9d646753ce6514f11f213dc3c1a3cc44452556d89cfb7bda15963d2411910", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "5: programs for machine learning. ", "mimetype": "text/plain", "start_char_idx": 67219, "end_char_idx": 67253, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "a7bb79eb-f866-4c98-b39a-8ad35b62c28c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library. ", "original_text": "Elsevier, 2014.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1894fc09-34e2-4a49-8b7a-ff17c07e67b9", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "On detecting clustered anomalies using sciforest.  In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana. ", "original_text": "5: programs for machine learning. "}, "hash": "d42ed45bcf5bffcb1f4e7327bb9b9efaf54d50e231f86e63c20ad2a41a2d5007", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "187c1407-a91b-4a60-b79d-afdfd6a01449", "node_type": "1", "metadata": {"window": "Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n", "original_text": "[17] Parikshit Ram and Alexander G Gray. "}, "hash": "dc98c2e16acbbcaf75b05ca769b141aeb0038528ca81d7209595c6c2e1244fa5", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Elsevier, 2014.\n\n", "mimetype": "text/plain", "start_char_idx": 67253, "end_char_idx": 67270, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "187c1407-a91b-4a60-b79d-afdfd6a01449", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n", "original_text": "[17] Parikshit Ram and Alexander G Gray. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7bb79eb-f866-4c98-b39a-8ad35b62c28c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 274-290.  Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library. ", "original_text": "Elsevier, 2014.\n\n"}, "hash": "3680661e2e8498c9b7b4d2adfc56dba7fcb91a0733697ebdba79a35536de8f4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3765e6aa-bda2-4af1-ada5-30f88d78e55c", "node_type": "1", "metadata": {"window": "[16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. ", "original_text": "Density estimation trees. "}, "hash": "8a9c0663de5b087237a0d25b3d4ed87245caca78e2313ebaa3ec235dcedc82f0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[17] Parikshit Ram and Alexander G Gray. ", "mimetype": "text/plain", "start_char_idx": 67270, "end_char_idx": 67311, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "3765e6aa-bda2-4af1-ada5-30f88d78e55c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. ", "original_text": "Density estimation trees. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "187c1407-a91b-4a60-b79d-afdfd6a01449", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Springer, 2010.\n\n [16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n", "original_text": "[17] Parikshit Ram and Alexander G Gray. "}, "hash": "078c556aeab9241217a77d7b72334ced7650c6def826470f440ad4c30487f128", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d11910aa-7eff-4000-a9d0-915e39a44700", "node_type": "1", "metadata": {"window": "C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution. ", "original_text": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n"}, "hash": "29109d63aa098e137df3dc02e82f71f8aaf7c8b94221fdfaab3c9be7d048ced0", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Density estimation trees. ", "mimetype": "text/plain", "start_char_idx": 67311, "end_char_idx": 67337, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "d11910aa-7eff-4000-a9d0-915e39a44700", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution. ", "original_text": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3765e6aa-bda2-4af1-ada5-30f88d78e55c", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[16] J Ross Quinlan.  C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. ", "original_text": "Density estimation trees. "}, "hash": "468422da3dad3fb4a85a19401100ff1f8450d748dd67a0e2611a2b5a580aac0b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "65a20dc9-4aa6-4348-88f1-62558e245cb5", "node_type": "1", "metadata": {"window": "5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "[18] Shebuti Rayana. "}, "hash": "e0aa49fd2cb9d4de286b46f4c9252e5d726ef35ae3bcef735ebf1f7744a42f2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n", "mimetype": "text/plain", "start_char_idx": 67337, "end_char_idx": 67462, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "65a20dc9-4aa6-4348-88f1-62558e245cb5", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "[18] Shebuti Rayana. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d11910aa-7eff-4000-a9d0-915e39a44700", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "C4.  5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution. ", "original_text": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n"}, "hash": "54b674a45a812e7c84c934454536ac976d0335ed0c294d9e95bd82265c42aca8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5d3bab97-2039-4f71-8e5c-c3795801748a", "node_type": "1", "metadata": {"window": "Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Odds library. "}, "hash": "ebda91787d1616de75e999f7816e5b3b60e4b0ab7170f01fc5e4e68ea4b78b19", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[18] Shebuti Rayana. ", "mimetype": "text/plain", "start_char_idx": 67462, "end_char_idx": 67483, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "5d3bab97-2039-4f71-8e5c-c3795801748a", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Odds library. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "65a20dc9-4aa6-4348-88f1-62558e245cb5", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "5: programs for machine learning.  Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "[18] Shebuti Rayana. "}, "hash": "cdc50bca61b82f308f874911ce22ed1c722c19445e8f896eb5b1b2b48c1dd309", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0b237053-8415-4fdd-86a7-67989029addf", "node_type": "1", "metadata": {"window": "[17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "http://odds.cs.stonybrook.edu, 2016.\n\n"}, "hash": "b4f359f5218c94b610c21bae496435fbbc41c085107c02e7a723c7e675451a48", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Odds library. ", "mimetype": "text/plain", "start_char_idx": 67483, "end_char_idx": 67497, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "0b237053-8415-4fdd-86a7-67989029addf", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "http://odds.cs.stonybrook.edu, 2016.\n\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5d3bab97-2039-4f71-8e5c-c3795801748a", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Elsevier, 2014.\n\n [17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Odds library. "}, "hash": "e8cca044147447d0f0cdc460891f8e502652b8567993ea614476d6b4b408b187", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "213c7806-ff98-430e-9c09-8460aad5b896", "node_type": "1", "metadata": {"window": "Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "[19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. "}, "hash": "93e692a84c734f9c7357aaf4bfad911cefe4cdd93608b72206908966fab4490a", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "http://odds.cs.stonybrook.edu, 2016.\n\n", "mimetype": "text/plain", "start_char_idx": 67497, "end_char_idx": 67535, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "213c7806-ff98-430e-9c09-8460aad5b896", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "[19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0b237053-8415-4fdd-86a7-67989029addf", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[17] Parikshit Ram and Alexander G Gray.  Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "http://odds.cs.stonybrook.edu, 2016.\n\n"}, "hash": "8a63114bf2039b991cd58f3e9adc674886b3b14469e12a85966326df98a697ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9d458519-cb3e-48fb-8c94-e96cbdd2d106", "node_type": "1", "metadata": {"window": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Estimating the support of a high-dimensional distribution. "}, "hash": "a7412a40dae4b995a156b77741bca6cdb340305e75ee10b53c78a7c9e3915200", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. ", "mimetype": "text/plain", "start_char_idx": 67535, "end_char_idx": 67632, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "9d458519-cb3e-48fb-8c94-e96cbdd2d106", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Estimating the support of a high-dimensional distribution. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "213c7806-ff98-430e-9c09-8460aad5b896", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "Density estimation trees.  In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "[19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson. "}, "hash": "2d181387e16561599286a3e98afe7582f6aab182bd190aaa78e95aefc406d34d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "058153db-4987-46ce-bc08-2e33abb37b5c", "node_type": "1", "metadata": {"window": "[18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Neural computation, 13(7):1443\u20131471, 2001."}, "hash": "ffb6e1919d44ee497a6d8d8a9d27fe1b652cd3f065ac76fa21a436b994008d39", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Estimating the support of a high-dimensional distribution. ", "mimetype": "text/plain", "start_char_idx": 67632, "end_char_idx": 67691, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, {"id_": "058153db-4987-46ce-bc08-2e33abb37b5c", "embedding": null, "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "[18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Neural computation, 13(7):1443\u20131471, 2001."}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "85f59e26-2c26-49fb-a30f-2994f0bcf676", "node_type": "4", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf"}, "hash": "44f69844255901cb18ca7eaf5964bbb2d6cda1a8720d0d00f656c3406f74c87d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9d458519-cb3e-48fb-8c94-e96cbdd2d106", "node_type": "1", "metadata": {"title": "Revisiting randomized choices in isolation forests", "authors": "Cortes et al.", "year": 2021, "file_path": "ad-papers-pdf/randomised_choices_in_isolation_forest.pdf", "window": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 627-635, 2011.\n\n [18] Shebuti Rayana.  Odds library.  http://odds.cs.stonybrook.edu, 2016.\n\n [19] Bernhard Sch\u00f6lkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.  Estimating the support of a high-dimensional distribution.  Neural computation, 13(7):1443\u20131471, 2001.", "original_text": "Estimating the support of a high-dimensional distribution. "}, "hash": "b27a3f69a405fe0e399f67803c3b81715c05feafc4b504ab514abb8bf1ee66cb", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Neural computation, 13(7):1443\u20131471, 2001.", "mimetype": "text/plain", "start_char_idx": 67691, "end_char_idx": 67733, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}]